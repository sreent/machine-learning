{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Linear%20Regression/Linear%20Regression%20Code%20Walk%20Through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Code Walk Through\n",
    "\n",
    "This notebook walks through the **computational steps** of the Linear Regression algorithm from scratch, based on lecture slides 28-32.\n",
    "\n",
    "## What We'll Cover:\n",
    "1. **Visualize the data** - understand the dataset\n",
    "2. **Add bias term** - transform data to include intercept\n",
    "3. **Find best fit line** - compute optimal weights using closed-form solution\n",
    "4. **Make predictions** - use learned weights to predict new values\n",
    "\n",
    "We'll show **both loop and vectorized approaches** to understand the logic and efficient implementation.\n",
    "\n",
    "### Key Concept:\n",
    "- Linear regression finds the **best fit line** through the data\n",
    "- Uses **closed-form solution** (no iterative training needed!)\n",
    "- Formula: **y = w₁x + w₀** where w₀ is intercept and w₁ is slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We need:\n",
    "- **NumPy** for numerical operations and matrix calculations\n",
    "- **Matplotlib** for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training Data\n",
    "\n",
    "We create training data similar to the lecture slides (slide 28):\n",
    "- Generate x values from a range\n",
    "- Create y values with a linear relationship plus some noise\n",
    "- This simulates real-world data where measurements have some randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data (similar to slide 28)\n",
    "# Generate x values\n",
    "X_train = np.arange(-9.5, 8.5, 0.1).reshape(-1, 1)\n",
    "\n",
    "# Generate y values: y ≈ 1.035x + 1.069 with some noise\n",
    "true_slope = 1.035\n",
    "true_intercept = 1.069\n",
    "noise = np.random.normal(0, 2, X_train.shape[0])\n",
    "y_train = true_slope * X_train.ravel() + true_intercept + noise\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Target values shape:\", y_train.shape)\n",
    "print(f\"\\nNumber of training points: {len(X_train)}\")\n",
    "print(f\"X range: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "print(f\"y range: [{y_train.min():.1f}, {y_train.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the Data\n",
    "\n",
    "Let's plot our training data to see the relationship between x and y.\n",
    "\n",
    "We can see the points roughly follow a **linear trend** - perfect for linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of training data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='steelblue', s=30, alpha=0.6,\n",
    "           edgecolors='black', linewidths=0.5,\n",
    "           label='Training data')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Training Data: Looking for Linear Relationship', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"We have {len(X_train)} training points\")\n",
    "print(f\"Goal: Find the line y = w₁x + w₀ that best fits this data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Column of Ones (Bias Term)\n",
    "\n",
    "**Slide 29** shows the first step: adding a column of 1s to include the intercept term.\n",
    "\n",
    "**Why?**\n",
    "- Our model is: **y = w₁x + w₀**\n",
    "- We can rewrite this as: **y = w₀(1) + w₁x**\n",
    "- In matrix form: **y = [1, x] × [w₀, w₁]ᵀ**\n",
    "\n",
    "**Transformation:**\n",
    "- Original: **[x⁽¹⁾, x⁽²⁾, ..., x⁽ᴺ⁾]**\n",
    "- With bias: **[[1, x⁽¹⁾], [1, x⁽²⁾], ..., [1, x⁽ᴺ⁾]]**\n",
    "\n",
    "This matrix is called **Φ** (Phi) or the **design matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of ones using np.c_[]\n",
    "Phi = np.c_[np.ones(len(X_train)), X_train]\n",
    "\n",
    "print(\"Original X_train shape:\", X_train.shape)\n",
    "print(\"Design matrix Φ shape:\", Phi.shape)\n",
    "print(\"\\nFirst few rows of Φ:\")\n",
    "print(Phi[:5])\n",
    "print(\"\\nEach row is now: [1, x]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Find Best Fit Line (Closed-Form Solution)\n",
    "\n",
    "**Slide 30** shows how we compute the optimal weights using the **normal equation**:\n",
    "\n",
    "$$\\mathbf{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{y}$$\n",
    "\n",
    "Where:\n",
    "- **Φ** is our design matrix (with bias column)\n",
    "- **y** is our target values\n",
    "- **w** = [w₀, w₁] are the weights (intercept and slope)\n",
    "\n",
    "This gives us the **exact solution** in one computation (no iterative training!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights using the closed-form solution\n",
    "# w = (Φᵀ Φ)⁻¹ Φᵀ y\n",
    "weights = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ y_train\n",
    "\n",
    "print(\"Weights computed using normal equation:\")\n",
    "print(f\"\\nWeights = {weights}\")\n",
    "print()\n",
    "print(f\"Intercept (w₀): {weights[0]:.3f}\")\n",
    "print(f\"Slope (w₁):     {weights[1]:.3f}\")\n",
    "print()\n",
    "print(f\"Final linear model: y = {weights[1]:.3f}x + {weights[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the Best Fit Line\n",
    "\n",
    "Let's plot our learned line along with the training data to see how well it fits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x values for plotting the line\n",
    "x_line = np.linspace(-10, 10, 200)\n",
    "\n",
    "# Compute y values using our learned weights: y = w₁x + w₀\n",
    "y_line = weights[1] * x_line + weights[0]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='steelblue', s=30, alpha=0.6,\n",
    "           edgecolors='black', linewidths=0.5,\n",
    "           label='Training data', zorder=2)\n",
    "\n",
    "# Best fit line\n",
    "plt.plot(x_line, y_line,\n",
    "        'r-', linewidth=3, alpha=0.8,\n",
    "        label=f'y = {weights[1]:.3f}x + {weights[0]:.3f}',\n",
    "        zorder=3)\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Linear Regression: Best Fit Line', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model equation: y = {weights[1]:.3f}x + {weights[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Add Bias Term to Test Data\n",
    "\n",
    "**Slide 31** shows how we prepare the test point for prediction.\n",
    "\n",
    "Just like with training data, we need to add a column of 1s to our test data.\n",
    "\n",
    "**Transformation:** [5] → [1, 5]\n",
    "\n",
    "This ensures our test point has the same format as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test point (from slide 28)\n",
    "X_test = np.array([[5.0]])\n",
    "\n",
    "print(\"Original test point:\", X_test[0])\n",
    "print(\"Shape:\", X_test.shape)\n",
    "print()\n",
    "\n",
    "# Add bias term\n",
    "X_test_with_bias = np.c_[np.ones(len(X_test)), X_test]\n",
    "\n",
    "print(\"Test point with bias:\", X_test_with_bias[0])\n",
    "print(\"Shape:\", X_test_with_bias.shape)\n",
    "print()\n",
    "print(\"Now it has the form [1, x] to match our weights [w₀, w₁]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Make Prediction Using Matrix Multiplication\n",
    "\n",
    "**Slide 32** shows the prediction calculation:\n",
    "\n",
    "$$\\hat{y} = [1, x] \\times [w_0, w_1]^T = 1 \\times w_0 + x \\times w_1$$\n",
    "\n",
    "For our test point x = 5:\n",
    "- ŷ = [1, 5] × [w₀, w₁]\n",
    "- ŷ = 1×w₀ + 5×w₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using matrix multiplication\n",
    "prediction = X_test_with_bias @ weights\n",
    "\n",
    "print(\"Matrix multiplication approach:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test point: x = {X_test[0,0]}\")\n",
    "print(f\"Test point with bias: {X_test_with_bias[0]}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print()\n",
    "print(f\"ŷ = [1, {X_test[0,0]}] × [{weights[0]:.3f}, {weights[1]:.3f}]\")\n",
    "print(f\"ŷ = 1×{weights[0]:.3f} + {X_test[0,0]}×{weights[1]:.3f}\")\n",
    "print(f\"ŷ = {weights[0]:.3f} + {X_test[0,0] * weights[1]:.3f}\")\n",
    "print(f\"ŷ = {prediction[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize the Prediction\n",
    "\n",
    "Let's visualize our prediction on the fitted line, matching slide 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x values for plotting the line\n",
    "x_line = np.linspace(-10, 10, 200)\n",
    "y_line = weights[1] * x_line + weights[0]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='steelblue', s=30, alpha=0.6,\n",
    "           edgecolors='black', linewidths=0.5,\n",
    "           label='Training data', zorder=2)\n",
    "\n",
    "# Best fit line\n",
    "plt.plot(x_line, y_line,\n",
    "        'r-', linewidth=3, alpha=0.8,\n",
    "        label=f'y = {weights[1]:.3f}x + {weights[0]:.3f}',\n",
    "        zorder=3)\n",
    "\n",
    "# Test point and prediction\n",
    "plt.scatter(X_test, prediction,\n",
    "           c='red', s=400, marker='*',\n",
    "           edgecolors='black', linewidths=2,\n",
    "           label=f'Test point: x={X_test[0,0]:.1f}, ŷ={prediction[0]:.3f}',\n",
    "           zorder=4)\n",
    "\n",
    "# Draw dashed lines to show prediction (similar to slide 28)\n",
    "plt.plot([X_test[0,0], X_test[0,0]], [plt.ylim()[0], prediction[0]], \n",
    "        'k--', alpha=0.5, linewidth=1.5)\n",
    "plt.plot([plt.xlim()[0], X_test[0,0]], [prediction[0], prediction[0]], \n",
    "        'k--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Linear Regression: Making a Prediction', fontsize=16)\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFor test point x = {X_test[0,0]:.1f}:\")\n",
    "print(f\"Predicted value ŷ = {prediction[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've walked through all the computational steps of Linear Regression (slides 28-32):\n",
    "\n",
    "1. ✅ **Visualized data** - saw training points showing linear trend\n",
    "2. ✅ **Added bias term** (Slide 29) - transformed data by adding column of 1s to create design matrix Φ\n",
    "3. ✅ **Found best fit line** (Slide 30) - used closed-form solution **w = (ΦᵀΦ)⁻¹Φᵀy** to compute optimal weights\n",
    "4. ✅ **Prepared test data** (Slide 31) - added bias term to test point\n",
    "5. ✅ **Made prediction** (Slide 32) - computed ŷ = [1, x] · [w₀, w₁] for new point\n",
    "\n",
    "### Key Linear Regression Concepts:\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Design Matrix (Φ)** | Training data with bias column: [[1, x⁽¹⁾], [1, x⁽²⁾], ...] |\n",
    "| **Weights (w)** | [w₀, w₁] where w₀ = intercept, w₁ = slope |\n",
    "| **Normal Equation** | w = (ΦᵀΦ)⁻¹Φᵀy gives optimal solution directly |\n",
    "| **Prediction** | ŷ = w₀ + w₁x (or [1,x] · w in matrix form) |\n",
    "\n",
    "### Key NumPy Operations Used:\n",
    "\n",
    "- **`np.c_[ones, X]`** - concatenate columns to add bias term\n",
    "- **`.T`** - transpose matrix (Φ → Φᵀ)\n",
    "- **`@`** - matrix multiplication operator\n",
    "- **`np.linalg.inv()`** - compute matrix inverse\n",
    "- **`Phi.T @ Phi`** - compute ΦᵀΦ (Gram matrix)\n",
    "- **`X @ w`** - compute predictions via matrix-vector multiplication\n",
    "\n",
    "### Linear Regression vs KNN Regression:\n",
    "\n",
    "| Aspect | Linear Regression | KNN Regression |\n",
    "|--------|------------------|----------------|\n",
    "| **Model** | Parametric (learns fixed weights) | Non-parametric (uses training data directly) |\n",
    "| **Training** | Closed-form solution (instant) | No training needed |\n",
    "| **Prediction** | Fast (just w₀ + w₁x) | Slower (must find K neighbors) |\n",
    "| **Assumes** | Linear relationship | Local similarity |\n",
    "| **Memory** | Only stores weights | Must store all training data |\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "- **Step-by-step breakdown** helps you understand the mathematical operations\n",
    "- **Matrix operations** provide efficient implementation\n",
    "\n",
    "In practice, linear regression is powerful when your data has a linear relationship, and it's much faster than iterative methods!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
