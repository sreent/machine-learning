{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Linear%20Regression/Linear%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Hands-On Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement a Linear Regression model from scratch, understand the mathematics behind it, and apply it to real data. Along the way, you'll answer conceptual questions and create visualizations to deepen your understanding.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the mathematics of Linear Regression\n",
    "- Implement a custom Linear Regression class from scratch\n",
    "- Visualize regression lines and prediction errors\n",
    "- Apply feature scaling and understand its importance\n",
    "- Tune models using train/validation/test splits\n",
    "- Compare custom implementation with scikit-learn\n",
    "- Analyze model performance and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a **parametric supervised learning algorithm** used for **regression tasks** (predicting continuous values).\n",
    "\n",
    "**Key Idea:**\n",
    "- Find a **linear function** that best fits the training data\n",
    "- Model: **y = w₁x + w₀** (for single feature) or **y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ** (for multiple features)\n",
    "- **w₀** is the intercept (bias term)\n",
    "- **w₁, w₂, ..., wₙ** are the slopes (weights)\n",
    "\n",
    "**How it works:**\n",
    "1. **Training:** Find optimal weights **w** using the **normal equation**: **w = (ΦᵀΦ)⁻¹Φᵀy**\n",
    "2. **Prediction:** Compute **ŷ = Φw** where Φ is the design matrix with bias column\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable\n",
    "- Fast training (closed-form solution)\n",
    "- Fast prediction\n",
    "- Works well when relationships are linear\n",
    "\n",
    "**Disadvantages:**\n",
    "- Assumes linear relationship\n",
    "- Sensitive to outliers\n",
    "- Can't capture complex non-linear patterns (without feature engineering)\n",
    "- Matrix inversion can be computationally expensive for very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: Linear Regression finds the best fit line by:\n>\n> A. Computing the closed-form solution using the normal equation w = (ΦᵀΦ)⁻¹Φᵀy\n>\n> B. Iteratively searching through all possible lines until convergence\n>\n> C. Using gradient descent to minimize the loss function over many epochs\n>\n> D. Randomly initializing weights and selecting the best performing set\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: A**\n\n**Explanation:**\n- **A is TRUE**: Linear Regression with the normal equation computes the optimal weights directly using w = (ΦᵀΦ)⁻¹Φᵀy. This is a closed-form solution that finds the global optimum in one calculation, without any iterative training.\n- **B is FALSE**: There's no exhaustive search through all possible lines. With infinite possible lines, this would be computationally impossible. The normal equation mathematically derives the optimal solution.\n- **C is FALSE**: While gradient descent CAN be used for Linear Regression (especially with large datasets), the standard implementation uses the normal equation which requires no iterations or epochs.\n- **D is FALSE**: Linear Regression is deterministic, not random. The normal equation guarantees finding the exact optimal weights without any random initialization or trial-and-error.\n\n**Key Insight**: The normal equation is what makes Linear Regression \"fast to train\" - it's a single matrix operation, unlike iterative optimization methods.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Complexity: From Simple to Complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While basic Linear Regression fits a straight line, we can create more complex models using **polynomial features**.\n",
    "\n",
    "**Examples:**\n",
    "- **Degree 1 (Linear):** y = w₀ + w₁x\n",
    "- **Degree 2 (Quadratic):** y = w₀ + w₁x + w₂x²\n",
    "- **Degree 3 (Cubic):** y = w₀ + w₁x + w₂x² + w₃x³\n",
    "\n",
    "As model complexity increases:\n",
    "- **Low complexity (underfit):** Model is too simple, high bias, misses patterns\n",
    "- **Right complexity:** Model generalizes well, balanced bias-variance\n",
    "- **High complexity (overfit):** Model is too complex, high variance, memorizes noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 50).reshape(-1, 1)\n",
    "y = 0.5 * X.ravel()**2 + X.ravel() + 2 + np.random.normal(0, 1, 50)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "degrees = [1, 2, 9]\n",
    "titles = ['Underfitting (Degree 1)', 'Good Fit (Degree 2)', 'Overfitting (Degree 9)']\n",
    "\n",
    "for ax, degree, title in zip(axes, degrees, titles):\n",
    "    # Fit polynomial regression\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    # Plot\n",
    "    X_plot = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    \n",
    "    ax.scatter(X, y, c='lightblue', s=50, edgecolors='black', label='Data')\n",
    "    ax.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'Degree {degree}')\n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-off in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias** = Error from overly simplistic assumptions\n",
    "- High bias → Underfitting → Model too simple\n",
    "- Example: Fitting a line to quadratic data\n",
    "\n",
    "**Variance** = Error from sensitivity to small fluctuations in training data\n",
    "- High variance → Overfitting → Model too complex\n",
    "- Example: High-degree polynomial that memorizes noise\n",
    "\n",
    "**Goal:** Find the sweet spot that minimizes **total error = bias² + variance + irreducible error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: A linear regression model with polynomial degree 9 has very low training error (MSE = 0.05) but high validation error (MSE = 2.50). This indicates:\n>\n> A. High bias (underfitting) - the model is too simple\n>\n> B. High variance (overfitting) - the model is too complex\n>\n> C. Perfect fit - the model generalizes well\n>\n> D. The validation set is too small to evaluate properly\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: B**\n\n**Explanation:**\n- **A is FALSE**: High bias (underfitting) would cause BOTH training and validation errors to be high. Here, training error is very low (0.05), so the model is definitely not too simple.\n- **B is TRUE**: The large gap between training error (0.05) and validation error (2.50) is the classic signature of overfitting. The degree-9 polynomial has memorized the training data (including noise) but fails to generalize to new data. This is high variance - the model is too complex.\n- **C is FALSE**: A well-generalizing model would have similar training and validation errors. The 50× gap (0.05 vs 2.50) shows severe overfitting, not good generalization.\n- **D is FALSE**: While validation set size matters, a 50× error gap is far too large to be explained by sampling variance alone. This clearly indicates a model complexity problem.\n\n**Key Insight**: When training error is much lower than validation error, you have overfitting (high variance). When BOTH are high, you have underfitting (high bias).\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Linear Regression's predictions aren't affected by feature scaling (unlike KNN), scaling is still important because:\n",
    "\n",
    "1. **Numerical Stability:** Computing (ΦᵀΦ)⁻¹ can be numerically unstable when features have very different scales\n",
    "2. **Regularization:** If using Ridge or Lasso regression, unscaled features will be penalized unfairly\n",
    "3. **Optimization:** When using gradient descent instead of the normal equation, convergence is much faster with scaled features\n",
    "4. **Interpretability:** Standardized coefficients can be compared to determine feature importance\n",
    "\n",
    "**Best Practice:** Always scale features for regression tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Z-Score Formula\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- **x** = original value\n",
    "- **μ** = mean of the feature\n",
    "- **σ** = standard deviation of the feature\n",
    "- **z** = standardized value\n",
    "\n",
    "### What This Does\n",
    "- Centers data around 0 (mean = 0)\n",
    "- Scales to unit variance (std = 1)\n",
    "- Preserves the distribution shape\n",
    "- Makes features comparable\n",
    "\n",
    "### Critical Rule for Train/Validation/Test Splits\n",
    "```python\n",
    "# 1. Fit scaler on training data ONLY\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Compute μ and σ from training data\n",
    "\n",
    "# 2. Transform all sets using SAME parameters\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)      # Use training μ and σ\n",
    "X_test_scaled = scaler.transform(X_test)    # Use training μ and σ\n",
    "```\n",
    "\n",
    "**Why?** To prevent **data leakage** - the model should not have any information about validation/test sets during training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: After properly fitting a StandardScaler on training data and transforming all sets, what should be TRUE about the scaled training features?\n>\n> A. All features should have values between 0 and 1\n>\n> B. All features should have the same variance as the original data\n>\n> C. All features should have approximately mean = 0 and standard deviation = 1\n>\n> D. All features should be normalized to have the same range as the target variable\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: C**\n\n**Explanation:**\n- **A is FALSE**: StandardScaler uses z-score normalization, which transforms data to have mean=0 and std=1. Values can be negative and exceed 1 (e.g., outliers might be ±3 or more). Min-max scaling (MinMaxScaler) produces [0,1] ranges, not StandardScaler.\n- **B is FALSE**: The entire purpose of standardization is to CHANGE the variance to 1. The original variance could have been anything (e.g., 100, 0.01, etc.), but after scaling, all features have variance ≈ 1.\n- **C is TRUE**: StandardScaler applies z = (x - μ)/σ to each feature, which centers the data at mean=0 and scales to std=1. This is exactly what z-score standardization does.\n- **D is FALSE**: Feature scaling is independent of the target variable. StandardScaler only looks at the feature distributions, not the target. Features and target can have completely different scales.\n\n**Key Insight**: StandardScaler (z-score) creates mean=0, std=1. MinMaxScaler creates values in [0,1]. They serve different purposes!\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Pseudocode\n",
    "\n",
    "```\n",
    "============================================\n",
    "Inputs\n",
    "============================================\n",
    "X       ← training features (N × d matrix)\n",
    "y       ← training targets (N × 1 vector)\n",
    "X_query ← examples to predict\n",
    "\n",
    "============================================\n",
    "----- fit -----\n",
    "============================================\n",
    "1. Store X and y\n",
    "2. Add bias column: Φ ← [1, X]  # (N × (d+1))\n",
    "3. Compute weights using normal equation:\n",
    "   w ← (ΦᵀΦ)⁻¹Φᵀy\n",
    "4. Store w\n",
    "\n",
    "============================================\n",
    "----- predict -----\n",
    "============================================\n",
    "For each query point in X_query:\n",
    "1. Add bias: Φ_query ← [1, X_query]\n",
    "2. Compute prediction: ŷ ← Φ_query · w\n",
    "3. Return ŷ\n",
    "```\n",
    "\n",
    "### Key Observations\n",
    "- **No iterations needed:** One-step solution via normal equation\n",
    "- **Fast prediction:** Just matrix multiplication\n",
    "- **Memory efficient:** Only stores weights (not all training data like KNN)\n",
    "- **Global model:** Learns one function for entire space (unlike local KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Custom Linear Regression Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a scaffold of the `MyLinearRegressor` class. Fill in the TODO sections to complete the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\n\nclass MyLinearRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Custom Linear Regression implementation using the normal equation.\n    \n    Parameters:\n    -----------\n    None\n    \n    Attributes:\n    -----------\n    weights_ : array of shape (n_features + 1,)\n        Learned weights including bias term\n    \"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y):\n        \"\"\"\n        Fit the linear regression model using the normal equation.\n        \n        Parameters:\n        -----------\n        X : array-like of shape (n_samples, n_features)\n            Training data\n        y : array-like of shape (n_samples,)\n            Target values\n        \n        Returns:\n        --------\n        self\n        \"\"\"\n        # TODO: Add column of ones for bias term\n        # Hint: Use np.c_[np.ones(len(X)), X] to create design matrix Phi\n        Phi = ___\n        \n        # TODO: Compute weights using normal equation: w = (Phi^T Phi)^{-1} Phi^T y\n        # Hint: Use @ for matrix multiplication, .T for transpose, np.linalg.inv() for inverse\n        self.weights_ = ___\n        \n        return self\n    \n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n        \n        Parameters:\n        -----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to predict\n        \n        Returns:\n        --------\n        y_pred : array of shape (n_samples,)\n            Predicted values\n        \"\"\"\n        # TODO: Add column of ones for bias term\n        Phi = ___\n        \n        # TODO: Compute predictions: y_pred = Phi @ weights\n        y_pred = ___\n        \n        return y_pred"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Once you have filled in the implementation, let's test our custom regressor on a simple dataset to ensure it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple test data\n",
    "np.random.seed(42)\n",
    "X_simple = np.array([[1], [2], [3], [4], [5]])\n",
    "y_simple = np.array([2, 4, 6, 8, 10])  # Perfect linear relationship: y = 2x\n",
    "\n",
    "# Fit model\n",
    "model = MyLinearRegressor()\n",
    "model.fit(X_simple, y_simple)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_simple)\n",
    "\n",
    "print(\"Learned weights (w0=intercept, w1=slope):\", model.weights_)\n",
    "print(\"Expected: [0, 2] or very close to it\")\n",
    "print(\"\\nPredictions:\", predictions)\n",
    "print(\"Actual:     \", y_simple)\n",
    "print(\"\\nMean Squared Error:\", np.mean((predictions - y_simple)**2))\n",
    "print(\"Expected: ~0 (perfect fit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: Unlike algorithms such as KNN, Linear Regression has very fast prediction time even with large datasets. Why?\n>\n> A. Linear Regression learns a parametric model (weights) during training. Prediction is just a simple matrix multiplication, regardless of training set size.\n>\n> B. Linear Regression stores a compressed version of the training data that requires less computation\n>\n> C. Linear Regression uses the normal equation which caches distances to training points\n>\n> D. Linear Regression only uses the K most important training examples for each prediction\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: A**\n\n**Explanation:**\n- **A is TRUE**: Linear Regression is a parametric model that learns fixed weights w during training. At prediction time, it computes ŷ = Φw (matrix multiplication), which is O(d) where d is the number of features. The training set size N is irrelevant at prediction time because the model doesn't reference training data - only the learned weights.\n- **B is FALSE**: Linear Regression doesn't store any training data (compressed or otherwise) after training. It only stores the weight vector w, which has size (d+1) regardless of how many training examples (N) were used.\n- **C is FALSE**: The normal equation w = (ΦᵀΦ)⁻¹Φᵀy is only used during training to compute weights. It doesn't cache distances or store anything for prediction time. Linear Regression doesn't use distance computations at all.\n- **D is FALSE**: This describes KNN behavior, not Linear Regression. Linear Regression uses ALL training data to learn weights during training, but uses NO training examples during prediction - it only uses the learned weights.\n\n**Key Insight**: Parametric models (Linear Regression) learn fixed parameters → O(d) prediction. Non-parametric models (KNN) reference training data → O(Nd) prediction.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the same synthetic dataset from the Code Walk Through to visualize how Linear Regression works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same data as in Code Walk Through\n",
    "np.random.seed(42)\n",
    "X_train = np.arange(-9.5, 8.5, 0.1).reshape(-1, 1)\n",
    "y_train = X_train.ravel() + 1 + np.random.normal(0, 2, len(X_train))\n",
    "\n",
    "print(f\"Training data: {len(X_train)} points\")\n",
    "print(f\"X range: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "print(f\"y range: [{y_train.min():.1f}, {y_train.max():.1f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Training Data: Linear Relationship with Noise', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Visualizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Fit your MyLinearRegressor on the training data\n# Hint: model = MyLinearRegressor()\n#       model.fit(X_train, y_train)\n\nmodel = ___\nmodel.fit(___, ___)\n\nprint(f\"Learned weights: {model.weights_}\")\nprint(f\"Model equation: y = {model.weights_[1]:.3f}x + {model.weights_[0]:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fit\n",
    "x_line = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
    "y_line = model.predict(x_line)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, edgecolors='black', linewidths=0.5, label='Training data')\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'Best fit: y={model.weights_[1]:.2f}x+{model.weights_[0]:.2f}')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Linear Regression: Best Fit Line', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Predictions: Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the model makes a prediction for a single test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Make a prediction for x = 5.0\n# Hint: X_test = np.array([[5.0]])\n#       y_pred = model.predict(X_test)\n\nX_test = np.array([[___]])\ny_pred = model.predict(___)\n\nprint(f\"For x = {X_test[0, 0]:.1f}:\")\nprint(f\"Predicted y = {y_pred[0]:.3f}\")\nprint(f\"Calculation: y = {model.weights_[1]:.3f} × {X_test[0, 0]:.1f} + {model.weights_[0]:.3f} = {y_pred[0]:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, label='Training data')\n",
    "plt.plot(x_line, y_line, 'k-', linewidth=2, label='Best fit line')\n",
    "plt.scatter(X_test, y_pred, c='red', s=200, marker='*', edgecolors='black', linewidths=2, \n",
    "           label=f'Prediction: x={X_test[0,0]:.1f}, ŷ={y_pred[0]:.2f}', zorder=5)\n",
    "plt.plot([X_train.min(), X_test[0,0]], [y_pred[0], y_pred[0]], 'r--', alpha=0.5, linewidth=1)\n",
    "plt.plot([X_test[0,0], X_test[0,0]], [y_train.min(), y_pred[0]], 'r--', alpha=0.5, linewidth=1)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Linear Regression: Making a Prediction', fontsize=16)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics: MSE and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- Measures average squared difference between actual and predicted values\n",
    "- Units are squared (e.g., if y is in dollars, MSE is in dollars²)\n",
    "- Heavily penalizes large errors (due to squaring)\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}}$$\n",
    "\n",
    "- Same units as the target variable\n",
    "- Easier to interpret (e.g., \"average error of $5000\")\n",
    "- Commonly used for regression evaluation\n",
    "\n",
    "### R² Score (Coefficient of Determination)\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "- Ranges from -∞ to 1 (1 is perfect, 0 means model is no better than predicting mean)\n",
    "- Represents proportion of variance explained by the model\n",
    "- Scale-independent (can compare across different datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Compute metrics on training data\n",
    "y_train_pred = model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Training Metrics:\")\n",
    "print(f\"  MSE:  {mse:.3f}\")\n",
    "print(f\"  RMSE: {rmse:.3f}\")\n",
    "print(f\"  R²:   {r2:.3f}\")\n",
    "print(f\"\\nInterpretation: The model's predictions are on average {rmse:.2f} units away from actual values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a Real Dataset: California Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply Linear Regression to a real-world dataset. We'll use the California Housing dataset, which contains information about California districts and median house values.\n",
    "\n",
    "**Dataset Features:**\n",
    "- MedInc: Median income in block group\n",
    "- HouseAge: Median house age in block group  \n",
    "- AveRooms: Average number of rooms per household\n",
    "- AveBedrms: Average number of bedrooms per household\n",
    "- Population: Block group population\n",
    "- AveOccup: Average number of household members\n",
    "- Latitude: Block group latitude\n",
    "- Longitude: Block group longitude\n",
    "\n",
    "**Target:** Median house value (in $100,000s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = y\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how features have **very different scales:**\n",
    "- MedInc ranges from ~0.5 to ~15\n",
    "- Population ranges from ~3 to ~35,000+\n",
    "- Latitude/Longitude are coordinates\n",
    "\n",
    "This is why feature scaling is important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why 3 splits?**\n",
    "- **Training set (60%):** Fit the model\n",
    "- **Validation set (20%):** Tune hyperparameters / compare models\n",
    "- **Test set (20%):** Final evaluation (touch only once!)\n",
    "\n",
    "**Important:** Test set simulates real-world unseen data. Never use it for model selection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\n\n# TODO: Split into Train (60%), Validation (20%), Test (20%)\n# Hint: First split into Train (60%) and Temp (40%)\n#       Then split Temp into equal halves for Validation and Test\n#       Use random_state=42 for reproducibility\n\n# Step 1: Split into Train (60%) and Temp (40%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=___, random_state=___)\n\n# Step 2: Split Temp into Validation (50% of 40% = 20%) and Test (50% of 40% = 20%)\nX_val, X_test, y_val, y_test = train_test_split(___, ___, test_size=___, random_state=___)\n\nprint(f\"Training set:   {len(X_train)} samples ({len(X_train)/len(X)*100:.0f}%)\")\nprint(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.0f}%)\")\nprint(f\"Test set:       {len(X_test)} samples ({len(X_test)/len(X)*100:.0f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling: Comparing Unscaled vs Scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression on Unscaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Fit MyLinearRegressor on unscaled training data\n# TODO: Predict on validation set\n# TODO: Compute RMSE and R² scores\n\nmodel_unscaled = ___\nmodel_unscaled.fit(___, ___)\n\ny_val_pred_unscaled = model_unscaled.predict(___)\n\nrmse_unscaled = np.sqrt(mean_squared_error(___, ___))\nr2_unscaled = r2_score(___, ___)\n\nprint(\"Performance on UNSCALED features:\")\nprint(f\"  Validation RMSE: {rmse_unscaled:.4f}\")\nprint(f\"  Validation R²:   {r2_unscaled:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear Regression on Scaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler\n\n# TODO: Create and fit StandardScaler on training data ONLY\n# TODO: Transform train and validation sets\n# Hint: scaler = StandardScaler()\n#       scaler.fit(X_train)\n#       X_train_scaled = scaler.transform(X_train)\n#       X_val_scaled = scaler.transform(X_val)\n\nscaler = ___\nscaler.fit(___)\n\nX_train_scaled = scaler.transform(___)\nX_val_scaled = scaler.transform(___)\n\nprint(\"Scaled training data statistics:\")\nprint(f\"  Mean: {X_train_scaled.mean(axis=0)}\")\nprint(f\"  Std:  {X_train_scaled.std(axis=0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Fit MyLinearRegressor on scaled training data\n# TODO: Predict on scaled validation set\n# TODO: Compute RMSE and R² scores\n\nmodel_scaled = ___\nmodel_scaled.fit(___, ___)\n\ny_val_pred_scaled = model_scaled.predict(___)\n\nrmse_scaled = np.sqrt(mean_squared_error(___, ___))\nr2_scaled = r2_score(___, ___)\n\nprint(\"Performance on SCALED features:\")\nprint(f\"  Validation RMSE: {rmse_scaled:.4f}\")\nprint(f\"  Validation R²:   {r2_scaled:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*50)\nprint(\"COMPARISON: Unscaled vs Scaled Features\")\nprint(\"=\"*50)\nprint(f\"Unscaled - RMSE: {rmse_unscaled:.4f} | R²: {r2_unscaled:.4f}\")\nprint(f\"Scaled   - RMSE: {rmse_scaled:.4f} | R²: {r2_scaled:.4f}\")\nprint(\"\\nNote: For Linear Regression with normal equation, scaling doesn't change predictions.\")\nprint(\"However, it improves numerical stability and is essential for regularization!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: You're evaluating a Linear Regression model and want to ensure your performance estimates are unbiased and reflect real-world generalization. Which practice is MOST important?\n>\n> A. Using the largest possible training set to maximize model performance\n>\n> B. Tuning hyperparameters directly on the test set to find the best configuration\n>\n> C. Keeping the test set completely unseen until final evaluation, using a separate validation set for model selection\n>\n> D. Fitting the StandardScaler on all data (train + validation + test) before splitting\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: C**\n\n**Explanation:**\n- **A is FALSE**: While a larger training set generally helps, this alone doesn't ensure unbiased evaluation. If you don't properly separate validation/test sets or if you leak information, performance estimates will be overly optimistic regardless of training set size.\n- **B is FALSE**: This is a critical error that causes data leakage! If you tune hyperparameters on the test set, you're indirectly \"training\" on it. Your test performance will be overly optimistic and won't reflect true generalization to new data. This defeats the purpose of having a test set.\n- **C is TRUE**: The test set must remain completely unseen until final evaluation to provide an unbiased estimate of real-world performance. Use the validation set for hyperparameter tuning and model selection, reserve the test set for the final evaluation only once.\n- **D is FALSE**: This causes data leakage! Fitting the scaler on test data means your model has \"seen\" test set statistics (mean, std). Always fit the scaler ONLY on training data, then transform validation/test sets using those learned parameters.\n\n**Key Insight**: Proper train/validation/test separation is crucial. Any information from validation/test sets that influences training or model selection will inflate performance estimates.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Scikit-Learn's Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our implementation matches scikit-learn's LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import LinearRegression as SklearnLR\n\n# Fit sklearn model\nsklearn_model = SklearnLR()\nsklearn_model.fit(X_train_scaled, y_train)\ny_val_pred_sklearn = sklearn_model.predict(X_val_scaled)\n\n# Compute metrics\nrmse_sklearn = np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn))\nr2_sklearn = r2_score(y_val, y_val_pred_sklearn)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON: Custom Implementation vs Scikit-Learn\")\nprint(\"=\"*60)\nprint(f\"Custom MyLinearRegressor - RMSE: {rmse_scaled:.4f} | R²: {r2_scaled:.4f}\")\nprint(f\"Sklearn LinearRegression - RMSE: {rmse_sklearn:.4f} | R²: {r2_sklearn:.4f}\")\nprint(f\"\\nDifference in RMSE: {abs(rmse_scaled - rmse_sklearn):.6f}\")\nprint(f\"Difference in R²:   {abs(r2_scaled - r2_sklearn):.6f}\")\nprint(\"\\n✓ Results should match (within numerical precision)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions vs Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create scatter plot of predictions vs actual\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val, y_val_pred_scaled, alpha=0.5, edgecolors='black', linewidths=0.5)\nplt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', linewidth=2, label='Perfect predictions')\nplt.xlabel('Actual Values', fontsize=14)\nplt.ylabel('Predicted Values', fontsize=14)\nplt.title(f'Predictions vs Actual (R² = {r2_scaled:.3f})', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Points close to the red line indicate accurate predictions.\")\nprint(\"Scatter away from the line shows prediction errors.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Prediction Errors: Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute residuals (errors)\nresiduals = y_val - y_val_pred_scaled\n\n# Create residual plot\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot 1: Residuals vs Predicted Values\naxes[0].scatter(y_val_pred_scaled, residuals, alpha=0.5, edgecolors='black', linewidths=0.5)\naxes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Predicted Values', fontsize=12)\naxes[0].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\naxes[0].set_title('Residual Plot', fontsize=14)\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Histogram of Residuals\naxes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\naxes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Residuals', fontsize=12)\naxes[1].set_ylabel('Frequency', fontsize=12)\naxes[1].set_title('Distribution of Residuals', fontsize=14)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Good residual plots should show:\")\nprint(\"  1. Residuals randomly scattered around 0 (no patterns)\")\nprint(\"  2. Approximately normal distribution\")\nprint(\"  3. Constant variance across all predicted values (homoscedasticity)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Outliers on Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is **sensitive to outliers** because it uses the **sum of squared errors** as its objective function. Outliers can significantly distort the regression line by pulling it towards themselves, leading to:\n\n",
    "- **Biased coefficient estimates**: The model tries to minimize error for all points, including outliers\n",
    "- **Reduced model accuracy**: The regression line may not represent the true relationship for most data points\n",
    "- **Poor generalization**: The model fits extreme points rather than the overall trend\n\n",
    "**Key Principle**: Our ML model should perform well in most cases. Therefore, outliers should often be detected and handled appropriately (removed, capped, or modeled separately) before training.\n\n",
    "Let's demonstrate the impact of outliers with a visual example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate clean data with a linear relationship\n",
    "np.random.seed(42)\n",
    "X_clean = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_clean = 2 * X_clean.ravel() + 1 + np.random.normal(0, 1, 50)\n",
    "\n",
    "# Add an outlier\n",
    "X_with_outlier = np.vstack([X_clean, [[8.0]]])\n",
    "y_with_outlier = np.append(y_clean, [5.0])  # This point is far below the trend\n",
    "\n",
    "# Fit models\n",
    "model_clean = MyLinearRegressor()\n",
    "model_clean.fit(X_clean, y_clean)\n",
    "\n",
    "model_with_outlier = MyLinearRegressor()\n",
    "model_with_outlier.fit(X_with_outlier, y_with_outlier)\n",
    "\n",
    "# Create predictions for plotting\n",
    "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_plot_clean = model_clean.predict(X_plot)\n",
    "y_plot_with_outlier = model_with_outlier.predict(X_plot)\n",
    "\n",
    "# Visualize the impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot without outlier\n",
    "ax1.scatter(X_clean, y_clean, alpha=0.6, edgecolors='black', linewidths=0.5, label='Training data')\n",
    "ax1.plot(X_plot, y_plot_clean, 'r-', linewidth=2, \n",
    "         label=f'Fit: y={model_clean.weights_[1]:.2f}x+{model_clean.weights_[0]:.2f}')\n",
    "ax1.set_xlabel('X', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('Linear Regression WITHOUT Outlier', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot with outlier\n",
    "ax2.scatter(X_clean, y_clean, alpha=0.6, edgecolors='black', linewidths=0.5, label='Training data')\n",
    "ax2.scatter([8.0], [5.0], color='red', s=200, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Outlier', zorder=5)\n",
    "ax2.plot(X_plot, y_plot_clean, 'g--', linewidth=2, alpha=0.5, label='Original fit (without outlier)')\n",
    "ax2.plot(X_plot, y_plot_with_outlier, 'r-', linewidth=2, \n",
    "         label=f'Fit WITH outlier: y={model_with_outlier.weights_[1]:.2f}x+{model_with_outlier.weights_[0]:.2f}')\n",
    "ax2.set_xlabel('X', fontsize=12)\n",
    "ax2.set_ylabel('y', fontsize=12)\n",
    "ax2.set_title('Linear Regression WITH Outlier', fontsize=14)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nImpact of the outlier:\")\n",
    "print(f\"  Slope changed from {model_clean.weights_[1]:.3f} to {model_with_outlier.weights_[1]:.3f}\")\n",
    "print(f\"  Intercept changed from {model_clean.weights_[0]:.3f} to {model_with_outlier.weights_[0]:.3f}\")\n",
    "print(f\"\\nThe regression line is 'pulled' toward the outlier, affecting predictions for all points!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: You notice one data point in your training set has an extremely high target value compared to similar inputs. What is the likely impact on your linear regression model?\n>\n> A. No impact - linear regression automatically detects and ignores outliers during training\n>\n> B. The regression line will be pulled toward the outlier, potentially degrading predictions for typical data points\n>\n> C. The model will generalize better because it learns to handle extreme cases\n>\n> D. Only the intercept will be affected, not the slope coefficients\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: B**\n\n**Explanation:**\n- **A is FALSE**: Linear regression has NO built-in outlier detection or robust loss function. The normal equation w = (ΦᵀΦ)⁻¹Φᵀy treats all points equally in the mathematical derivation. Unlike some algorithms (e.g., RANSAC or Huber regression), standard Linear Regression doesn't identify or downweight outliers.\n- **B is TRUE**: Linear regression minimizes the sum of squared errors: Σ(yᵢ - ŷᵢ)². Because errors are squared, outliers with large residuals contribute disproportionately to the loss (e.g., error of 10 contributes 100 to the loss vs. error of 1 contributing 1). The model shifts the regression line to reduce the outlier's massive error, which worsens predictions for the majority of normal points.\n- **C is FALSE**: Outliers are typically noise or data quality issues, not valuable extreme cases. The model \"learning\" from an outlier means it's memorizing noise, which hurts generalization. If the outlier appears due to measurement error or data entry mistake, the model is fitting invalid data.\n- **D is FALSE**: Both intercept (w₀) and slope coefficients (w₁, w₂, ...) are affected by outliers. The entire regression hyperplane can rotate and shift. The normal equation computes all weights simultaneously, and an outlier influences the entire (ΦᵀΦ)⁻¹Φᵀy calculation.\n\n**Key Insight**: Linear Regression's squared loss makes it highly sensitive to outliers. Use robust regression methods (Huber, RANSAC) or remove outliers before training for better performance on typical data.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finalized our approach (using scaled features), let's evaluate on the test set **one time only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Transform test set using the SAME scaler fitted on training data\n# TODO: Predict on test set\n# TODO: Compute final RMSE and R² scores\n\nX_test_scaled = scaler.transform(___)\n\ny_test_pred = model_scaled.predict(___)\n\ntest_rmse = np.sqrt(mean_squared_error(___, ___))\ntest_r2 = r2_score(___, ___)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL TEST SET EVALUATION\")\nprint(\"=\"*60)\nprint(f\"Test RMSE: {test_rmse:.4f}\")\nprint(f\"Test R²:   {test_r2:.4f}\")\nprint(f\"\\nInterpretation: Our model's predictions are on average\")\nprint(f\"{test_rmse:.2f} × $100,000 = ${test_rmse*100000:.0f} away from actual house values.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we standardized our features, we can compare the magnitude of learned weights to understand feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract feature weights (excluding bias)\nfeature_weights = model_scaled.weights_[1:]  # Skip bias term\n\n# Create DataFrame for visualization\nimportance_df = pd.DataFrame({\n    'Feature': housing.feature_names,\n    'Weight': feature_weights\n}).sort_values('Weight', key=abs, ascending=False)\n\n# Plot\nplt.figure(figsize=(10, 6))\ncolors = ['green' if x > 0 else 'red' for x in importance_df['Weight']]\nplt.barh(importance_df['Feature'], importance_df['Weight'], color=colors, edgecolor='black')\nplt.xlabel('Standardized Weight', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance (Standardized Weights)', fontsize=14)\nplt.axvline(x=0, color='black', linewidth=0.8)\nplt.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFeature Importance:\")\nprint(importance_df.to_string(index=False))\nprint(\"\\nGreen = Positive correlation (↑ feature → ↑ house value)\")\nprint(\"Red   = Negative correlation (↑ feature → ↓ house value)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: In the California Housing dataset with standardized features, you observe these weights: MedInc: +0.82, Latitude: -0.15, HouseAge: +0.05. What is the MOST appropriate interpretation?\n>\n> A. Median income causes house values to increase by $82,000\n>\n> B. Latitude has minimal impact because the weight is negative\n>\n> C. HouseAge should be removed from the model because its weight is small\n>\n> D. Median income has the strongest association with house values among these three features\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: D**\n\n**Explanation:**\n- **A is FALSE**: Correlation ≠ causation. A weight indicates association, not causation. Also, with standardized features, weights represent the change in target per 1 standard deviation change in the feature, not per unit change. You cannot directly convert the weight to a dollar amount without considering the feature's scale and the causality question.\n- **B is FALSE**: The magnitude matters more than the sign. |−0.15| > |+0.05|, so Latitude has more impact than HouseAge. The negative sign just means inverse relationship: as Latitude increases (moving north in California), house values tend to decrease (makes sense - San Diego area vs. rural northern areas).\n- **C is FALSE**: Small weight doesn't automatically mean the feature should be removed. HouseAge might still provide value and removing it could hurt generalization. Feature selection should be based on validation performance, not just weight magnitude. Also, even small weights can be statistically significant.\n- **D is TRUE**: With standardized features, weight magnitudes are directly comparable. |0.82| > |−0.15| > |0.05| indicates MedInc has the strongest linear association with house values. When features are on the same scale (mean=0, std=1), larger absolute weights indicate stronger relationships.\n\n**Key Insight**: Standardized weights allow direct comparison of feature importance. But remember: large weight = strong association, not causation!\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Linear Regression is parametric and efficient**\n",
    "   - Learns fixed weights during training\n",
    "   - Fast prediction (just matrix multiplication)\n",
    "   - Closed-form solution via normal equation\n",
    "\n",
    "2. **Always scale your features**\n",
    "   - Fit scaler on training data ONLY\n",
    "   - Transform all sets using same parameters\n",
    "   - Essential for regularization and numerical stability\n",
    "\n",
    "3. **Use proper train/validation/test splits**\n",
    "   - Training: Fit the model\n",
    "   - Validation: Tune hyperparameters/compare models\n",
    "   - Test: Final evaluation (touch once!)\n",
    "\n",
    "4. **Understand bias-variance tradeoff**\n",
    "   - Simple models (low degree) → high bias (underfit)\n",
    "   - Complex models (high degree) → high variance (overfit)\n",
    "   - Find the sweet spot using validation set\n",
    "\n",
    "5. **Analyze residuals**\n",
    "   - Check for patterns in residual plots\n",
    "   - Validate assumptions (normality, homoscedasticity)\n",
    "   - Identify areas where model struggles\n",
    "\n",
    "### When to Use Linear Regression\n",
    "\n",
    "✅ **Good for:**\n",
    "- Relationships that are approximately linear\n",
    "- When interpretability is important\n",
    "- Baseline model for comparison\n",
    "- Large datasets (very efficient)\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Highly non-linear relationships (without feature engineering)\n",
    "- Data with many outliers (consider robust regression)\n",
    "- When features are highly collinear (consider Ridge/Lasso)\n",
    "\n",
    "### Linear Regression vs KNN Regression\n",
    "\n",
    "| Aspect | Linear Regression | KNN Regression |\n",
    "|--------|------------------|----------------|\n",
    "| **Model Type** | Parametric (learns weights) | Non-parametric (instance-based) |\n",
    "| **Training Time** | Fast (closed-form) | Instant (lazy learner) |\n",
    "| **Prediction Time** | Very fast O(d) | Slower O(Nd) |\n",
    "| **Memory** | Stores only weights | Stores all training data |\n",
    "| **Assumes** | Linear relationship | Local similarity |\n",
    "| **Interpretability** | High (can analyze weights) | Low (black box) |\n",
    "| **Handles Non-linearity** | Requires feature engineering | Naturally handles it |\n",
    "| **Outlier Sensitivity** | High | Medium (depends on K) |\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "- ✅ Always split data into train/validation/test\n",
    "- ✅ Standardize features (fit on train, transform all)\n",
    "- ✅ Analyze residuals to validate assumptions\n",
    "- ✅ Check for multicollinearity (VIF scores)\n",
    "- ✅ Compare with baseline models\n",
    "- ✅ Use cross-validation for robust evaluation\n",
    "- ✅ Interpret feature weights (if features are scaled)\n",
    "- ✅ Watch for data leakage (never fit on validation/test)\n",
    "- ✅ Consider regularization (Ridge/Lasso) for many features\n",
    "- ✅ Evaluate on multiple metrics (RMSE, R², MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Final Question**: You have three Linear Regression models with validation results: Model A (R²=0.85, RMSE=0.52), Model B (R²=0.82, RMSE=0.48), Model C (R²=0.88, RMSE=0.61). Which model should you choose for deployment and why?\n>\n> A. Model A because it has the best balance of R² and RMSE\n>\n> B. Model C because it has the highest R² score\n>\n> C. They are all equivalent since R² and RMSE measure the same thing\n>\n> D. Model B because RMSE directly measures prediction error in the target's units, which matters more for real-world impact\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: D**\n\n**Explanation:**\n- **A is FALSE**: While Model A has moderate scores on both metrics, \"balance\" isn't the goal - minimizing real-world prediction error is. Model B has the lowest RMSE (0.48), meaning its predictions are closest to actual values on average, which is what matters for deployment.\n- **B is FALSE**: R² measures variance explained, but doesn't tell you the magnitude of errors in meaningful units. Model C has the highest R² (0.88) but also the highest RMSE (0.61), meaning its predictions are actually WORSE on average. A high R² with high RMSE can occur when the model captures the overall trend but makes large errors.\n- **C is FALSE**: R² and RMSE measure different things. R² is scale-independent (compares model to baseline of predicting mean), while RMSE is in the target's units (e.g., dollars, years). You can have high R² with high RMSE if the target has high variance. They provide complementary information.\n- **D is TRUE**: RMSE = 0.48 means predictions are off by 0.48 × $100,000 = $48,000 on average. Model B's RMSE of 0.48 is better than Model A's 0.52 ($52,000 error) and Model C's 0.61 ($61,000 error). For deployment, you want to minimize actual prediction errors (RMSE), not just maximize variance explained (R²).\n\n**Key Insight**: For model selection, prioritize metrics that reflect real-world impact. RMSE in interpretable units (dollars, days, etc.) is often more actionable than R².\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the Linear Regression Hands-On Lab! You now understand:\n",
    "\n",
    "- ✅ The mathematics behind Linear Regression\n",
    "- ✅ How to implement it from scratch\n",
    "- ✅ The importance of feature scaling\n",
    "- ✅ How to properly split and evaluate models\n",
    "- ✅ Bias-variance tradeoff concepts\n",
    "- ✅ Residual analysis and diagnostics\n",
    "- ✅ When to use Linear Regression vs other algorithms\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore regularization (Ridge and Lasso regression)\n",
    "- Learn about polynomial regression for non-linear relationships\n",
    "- Study logistic regression for classification tasks\n",
    "- Practice with different datasets to build intuition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}