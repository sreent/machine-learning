{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Linear%20Regression/Stock%20Price%20Prediction%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608PP8jDFKTx"
      },
      "source": [
        "# Case Study: Stock Price Prediction with Linear Regression\n",
        "\n",
        "## The Challenge: Building a Unified Model Across Different Stocks\n",
        "\n",
        "**Business Problem:**  \n",
        "We want to predict stock prices **4 weeks ahead** for FTSE 100 companies using historical data. However, we face a critical challenge:\n",
        "\n",
        "- **Stock AAF.L** trades around **68 pence (¬£0.68)**\n",
        "- **Stock WTB.L** trades around **3,000 pence (¬£30.00)**\n",
        "\n",
        "*Note: FTSE 100 prices are quoted in GBX (pence), not GBP (pounds)*\n",
        "\n",
        "**Why is this a problem?**\n",
        "\n",
        "If we naively use raw prices as both features and targets:\n",
        "- A 5p prediction error on AAF.L (68p) = **7.4% error** üò±\n",
        "- A 5p prediction error on WTB.L (3,000p) = **0.17% error** ‚úÖ\n",
        "\n",
        "The model would optimize for expensive stocks and perform terribly on cheaper ones!\n",
        "\n",
        "**The Solution: Think Like a Quant Trader**\n",
        "\n",
        "Professional traders don't care about absolute price movements - they care about **percentage returns**:\n",
        "\n",
        "$$\\text{Return} = \\frac{\\text{Price}_{today} - \\text{Price}_{yesterday}}{\\text{Price}_{yesterday}}$$\n",
        "\n",
        "- A **5% gain** is meaningful whether on a 68p or 3,000p stock\n",
        "- Returns are **scale-invariant** - perfect for a unified model!\n",
        "- Returns are **additive** over time (log returns)\n",
        "\n",
        "**What We'll Build:**\n",
        "1. Load ~580K rows of FTSE 100 daily OHLC data (2019-2025)\n",
        "2. Engineer financial features: returns, volatility, momentum, moving averages\n",
        "3. Create lagged features (past returns predict future returns)\n",
        "4. Handle time-series data properly (no future data leakage!)\n",
        "5. Build a unified linear regression model predicting **4-week returns**\n",
        "6. Evaluate performance and understand limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP2lzXVgFKTx"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this case study, you will:\n",
        "\n",
        "‚úÖ Understand why **returns** (not prices) are used in financial ML  \n",
        "‚úÖ Learn proper **time-series cross-validation** to avoid lookahead bias  \n",
        "‚úÖ Engineer **domain-specific features** for stock prediction  \n",
        "‚úÖ Handle **multiple entities** (100 stocks) in a unified model  \n",
        "‚úÖ Interpret model performance in financial context  \n",
        "‚úÖ Understand the **challenges and limitations** of linear models for finance\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ TL;DR - The 4-Step Pipeline\n",
        "\n",
        "If you're already familiar with ML, here's the quick overview:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 1. DATA TRANSFORMATION: Prices ‚Üí Percentage Returns             ‚îÇ\n",
        "‚îÇ    Why: 68p stock ‚â† 3,000p stock, but 5% return = 5% return     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                              ‚Üì\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 2. FEATURE ENGINEERING: Lags (1/5/10/20d) + Volatility + MA     ‚îÇ\n",
        "‚îÇ    Critical: Sort by [ticker, date] before ANY calculations!    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                              ‚Üì\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 3. TIME-SERIES SPLIT: Train (2019-2023) ‚Üí Val (2024) ‚Üí Test     ‚îÇ\n",
        "‚îÇ    Never random split! Would leak future into past              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                              ‚Üì\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ 4. EVALUATION: Multiple Metrics (Both Matter!)                  ‚îÇ\n",
        "‚îÇ    ‚Ä¢ R¬≤ measures accuracy of predicted magnitudes               ‚îÇ\n",
        "‚îÇ    ‚Ä¢ Hit Rate measures directional correctness (UP vs DOWN)     ‚îÇ\n",
        "‚îÇ    For trading: 55% Hit Rate can be profitable even if R¬≤ low   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Key Results to Expect:**\n",
        "- R¬≤: ~0.01-0.05 (low is normal for finance!)\n",
        "- Hit Rate: 51-55% (slight edge over 50% random)\n",
        "- Learning: Domain knowledge matters more than algorithm complexity\n",
        "\n",
        "Now let's dive into the details! üëá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aY2mXN9FKTy"
      },
      "source": [
        "## Step 1: Import Libraries and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qQBuLACFKTy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 4)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQldc60bFKTz"
      },
      "outputs": [],
      "source": [
        "# Load the FTSE 100 dataset directly from Google Drive\n",
        "file_id = '14eS3EEKXmDFy-tMXAjqzF8jT6SwZWTJD'\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "df = pd.read_csv(url, parse_dates=['date'])\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Number of unique tickers: {df['ticker'].nunique()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ix8hPAlFKTz"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nBasic statistics:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O9H6XG8FKTz"
      },
      "source": [
        "## Step 2: Exploratory Data Analysis - The Price Scale Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuSeJ34yFKT0"
      },
      "outputs": [],
      "source": [
        "# Analyze price ranges across different stocks\n",
        "price_stats = df.groupby('ticker')['adjclose'].agg(['min', 'mean', 'max', 'std']).sort_values('mean')\n",
        "\n",
        "print(\"Price statistics by ticker (sorted by mean price):\")\n",
        "print(\"\\nCheapest stocks:\")\n",
        "print(price_stats.head(5))\n",
        "print(\"\\nMost expensive stocks:\")\n",
        "print(price_stats.tail(5))\n",
        "\n",
        "# Visualize the problem\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Price range distribution\n",
        "axes[0].hist(price_stats['mean'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Mean Adjusted Close Price (pence)', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Stocks', fontsize=12)\n",
        "axes[0].set_title('Distribution of Average Stock Prices\\n(Why we can\\'t use raw prices!)', fontsize=14)\n",
        "axes[0].axvline(price_stats['mean'].median(), color='red', linestyle='--',\n",
        "               linewidth=2, label=f'Median: {price_stats[\"mean\"].median():.0f}p')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Example time series for cheap vs expensive stocks\n",
        "cheap_stock = price_stats.index[0]  # Cheapest\n",
        "expensive_stock = price_stats.index[-1]  # Most expensive\n",
        "\n",
        "cheap_data = df[df['ticker'] == cheap_stock].set_index('date')['adjclose']\n",
        "expensive_data = df[df['ticker'] == expensive_stock].set_index('date')['adjclose']\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2_twin = ax2.twinx()\n",
        "\n",
        "ax2.plot(cheap_data.index, cheap_data.values, 'b-', linewidth=1.5, label=f'{cheap_stock} (Cheap)', alpha=0.7)\n",
        "ax2_twin.plot(expensive_data.index, expensive_data.values, 'r-', linewidth=1.5, label=f'{expensive_stock} (Expensive)', alpha=0.7)\n",
        "\n",
        "ax2.set_xlabel('Date', fontsize=12)\n",
        "ax2.set_ylabel(f'{cheap_stock} Price (pence)', color='b', fontsize=12)\n",
        "ax2_twin.set_ylabel(f'{expensive_stock} Price (pence)', color='r', fontsize=12)\n",
        "ax2.set_title('Different Price Scales = Need Returns!', fontsize=14)\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "ax2_twin.tick_params(axis='y', labelcolor='r')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Combine legends\n",
        "lines1, labels1 = ax2.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüí° Key Insight: Prices range from {price_stats['mean'].min():.0f}p to {price_stats['mean'].max():.0f}p\")\n",
        "print(f\"   (¬£{price_stats['mean'].min()/100:.2f} to ¬£{price_stats['mean'].max()/100:.2f})\")\n",
        "print(f\"   This {price_stats['mean'].max() / price_stats['mean'].min():.0f}x difference makes raw prices unusable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question**: You're building a unified model for FTSE 100 stocks where prices range from 68p to 3,000p. Why is using percentage returns essential instead of raw prices?\n",
        ">\n",
        "> A. Returns ensure all features scale to [0,1] range, which improves gradient descent convergence rates\n",
        ">\n",
        "> B. Returns make movements comparable: 5p is 7.4% for 68p stocks vs 0.17% for 3,000p stocks\n",
        ">\n",
        "> C. Linear regression's normal equation requires all features to use identical measurement units\n",
        ">\n",
        "> D. Returns reduce overfitting by normalizing the target variable to have zero mean\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation:**\n",
        "- **B is TRUE**: Raw prices have different meanings across stocks. A 5p error on a 68p stock is a massive 7.4% mistake, while the same 5p error on a 3,000p stock is only 0.17%. The model would optimize for expensive stocks (where absolute errors are small percentages) and ignore cheap stocks (where absolute errors are huge percentages). Returns put everything on the same scale: a 5% gain means the same thing for all stocks.\n",
        "\n",
        "- **A is FALSE**: Returns don't scale to [0,1] - they can be negative (stock goes down) and can exceed 100% in theory. That's min-max scaling, not percentage returns. Also, we're using the normal equation (closed-form solution), not gradient descent, so convergence isn't relevant here.\n",
        "\n",
        "- **C is FALSE**: Linear regression works fine with mixed units mathematically - the normal equation w = (Œ¶·µÄŒ¶)‚Åª¬πŒ¶·µÄy computes regardless of units. The issue is MODEL QUALITY and INTERPRETABILITY, not mathematical requirements.\n",
        "\n",
        "- **D is FALSE**: While returns do tend to be centered around zero, this isn't about overfitting prevention. Overfitting is controlled through regularization, feature selection, and validation. The key issue is comparability across stocks with different price scales.\n",
        "\n",
        "**Key Insight**: For multi-entity models, use **relative measures** (percentages, ratios) not **absolute measures** (raw values) to ensure fair comparison across all entities!\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "AcYTMC7lFKT0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD1ifuMWFKT0"
      },
      "source": [
        "## Step 3: Feature Engineering - The Solution\n",
        "\n",
        "### Step 3a: Understanding the Approach (Concept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxxrGmAvFKT0"
      },
      "source": [
        "### Feature Engineering Design Principles\n",
        "\n",
        "**CRITICAL: Sorting Before Feature Calculation**\n",
        "\n",
        "Before calculating ANY time-series features, we **MUST** sort by `['ticker', 'date']`:\n",
        "\n",
        "```python\n",
        "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
        "grouped = df.groupby('ticker')\n",
        "```\n",
        "\n",
        "**Why is this absolutely critical?**\n",
        "\n",
        "‚ùå **Without proper sorting, disasters happen:**\n",
        "\n",
        "1. **Wrong lag values**: `shift(1)` gives you a random previous row, not yesterday!\n",
        "   - Example: If data is unsorted, \"yesterday's return\" might actually be from 6 months ago or a different stock\n",
        "   \n",
        "2. **Mixed ticker data**: Rolling windows could include multiple stocks!\n",
        "   - Example: 5-day volatility for AAF.L accidentally includes days from WTB.L\n",
        "   \n",
        "3. **Time travel paradoxes**: Future data leaks into past features\n",
        "   - Example: \"Yesterday's\" return is actually from next month\n",
        "\n",
        "4. **Incorrect momentum**: Cumulative returns calculated across time gaps\n",
        "   - Example: 5-day return spans from Monday to Friday 2 years later\n",
        "\n",
        "**Correct approach:**\n",
        "```python\n",
        "# Step 1: Sort by ticker AND date\n",
        "df.sort_values(['ticker', 'date'])  # Ensures each ticker's data is chronological\n",
        "\n",
        "# Step 2: Group by ticker before ANY time-series operations\n",
        "grouped = df.groupby('ticker')\n",
        "\n",
        "# Step 3: Use grouped transforms (automatically respects ticker boundaries)\n",
        "grouped['adjclose'].transform(lambda x: x.shift(1))  # Safe! Won't mix tickers\n",
        "```\n",
        "\n",
        "**Visual example of the problem:**\n",
        "\n",
        "| Before Sorting (WRONG) | After Sorting (CORRECT) |\n",
        "|------------------------|-------------------------|\n",
        "| 2024-01-05, WTB.L, 3000p | 2024-01-01, AAF.L, 68p |\n",
        "| 2024-01-01, AAF.L, 68p   | 2024-01-02, AAF.L, 70p |\n",
        "| 2024-01-03, WTB.L, 3020p | 2024-01-03, AAF.L, 69p |\n",
        "| 2024-01-02, AAF.L, 70p   | 2024-01-01, WTB.L, 3000p |\n",
        "| 2024-01-03, AAF.L, 69p   | 2024-01-03, WTB.L, 3020p |\n",
        "|                          | 2024-01-05, WTB.L, 3010p |\n",
        "\n",
        "Without sorting, `shift(1)` on row 2 gives you WTB.L's price (3000p) as AAF.L's \"previous day\"! üí•\n",
        "\n",
        "**Why Percentage-Based Features?**\n",
        "\n",
        "For a unified model across stocks with vastly different prices (68p vs 3,000p), we need features that are **comparable** across tickers:\n",
        "\n",
        "1. **Returns** (not prices):\n",
        "   - A 5p move on AAF.L (68p) = 7.4% change\n",
        "   - A 5p move on WTB.L (3,000p) = 0.17% change\n",
        "   - Solution: Use percentage returns so both are on the same scale\n",
        "\n",
        "2. **Lagged Returns at Meaningful Intervals**:\n",
        "   - Not arbitrary (1,2,3,4,5 days) but trading-relevant periods\n",
        "   - **1 day**: Yesterday's performance\n",
        "   - **5 days**: 1 week ago (captures weekly patterns)\n",
        "   - **10 days**: 2 weeks ago (medium-term trends)\n",
        "   - **20 days**: 4 weeks ago (matches our prediction horizon)\n",
        "\n",
        "3. **Volatility as Percentage**:\n",
        "   - Standard deviation of returns (already in percentage terms)\n",
        "   - 2% daily volatility means the same thing for all stocks\n",
        "\n",
        "4. **Momentum as Percentage Change**:\n",
        "   - (Price_t / Price_t-k) - 1 gives percentage gain/loss\n",
        "   - +10% momentum is comparable whether on 68p or 3,000p stock\n",
        "\n",
        "5. **Moving Averages Normalized**:\n",
        "   - Distance from MA as percentage of current price\n",
        "   - (Price - MA) / Price shows relative deviation\n",
        "   - Being 5% above MA means the same thing for all tickers\n",
        "\n",
        "6. **Target: 4-Week Percentage Change**:\n",
        "   - (Price_t+20 - Price_t) / Price_t\n",
        "   - Predicting percentage returns (not absolute price changes)\n",
        "\n",
        "**Why Use Adjusted Close?**\n",
        "\n",
        "We use `adjclose` (adjusted close) instead of `close` because:\n",
        "- **Stock splits**: A 2-for-1 split halves the raw close price overnight. Adjusted close adjusts all historical prices so returns remain accurate.\n",
        "- **Dividends**: When a stock pays dividends, the price drops by approximately that amount. Adjusted close factors this in for total return.\n",
        "- **Accurate backtesting**: Using raw close prices gives misleading historical returns. Adjusted close gives true investment returns.\n",
        "\n",
        "**Mathematical Note: Log Returns vs Simple Returns**\n",
        "\n",
        "For small changes (<10%), log return ‚âà simple percentage return:\n",
        "- Simple return: (P_t - P_t-1) / P_t-1 = 0.05 means +5%\n",
        "- Log return: ln(P_t / P_t-1) = 0.0488 ‚âà 5%\n",
        "\n",
        "For consistency and because our daily returns are typically < 5%, we use log returns for features but simple percentage returns for momentum (cumulative) calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question**: Your model achieves 85% training R¬≤ but only 8% validation R¬≤ on stock returns. You discover you forgot to sort by ['ticker', 'date'] before calculating lagged features. What likely happened?\n",
        ">\n",
        "> A. Features mixed data across stocks and time periods, causing severe data leakage and overfitting\n",
        ">\n",
        "> B. Pandas groupby operations failed silently, producing NaN values that corrupted your training process\n",
        ">\n",
        "> C. The model trained slower due to cache misses, but predictions remain mathematically identical\n",
        ">\n",
        "> D. Only the first ticker's features are corrupted; other tickers processed correctly sequentially\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: A**\n",
        "\n",
        "**Explanation:**\n",
        "- **A is TRUE**: Without sorting, shift(1) gives you the previous ROW, not the previous DAY. Features like \"yesterday's return\" could actually be from 6 months in the future or from a completely different stock. This creates massive data leakage - your model is \"learning\" from randomly mixed future/past data and different stocks, which explains the 85% training R¬≤ (it's memorizing corrupted patterns). But on properly ordered validation data, these patterns don't exist, causing the collapse to 8%.\n",
        "\n",
        "- **B is FALSE**: Pandas groupby doesn't require pre-sorted data and won't fail silently. It will compute the operations, just with wrong results because the temporal order is scrambled. You'd get numerical values, not NaN errors.\n",
        "\n",
        "- **C is FALSE**: The predictions are fundamentally WRONG, not just slower. Your \"lag_1d\" feature contains random data from the wrong time periods and wrong stocks. This isn't a performance issue - it's a correctness catastrophe.\n",
        "\n",
        "- **D is FALSE**: ALL tickers are affected equally. There's no mechanism where the first ticker suffers while others are fine. Each ticker's time series is scrambled if you don't sort properly first.\n",
        "\n",
        "**Key Insight**: For time-series features, **sort by [entity, time] BEFORE any shift/rolling operations** - this isn't optional, it's mandatory for correctness!\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "KMhEQbQfFKT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atXjdJODFKT1"
      },
      "outputs": [],
      "source": [
        "def engineer_features(df):\n",
        "    \"\"\"\n",
        "    Engineer financial features from OHLC data.\n",
        "    All features are percentage-based and scale-invariant for comparing across tickers!\n",
        "\n",
        "    Important: Uses 'adjclose' (adjusted close) instead of 'close' to account for\n",
        "    stock splits and dividends, ensuring accurate historical returns.\n",
        "\n",
        "    Key Design Principles:\n",
        "    - Use meaningful lag intervals: 1, 5, 10, 20 trading days (1 day, 1 week, 2 weeks, 4 weeks)\n",
        "    - All features in percentage terms for apple-to-apple comparison\n",
        "    - Volatility, momentum, and moving averages normalized by price\n",
        "    \"\"\"\n",
        "    # ‚ö†Ô∏è CRITICAL: Sort by ticker and date to ensure proper time series order\n",
        "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
        "\n",
        "    # Group by ticker for time-series operations\n",
        "    grouped = df.groupby('ticker')\n",
        "\n",
        "    # ========================================\n",
        "    # 1. DAILY RETURN (base feature)\n",
        "    # ========================================\n",
        "    # Daily percentage return (log return ‚âà percentage for small changes)\n",
        "    df['return_1d'] = grouped['adjclose'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "\n",
        "    # ========================================\n",
        "    # 2. LAGGED RETURNS (past performance at key intervals)\n",
        "    # ========================================\n",
        "    # Meaningful trading intervals: 1 day, 1 week, 2 weeks, 4 weeks\n",
        "    df['return_lag_1d'] = grouped['return_1d'].shift(1)      # Yesterday\n",
        "    df['return_lag_5d'] = grouped['return_1d'].shift(5)      # 1 week ago\n",
        "    df['return_lag_10d'] = grouped['return_1d'].shift(10)    # 2 weeks ago\n",
        "    df['return_lag_20d'] = grouped['return_1d'].shift(20)    # 4 weeks ago\n",
        "\n",
        "    # Cumulative returns over these periods (percentage change)\n",
        "    df['return_cum_5d'] = grouped['adjclose'].transform(lambda x: (x / x.shift(5)) - 1)   # 1-week return\n",
        "    df['return_cum_10d'] = grouped['adjclose'].transform(lambda x: (x / x.shift(10)) - 1) # 2-week return\n",
        "    df['return_cum_20d'] = grouped['adjclose'].transform(lambda x: (x / x.shift(20)) - 1) # 4-week return\n",
        "\n",
        "    # ========================================\n",
        "    # 3. VOLATILITY (risk measure in percentage)\n",
        "    # ========================================\n",
        "    # 5-day rolling standard deviation of returns (already in percentage terms)\n",
        "    df['volatility_5d'] = grouped['return_1d'].transform(lambda x: x.shift(1).rolling(5).std())\n",
        "\n",
        "    # 20-day rolling volatility (monthly volatility)\n",
        "    df['volatility_20d'] = grouped['return_1d'].transform(lambda x: x.shift(1).rolling(20).std())\n",
        "\n",
        "    # Intraday range as percentage of adjusted close (high-low spread)\n",
        "    df['range_hl_pct'] = (df['high'] - df['low']) / df['adjclose']\n",
        "\n",
        "    # ========================================\n",
        "    # 4. MOMENTUM INDICATORS (percentage-based)\n",
        "    # ========================================\n",
        "    # Already calculated above as return_cum_5d, return_cum_10d, return_cum_20d\n",
        "    # These represent momentum: positive = upward trend, negative = downward trend\n",
        "\n",
        "    # ========================================\n",
        "    # 5. MOVING AVERAGES (trend indicators in percentage)\n",
        "    # ========================================\n",
        "    # Distance from 5-day MA (short-term trend)\n",
        "    df['ma_5d'] = grouped['adjclose'].transform(lambda x: x.rolling(5).mean())\n",
        "    df['dist_from_ma5_pct'] = (df['adjclose'] - df['ma_5d']) / df['adjclose']\n",
        "\n",
        "    # Distance from 20-day MA (medium-term trend)\n",
        "    df['ma_20d'] = grouped['adjclose'].transform(lambda x: x.rolling(20).mean())\n",
        "    df['dist_from_ma20_pct'] = (df['adjclose'] - df['ma_20d']) / df['adjclose']\n",
        "\n",
        "    # ========================================\n",
        "    # 6. INTRADAY BEHAVIOR (percentage-based)\n",
        "    # ========================================\n",
        "    # Intraday return (open to adjusted close) as percentage\n",
        "    df['return_intraday'] = (df['adjclose'] - df['open']) / df['open']\n",
        "\n",
        "    # Overnight return (previous adjusted close to today's open) as percentage\n",
        "    df['return_overnight'] = grouped.apply(\n",
        "        lambda x: (x['open'] - x['adjclose'].shift(1)) / x['adjclose'].shift(1)\n",
        "    ).reset_index(level=0, drop=True)\n",
        "\n",
        "    # ========================================\n",
        "    # 7. TARGET: 4-WEEK FUTURE RETURN (percentage change)\n",
        "    # ========================================\n",
        "    # This is what we want to predict!\n",
        "    # Percentage change 20 trading days (4 weeks) into the future\n",
        "    df['target_return_4w'] = grouped['adjclose'].transform(lambda x: (x.shift(-20) - x) / x)\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Engineering features...\")\n",
        "df_features = engineer_features(df.copy())\n",
        "print(f\"‚úÖ Created {len(df_features.columns) - len(df.columns)} new features\")\n",
        "print(f\"\\nNew columns: {list(df_features.columns[len(df.columns):])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq8nhuoYFKT1"
      },
      "outputs": [],
      "source": [
        "# Inspect the engineered features\n",
        "print(\"Sample of engineered features:\")\n",
        "display_cols = ['date', 'ticker', 'adjclose', 'return_1d', 'return_lag_1d', 'return_lag_5d',\n",
        "                'return_cum_5d', 'volatility_5d', 'dist_from_ma20_pct', 'target_return_4w']\n",
        "df_features[display_cols].head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCojTmf-FKT1"
      },
      "source": [
        "## Step 4: Visualize Returns vs Prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mC6DrLSFKT1"
      },
      "outputs": [],
      "source": [
        "# Compare distributions: prices vs returns\n",
        "# Pick two stocks with very different price levels\n",
        "cheap_ticker = price_stats.index[0]\n",
        "expensive_ticker = price_stats.index[-1]\n",
        "\n",
        "cheap_df = df_features[df_features['ticker'] == cheap_ticker].dropna()\n",
        "expensive_df = df_features[df_features['ticker'] == expensive_ticker].dropna()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Row 1: Prices\n",
        "axes[0, 0].hist(cheap_df['adjclose'], bins=50, alpha=0.7, edgecolor='black', label=cheap_ticker)\n",
        "axes[0, 0].set_xlabel('Adjusted Close Price (¬£)', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0, 0].set_title(f'{cheap_ticker} Price Distribution', fontsize=14)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].hist(expensive_df['adjclose'], bins=50, alpha=0.7, color='orange', edgecolor='black', label=expensive_ticker)\n",
        "axes[0, 1].set_xlabel('Adjusted Close Price (¬£)', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0, 1].set_title(f'{expensive_ticker} Price Distribution', fontsize=14)\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Row 2: Returns (COMPARABLE!)\n",
        "axes[1, 0].hist(cheap_df['return_1d'].dropna() * 100, bins=50, alpha=0.7, edgecolor='black', label=cheap_ticker)\n",
        "axes[1, 0].set_xlabel('Daily Return (%)', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 0].set_title(f'{cheap_ticker} Return Distribution\\n(Now comparable!)', fontsize=14)\n",
        "axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
        "axes[1, 0].set_xlim(-10, 10)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(expensive_df['return_1d'].dropna() * 100, bins=50, alpha=0.7, color='orange', edgecolor='black', label=expensive_ticker)\n",
        "axes[1, 1].set_xlabel('Daily Return (%)', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 1].set_title(f'{expensive_ticker} Return Distribution\\n(Similar scale!)', fontsize=14)\n",
        "axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
        "axes[1, 1].set_xlim(-10, 10)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüí° Key Insight: Returns put both stocks on the SAME scale!\")\n",
        "print(f\"   {cheap_ticker} return std: {cheap_df['return_1d'].std()*100:.2f}%\")\n",
        "print(f\"   {expensive_ticker} return std: {expensive_df['return_1d'].std()*100:.2f}%\")\n",
        "print(f\"   These are comparable, unlike prices!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beCfiFNFKT1"
      },
      "source": [
        "## Step 5: Prepare Data for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPv2sU38FKT1"
      },
      "outputs": [],
      "source": [
        "# Define feature columns (what we'll use to predict)\n",
        "#\n",
        "# ‚öôÔ∏è FEATURE SELECTION RATIONALE:\n",
        "# We chose these 15 features based on:\n",
        "# 1. Trading relevance: Momentum, volatility, and trends are what traders actually use\n",
        "# 2. Different time horizons: 1/5/10/20 days capture short to medium-term patterns\n",
        "# 3. Avoiding redundancy: Each feature provides unique information\n",
        "# 4. Computational efficiency: Enough features to capture patterns, not so many to overfit\n",
        "#\n",
        "# More features isn't always better! Too many can lead to:\n",
        "# - Overfitting (memorizing noise)\n",
        "# - Multicollinearity (correlated features confuse the model)\n",
        "# - Slower training and prediction\n",
        "\n",
        "feature_cols = [\n",
        "    # Lagged returns at meaningful intervals: 1 day, 1 week, 2 weeks, 4 weeks\n",
        "    'return_lag_1d',\n",
        "    'return_lag_5d',\n",
        "    'return_lag_10d',\n",
        "    'return_lag_20d',\n",
        "\n",
        "    # Cumulative returns (momentum over periods)\n",
        "    'return_cum_5d',    # 1-week momentum\n",
        "    'return_cum_10d',   # 2-week momentum\n",
        "    'return_cum_20d',   # 4-week momentum\n",
        "\n",
        "    # Volatility (risk measures in percentage)\n",
        "    'volatility_5d',    # Short-term volatility\n",
        "    'volatility_20d',   # Medium-term volatility\n",
        "    'range_hl_pct',     # Intraday range\n",
        "\n",
        "    # Moving averages (trend indicators in percentage)\n",
        "    'dist_from_ma5_pct',   # Distance from 5-day MA\n",
        "    'dist_from_ma20_pct',  # Distance from 20-day MA\n",
        "\n",
        "    # Intraday behavior (percentage)\n",
        "    'return_intraday',\n",
        "    'return_overnight'\n",
        "]\n",
        "\n",
        "target_col = 'target_return_4w'\n",
        "\n",
        "# Remove rows with missing values (due to lagging and rolling windows)\n",
        "df_clean = df_features[feature_cols + [target_col, 'date', 'ticker', 'adjclose']].dropna()\n",
        "\n",
        "print(f\"Original dataset: {len(df_features)} rows\")\n",
        "print(f\"After removing NaN: {len(df_clean)} rows ({len(df_clean)/len(df_features)*100:.1f}% retained)\")\n",
        "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
        "print(f\"Target: {target_col}\")\n",
        "print(f\"\\nDate range: {df_clean['date'].min()} to {df_clean['date'].max()}\")\n",
        "\n",
        "print(f\"\\nüí° All features are now percentage-based and comparable across tickers!\")\n",
        "print(f\"   - Using adjclose (not close) to account for splits and dividends\")\n",
        "print(f\"   - Lagged returns: 1, 5, 10, 20 days (not 1,2,3,4,5)\")\n",
        "print(f\"   - Volatility: Standard deviation of returns (already %)\")\n",
        "print(f\"   - Momentum: Cumulative returns over 5/10/20 days (%)\")\n",
        "print(f\"   - Moving averages: Distance as % of current price\")\n",
        "print(f\"   - Target: 4-week percentage change\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß DATA QUALITY: Remove problematic tickers and winsorize extreme returns\n",
        "#\n",
        "# Problem: Penny stocks and data errors cause extreme returns (>10,000%)\n",
        "# Solution: 2-step approach\n",
        "#   1. Remove tickers that are consistently problematic\n",
        "#   2. Winsorize remaining extreme individual returns\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA QUALITY: Removing Outliers and Winsorization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nOriginal data: {len(df_clean):,} rows, {df_clean['ticker'].nunique()} tickers\")\n",
        "\n",
        "# Step 1: Identify and remove problematic tickers\n",
        "extreme_threshold = 1.0  # Returns > 100% or < -90%\n",
        "extreme_mask = df_clean[target_col].abs() > extreme_threshold\n",
        "\n",
        "# Count extreme returns per ticker\n",
        "ticker_extreme_counts = df_clean[extreme_mask].groupby('ticker').size().sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nStep 1: Remove consistently problematic tickers\")\n",
        "print(f\"  Tickers with extreme returns:\")\n",
        "if len(ticker_extreme_counts) > 0:\n",
        "    print(f\"    Total: {len(ticker_extreme_counts)} tickers have at least 1 extreme return\")\n",
        "    print(f\"    Top 5 worst:\")\n",
        "    for ticker, count in ticker_extreme_counts.head().items():\n",
        "        ticker_total = len(df_clean[df_clean['ticker'] == ticker])\n",
        "        print(f\"      {ticker}: {count} extreme returns out of {ticker_total} ({count/ticker_total*100:.1f}%)\")\n",
        "\n",
        "    # Remove tickers with 3+ extreme returns (too problematic)\n",
        "    removal_threshold = 3\n",
        "    problematic_tickers = ticker_extreme_counts[ticker_extreme_counts >= removal_threshold].index\n",
        "    print(f\"\\n  Removing {len(problematic_tickers)} tickers with {removal_threshold}+ extreme returns\")\n",
        "\n",
        "    df_clean = df_clean[~df_clean['ticker'].isin(problematic_tickers)]\n",
        "    print(f\"  After removal: {len(df_clean):,} rows, {df_clean['ticker'].nunique()} tickers\")\n",
        "\n",
        "# Step 2: Winsorize remaining extreme returns at 1st/99th percentiles\n",
        "print(f\"\\nStep 2: Winsorize remaining extreme values\")\n",
        "print(f\"  Before winsorization:\")\n",
        "print(f\"    Min: {df_clean[target_col].min()*100:.2f}%, Max: {df_clean[target_col].max()*100:.2f}%\")\n",
        "print(f\"    1st percentile: {df_clean[target_col].quantile(0.01)*100:.2f}%\")\n",
        "print(f\"    99th percentile: {df_clean[target_col].quantile(0.99)*100:.2f}%\")\n",
        "\n",
        "extreme_count = (df_clean[target_col].abs() > extreme_threshold).sum()\n",
        "print(f\"    Remaining extreme values: {extreme_count} ({extreme_count/len(df_clean)*100:.2f}%)\")\n",
        "\n",
        "# Winsorize at 1st and 99th percentiles\n",
        "lower_bound = df_clean[target_col].quantile(0.01)\n",
        "upper_bound = df_clean[target_col].quantile(0.99)\n",
        "df_clean[target_col] = df_clean[target_col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "print(f\"\\n  After winsorization:\")\n",
        "print(f\"    Min: {df_clean[target_col].min()*100:.2f}%, Max: {df_clean[target_col].max()*100:.2f}%\")\n",
        "print(f\"    Capped at: [{lower_bound*100:.2f}%, {upper_bound*100:.2f}%]\")\n",
        "\n",
        "extreme_count_after = (df_clean[target_col].abs() > extreme_threshold).sum()\n",
        "print(f\"    Remaining extreme values: {extreme_count_after} (should be 0)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Data quality fixed! RMSE will now be meaningful.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "MRPZHKLw4d7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß CRITICAL FIX: Winsorize extreme returns\n",
        "#\n",
        "# Problem: 0.77% of data has extreme returns (>10,000%) due to penny stocks,\n",
        "# data errors, or corporate actions. These outliers destroy RMSE.\n",
        "#\n",
        "# Solution: Cap (winsorize) returns at 99th percentile\n",
        "# This is standard practice in quantitative finance!\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RETURN WINSORIZATION: Handling Extreme Outliers\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check distribution before winsorization\n",
        "print(f\"\\nBefore winsorization:\")\n",
        "print(f\"  Target min: {df_clean[target_col].min()*100:.2f}%\")\n",
        "print(f\"  Target max: {df_clean[target_col].max()*100:.2f}%\")\n",
        "print(f\"  Target 1st percentile: {df_clean[target_col].quantile(0.01)*100:.2f}%\")\n",
        "print(f\"  Target 99th percentile: {df_clean[target_col].quantile(0.99)*100:.2f}%\")\n",
        "\n",
        "# Count extreme values\n",
        "extreme_positive = (df_clean[target_col] > 1.0).sum()  # > 100%\n",
        "extreme_negative = (df_clean[target_col] < -0.9).sum()  # < -90%\n",
        "print(f\"\\n  Extreme positive returns (>100%): {extreme_positive} ({extreme_positive/len(df_clean)*100:.2f}%)\")\n",
        "print(f\"  Extreme negative returns (<-90%): {extreme_negative} ({extreme_negative/len(df_clean)*100:.2f}%)\")\n",
        "\n",
        "# Winsorize at 1st and 99th percentiles\n",
        "lower_bound = df_clean[target_col].quantile(0.01)\n",
        "upper_bound = df_clean[target_col].quantile(0.99)\n",
        "\n",
        "df_clean[target_col] = df_clean[target_col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "print(f\"\\nAfter winsorization:\")\n",
        "print(f\"  Target min: {df_clean[target_col].min()*100:.2f}%\")\n",
        "print(f\"  Target max: {df_clean[target_col].max()*100:.2f}%\")\n",
        "print(f\"  Capped at: [{lower_bound*100:.2f}%, {upper_bound*100:.2f}%]\")\n",
        "\n",
        "print(f\"\\n‚úÖ Extreme outliers winsorized! RMSE will now be meaningful.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "MjgiqR774d7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question**: A colleague suggests random train/test split instead of chronological \"to ensure balanced representation across all time periods.\" What's the critical flaw?\n",
        ">\n",
        "> A. Random splits are fine for time-series; chronological splits waste data from early periods\n",
        ">\n",
        "> B. The model would train on future data to predict the past, causing unrealistic performance estimates\n",
        ">\n",
        "> C. Time-series autocorrelation requires consecutive samples; random splitting breaks these dependencies\n",
        ">\n",
        "> D. Random splits create unequal ticker representation, biasing the model toward high-volume stocks\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation:**\n",
        "- **B is TRUE**: Random splitting creates temporal leakage. You'd have training examples from 2023 predicting test examples from 2021. In real trading, you only know the past, not the future. This violates causality and produces models that look great in testing (R¬≤ = 0.80+) but fail completely in live deployment when they only have past data. Your validation metrics become meaningless.\n",
        "\n",
        "- **A is FALSE**: While random splits do use all time periods, this is exactly the problem! You're letting the model \"peek\" at the future. Real trading requires predicting forward in time, not randomly across time. Chronological splits simulate this correctly.\n",
        "\n",
        "- **C is FALSE**: While autocorrelation exists, we capture it through lagged features explicitly. The main issue isn't about \"breaking dependencies\" - it's about temporal causality violation. Even without autocorrelation, you still can't use future data to predict the past.\n",
        "\n",
        "- **D is FALSE**: Each stock has the same number of trading days (markets operate on a shared calendar). Ticker representation isn't affected by split method. The critical flaw is temporal leakage, not class imbalance.\n",
        "\n",
        "**Key Insight**: For time-series, ALWAYS split chronologically. Random splits = data leakage that produces misleading performance metrics!\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "O5qBPtDnFKT2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz2S7YKXFKT2"
      },
      "source": [
        "## Step 6: Time-Series Split (CRITICAL!)\n",
        "\n",
        "### ‚ö†Ô∏è Why we CAN'T use random train/test split for time series:\n",
        "\n",
        "**Random split = Future data leakage!**\n",
        "\n",
        "Imagine it's January 2022, and you randomly select:\n",
        "- Training: Some data from 2020, 2021, **2022, 2023**\n",
        "- Testing: Some data from 2020, **2021**, 2022\n",
        "\n",
        "Your model is using **future data** (2023) to predict the **past** (2021)! This would never work in real trading.\n",
        "\n",
        "**Correct approach: Time-based split**\n",
        "- Train: 2019-07-01 to 2023-12-31 (4.5 years)\n",
        "- Validation: 2024-01-01 to 2024-12-31 (1 year)\n",
        "- Test: 2025-01-01 onwards (most recent)\n",
        "\n",
        "This simulates real-world deployment: train on past, predict future!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH2h-gU-FKT2"
      },
      "outputs": [],
      "source": [
        "# Time-based split\n",
        "train_end = '2023-12-31'\n",
        "val_end = '2024-12-31'\n",
        "\n",
        "train_df = df_clean[df_clean['date'] <= train_end]\n",
        "val_df = df_clean[(df_clean['date'] > train_end) & (df_clean['date'] <= val_end)]\n",
        "test_df = df_clean[df_clean['date'] > val_end]\n",
        "\n",
        "print(\"Time-Series Split:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training:   {train_df['date'].min()} to {train_df['date'].max()} | {len(train_df):,} rows\")\n",
        "print(f\"Validation: {val_df['date'].min()} to {val_df['date'].max()} | {len(val_df):,} rows\")\n",
        "print(f\"Test:       {test_df['date'].min()} to {test_df['date'].max()} | {len(test_df):,} rows\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract features and targets\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[target_col].values\n",
        "\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df[target_col].values\n",
        "\n",
        "X_test = test_df[feature_cols].values\n",
        "y_test = test_df[target_col].values\n",
        "\n",
        "print(f\"\\nX_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape:   {X_val.shape}\")\n",
        "print(f\"X_test shape:  {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMfeFctUFKT2"
      },
      "source": [
        "## Step 7: Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeRtEghYFKT2"
      },
      "outputs": [],
      "source": [
        "# Standardize features using training data statistics\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling completed!\")\n",
        "print(f\"\\nTraining set feature statistics (after scaling):\")\n",
        "print(f\"  Mean: {X_train_scaled.mean(axis=0).round(6)}\")\n",
        "print(f\"  Std:  {X_train_scaled.std(axis=0).round(6)}\")\n",
        "print(f\"\\n‚úÖ All features now have mean ‚âà 0 and std ‚âà 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ku2sMQBFKT2"
      },
      "source": [
        "## Step 8: Train Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhOIoBNQFKT2"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_val_pred = model.predict(X_val_scaled)\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"\\nModel parameters:\")\n",
        "print(f\"  Intercept: {model.intercept_:.6f}\")\n",
        "print(f\"  Number of coefficients: {len(model.coef_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TLKAJZOFKT2"
      },
      "source": [
        "## Step 9: Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7W0pC4tFKT2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, set_name=\"\"):\n",
        "    \"\"\"\n",
        "    Evaluate regression model with financial metrics.\n",
        "    \"\"\"\n",
        "    # Standard metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Financial metrics (convert log returns to percentage)\n",
        "    error_pct = np.abs(y_true - y_pred) * 100  # Absolute error in %\n",
        "\n",
        "    print(f\"\\n{set_name} Performance:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"  R¬≤ Score:          {r2:.4f}\")\n",
        "    print(f\"  RMSE (log return): {rmse:.4f}\")\n",
        "    print(f\"  MAE (log return):  {mae:.4f}\")\n",
        "    print(f\"\\n  RMSE (%):          {rmse*100:.2f}%\")\n",
        "    print(f\"  MAE (%):           {mae*100:.2f}%\")\n",
        "    print(f\"  Mean Abs Error:    {error_pct.mean():.2f}%\")\n",
        "    print(f\"  Median Abs Error:  {np.median(error_pct):.2f}%\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return {\n",
        "        'r2': r2,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'rmse_pct': rmse * 100,\n",
        "        'mae_pct': mae * 100\n",
        "    }\n",
        "\n",
        "# Evaluate on all sets\n",
        "train_metrics = evaluate_model(y_train, y_train_pred, \"TRAINING\")\n",
        "val_metrics = evaluate_model(y_val, y_val_pred, \"VALIDATION\")\n",
        "test_metrics = evaluate_model(y_test, y_test_pred, \"TEST\")\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<20} {'Train':>12} {'Validation':>12} {'Test':>12}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'R¬≤ Score':<20} {train_metrics['r2']:>12.4f} {val_metrics['r2']:>12.4f} {test_metrics['r2']:>12.4f}\")\n",
        "print(f\"{'RMSE (%)':<20} {train_metrics['rmse_pct']:>12.2f} {val_metrics['rmse_pct']:>12.2f} {test_metrics['rmse_pct']:>12.2f}\")\n",
        "print(f\"{'MAE (%)':<20} {train_metrics['mae_pct']:>12.2f} {val_metrics['mae_pct']:>12.2f} {test_metrics['mae_pct']:>12.2f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç DIAGNOSTIC: Check for extreme outliers causing inflated RMSE\n",
        "print(\"=\"*70)\n",
        "print(\"DIAGNOSTIC: Checking for data quality issues\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check for NaN or Inf values\n",
        "print(f\"\\nTraining predictions:\")\n",
        "print(f\"  Contains NaN: {np.isnan(y_train_pred).any()}\")\n",
        "print(f\"  Contains Inf: {np.isinf(y_train_pred).any()}\")\n",
        "print(f\"  Min: {y_train_pred.min():.4f}\")\n",
        "print(f\"  Max: {y_train_pred.max():.4f}\")\n",
        "\n",
        "print(f\"\\nTraining actuals:\")\n",
        "print(f\"  Contains NaN: {np.isnan(y_train).any()}\")\n",
        "print(f\"  Contains Inf: {np.isinf(y_train).any()}\")\n",
        "print(f\"  Min: {y_train.min():.4f}\")\n",
        "print(f\"  Max: {y_train.max():.4f}\")\n",
        "\n",
        "# Calculate errors\n",
        "train_errors = np.abs(y_train - y_train_pred)\n",
        "print(f\"\\nTraining errors:\")\n",
        "print(f\"  Mean: {train_errors.mean():.4f}\")\n",
        "print(f\"  Median: {np.median(train_errors):.4f}\")\n",
        "print(f\"  95th percentile: {np.percentile(train_errors, 95):.4f}\")\n",
        "print(f\"  99th percentile: {np.percentile(train_errors, 99):.4f}\")\n",
        "print(f\"  Max: {train_errors.max():.4f}\")\n",
        "\n",
        "# Find extreme outliers (> 100% error = 1.0 in decimal)\n",
        "extreme_mask = train_errors > 1.0\n",
        "n_extreme = extreme_mask.sum()\n",
        "print(f\"\\nExtreme outliers (error > 100%):\")\n",
        "print(f\"  Count: {n_extreme} / {len(train_errors)} ({n_extreme/len(train_errors)*100:.2f}%)\")\n",
        "\n",
        "if n_extreme > 0:\n",
        "    print(f\"  These {n_extreme} outliers are inflating your RMSE!\")\n",
        "    print(f\"\\n  Top 5 worst predictions:\")\n",
        "    worst_indices = np.argsort(train_errors)[-5:][::-1]\n",
        "    for i, idx in enumerate(worst_indices, 1):\n",
        "        print(f\"    {i}. Actual: {y_train[idx]*100:>8.2f}%, Predicted: {y_train_pred[idx]*100:>8.2f}%, Error: {train_errors[idx]*100:>8.2f}%\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "u2xBdgiQ4d7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr2MHCc1FKT2"
      },
      "source": [
        "## Step 10: Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YsD1RWjFKT3"
      },
      "outputs": [],
      "source": [
        "# Analyze feature importance (from standardized coefficients)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Coefficient': model.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, edgecolor='black')\n",
        "plt.xlabel('Standardized Coefficient', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Feature Importance for 4-Week Return Prediction', fontsize=14)\n",
        "plt.axvline(x=0, color='black', linewidth=1)\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 Most Important Features:\")\n",
        "print(feature_importance.head())\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   Green = Positive coefficient (higher feature ‚Üí higher future return)\")\n",
        "print(\"   Red   = Negative coefficient (higher feature ‚Üí lower future return)\")\n",
        "print(\"   Magnitude = Strength of relationship\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOiWAQMeFKT3"
      },
      "source": [
        "## Step 11: Visualize Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question**: Your model achieves R¬≤ = 0.03 and Hit Rate = 54% on stock prediction. A stakeholder calls it \"worthless because R¬≤ is below 0.70.\" Your response?\n",
        ">\n",
        "> A. Agree - models below R¬≤ = 0.70 fail to meet quantitative finance industry standards\n",
        ">\n",
        "> B. Disagree - in noisy markets, R¬≤ = 0.03 is normal; 54% Hit Rate beats random and could profit\n",
        ">\n",
        "> C. Agree - low validation R¬≤ versus expected performance indicates severe overfitting problems\n",
        ">\n",
        "> D. Disagree - R¬≤ is meaningless for finance; only Hit Rate matters for trading profitability\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation:**\n",
        "- **B is TRUE**: Financial markets have extremely low signal-to-noise ratios. Professional quant funds often achieve R¬≤ = 0.01-0.10. The critical signal here is Hit Rate = 54% vs 50% random - a 4 percentage point edge. Consistently predicting direction correctly 54% of the time, with proper risk management, position sizing, and diversification, can generate significant profits. Many successful trading strategies operate in the 51-56% hit rate range.\n",
        "\n",
        "- **A is FALSE**: There's no \"R¬≤ = 0.70 industry standard\" for financial prediction. That might apply to physics or engineering, but markets have too much random noise (news, sentiment, macro shocks). R¬≤ = 0.03 means you explain 3% of variance - small but meaningful in finance.\n",
        "\n",
        "- **C is FALSE**: Low R¬≤ isn't evidence of overfitting - it's evidence of noisy data. Overfitting shows as HIGH training R¬≤ with LOW validation R¬≤. Here, both are low, which is expected in finance and suggests the model generalizes appropriately.\n",
        "\n",
        "- **D is FALSE**: Both metrics provide value. R¬≤ tells you magnitude prediction accuracy (useful for portfolio optimization, position sizing). Hit Rate tells you directional accuracy (useful for long/short signals). Use both for complete evaluation.\n",
        "\n",
        "**Key Insight**: In noisy domains, judge models by domain standards. A 4% edge over random (54% vs 50%) is more valuable than high R¬≤ on clean academic datasets!\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "XWivJiCEFKT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2gfo5pXFKT3"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: Predicted vs Actual (Validation Set)\n",
        "axes[0, 0].scatter(y_val * 100, y_val_pred * 100, alpha=0.3, s=10)\n",
        "axes[0, 0].plot([y_val.min()*100, y_val.max()*100], [y_val.min()*100, y_val.max()*100],\n",
        "               'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0, 0].set_xlabel('Actual 4-Week Return (%)', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Predicted 4-Week Return (%)', fontsize=12)\n",
        "axes[0, 0].set_title(f'Validation Set: Predictions vs Actual\\nR¬≤ = {val_metrics[\"r2\"]:.4f}', fontsize=14)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].axhline(0, color='gray', linewidth=0.5)\n",
        "axes[0, 0].axvline(0, color='gray', linewidth=0.5)\n",
        "\n",
        "# Plot 2: Residual Distribution\n",
        "residuals_val = (y_val - y_val_pred) * 100\n",
        "axes[0, 1].hist(residuals_val, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Prediction Error (%)', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0, 1].set_title(f'Distribution of Prediction Errors\\nMean: {residuals_val.mean():.2f}% | Std: {residuals_val.std():.2f}%', fontsize=14)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Error vs Predicted Return\n",
        "axes[1, 0].scatter(y_val_pred * 100, residuals_val, alpha=0.3, s=10)\n",
        "axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "axes[1, 0].set_xlabel('Predicted Return (%)', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Prediction Error (%)', fontsize=12)\n",
        "axes[1, 0].set_title('Residual Plot: Check for Patterns', fontsize=14)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Cumulative Error Over Time\n",
        "val_df_copy = val_df.copy()\n",
        "val_df_copy['predicted'] = y_val_pred\n",
        "val_df_copy['error'] = np.abs(y_val - y_val_pred) * 100\n",
        "error_by_date = val_df_copy.groupby('date')['error'].mean().sort_index()\n",
        "axes[1, 1].plot(error_by_date.index, error_by_date.values, linewidth=1.5)\n",
        "axes[1, 1].axhline(error_by_date.mean(), color='red', linestyle='--', linewidth=2,\n",
        "                  label=f'Mean: {error_by_date.mean():.2f}%')\n",
        "axes[1, 1].set_xlabel('Date', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Mean Absolute Error (%)', fontsize=12)\n",
        "axes[1, 1].set_title('Prediction Error Over Time', fontsize=14)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tMkCWfrFKT3"
      },
      "source": [
        "## Step 12: Directional Accuracy (Trading Perspective)\n",
        "\n",
        "### What is Directional Accuracy?\n",
        "\n",
        "**Directional Accuracy = Hit Rate = Win Rate**\n",
        "\n",
        "In trading terminology, these all mean the same thing:\n",
        "- **What percentage of predictions correctly identified the direction?**\n",
        "- If actual return is +5% and we predicted +2%, that's a **HIT** ‚úÖ (both positive)\n",
        "- If actual return is -3% and we predicted +1%, that's a **MISS** ‚ùå (wrong direction)\n",
        "\n",
        "**Why it matters more than R¬≤ for trading:**\n",
        "\n",
        "1. **Profitability depends on direction**:\n",
        "   - If you predict stock will go UP ‚Üí you BUY\n",
        "   - If you predict stock will go DOWN ‚Üí you SELL (or short)\n",
        "   - The exact magnitude matters less than getting the direction right\n",
        "\n",
        "2. **Baseline is 50%**:\n",
        "   - Random guessing = 50% hit rate (coin flip)\n",
        "   - 55% hit rate = consistently beating random (potentially profitable!)\n",
        "   - 60%+ hit rate = very strong signal (professional quant level)\n",
        "\n",
        "3. **Hit rate + Risk management = Strategy**:\n",
        "   - Even 51% hit rate can be profitable with proper position sizing\n",
        "   - Combined with stop losses and profit targets\n",
        "   - Diversified across many stocks to reduce idiosyncratic risk\n",
        "\n",
        "**Example:**\n",
        "- Our model predicts 4-week returns for 1,000 stock-days\n",
        "- 550 times we correctly predict UP or DOWN\n",
        "- 450 times we get the direction wrong\n",
        "- **Hit rate = 550 / 1,000 = 55%** ‚úÖ Beats random!\n",
        "\n",
        "This is what we calculate below: Can our linear regression model beat a 50% coin flip?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HEdGyGNFKT3"
      },
      "outputs": [],
      "source": [
        "# For trading, direction matters more than exact magnitude!\n",
        "# Can we predict if stock will go UP or DOWN?\n",
        "\n",
        "def directional_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate what percentage of time we correctly predict direction.\n",
        "    Also known as: Hit Rate, Win Rate\n",
        "    \"\"\"\n",
        "    # Convert to up/down signals\n",
        "    actual_direction = (y_true > 0).astype(int)  # 1 if up, 0 if down\n",
        "    pred_direction = (y_pred > 0).astype(int)\n",
        "\n",
        "    accuracy = (actual_direction == pred_direction).mean()\n",
        "\n",
        "    return accuracy, actual_direction, pred_direction\n",
        "\n",
        "# Calculate directional accuracy\n",
        "train_dir_acc, _, _ = directional_accuracy(y_train, y_train_pred)\n",
        "val_dir_acc, val_actual_dir, val_pred_dir = directional_accuracy(y_val, y_val_pred)\n",
        "test_dir_acc, _, _ = directional_accuracy(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DIRECTIONAL ACCURACY / HIT RATE (Can we predict UP vs DOWN?)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training:   {train_dir_acc*100:.2f}%\")\n",
        "print(f\"Validation: {val_dir_acc*100:.2f}%\")\n",
        "print(f\"Test:       {test_dir_acc*100:.2f}%\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBaseline (random guessing / coin flip): 50.00%\")\n",
        "print(f\"Our model beats random: {val_dir_acc > 0.5}\")\n",
        "\n",
        "if val_dir_acc > 0.5:\n",
        "    edge = (val_dir_acc - 0.5) * 100\n",
        "    print(f\"Edge over random: +{edge:.2f} percentage points\")\n",
        "    print(f\"\\nüí° A {edge:.2f}% edge, when consistently applied with proper risk\")\n",
        "    print(f\"   management across many trades, can be highly profitable!\")\n",
        "\n",
        "# Confusion matrix for validation set\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(val_actual_dir, val_pred_dir)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted DOWN', 'Predicted UP'],\n",
        "            yticklabels=['Actual DOWN', 'Actual UP'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.ylabel('Actual Direction', fontsize=12)\n",
        "plt.xlabel('Predicted Direction', fontsize=12)\n",
        "plt.title(f'Confusion Matrix: Direction Prediction\\nHit Rate: {val_dir_acc*100:.2f}%', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report (Direction):\")\n",
        "print(classification_report(val_actual_dir, val_pred_dir,\n",
        "                          target_names=['DOWN', 'UP']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuGErCUgFKT3"
      },
      "source": [
        "## Step 13: Case Study - Individual Stock Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0ZNxIb0FKT3"
      },
      "outputs": [],
      "source": [
        "# Let's examine predictions for specific stocks\n",
        "# Pick 3 stocks: cheap, medium, expensive\n",
        "sample_tickers = [price_stats.index[0], price_stats.index[len(price_stats)//2], price_stats.index[-1]]\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "\n",
        "for idx, ticker in enumerate(sample_tickers):\n",
        "    # Get validation data for this ticker\n",
        "    ticker_val = val_df[val_df['ticker'] == ticker].copy()\n",
        "    ticker_indices = val_df[val_df['ticker'] == ticker].index\n",
        "    ticker_val['predicted'] = y_val_pred[val_df['ticker'] == ticker]\n",
        "\n",
        "    if len(ticker_val) > 0:\n",
        "        # Plot actual vs predicted returns over time\n",
        "        ax = axes[idx]\n",
        "        ax.plot(ticker_val['date'], ticker_val['target_return_4w'] * 100,\n",
        "               'b-', linewidth=2, label='Actual 4W Return', alpha=0.7)\n",
        "        ax.plot(ticker_val['date'], ticker_val['predicted'] * 100,\n",
        "               'r--', linewidth=2, label='Predicted 4W Return', alpha=0.7)\n",
        "        ax.axhline(0, color='black', linewidth=0.5)\n",
        "        ax.set_xlabel('Date', fontsize=11)\n",
        "        ax.set_ylabel('4-Week Return (%)', fontsize=11)\n",
        "\n",
        "        # Calculate R¬≤ for this ticker\n",
        "        ticker_r2 = r2_score(ticker_val['target_return_4w'], ticker_val['predicted'])\n",
        "        avg_price = price_stats.loc[ticker, 'mean']\n",
        "\n",
        "        ax.set_title(f'{ticker} (Avg Price: {avg_price:.0f}p = ¬£{avg_price/100:.2f}) | R¬≤ = {ticker_r2:.4f}', fontsize=13)\n",
        "        ax.legend(loc='upper left')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight: The model works across ALL price levels!\")\n",
        "print(\"   Returns normalize the different scales, enabling a unified model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exp2FpBoFKT3"
      },
      "source": [
        "## Step 14: Understanding Model Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZoVYvHdFKT3"
      },
      "source": [
        "### Why Stock Prediction is Hard (Even with ML)\n",
        "\n",
        "Our model's R¬≤ is likely **low** (< 0.05). This is **normal** for stock prediction! Here's why:\n",
        "\n",
        "#### 1. **Efficient Market Hypothesis (EMH)**\n",
        "- Markets are highly efficient\n",
        "- Past prices already incorporate all historical information\n",
        "- If prediction were easy, everyone would do it and eliminate the opportunity\n",
        "\n",
        "#### 2. **Noise-to-Signal Ratio**\n",
        "Stock returns = Predictable signal + Random noise\n",
        "\n",
        "$$r_t = \\underbrace{\\mu + \\beta_1 x_1 + ... + \\beta_n x_n}_{\\text{Signal (tiny)}} + \\underbrace{\\epsilon_t}_{\\text{Noise (huge)}}$$\n",
        "\n",
        "The noise dominates! Even a \"good\" R¬≤ of 0.01 means your model explains 1% of variance.\n",
        "\n",
        "#### 3. **Linear Regression Limitations**\n",
        "- Assumes linear relationships (markets are non-linear)\n",
        "- Can't capture regime changes (bull markets, crashes, volatility spikes)\n",
        "- Ignores cross-stock interactions\n",
        "- No consideration of external events (news, earnings, macro data)\n",
        "\n",
        "#### 4. **What Professional Quants Do Differently**\n",
        "- Use **hundreds of features** (alternative data, news sentiment, options flow)\n",
        "- Employ **ensemble models** (Random Forests, Gradient Boosting, Neural Networks)\n",
        "- Implement **regime detection** (separate models for different market conditions)\n",
        "- Focus on **directional accuracy** and **risk-adjusted returns**, not R¬≤\n",
        "- Trade **many stocks** to diversify away idiosyncratic risk\n",
        "- Operate at **higher frequencies** (daily, hourly, minute-level)\n",
        "\n",
        "#### 5. **The Real Benchmark**\n",
        "Even a **slight** edge (51% directional accuracy instead of 50%) can be profitable with:\n",
        "- Proper risk management\n",
        "- Position sizing\n",
        "- Transaction cost optimization\n",
        "- Portfolio diversification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtVSMPbEFKT8"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### ‚úÖ What We Learned\n",
        "\n",
        "1. **Scale Invariance is Critical**\n",
        "   - Raw prices vary ~44x across stocks (68p to 3,000p, or ¬£0.68 to ¬£30)\n",
        "   - Returns normalize this: both cheap and expensive stocks have ~2% daily volatility\n",
        "   - Unified model trains on all stocks equally\n",
        "   - **Important**: FTSE 100 prices are quoted in GBX (pence), not GBP (pounds)\n",
        "\n",
        "2. **Time-Series Require Special Handling**\n",
        "   - Never use random train/test splits (causes future data leakage)\n",
        "   - Always split chronologically: train on past, test on future\n",
        "   - Lagged features create dependencies that must be respected\n",
        "\n",
        "3. **Domain Knowledge Matters**\n",
        "   - Financial features (momentum, volatility, moving averages) are industry standard\n",
        "   - Use meaningful lag intervals: 1, 5, 10, 20 days (not arbitrary 1,2,3,4,5)\n",
        "   - Feature engineering dramatically improves over raw prices\n",
        "   - Understanding the domain helps interpret model limitations\n",
        "\n",
        "4. **Linear Regression for Finance**\n",
        "   - **Strengths**: Fast, interpretable, works across all stocks\n",
        "   - **Weaknesses**: Can't capture non-linearities, market regimes, or external shocks\n",
        "   - **Reality**: Low R¬≤ is expected and doesn't mean failure\n",
        "\n",
        "5. **Prediction vs Trading**\n",
        "   - Directional accuracy matters more than exact values\n",
        "   - Even 51% accuracy can be profitable with proper risk management\n",
        "   - Real trading requires transaction costs, slippage, and position sizing\n",
        "\n",
        "### üöÄ Extensions to Explore\n",
        "\n",
        "1. **Better Models**: Try Ridge/Lasso regression, Random Forests, XGBoost\n",
        "2. **More Features**: Add sector indicators, market-wide features, volatility indices\n",
        "3. **Different Targets**: Predict volatility, drawdowns, or classification (up/down)\n",
        "4. **Walk-Forward Analysis**: Retrain model periodically to adapt to changing markets\n",
        "5. **Portfolio Optimization**: Use predictions to construct long/short portfolios\n",
        "\n",
        "### üìö Key Concepts Applied\n",
        "\n",
        "| Concept | How We Used It |\n",
        "|---------|---------------|\n",
        "| **Linear Regression** | Unified model across 100 stocks |\n",
        "| **Feature Engineering** | Percentage-based returns, lagged features (1,5,10,20 days), momentum, volatility |\n",
        "| **Feature Scaling** | StandardScaler on all features |\n",
        "| **Time-Series Split** | Chronological train/val/test |\n",
        "| **Model Evaluation** | R¬≤, RMSE, MAE, directional accuracy |\n",
        "| **Domain Knowledge** | Financial returns, trading metrics, GBX pricing |\n",
        "\n",
        "### ‚ö†Ô∏è Important Warnings\n",
        "\n",
        "**DO NOT** use this model for real trading without:\n",
        "- Understanding market microstructure (bid-ask spreads, slippage)\n",
        "- Implementing proper risk management (stop losses, position sizing)\n",
        "- Accounting for transaction costs (commissions, taxes)\n",
        "- Testing on more comprehensive data (multiple market cycles)\n",
        "- Consulting with financial professionals\n",
        "\n",
        "**This is for educational purposes only!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8YWSfogFKT8"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this case study, we tackled a real-world financial ML problem:\n",
        "\n",
        "1. ‚úÖ Identified the **scale problem** with stock prices\n",
        "2. ‚úÖ Solved it using **percentage returns** instead of absolute prices\n",
        "3. ‚úÖ Engineered **domain-specific features** (momentum, volatility, moving averages)\n",
        "4. ‚úÖ Properly handled **time-series data** (chronological splits, no lookahead bias)\n",
        "5. ‚úÖ Built a **unified linear regression model** for 100 different stocks\n",
        "6. ‚úÖ Evaluated with **financial metrics** (directional accuracy, risk-adjusted performance)\n",
        "7. ‚úÖ Understood **model limitations** and real-world challenges\n",
        "\n",
        "**Most importantly**: We learned that **feature engineering and domain knowledge matter more than algorithm complexity** in many real-world problems!\n",
        "\n",
        "Linear Regression, while simple, taught us:\n",
        "- The importance of proper data transformation\n",
        "- How to handle multi-entity datasets\n",
        "- Why time-series require special treatment\n",
        "- That \"low R¬≤\" doesn't mean \"bad model\" in noisy domains\n",
        "\n",
        "These lessons apply far beyond stock prediction - to demand forecasting, sensor data, medical time series, and more!\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ What's Next? Extensions & Advanced Topics\n",
        "\n",
        "### üìà Immediate Improvements (Low-Hanging Fruit)\n",
        "\n",
        "1. **Regularization** (Ridge/Lasso)\n",
        "   ```python\n",
        "   from sklearn.linear_model import Ridge, Lasso\n",
        "   # Prevents overfitting by penalizing large coefficients\n",
        "   ridge_model = Ridge(alpha=1.0)  # L2 regularization\n",
        "   lasso_model = Lasso(alpha=0.1)  # L1 regularization + feature selection\n",
        "   ```\n",
        "   **Why**: Handles multicollinearity and reduces overfitting\n",
        "\n",
        "2. **Polynomial Features**\n",
        "   ```python\n",
        "   from sklearn.preprocessing import PolynomialFeatures\n",
        "   # Capture non-linear relationships\n",
        "   poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "   X_poly = poly.fit_transform(X_train_scaled)\n",
        "   ```\n",
        "   **Why**: Markets aren't perfectly linear!\n",
        "\n",
        "3. **Cross-Validation** (Time-Series Specific)\n",
        "   ```python\n",
        "   from sklearn.model_selection import TimeSeriesSplit\n",
        "   # Proper validation for time-series data\n",
        "   tscv = TimeSeriesSplit(n_splits=5)\n",
        "   for train_idx, val_idx in tscv.split(X):\n",
        "       # Train on past, validate on future (multiple times)\n",
        "   ```\n",
        "   **Why**: More robust performance estimates\n",
        "\n",
        "### üéØ Advanced Modeling\n",
        "\n",
        "4. **Ensemble Methods**\n",
        "   - **Random Forest**: Captures non-linearities, feature interactions\n",
        "   - **XGBoost/LightGBM**: State-of-the-art for tabular data\n",
        "   - **Stacking**: Combine multiple model predictions\n",
        "\n",
        "5. **Feature Engineering 2.0**\n",
        "   - Sector indicators (one-hot encode industry)\n",
        "   - Market-wide features (VIX volatility index, market returns)\n",
        "   - Cross-stock features (correlation with sector, relative strength)\n",
        "   - Fundamental data (P/E ratios, earnings)\n",
        "   - Alternative data (news sentiment, social media)\n",
        "\n",
        "6. **Different Prediction Targets**\n",
        "   - **Classification**: UP/DOWN binary (better for trading signals)\n",
        "   - **Volatility prediction**: Predict risk, not returns\n",
        "   - **Probability distributions**: Quantile regression\n",
        "\n",
        "### üî¨ Production-Ready Enhancements\n",
        "\n",
        "7. **Walk-Forward Analysis**\n",
        "   ```\n",
        "   Train on 2019-2020 ‚Üí Test on 2021\n",
        "   Train on 2019-2021 ‚Üí Test on 2022\n",
        "   Train on 2019-2022 ‚Üí Test on 2023\n",
        "   ...\n",
        "   ```\n",
        "   **Why**: Markets change! Retrain periodically\n",
        "\n",
        "8. **Transaction Cost Modeling**\n",
        "   - Bid-ask spreads\n",
        "   - Commissions\n",
        "   - Market impact (your trades move prices!)\n",
        "   - Realistic profit/loss calculation\n",
        "\n",
        "9. **Risk Management**\n",
        "   - Position sizing (Kelly criterion)\n",
        "   - Stop losses\n",
        "   - Portfolio diversification\n",
        "   - Max drawdown constraints\n",
        "\n",
        "### üí° Research Directions\n",
        "\n",
        "10. **Regime Detection**\n",
        "    - Separate models for bull/bear/sideways markets\n",
        "    - Use Hidden Markov Models or changepoint detection\n",
        "    - Adapt strategy to market conditions\n",
        "\n",
        "11. **Meta-Labeling**\n",
        "    - First model: Predict magnitude\n",
        "    - Second model: Predict if first model is confident enough to trade\n",
        "    - Reduces false positives\n",
        "\n",
        "12. **Portfolio Optimization**\n",
        "    - Use predictions to construct optimal portfolios\n",
        "    - Mean-variance optimization\n",
        "    - Risk parity\n",
        "    - Long/short strategies\n",
        "\n",
        "### üìö Recommended Learning Path\n",
        "\n",
        "**If you're new to finance ML:**\n",
        "1. Study this notebook thoroughly\n",
        "2. Try Ridge/Lasso regularization\n",
        "3. Experiment with different features\n",
        "4. Read \"Advances in Financial Machine Learning\" by Marcos L√≥pez de Prado\n",
        "\n",
        "**If you're experienced:**\n",
        "1. Implement walk-forward validation\n",
        "2. Build ensemble models\n",
        "3. Add transaction costs and risk management\n",
        "4. Research regime detection\n",
        "\n",
        "### ‚ö†Ô∏è Final Reminder\n",
        "\n",
        "Remember: This is **educational**. Real quantitative trading requires:\n",
        "- Deep understanding of markets and microstructure\n",
        "- Rigorous backtesting with realistic assumptions\n",
        "- Risk management systems\n",
        "- Regulatory compliance\n",
        "- Continuous monitoring and adaptation\n",
        "\n",
        "**But** the skills you learned here - domain knowledge, feature engineering, proper evaluation - apply to *any* time-series prediction problem!\n",
        "\n",
        "---\n",
        "\n",
        "**Want to explore more?**\n",
        "- Check other case studies in this repository\n",
        "- Try applying these techniques to your own time-series data\n",
        "- Join online communities (Quantitative Finance Stack Exchange, r/algotrading)\n",
        "\n",
        "**Happy modeling!** üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
