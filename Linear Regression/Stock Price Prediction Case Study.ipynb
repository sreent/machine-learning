{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Linear%20Regression/Stock%20Price%20Prediction%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Stock Price Prediction with Linear Regression\n",
    "\n",
    "## The Challenge: Building a Unified Model Across Different Stocks\n",
    "\n",
    "**Business Problem:**  \n",
    "We want to predict stock prices **4 weeks ahead** for FTSE 100 companies using historical data. However, we face a critical challenge:\n",
    "\n",
    "- **Stock AAF.L** trades around **Â£70**\n",
    "- **Stock WTB.L** trades around **Â£3,000**\n",
    "\n",
    "**Why is this a problem?**\n",
    "\n",
    "If we naively use raw prices as both features and targets:\n",
    "- A Â£5 prediction error on AAF.L (Â£70) = **7% error** ðŸ˜±\n",
    "- A Â£5 prediction error on WTB.L (Â£3,000) = **0.17% error** âœ…\n",
    "\n",
    "The model would optimize for expensive stocks and perform terribly on cheaper ones!\n",
    "\n",
    "**The Solution: Think Like a Quant Trader**\n",
    "\n",
    "Professional traders don't care about absolute price movements - they care about **percentage returns**:\n",
    "\n",
    "$$\\text{Return} = \\frac{\\text{Price}_{today} - \\text{Price}_{yesterday}}{\\text{Price}_{yesterday}}$$\n",
    "\n",
    "- A **5% gain** is meaningful whether on a Â£70 or Â£3,000 stock\n",
    "- Returns are **scale-invariant** - perfect for a unified model!\n",
    "- Returns are **additive** over time (log returns)\n",
    "\n",
    "**What We'll Build:**\n",
    "1. Load ~580K rows of FTSE 100 daily OHLC data (2019-2025)\n",
    "2. Engineer financial features: returns, volatility, momentum, moving averages\n",
    "3. Create lagged features (past returns predict future returns)\n",
    "4. Handle time-series data properly (no future data leakage!)\n",
    "5. Build a unified linear regression model predicting **4-week returns**\n",
    "6. Evaluate performance and understand limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this case study, you will:\n",
    "\n",
    "âœ… Understand why **returns** (not prices) are used in financial ML  \n",
    "âœ… Learn proper **time-series cross-validation** to avoid lookahead bias  \n",
    "âœ… Engineer **domain-specific features** for stock prediction  \n",
    "âœ… Handle **multiple entities** (100 stocks) in a unified model  \n",
    "âœ… Interpret model performance in financial context  \n",
    "âœ… Understand the **challenges and limitations** of linear models for finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FTSE 100 dataset\n",
    "df = pd.read_csv('ftse100.csv', parse_dates=['date'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Number of unique tickers: {df['ticker'].nunique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploratory Data Analysis - The Price Scale Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price ranges across different stocks\n",
    "price_stats = df.groupby('ticker')['close'].agg(['min', 'mean', 'max', 'std']).sort_values('mean')\n",
    "\n",
    "print(\"Price statistics by ticker (sorted by mean price):\")\n",
    "print(\"\\nCheapest stocks:\")\n",
    "print(price_stats.head(5))\n",
    "print(\"\\nMost expensive stocks:\")\n",
    "print(price_stats.tail(5))\n",
    "\n",
    "# Visualize the problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Price range distribution\n",
    "axes[0].hist(price_stats['mean'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Mean Close Price (Â£)', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Stocks', fontsize=12)\n",
    "axes[0].set_title('Distribution of Average Stock Prices\\n(Why we can\\'t use raw prices!)', fontsize=14)\n",
    "axes[0].axvline(price_stats['mean'].median(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Median: Â£{price_stats[\"mean\"].median():.2f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Example time series for cheap vs expensive stocks\n",
    "cheap_stock = price_stats.index[0]  # Cheapest\n",
    "expensive_stock = price_stats.index[-1]  # Most expensive\n",
    "\n",
    "cheap_data = df[df['ticker'] == cheap_stock].set_index('date')['close']\n",
    "expensive_data = df[df['ticker'] == expensive_stock].set_index('date')['close']\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2_twin = ax2.twinx()\n",
    "\n",
    "ax2.plot(cheap_data.index, cheap_data.values, 'b-', linewidth=1.5, label=f'{cheap_stock} (Cheap)', alpha=0.7)\n",
    "ax2_twin.plot(expensive_data.index, expensive_data.values, 'r-', linewidth=1.5, label=f'{expensive_stock} (Expensive)', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel(f'{cheap_stock} Price (Â£)', color='b', fontsize=12)\n",
    "ax2_twin.set_ylabel(f'{expensive_stock} Price (Â£)', color='r', fontsize=12)\n",
    "ax2.set_title('Different Price Scales = Need Returns!', fontsize=14)\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='r')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insight: Prices range from Â£{price_stats['mean'].min():.2f} to Â£{price_stats['mean'].max():.2f}\")\n",
    "print(f\"   This {price_stats['mean'].max() / price_stats['mean'].min():.0f}x difference makes raw prices unusable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering - The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Financial Returns\n",
    "\n",
    "**Simple Return (what traders report):**\n",
    "$$r_t = \\frac{P_t - P_{t-1}}{P_{t-1}} = \\frac{P_t}{P_{t-1}} - 1$$\n",
    "\n",
    "**Log Return (what quants use):**\n",
    "$$r_t = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right) = \\ln(P_t) - \\ln(P_{t-1})$$\n",
    "\n",
    "**Why log returns?**\n",
    "1. **Time-additivity**: Total return over n periods = sum of daily log returns\n",
    "2. **Symmetry**: +10% gain and -10% loss have similar magnitudes in log space\n",
    "3. **Statistical properties**: More normally distributed (better for linear models)\n",
    "4. **Numerical stability**: No division by potentially small numbers\n",
    "\n",
    "For small returns (< 10%), they're approximately equal: $\\ln(1 + r) \\approx r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer financial features from OHLC data.\n",
    "    All features are designed to be scale-invariant!\n",
    "    \"\"\"\n",
    "    # Sort by ticker and date to ensure proper time series order\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Group by ticker for time-series operations\n",
    "    grouped = df.groupby('ticker')\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. RETURNS (our main features and target)\n",
    "    # ========================================\n",
    "    # Daily log return (today's feature)\n",
    "    df['return_1d'] = grouped['close'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    \n",
    "    # Intraday return (open to close)\n",
    "    df['return_intraday'] = np.log(df['close'] / df['open'])\n",
    "    \n",
    "    # Overnight return (previous close to today's open)\n",
    "    df['return_overnight'] = grouped.apply(lambda x: np.log(x['open'] / x['close'].shift(1))).reset_index(level=0, drop=True)\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. LAGGED RETURNS (past performance)\n",
    "    # ========================================\n",
    "    # Past 5 days of returns\n",
    "    for lag in [1, 2, 3, 4, 5]:\n",
    "        df[f'return_lag_{lag}d'] = grouped['return_1d'].shift(lag)\n",
    "    \n",
    "    # Past week average return\n",
    "    df['return_mean_5d'] = grouped['return_1d'].transform(lambda x: x.shift(1).rolling(5).mean())\n",
    "    \n",
    "    # Past 4-week return (starting point for our prediction)\n",
    "    df['return_4w'] = grouped['close'].transform(lambda x: np.log(x / x.shift(20)))\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. VOLATILITY (risk measure)\n",
    "    # ========================================\n",
    "    # 5-day rolling standard deviation of returns\n",
    "    df['volatility_5d'] = grouped['return_1d'].transform(lambda x: x.shift(1).rolling(5).std())\n",
    "    \n",
    "    # Intraday range (high-low volatility proxy)\n",
    "    df['range_hl'] = (df['high'] - df['low']) / df['close']\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. MOMENTUM INDICATORS\n",
    "    # ========================================\n",
    "    # 5-day momentum (comparing to 5 days ago)\n",
    "    df['momentum_5d'] = grouped['close'].transform(lambda x: (x / x.shift(5)) - 1)\n",
    "    \n",
    "    # 10-day momentum\n",
    "    df['momentum_10d'] = grouped['close'].transform(lambda x: (x / x.shift(10)) - 1)\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. MOVING AVERAGES (trend indicators)\n",
    "    # ========================================\n",
    "    # Distance from 20-day moving average (normalized)\n",
    "    df['ma_20d'] = grouped['close'].transform(lambda x: x.rolling(20).mean())\n",
    "    df['dist_from_ma20'] = (df['close'] - df['ma_20d']) / df['ma_20d']\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. TARGET: 4-WEEK FUTURE RETURN\n",
    "    # ========================================\n",
    "    # This is what we want to predict!\n",
    "    # Shift by -20 trading days (4 weeks * 5 days/week)\n",
    "    df['target_return_4w'] = grouped['close'].transform(lambda x: np.log(x.shift(-20) / x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Engineering features...\")\n",
    "df_features = engineer_features(df.copy())\n",
    "print(f\"âœ… Created {len(df_features.columns) - len(df.columns)} new features\")\n",
    "print(f\"\\nNew columns: {list(df_features.columns[len(df.columns):])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the engineered features\n",
    "print(\"Sample of engineered features:\")\n",
    "display_cols = ['date', 'ticker', 'close', 'return_1d', 'return_lag_1d', 'volatility_5d', \n",
    "                'momentum_5d', 'dist_from_ma20', 'target_return_4w']\n",
    "df_features[display_cols].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Returns vs Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions: prices vs returns\n",
    "# Pick two stocks with very different price levels\n",
    "cheap_ticker = price_stats.index[0]\n",
    "expensive_ticker = price_stats.index[-1]\n",
    "\n",
    "cheap_df = df_features[df_features['ticker'] == cheap_ticker].dropna()\n",
    "expensive_df = df_features[df_features['ticker'] == expensive_ticker].dropna()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Row 1: Prices\n",
    "axes[0, 0].hist(cheap_df['close'], bins=50, alpha=0.7, edgecolor='black', label=cheap_ticker)\n",
    "axes[0, 0].set_xlabel('Close Price (Â£)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title(f'{cheap_ticker} Price Distribution', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(expensive_df['close'], bins=50, alpha=0.7, color='orange', edgecolor='black', label=expensive_ticker)\n",
    "axes[0, 1].set_xlabel('Close Price (Â£)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title(f'{expensive_ticker} Price Distribution', fontsize=14)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Returns (COMPARABLE!)\n",
    "axes[1, 0].hist(cheap_df['return_1d'].dropna() * 100, bins=50, alpha=0.7, edgecolor='black', label=cheap_ticker)\n",
    "axes[1, 0].set_xlabel('Daily Return (%)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title(f'{cheap_ticker} Return Distribution\\n(Now comparable!)', fontsize=14)\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlim(-10, 10)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(expensive_df['return_1d'].dropna() * 100, bins=50, alpha=0.7, color='orange', edgecolor='black', label=expensive_ticker)\n",
    "axes[1, 1].set_xlabel('Daily Return (%)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title(f'{expensive_ticker} Return Distribution\\n(Similar scale!)', fontsize=14)\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlim(-10, 10)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insight: Returns put both stocks on the SAME scale!\")\n",
    "print(f\"   {cheap_ticker} return std: {cheap_df['return_1d'].std()*100:.2f}%\")\n",
    "print(f\"   {expensive_ticker} return std: {expensive_df['return_1d'].std()*100:.2f}%\")\n",
    "print(f\"   These are comparable, unlike prices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (what we'll use to predict)\n",
    "feature_cols = [\n",
    "    # Lagged returns\n",
    "    'return_lag_1d', 'return_lag_2d', 'return_lag_3d', 'return_lag_4d', 'return_lag_5d',\n",
    "    'return_mean_5d',\n",
    "    \n",
    "    # Volatility\n",
    "    'volatility_5d',\n",
    "    'range_hl',\n",
    "    \n",
    "    # Momentum\n",
    "    'momentum_5d',\n",
    "    'momentum_10d',\n",
    "    \n",
    "    # Moving averages\n",
    "    'dist_from_ma20',\n",
    "    \n",
    "    # Intraday behavior\n",
    "    'return_intraday',\n",
    "    'return_overnight'\n",
    "]\n",
    "\n",
    "target_col = 'target_return_4w'\n",
    "\n",
    "# Remove rows with missing values (due to lagging and rolling windows)\n",
    "df_clean = df_features[feature_cols + [target_col, 'date', 'ticker', 'close']].dropna()\n",
    "\n",
    "print(f\"Original dataset: {len(df_features)} rows\")\n",
    "print(f\"After removing NaN: {len(df_clean)} rows ({len(df_clean)/len(df_features)*100:.1f}% retained)\")\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"\\nDate range: {df_clean['date'].min()} to {df_clean['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Time-Series Split (CRITICAL!)\n",
    "\n",
    "### âš ï¸ Why we CAN'T use random train/test split for time series:\n",
    "\n",
    "**Random split = Future data leakage!**\n",
    "\n",
    "Imagine it's January 2022, and you randomly select:\n",
    "- Training: Some data from 2020, 2021, **2022, 2023**\n",
    "- Testing: Some data from 2020, **2021**, 2022\n",
    "\n",
    "Your model is using **future data** (2023) to predict the **past** (2021)! This would never work in real trading.\n",
    "\n",
    "**Correct approach: Time-based split**\n",
    "- Train: 2019-07-01 to 2023-12-31 (4.5 years)\n",
    "- Validation: 2024-01-01 to 2024-12-31 (1 year)\n",
    "- Test: 2025-01-01 onwards (most recent)\n",
    "\n",
    "This simulates real-world deployment: train on past, predict future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split\n",
    "train_end = '2023-12-31'\n",
    "val_end = '2024-12-31'\n",
    "\n",
    "train_df = df_clean[df_clean['date'] <= train_end]\n",
    "val_df = df_clean[(df_clean['date'] > train_end) & (df_clean['date'] <= val_end)]\n",
    "test_df = df_clean[df_clean['date'] > val_end]\n",
    "\n",
    "print(\"Time-Series Split:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training:   {train_df['date'].min()} to {train_df['date'].max()} | {len(train_df):,} rows\")\n",
    "print(f\"Validation: {val_df['date'].min()} to {val_df['date'].max()} | {len(val_df):,} rows\")\n",
    "print(f\"Test:       {test_df['date'].min()} to {test_df['date'].max()} | {len(test_df):,} rows\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract features and targets\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[target_col].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df[target_col].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape:   {X_val.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features using training data statistics\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nTraining set feature statistics (after scaling):\")\n",
    "print(f\"  Mean: {X_train_scaled.mean(axis=0).round(6)}\")\n",
    "print(f\"  Std:  {X_train_scaled.std(axis=0).round(6)}\")\n",
    "print(f\"\\nâœ… All features now have mean â‰ˆ 0 and std â‰ˆ 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Intercept: {model.intercept_:.6f}\")\n",
    "print(f\"  Number of coefficients: {len(model.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, set_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate regression model with financial metrics.\n",
    "    \"\"\"\n",
    "    # Standard metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Financial metrics (convert log returns to percentage)\n",
    "    error_pct = np.abs(y_true - y_pred) * 100  # Absolute error in %\n",
    "    \n",
    "    print(f\"\\n{set_name} Performance:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"  RÂ² Score:          {r2:.4f}\")\n",
    "    print(f\"  RMSE (log return): {rmse:.4f}\")\n",
    "    print(f\"  MAE (log return):  {mae:.4f}\")\n",
    "    print(f\"\\n  RMSE (%):          {rmse*100:.2f}%\")\n",
    "    print(f\"  MAE (%):           {mae*100:.2f}%\")\n",
    "    print(f\"  Mean Abs Error:    {error_pct.mean():.2f}%\")\n",
    "    print(f\"  Median Abs Error:  {np.median(error_pct):.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'rmse_pct': rmse * 100,\n",
    "        'mae_pct': mae * 100\n",
    "    }\n",
    "\n",
    "# Evaluate on all sets\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, \"TRAINING\")\n",
    "val_metrics = evaluate_model(y_val, y_val_pred, \"VALIDATION\")\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, \"TEST\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<20} {'Train':>12} {'Validation':>12} {'Test':>12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'RÂ² Score':<20} {train_metrics['r2']:>12.4f} {val_metrics['r2']:>12.4f} {test_metrics['r2']:>12.4f}\")\n",
    "print(f\"{'RMSE (%)':<20} {train_metrics['rmse_pct']:>12.2f} {val_metrics['rmse_pct']:>12.2f} {test_metrics['rmse_pct']:>12.2f}\")\n",
    "print(f\"{'MAE (%)':<20} {train_metrics['mae_pct']:>12.2f} {val_metrics['mae_pct']:>12.2f} {test_metrics['mae_pct']:>12.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance (from standardized coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, edgecolor='black')\n",
    "plt.xlabel('Standardized Coefficient', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance for 4-Week Return Prediction', fontsize=14)\n",
    "plt.axvline(x=0, color='black', linewidth=1)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   Green = Positive coefficient (higher feature â†’ higher future return)\")\n",
    "print(\"   Red   = Negative coefficient (higher feature â†’ lower future return)\")\n",
    "print(\"   Magnitude = Strength of relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Predicted vs Actual (Validation Set)\n",
    "axes[0, 0].scatter(y_val * 100, y_val_pred * 100, alpha=0.3, s=10)\n",
    "axes[0, 0].plot([y_val.min()*100, y_val.max()*100], [y_val.min()*100, y_val.max()*100], \n",
    "               'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual 4-Week Return (%)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Predicted 4-Week Return (%)', fontsize=12)\n",
    "axes[0, 0].set_title(f'Validation Set: Predictions vs Actual\\nRÂ² = {val_metrics[\"r2\"]:.4f}', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(0, color='gray', linewidth=0.5)\n",
    "axes[0, 0].axvline(0, color='gray', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Residual Distribution\n",
    "residuals_val = (y_val - y_val_pred) * 100\n",
    "axes[0, 1].hist(residuals_val, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Prediction Error (%)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title(f'Distribution of Prediction Errors\\nMean: {residuals_val.mean():.2f}% | Std: {residuals_val.std():.2f}%', fontsize=14)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Error vs Predicted Return\n",
    "axes[1, 0].scatter(y_val_pred * 100, residuals_val, alpha=0.3, s=10)\n",
    "axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Return (%)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Prediction Error (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Residual Plot: Check for Patterns', fontsize=14)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Cumulative Error Over Time\n",
    "val_df_copy = val_df.copy()\n",
    "val_df_copy['predicted'] = y_val_pred\n",
    "val_df_copy['error'] = np.abs(y_val - y_val_pred) * 100\n",
    "error_by_date = val_df_copy.groupby('date')['error'].mean().sort_index()\n",
    "axes[1, 1].plot(error_by_date.index, error_by_date.values, linewidth=1.5)\n",
    "axes[1, 1].axhline(error_by_date.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                  label=f'Mean: {error_by_date.mean():.2f}%')\n",
    "axes[1, 1].set_xlabel('Date', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Mean Absolute Error (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Prediction Error Over Time', fontsize=14)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Directional Accuracy (Trading Perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trading, direction matters more than exact magnitude!\n",
    "# Can we predict if stock will go UP or DOWN?\n",
    "\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate what percentage of time we correctly predict direction.\n",
    "    \"\"\"\n",
    "    # Convert to up/down signals\n",
    "    actual_direction = (y_true > 0).astype(int)  # 1 if up, 0 if down\n",
    "    pred_direction = (y_pred > 0).astype(int)\n",
    "    \n",
    "    accuracy = (actual_direction == pred_direction).mean()\n",
    "    \n",
    "    return accuracy, actual_direction, pred_direction\n",
    "\n",
    "# Calculate directional accuracy\n",
    "train_dir_acc, _, _ = directional_accuracy(y_train, y_train_pred)\n",
    "val_dir_acc, val_actual_dir, val_pred_dir = directional_accuracy(y_val, y_val_pred)\n",
    "test_dir_acc, _, _ = directional_accuracy(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIRECTIONAL ACCURACY (Can we predict UP vs DOWN?)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training:   {train_dir_acc*100:.2f}%\")\n",
    "print(f\"Validation: {val_dir_acc*100:.2f}%\")\n",
    "print(f\"Test:       {test_dir_acc*100:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBaseline (random guessing): 50.00%\")\n",
    "print(f\"Our model beats random: {val_dir_acc > 0.5}\")\n",
    "\n",
    "# Confusion matrix for validation set\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(val_actual_dir, val_pred_dir)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted DOWN', 'Predicted UP'],\n",
    "            yticklabels=['Actual DOWN', 'Actual UP'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.ylabel('Actual Direction', fontsize=12)\n",
    "plt.xlabel('Predicted Direction', fontsize=12)\n",
    "plt.title(f'Confusion Matrix: Direction Prediction\\nAccuracy: {val_dir_acc*100:.2f}%', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report (Direction):\")\n",
    "print(classification_report(val_actual_dir, val_pred_dir, \n",
    "                          target_names=['DOWN', 'UP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Case Study - Individual Stock Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine predictions for specific stocks\n",
    "# Pick 3 stocks: cheap, medium, expensive\n",
    "sample_tickers = [price_stats.index[0], price_stats.index[len(price_stats)//2], price_stats.index[-1]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for idx, ticker in enumerate(sample_tickers):\n",
    "    # Get validation data for this ticker\n",
    "    ticker_val = val_df[val_df['ticker'] == ticker].copy()\n",
    "    ticker_indices = val_df[val_df['ticker'] == ticker].index\n",
    "    ticker_val['predicted'] = y_val_pred[val_df['ticker'] == ticker]\n",
    "    \n",
    "    if len(ticker_val) > 0:\n",
    "        # Plot actual vs predicted returns over time\n",
    "        ax = axes[idx]\n",
    "        ax.plot(ticker_val['date'], ticker_val['target_return_4w'] * 100, \n",
    "               'b-', linewidth=2, label='Actual 4W Return', alpha=0.7)\n",
    "        ax.plot(ticker_val['date'], ticker_val['predicted'] * 100, \n",
    "               'r--', linewidth=2, label='Predicted 4W Return', alpha=0.7)\n",
    "        ax.axhline(0, color='black', linewidth=0.5)\n",
    "        ax.set_xlabel('Date', fontsize=11)\n",
    "        ax.set_ylabel('4-Week Return (%)', fontsize=11)\n",
    "        \n",
    "        # Calculate RÂ² for this ticker\n",
    "        ticker_r2 = r2_score(ticker_val['target_return_4w'], ticker_val['predicted'])\n",
    "        avg_price = price_stats.loc[ticker, 'mean']\n",
    "        \n",
    "        ax.set_title(f'{ticker} (Avg Price: Â£{avg_price:.2f}) | RÂ² = {ticker_r2:.4f}', fontsize=13)\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: The model works across ALL price levels!\")\n",
    "print(\"   Returns normalize the different scales, enabling a unified model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Understanding Model Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Stock Prediction is Hard (Even with ML)\n",
    "\n",
    "Our model's RÂ² is likely **low** (< 0.05). This is **normal** for stock prediction! Here's why:\n",
    "\n",
    "#### 1. **Efficient Market Hypothesis (EMH)**\n",
    "- Markets are highly efficient\n",
    "- Past prices already incorporate all historical information\n",
    "- If prediction were easy, everyone would do it and eliminate the opportunity\n",
    "\n",
    "#### 2. **Noise-to-Signal Ratio**\n",
    "Stock returns = Predictable signal + Random noise\n",
    "\n",
    "$$r_t = \\underbrace{\\mu + \\beta_1 x_1 + ... + \\beta_n x_n}_{\\text{Signal (tiny)}} + \\underbrace{\\epsilon_t}_{\\text{Noise (huge)}}$$\n",
    "\n",
    "The noise dominates! Even a \"good\" RÂ² of 0.01 means your model explains 1% of variance.\n",
    "\n",
    "#### 3. **Linear Regression Limitations**\n",
    "- Assumes linear relationships (markets are non-linear)\n",
    "- Can't capture regime changes (bull markets, crashes, volatility spikes)\n",
    "- Ignores cross-stock interactions\n",
    "- No consideration of external events (news, earnings, macro data)\n",
    "\n",
    "#### 4. **What Professional Quants Do Differently**\n",
    "- Use **hundreds of features** (alternative data, news sentiment, options flow)\n",
    "- Employ **ensemble models** (Random Forests, Gradient Boosting, Neural Networks)\n",
    "- Implement **regime detection** (separate models for different market conditions)\n",
    "- Focus on **directional accuracy** and **risk-adjusted returns**, not RÂ²\n",
    "- Trade **many stocks** to diversify away idiosyncratic risk\n",
    "- Operate at **higher frequencies** (daily, hourly, minute-level)\n",
    "\n",
    "#### 5. **The Real Benchmark**\n",
    "Even a **slight** edge (51% directional accuracy instead of 50%) can be profitable with:\n",
    "- Proper risk management\n",
    "- Position sizing\n",
    "- Transaction cost optimization\n",
    "- Portfolio diversification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### âœ… What We Learned\n",
    "\n",
    "1. **Scale Invariance is Critical**\n",
    "   - Raw prices vary 100x across stocks (Â£30 to Â£3,000)\n",
    "   - Returns normalize this: both cheap and expensive stocks have ~2% daily volatility\n",
    "   - Unified model trains on all stocks equally\n",
    "\n",
    "2. **Time-Series Require Special Handling**\n",
    "   - Never use random train/test splits (causes future data leakage)\n",
    "   - Always split chronologically: train on past, test on future\n",
    "   - Lagged features create dependencies that must be respected\n",
    "\n",
    "3. **Domain Knowledge Matters**\n",
    "   - Financial features (momentum, volatility, moving averages) are industry standard\n",
    "   - Feature engineering dramatically improves over raw prices\n",
    "   - Understanding the domain helps interpret model limitations\n",
    "\n",
    "4. **Linear Regression for Finance**\n",
    "   - **Strengths**: Fast, interpretable, works across all stocks\n",
    "   - **Weaknesses**: Can't capture non-linearities, market regimes, or external shocks\n",
    "   - **Reality**: Low RÂ² is expected and doesn't mean failure\n",
    "\n",
    "5. **Prediction vs Trading**\n",
    "   - Directional accuracy matters more than exact values\n",
    "   - Even 51% accuracy can be profitable with proper risk management\n",
    "   - Real trading requires transaction costs, slippage, and position sizing\n",
    "\n",
    "### ðŸš€ Extensions to Explore\n",
    "\n",
    "1. **Better Models**: Try Ridge/Lasso regression, Random Forests, XGBoost\n",
    "2. **More Features**: Add sector indicators, market-wide features, volatility indices\n",
    "3. **Different Targets**: Predict volatility, drawdowns, or classification (up/down)\n",
    "4. **Walk-Forward Analysis**: Retrain model periodically to adapt to changing markets\n",
    "5. **Portfolio Optimization**: Use predictions to construct long/short portfolios\n",
    "\n",
    "### ðŸ“š Key Concepts Applied\n",
    "\n",
    "| Concept | How We Used It |\n",
    "|---------|---------------|\n",
    "| **Linear Regression** | Unified model across 100 stocks |\n",
    "| **Feature Engineering** | Returns, lagged features, momentum, volatility |\n",
    "| **Feature Scaling** | StandardScaler on all features |\n",
    "| **Time-Series Split** | Chronological train/val/test |\n",
    "| **Model Evaluation** | RÂ², RMSE, MAE, directional accuracy |\n",
    "| **Domain Knowledge** | Financial returns, trading metrics |\n",
    "\n",
    "### âš ï¸ Important Warnings\n",
    "\n",
    "**DO NOT** use this model for real trading without:\n",
    "- Understanding market microstructure (bid-ask spreads, slippage)\n",
    "- Implementing proper risk management (stop losses, position sizing)\n",
    "- Accounting for transaction costs (commissions, taxes)\n",
    "- Testing on more comprehensive data (multiple market cycles)\n",
    "- Consulting with financial professionals\n",
    "\n",
    "**This is for educational purposes only!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Compare Against Naive Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does our model compare to simple baselines?\n",
    "\n",
    "# Baseline 1: Always predict 0% return (market-neutral)\n",
    "y_val_baseline_zero = np.zeros_like(y_val)\n",
    "\n",
    "# Baseline 2: Predict mean historical return\n",
    "y_val_baseline_mean = np.full_like(y_val, y_train.mean())\n",
    "\n",
    "# Baseline 3: Momentum - assume next 4 weeks = last 4 weeks\n",
    "y_val_baseline_momentum = val_df['return_4w'].values\n",
    "\n",
    "# Evaluate baselines\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Our Model vs Simple Baselines\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = [\n",
    "    ('Baseline: Always 0%', y_val_baseline_zero),\n",
    "    ('Baseline: Mean Return', y_val_baseline_mean),\n",
    "    ('Baseline: Momentum', y_val_baseline_momentum),\n",
    "    ('Our Linear Regression', y_val_pred)\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, preds in models:\n",
    "    r2 = r2_score(y_val, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds)) * 100\n",
    "    mae = mean_absolute_error(y_val, preds) * 100\n",
    "    dir_acc, _, _ = directional_accuracy(y_val, preds)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RÂ²': r2,\n",
    "        'RMSE (%)': rmse,\n",
    "        'MAE (%)': mae,\n",
    "        'Dir. Acc (%)': dir_acc * 100\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: RÂ² comparison\n",
    "axes[0].bar(range(len(results_df)), results_df['RÂ²'], \n",
    "           color=['gray', 'gray', 'gray', 'green'], edgecolor='black')\n",
    "axes[0].set_xticks(range(len(results_df)))\n",
    "axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('RÂ² Score', fontsize=12)\n",
    "axes[0].set_title('Model Comparison: RÂ² Score', fontsize=14)\n",
    "axes[0].axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Directional accuracy\n",
    "axes[1].bar(range(len(results_df)), results_df['Dir. Acc (%)'], \n",
    "           color=['gray', 'gray', 'gray', 'green'], edgecolor='black')\n",
    "axes[1].set_xticks(range(len(results_df)))\n",
    "axes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Directional Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Model Comparison: Direction Prediction', fontsize=14)\n",
    "axes[1].axhline(50, color='red', linestyle='--', linewidth=2, label='Random Guess')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Even if RÂ² seems low, beating simple baselines is valuable!\")\n",
    "print(\"   Financial markets are noisy - any edge over random is significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this case study, we tackled a real-world financial ML problem:\n",
    "\n",
    "1. âœ… Identified the **scale problem** with stock prices\n",
    "2. âœ… Solved it using **percentage returns** instead of absolute prices\n",
    "3. âœ… Engineered **domain-specific features** (momentum, volatility, moving averages)\n",
    "4. âœ… Properly handled **time-series data** (chronological splits, no lookahead bias)\n",
    "5. âœ… Built a **unified linear regression model** for 100 different stocks\n",
    "6. âœ… Evaluated with **financial metrics** (directional accuracy, risk-adjusted performance)\n",
    "7. âœ… Understood **model limitations** and real-world challenges\n",
    "\n",
    "**Most importantly**: We learned that **feature engineering and domain knowledge matter more than algorithm complexity** in many real-world problems!\n",
    "\n",
    "Linear Regression, while simple, taught us:\n",
    "- The importance of proper data transformation\n",
    "- How to handle multi-entity datasets\n",
    "- Why time-series require special treatment\n",
    "- That \"low RÂ²\" doesn't mean \"bad model\" in noisy domains\n",
    "\n",
    "These lessons apply far beyond stock prediction - to demand forecasting, sensor data, medical time series, and more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
