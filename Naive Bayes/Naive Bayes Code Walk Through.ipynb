{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Naive%20Bayes/Naive%20Bayes%20Code%20Walk%20Through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk04sf6YhdHV"
      },
      "source": [
        "# Naïve Bayes: Code Walk Through\n",
        "\n",
        "This notebook provides a step-by-step computational walkthrough of **Naïve Bayes** classification using two variants:\n",
        "- **Gaussian Naïve Bayes** for continuous features\n",
        "- **Multinomial Naïve Bayes** for discrete count data (text classification)\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How **Bayes' Theorem** forms the foundation of Naïve Bayes classification\n",
        "- How the **naïve assumption** of conditional independence simplifies computation\n",
        "- How to compute **prior** and **conditional probabilities** from training data\n",
        "- How **Gaussian Naïve Bayes** models continuous features using mean and variance\n",
        "- How **Multinomial Naïve Bayes** handles text classification with word counts\n",
        "- How **Laplace smoothing** prevents zero probabilities\n",
        "- How to visualize decision boundaries and probability distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tG7jTDKhdHW"
      },
      "source": [
        "---\n",
        "# Part 1: Gaussian Naïve Bayes\n",
        "\n",
        "Gaussian Naïve Bayes assumes that features follow a **normal (Gaussian) distribution** for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVZvcTiKhdHW"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45QgFthJhdHW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLjblZBehdHX"
      },
      "source": [
        "## 2. Generate Synthetic Binary Classification Data\n",
        "\n",
        "We'll create a 2D dataset with two classes, following the pattern from the lecture slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QccywjNFhdHX"
      },
      "outputs": [],
      "source": [
        "# Generate two-class data\n",
        "N = 50  # samples per class\n",
        "D = 2   # features\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Class 0: centered around (1.5, 2.0)\n",
        "class_0 = np.hstack((\n",
        "    1.5 + 1.25 * np.random.randn(N, 1),\n",
        "    2.0 + 1.25 * np.random.randn(N, 1)\n",
        "))\n",
        "\n",
        "# Class 1: centered around (4.0, 4.0)\n",
        "class_1 = np.hstack((\n",
        "    4.0 + 1.0 * np.random.randn(N, 1),\n",
        "    4.0 + 1.0 * np.random.randn(N, 1)\n",
        "))\n",
        "\n",
        "# Combine into training set\n",
        "X_train = np.vstack((class_0, class_1))  # shape (2N, 2)\n",
        "y_train = np.concatenate([np.zeros(N), np.ones(N)])  # shape (2N,)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y_train.astype(int))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1-v9zoQhdHX"
      },
      "source": [
        "## 3. Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykbvs4-zhdHX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', label='Class 0', edgecolors='k', s=50)\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', label='Class 1', edgecolors='k', s=50)\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Binary Classification Data', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvR6dWqhdHY"
      },
      "source": [
        "## 4. Understanding Bayes' Theorem\n",
        "\n",
        "**Bayes' Theorem** allows us to compute the probability of a class given the features:\n",
        "\n",
        "$$P(y|\\vec{x}) = \\frac{P(\\vec{x}|y) \\cdot P(y)}{P(\\vec{x})}$$\n",
        "\n",
        "Where:\n",
        "- $P(y|\\vec{x})$ is the **posterior probability** (probability of class $y$ given features $\\vec{x}$)\n",
        "- $P(\\vec{x}|y)$ is the **likelihood** (probability of observing $\\vec{x}$ given class $y$)\n",
        "- $P(y)$ is the **prior probability** (probability of class $y$)\n",
        "- $P(\\vec{x})$ is the **evidence** (probability of observing $\\vec{x}$)\n",
        "\n",
        "### The Naïve Assumption\n",
        "\n",
        "Naïve Bayes assumes that **features are conditionally independent** given the class:\n",
        "\n",
        "$$P(\\vec{x}|y) = P(x_1, x_2, ..., x_D|y) \\approx P(x_1|y) \\cdot P(x_2|y) \\cdots P(x_D|y)$$\n",
        "\n",
        "This simplifies to:\n",
        "\n",
        "$$P(y|\\vec{x}) \\propto P(y) \\prod_{i=1}^{D} P(x_i|y)$$\n",
        "\n",
        "Since $P(\\vec{x})$ is constant for all classes, we can ignore it when comparing probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAM81frYhdHY"
      },
      "source": [
        "## 5. Calculate Prior Probabilities\n",
        "\n",
        "The **prior probability** $P(y)$ is simply the fraction of training samples in each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do7FKPWPhdHY"
      },
      "outputs": [],
      "source": [
        "# Identify unique classes\n",
        "classes = np.unique(y_train)\n",
        "print(f\"Classes: {classes}\")\n",
        "\n",
        "# Calculate prior probabilities\n",
        "priors = {}\n",
        "for c in classes:\n",
        "    priors[c] = np.sum(y_train == c) / len(y_train)\n",
        "    print(f\"P(y={int(c)}) = {priors[c]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfC00aqmhdHY"
      },
      "source": [
        "## 6. Fit Gaussian Distributions for Each Feature\n",
        "\n",
        "For **Gaussian Naïve Bayes**, we assume each feature $x_i$ follows a normal distribution:\n",
        "\n",
        "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{y,i}}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma^2_{y,i}}\\right)$$\n",
        "\n",
        "We need to calculate:\n",
        "- **Mean** $\\mu_{y,i}$: average value of feature $i$ for class $y$\n",
        "- **Variance** $\\sigma^2_{y,i}$: variance of feature $i$ for class $y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppr6QV1OhdHY"
      },
      "outputs": [],
      "source": [
        "# Calculate means and variances for each class and feature\n",
        "stats = {}\n",
        "\n",
        "for c in classes:\n",
        "    # Filter data for class c\n",
        "    X_c = X_train[y_train == c]\n",
        "\n",
        "    # Calculate mean and variance for each feature\n",
        "    means = np.mean(X_c, axis=0)\n",
        "    variances = np.var(X_c, axis=0)\n",
        "\n",
        "    # Store in dictionary\n",
        "    stats[c] = {'means': means, 'variances': variances}\n",
        "\n",
        "    print(f\"\\nClass {int(c)}:\")\n",
        "    print(f\"  Means:     {means}\")\n",
        "    print(f\"  Variances: {variances}\")\n",
        "    print(f\"  Std devs:  {np.sqrt(variances)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y193BPShdHY"
      },
      "source": [
        "## 7. Visualize the Gaussian Distributions\n",
        "\n",
        "Let's visualize the fitted Gaussian distributions for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJFMiL3ThdHY"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for feature_idx in range(2):\n",
        "    ax = axes[feature_idx]\n",
        "\n",
        "    # Plot histogram for each class\n",
        "    for c in classes:\n",
        "        X_c = X_train[y_train == c]\n",
        "        color = 'skyblue' if c == 0 else 'orange'\n",
        "        label = f'Class {int(c)}'\n",
        "\n",
        "        # Histogram\n",
        "        ax.hist(X_c[:, feature_idx], bins=15, alpha=0.5, color=color,\n",
        "                edgecolor='black', density=True, label=f'{label} (data)')\n",
        "\n",
        "        # Fitted Gaussian\n",
        "        mu = stats[c]['means'][feature_idx]\n",
        "        sigma = np.sqrt(stats[c]['variances'][feature_idx])\n",
        "        x_range = np.linspace(X_train[:, feature_idx].min() - 1,\n",
        "                              X_train[:, feature_idx].max() + 1, 200)\n",
        "        pdf = norm.pdf(x_range, mu, sigma)\n",
        "        ax.plot(x_range, pdf, linewidth=2, color=color,\n",
        "                linestyle='--', label=f'{label} N({mu:.2f}, {sigma:.2f})')\n",
        "\n",
        "    ax.set_xlabel(f'$x_{{{feature_idx+1}}}$', fontsize=12)\n",
        "    ax.set_ylabel('Probability Density', fontsize=12)\n",
        "    ax.set_title(f'Feature {feature_idx+1}: Gaussian Distributions', fontsize=13)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-iuWAnhdHY"
      },
      "source": [
        "## 8. Calculate Likelihood for a Test Point\n",
        "\n",
        "Let's make a prediction for a test point: $\\vec{x}_{test} = [4.0, 3.0]^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvWkLMDghdHY"
      },
      "outputs": [],
      "source": [
        "# Test point\n",
        "x_test = np.array([4.0, 3.0])\n",
        "print(f\"Test point: {x_test}\")\n",
        "\n",
        "# Calculate likelihood for each class\n",
        "def gaussian_pdf(x, mean, variance):\n",
        "    \"\"\"Calculate Gaussian probability density.\"\"\"\n",
        "    return (1 / np.sqrt(2 * np.pi * variance)) * \\\n",
        "           np.exp(-((x - mean) ** 2) / (2 * variance))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CALCULATING LIKELIHOODS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for c in classes:\n",
        "    print(f\"\\nClass {int(c)}:\")\n",
        "\n",
        "    # Calculate P(x_i|y) for each feature\n",
        "    for i in range(len(x_test)):\n",
        "        mu = stats[c]['means'][i]\n",
        "        var = stats[c]['variances'][i]\n",
        "        likelihood = gaussian_pdf(x_test[i], mu, var)\n",
        "        print(f\"  P(x_{i+1}={x_test[i]:.1f}|y={int(c)}) = {likelihood:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa7_h1k_hdHY"
      },
      "source": [
        "## 9. Calculate Posterior Probabilities\n",
        "\n",
        "Using the naïve assumption:\n",
        "\n",
        "$$P(y|\\vec{x}) \\propto P(y) \\prod_{i=1}^{D} P(x_i|y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2pnD26JhdHY"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"CALCULATING POSTERIOR PROBABILITIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "posteriors = {}\n",
        "\n",
        "for c in classes:\n",
        "    # Start with prior\n",
        "    posterior = priors[c]\n",
        "    print(f\"\\nClass {int(c)}:\")\n",
        "    print(f\"  Prior: P(y={int(c)}) = {posterior:.6f}\")\n",
        "\n",
        "    # Multiply by likelihood of each feature\n",
        "    for i in range(len(x_test)):\n",
        "        mu = stats[c]['means'][i]\n",
        "        var = stats[c]['variances'][i]\n",
        "        likelihood = gaussian_pdf(x_test[i], mu, var)\n",
        "        posterior *= likelihood\n",
        "        print(f\"  After feature {i+1}: {posterior:.10f}\")\n",
        "\n",
        "    posteriors[c] = posterior\n",
        "    print(f\"  \" + \"-\" * 50)\n",
        "    print(f\"  Final: P(y={int(c)}|x) ∝ {posterior:.10f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XetrTKelhdHZ"
      },
      "source": [
        "## 10. Make Prediction\n",
        "\n",
        "The predicted class is the one with the highest posterior probability:\n",
        "\n",
        "$$\\hat{y} = \\arg\\max_y P(y|\\vec{x})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIA5ZpEXhdHZ"
      },
      "outputs": [],
      "source": [
        "# Find class with maximum posterior\n",
        "predicted_class = max(posteriors, key=posteriors.get)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTest point: {x_test}\")\n",
        "print(f\"\\nPosterior probabilities (unnormalized):\")\n",
        "for c in classes:\n",
        "    print(f\"  P(y={int(c)}|x) ∝ {posteriors[c]:.10f}\")\n",
        "print(f\"\\nPredicted class: {int(predicted_class)}\")\n",
        "\n",
        "# Visualize prediction\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', label='Class 0', edgecolors='k', s=50)\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', label='Class 1', edgecolors='k', s=50)\n",
        "plt.scatter(x_test[0], x_test[1], c='red', marker='X', s=200,\n",
        "            edgecolors='black', linewidths=2, label='Test Point', zorder=5)\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title(f'Prediction: Class {int(predicted_class)}', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F35X7VRhdHZ"
      },
      "source": [
        "## 11. Visualize Decision Boundary\n",
        "\n",
        "Let's visualize the decision boundary created by our Gaussian Naïve Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxPrp0L5hdHZ"
      },
      "outputs": [],
      "source": [
        "# Create a mesh for visualization\n",
        "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                        np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "# Calculate posterior probability for each point in the mesh\n",
        "def predict_proba_class_1(X):\n",
        "    \"\"\"Predict probability of class 1 for array of points.\"\"\"\n",
        "    if X.ndim == 1:\n",
        "        X = X.reshape(1, -1)\n",
        "\n",
        "    probs_class_1 = []\n",
        "    for x in X:\n",
        "        posteriors_temp = {}\n",
        "        for c in classes:\n",
        "            posterior = priors[c]\n",
        "            for i in range(len(x)):\n",
        "                mu = stats[c]['means'][i]\n",
        "                var = stats[c]['variances'][i]\n",
        "                posterior *= gaussian_pdf(x[i], mu, var)\n",
        "            posteriors_temp[c] = posterior\n",
        "\n",
        "        # Normalize posteriors\n",
        "        total = sum(posteriors_temp.values())\n",
        "        prob_class_1 = posteriors_temp[1.0] / total if total > 0 else 0.5\n",
        "        probs_class_1.append(prob_class_1)\n",
        "\n",
        "    return np.array(probs_class_1)\n",
        "\n",
        "# Predict for mesh\n",
        "mesh_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "Z = predict_proba_class_1(mesh_points)\n",
        "Z = Z.reshape(xx1.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.contourf(xx1, xx2, Z, levels=20, cmap='RdBu_r', alpha=0.6)\n",
        "plt.colorbar(label='P(y=1|x)')\n",
        "plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2, linestyles='dashed')\n",
        "\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', label='Class 0', edgecolors='k', s=50)\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', label='Class 1', edgecolors='k', s=50)\n",
        "\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Gaussian Naïve Bayes Decision Boundary', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWtM9G5mhdHZ"
      },
      "source": [
        "## 12. Comparison with scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcoJmZVphdHZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train sklearn model\n",
        "sklearn_model = GaussianNB()\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test point\n",
        "sklearn_pred = sklearn_model.predict(x_test.reshape(1, -1))[0]\n",
        "sklearn_proba = sklearn_model.predict_proba(x_test.reshape(1, -1))[0]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPARISON WITH SCIKIT-LEARN\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nOur prediction:      Class {int(predicted_class)}\")\n",
        "print(f\"sklearn prediction:  Class {int(sklearn_pred)}\")\n",
        "print(f\"\\nsklearn probabilities:\")\n",
        "print(f\"  P(y=0|x) = {sklearn_proba[0]:.6f}\")\n",
        "print(f\"  P(y=1|x) = {sklearn_proba[1]:.6f}\")\n",
        "\n",
        "# Compare parameters\n",
        "print(f\"\\nParameter comparison:\")\n",
        "print(f\"\\nOur means:\")\n",
        "for c in classes:\n",
        "    print(f\"  Class {int(c)}: {stats[c]['means']}\")\n",
        "print(f\"\\nsklearn means (theta_):\")\n",
        "print(sklearn_model.theta_)\n",
        "\n",
        "print(f\"\\nOur variances:\")\n",
        "for c in classes:\n",
        "    print(f\"  Class {int(c)}: {stats[c]['variances']}\")\n",
        "print(f\"\\nsklearn variances (var_):\")\n",
        "print(sklearn_model.var_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy9dMyiAhdHa"
      },
      "source": [
        "---\n",
        "# Part 2: Multinomial Naïve Bayes\n",
        "\n",
        "Multinomial Naïve Bayes is used for **discrete count data**, most commonly for **text classification**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhKcnQMWhdHa"
      },
      "source": [
        "## 13. Create Text Dataset\n",
        "\n",
        "Following the lecture slides example, we'll create a simple spam/ham email dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVVgeIV3hdHa"
      },
      "outputs": [],
      "source": [
        "# Training emails\n",
        "spam_emails = [\n",
        "    \"free cash now\",\n",
        "    \"limited offer\",\n",
        "    \"cash prize waiting\"\n",
        "]\n",
        "\n",
        "ham_emails = [\n",
        "    \"we meet tomorrow can we\",\n",
        "    \"have you seen my book\",\n",
        "    \"i will bring cash later\"\n",
        "]\n",
        "\n",
        "# Combine\n",
        "emails = spam_emails + ham_emails\n",
        "labels_text = ['spam'] * len(spam_emails) + ['ham'] * len(ham_emails)\n",
        "labels_numeric = np.array([1] * len(spam_emails) + [0] * len(ham_emails))\n",
        "\n",
        "print(\"Training Emails:\")\n",
        "for i, (email, label) in enumerate(zip(emails, labels_text)):\n",
        "    print(f\"{i+1}. [{label.upper():>4}] {email}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh9W3FCdhdHa"
      },
      "source": [
        "## 14. Build Vocabulary and Bag-of-Words Representation\n",
        "\n",
        "We'll represent each email as a **bag-of-words** vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXcnSMvOhdHa"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "vocabulary = set()\n",
        "for email in emails:\n",
        "    words = email.split()\n",
        "    vocabulary.update(words)\n",
        "\n",
        "vocabulary = sorted(list(vocabulary))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "print(f\"Vocabulary (|V| = {len(vocabulary)}):\")\n",
        "print(vocabulary)\n",
        "print(f\"\\nWord to index mapping:\")\n",
        "for word, idx in word_to_idx.items():\n",
        "    print(f\"  {idx:2d}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdFEVxWQhdHa"
      },
      "outputs": [],
      "source": [
        "# Convert emails to bag-of-words representation\n",
        "def email_to_bow(email, word_to_idx):\n",
        "    \"\"\"Convert email to bag-of-words vector.\"\"\"\n",
        "    bow = np.zeros(len(word_to_idx))\n",
        "    for word in email.split():\n",
        "        if word in word_to_idx:\n",
        "            bow[word_to_idx[word]] += 1\n",
        "    return bow\n",
        "\n",
        "# Create feature matrix\n",
        "X_text = np.array([email_to_bow(email, word_to_idx) for email in emails])\n",
        "\n",
        "print(\"Bag-of-Words Representation:\")\n",
        "print(f\"Shape: {X_text.shape}\\n\")\n",
        "print(\"Features (words):\")\n",
        "print(\" \".join(f\"{word:>8}\" for word in vocabulary))\n",
        "print(\"-\" * (9 * len(vocabulary)))\n",
        "for i, (email, label, bow) in enumerate(zip(emails, labels_text, X_text)):\n",
        "    bow_str = \" \".join(f\"{int(count):>8}\" for count in bow)\n",
        "    print(f\"{bow_str}  <- [{label.upper():>4}] {email}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApqIbXtEhdHa"
      },
      "source": [
        "## 15. Calculate Prior Probabilities for Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s0sJderhdHa"
      },
      "outputs": [],
      "source": [
        "# Calculate priors\n",
        "n_spam = np.sum(labels_numeric == 1)\n",
        "n_ham = np.sum(labels_numeric == 0)\n",
        "n_total = len(labels_numeric)\n",
        "\n",
        "prior_spam = n_spam / n_total\n",
        "prior_ham = n_ham / n_total\n",
        "\n",
        "print(\"Prior Probabilities:\")\n",
        "print(f\"  P(spam) = {n_spam}/{n_total} = {prior_spam:.3f}\")\n",
        "print(f\"  P(ham)  = {n_ham}/{n_total} = {prior_ham:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBNeBTTchdHa"
      },
      "source": [
        "## 16. Calculate Word Likelihoods with Laplace Smoothing\n",
        "\n",
        "For Multinomial Naïve Bayes:\n",
        "\n",
        "$$P(word|class) = \\frac{\\text{Count}(word|class) + \\alpha}{\\text{Total words in class} + \\alpha \\cdot |V|}$$\n",
        "\n",
        "Where:\n",
        "- $\\alpha$ is the **Laplace smoothing** parameter (usually $\\alpha = 1$)\n",
        "- $|V|$ is the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPCfd0Y8hdHa"
      },
      "outputs": [],
      "source": [
        "# Laplace smoothing parameter\n",
        "alpha = 1.0\n",
        "\n",
        "# Calculate word counts for each class\n",
        "word_counts_spam = X_text[labels_numeric == 1].sum(axis=0)\n",
        "word_counts_ham = X_text[labels_numeric == 0].sum(axis=0)\n",
        "\n",
        "# Total word counts\n",
        "total_words_spam = word_counts_spam.sum()\n",
        "total_words_ham = word_counts_ham.sum()\n",
        "\n",
        "print(f\"Word counts per class:\\n\")\n",
        "print(f\"Spam: {word_counts_spam} (total = {total_words_spam})\")\n",
        "print(f\"Ham:  {word_counts_ham} (total = {total_words_ham})\")\n",
        "\n",
        "# Calculate likelihoods with Laplace smoothing\n",
        "vocab_size = len(vocabulary)\n",
        "likelihood_spam = (word_counts_spam + alpha) / (total_words_spam + alpha * vocab_size)\n",
        "likelihood_ham = (word_counts_ham + alpha) / (total_words_ham + alpha * vocab_size)\n",
        "\n",
        "print(f\"\\nWord Likelihoods (with α={alpha}):\")\n",
        "print(f\"\\n{'Word':<10} P(word|spam)  P(word|ham)\")\n",
        "print(\"-\" * 40)\n",
        "for i, word in enumerate(vocabulary):\n",
        "    print(f\"{word:<10} {likelihood_spam[i]:.6f}    {likelihood_ham[i]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXpNK60xhdHa"
      },
      "source": [
        "## 17. Make Prediction on New Email\n",
        "\n",
        "Let's predict whether \"**limited cash offer now free cash**\" is spam or ham."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGbQQ2T0hdHa"
      },
      "outputs": [],
      "source": [
        "# New email to classify\n",
        "new_email = \"limited cash offer now free cash\"\n",
        "print(f\"New email: \\\"{new_email}\\\"\\n\")\n",
        "\n",
        "# Convert to bag-of-words\n",
        "new_bow = email_to_bow(new_email, word_to_idx)\n",
        "print(\"Bag-of-words representation:\")\n",
        "for word, count in zip(vocabulary, new_bow):\n",
        "    if count > 0:\n",
        "        print(f\"  {word}: {int(count)}\")\n",
        "\n",
        "# Calculate log posterior for spam\n",
        "log_posterior_spam = np.log(prior_spam)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CALCULATING LOG POSTERIOR FOR SPAM\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"log P(spam) = {log_posterior_spam:.6f}\")\n",
        "\n",
        "for i, word in enumerate(vocabulary):\n",
        "    if new_bow[i] > 0:\n",
        "        contribution = new_bow[i] * np.log(likelihood_spam[i])\n",
        "        log_posterior_spam += contribution\n",
        "        print(f\"+ {int(new_bow[i])} × log P({word}|spam) = {int(new_bow[i])} × {np.log(likelihood_spam[i]):.6f} = {contribution:.6f}\")\n",
        "\n",
        "print(f\"{'-'*60}\")\n",
        "print(f\"Total log P(spam|email) = {log_posterior_spam:.6f}\")\n",
        "\n",
        "# Calculate log posterior for ham\n",
        "log_posterior_ham = np.log(prior_ham)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CALCULATING LOG POSTERIOR FOR HAM\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"log P(ham) = {log_posterior_ham:.6f}\")\n",
        "\n",
        "for i, word in enumerate(vocabulary):\n",
        "    if new_bow[i] > 0:\n",
        "        contribution = new_bow[i] * np.log(likelihood_ham[i])\n",
        "        log_posterior_ham += contribution\n",
        "        print(f\"+ {int(new_bow[i])} × log P({word}|ham) = {int(new_bow[i])} × {np.log(likelihood_ham[i]):.6f} = {contribution:.6f}\")\n",
        "\n",
        "print(f\"{'-'*60}\")\n",
        "print(f\"Total log P(ham|email) = {log_posterior_ham:.6f}\")\n",
        "\n",
        "# Make prediction\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"PREDICTION\")\n",
        "print(f\"{'='*60}\")\n",
        "prediction = \"spam\" if log_posterior_spam > log_posterior_ham else \"ham\"\n",
        "print(f\"\\nlog P(spam|email) = {log_posterior_spam:.6f}\")\n",
        "print(f\"log P(ham|email)  = {log_posterior_ham:.6f}\")\n",
        "print(f\"\\nPredicted class: {prediction.upper()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84inWVcDhdHa"
      },
      "source": [
        "## 18. Comparison with scikit-learn for Multinomial NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaWkottVhdHd"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train sklearn model\n",
        "sklearn_mnb = MultinomialNB(alpha=alpha)\n",
        "sklearn_mnb.fit(X_text, labels_numeric)\n",
        "\n",
        "# Predict on new email\n",
        "sklearn_pred = sklearn_mnb.predict(new_bow.reshape(1, -1))[0]\n",
        "sklearn_proba = sklearn_mnb.predict_proba(new_bow.reshape(1, -1))[0]\n",
        "sklearn_log_proba = sklearn_mnb.predict_log_proba(new_bow.reshape(1, -1))[0]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPARISON WITH SCIKIT-LEARN\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nOur prediction:      {prediction.upper()}\")\n",
        "print(f\"sklearn prediction:  {'SPAM' if sklearn_pred == 1 else 'HAM'}\")\n",
        "\n",
        "print(f\"\\nOur log probabilities:\")\n",
        "print(f\"  log P(ham|email)  = {log_posterior_ham:.6f}\")\n",
        "print(f\"  log P(spam|email) = {log_posterior_spam:.6f}\")\n",
        "\n",
        "print(f\"\\nsklearn log probabilities:\")\n",
        "print(f\"  log P(ham|email)  = {sklearn_log_proba[0]:.6f}\")\n",
        "print(f\"  log P(spam|email) = {sklearn_log_proba[1]:.6f}\")\n",
        "\n",
        "print(f\"\\nsklearn probabilities (normalized):\")\n",
        "print(f\"  P(ham|email)  = {sklearn_proba[0]:.6f}\")\n",
        "print(f\"  P(spam|email) = {sklearn_proba[1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm3AQsZBhdHd"
      },
      "source": [
        "## 19. Understanding Numerical Underflow\n",
        "\n",
        "When multiplying many small probabilities, we risk **numerical underflow** (values becoming too small to represent).\n",
        "\n",
        "**Solution**: Use **log probabilities** and addition instead:\n",
        "\n",
        "$$\\log P(y|\\vec{x}) = \\log P(y) + \\sum_{i=1}^{D} \\log P(x_i|y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLV-1nmYhdHd"
      },
      "outputs": [],
      "source": [
        "# Demonstrate numerical underflow\n",
        "print(\"Demonstrating Numerical Underflow:\")\n",
        "print(f\"\\n0.1 ** 10   = {0.1 ** 10:.2e}\")\n",
        "print(f\"0.1 ** 100  = {0.1 ** 100:.2e}\")\n",
        "print(f\"0.1 ** 1000 = {0.1 ** 1000:.2e}  <- Underflow!\")\n",
        "\n",
        "print(f\"\\nUsing logs:\")\n",
        "print(f\"10   × log(0.1) = {10 * np.log(0.1):.6f}\")\n",
        "print(f\"100  × log(0.1) = {100 * np.log(0.1):.6f}\")\n",
        "print(f\"1000 × log(0.1) = {1000 * np.log(0.1):.6f}  <- No underflow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWXbKgtfhdHe"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this walkthrough, we explored:\n",
        "\n",
        "### Gaussian Naïve Bayes\n",
        "1. **Built from Bayes' Theorem** with the naïve conditional independence assumption\n",
        "2. **Computed prior probabilities** from class frequencies in training data\n",
        "3. **Fitted Gaussian distributions** (mean and variance) for each feature per class\n",
        "4. **Calculated likelihoods** using the Gaussian probability density function\n",
        "5. **Made predictions** by selecting the class with highest posterior probability\n",
        "6. **Visualized decision boundaries** showing how the classifier separates classes\n",
        "\n",
        "### Multinomial Naïve Bayes\n",
        "1. **Represented text as bag-of-words** vectors (word count features)\n",
        "2. **Calculated word likelihoods** with Laplace smoothing to prevent zero probabilities\n",
        "3. **Used log probabilities** to avoid numerical underflow\n",
        "4. **Classified new documents** by comparing log posterior probabilities\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Bayes' Theorem** | $P(y|\\vec{x}) = \\frac{P(\\vec{x}|y) \\cdot P(y)}{P(\\vec{x})}$ |\n",
        "| **Naïve Assumption** | Features are conditionally independent: $P(\\vec{x}|y) \\approx \\prod_{i=1}^{D} P(x_i|y)$ |\n",
        "| **Prior** | $P(y)$ - probability of a class before seeing features |\n",
        "| **Likelihood** | $P(\\vec{x}|y)$ - probability of features given a class |\n",
        "| **Posterior** | $P(y|\\vec{x})$ - probability of class given features |\n",
        "| **Laplace Smoothing** | Add $\\alpha$ to counts to avoid zero probabilities |\n",
        "| **Log Probabilities** | Use $\\log$ to prevent numerical underflow |\n",
        "\n",
        "### When to Use Naïve Bayes\n",
        "\n",
        "**Advantages:**\n",
        "- Fast training and prediction\n",
        "- Works well with high-dimensional data\n",
        "- Performs well with small training sets\n",
        "- Provides probabilistic predictions\n",
        "- Simple and interpretable\n",
        "\n",
        "**Best suited for:**\n",
        "- Text classification (spam detection, sentiment analysis)\n",
        "- Document categorization\n",
        "- Real-time prediction\n",
        "- When features are relatively independent\n",
        "\n",
        "**Limitations:**\n",
        "- Assumes feature independence (often violated in practice)\n",
        "- Gaussian NB is sensitive to outliers\n",
        "- Can be outperformed by more complex models on large datasets\n",
        "\n",
        "### Comparison with Other Algorithms\n",
        "\n",
        "| Algorithm | Training Speed | Prediction Speed | Handles High Dimensions | Probabilistic | Feature Independence |\n",
        "|-----------|---------------|------------------|------------------------|---------------|---------------------|\n",
        "| **Naïve Bayes** | Very Fast | Very Fast | Excellent | Yes | Assumes |\n",
        "| **Logistic Regression** | Fast | Fast | Good | Yes | Not required |\n",
        "| **KNN** | None | Slow | Poor | No | Not required |\n",
        "| **Decision Trees** | Medium | Fast | Good | No | Not required |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}