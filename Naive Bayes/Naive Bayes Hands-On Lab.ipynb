{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Naive%20Bayes/Naive%20Bayes%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Hands-On Lab\n",
    "\n",
    "In this hands-on lab, you will implement the Naive Bayes classifier from scratch, gaining a deep understanding of probabilistic classification. You'll work with both **Gaussian Naive Bayes** for continuous features and **Multinomial Naive Bayes** for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand Bayes Theorem**: Apply the fundamental formula for probabilistic inference\n",
    "2. **Implement Gaussian Naive Bayes**: Build a classifier for continuous features from scratch\n",
    "3. **Implement Multinomial Naive Bayes**: Create a text classifier using bag-of-words representation\n",
    "4. **Handle numerical stability**: Use log probabilities to avoid numerical underflow\n",
    "5. **Apply Laplace smoothing**: Prevent zero probability issues in classification\n",
    "6. **Visualize decision boundaries**: Understand how Naive Bayes separates classes\n",
    "7. **Compare with scikit-learn**: Validate your implementation against the library version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Overview\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes Theorem**:\n",
    "\n",
    "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
    "\n",
    "Where:\n",
    "- $P(y|X)$ is the **posterior probability** - probability of class $y$ given features $X$\n",
    "- $P(X|y)$ is the **likelihood** - probability of features $X$ given class $y$\n",
    "- $P(y)$ is the **prior probability** - probability of class $y$ before seeing the data\n",
    "- $P(X)$ is the **evidence** - probability of the features (normalizing constant)\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "The \"naive\" in Naive Bayes comes from the **conditional independence assumption**: given the class label, all features are assumed to be independent of each other.\n",
    "\n",
    "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y) = \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "This simplification makes the algorithm computationally efficient and surprisingly effective in practice.\n",
    "\n",
    "### Classification Decision\n",
    "\n",
    "To classify a new sample, we compute:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "Since $P(X)$ is constant for all classes, we can ignore it for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "For **continuous features**, we assume each feature follows a Gaussian (normal) distribution within each class:\n",
    "\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_{y,i}$ is the mean of feature $i$ for class $y$\n",
    "- $\\sigma_{y,i}^2$ is the variance of feature $i$ for class $y$\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "\n",
    "For **count data** (like word frequencies in text), we use the multinomial distribution:\n",
    "\n",
    "$$P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{y,i}$ is the count of feature $i$ in class $y$\n",
    "- $N_y$ is the total count of all features in class $y$\n",
    "- $\\alpha$ is the Laplace smoothing parameter\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Stability: Log Probabilities\n",
    "\n",
    "Multiplying many small probabilities can lead to **numerical underflow**. To avoid this, we work with **log probabilities**:\n",
    "\n",
    "$$\\log P(y|X) \\propto \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i|y)$$\n",
    "\n",
    "For Gaussian likelihood, the log probability becomes:\n",
    "\n",
    "$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## When to Use Naive Bayes\n\nNaive Bayes is a fast, probabilistic classifier that excels in specific scenarios. Understanding when to use it is crucial for effective model selection.\n\n### \u2705 Use Naive Bayes When:\n\n**1. Text Classification Problems**\n- Spam detection, sentiment analysis, document categorization\n- High-dimensional sparse features (thousands of words)\n- Example: Email with 10,000 vocabulary words, most zero \u2192 Perfect for Multinomial NB\n\n**2. Need Fast Training and Prediction**\n- Training is O(N \u00d7 d) - just counting and computing means\n- Prediction is O(d) - no iterative optimization\n- Example: Real-time spam filtering processing millions of emails daily\n\n**3. Limited Training Data**\n- Naive Bayes needs fewer samples than discriminative models\n- Works well with hundreds of samples\n- Example: Medical diagnosis with only 200 patient records\n\n**4. Features Are (Approximately) Conditionally Independent**\n- Works surprisingly well even when independence is violated\n- Best when features provide complementary information\n- Example: Different medical tests measuring different aspects of health\n\n**5. Need Probabilistic Outputs**\n- Provides P(y|x) directly from Bayes theorem\n- Useful for ranking or threshold tuning\n- Example: Prioritize emails by spam probability, not just spam/not-spam\n\n**6. Baseline Model for Comparison**\n- Quick to implement and train\n- Establishes performance floor for complex models\n- Example: Before trying deep learning, check if Naive Bayes gets 90% accuracy\n\n### \u274c Don't Use Naive Bayes When:\n\n**1. Features Are Highly Correlated**\n- Independence assumption severely violated\n- Correlated features get \"double-counted\"\n- **Better alternatives**: Logistic Regression, Random Forest, or PCA first\n- Example: Using both \"temperature in Celsius\" and \"temperature in Fahrenheit\"\n\n**2. Need to Capture Feature Interactions**\n- Cannot learn \"A AND B\" patterns\n- Each feature contributes independently\n- **Better alternatives**: Decision Trees, Neural Networks\n- Example: XOR problem - (0,0)\u21920, (1,1)\u21920, (0,1)\u21921, (1,0)\u21921\n\n**3. Continuous Features Don't Follow Gaussian Distribution**\n- Gaussian NB assumes normal distribution per class\n- Multimodal or heavily skewed data violates this\n- **Better alternatives**: Transform features, use kernel density estimation, or different model\n- Example: Income data (highly right-skewed)\n\n**4. Need Well-Calibrated Probabilities**\n- Naive Bayes probabilities are often overconfident\n- Pushes probabilities toward 0 or 1\n- **Better alternatives**: Logistic Regression, or apply Platt scaling\n- Example: When 0.7 predicted probability should actually mean 70% success rate\n\n**5. Complex Decision Boundaries Required**\n- Decision boundary is always linear in log-probability space\n- Cannot capture highly non-linear patterns\n- **Better alternatives**: SVM with RBF kernel, Neural Networks\n- Example: Concentric circles classification\n\n### Quick Decision Tree\n\n```\nIs it a text classification problem?\n\u251c\u2500 Yes \u2192 Multinomial NB (excellent choice!)\n\u2514\u2500 No\n    \u251c\u2500 Are features continuous and roughly Gaussian?\n    \u2502   \u251c\u2500 Yes \u2192 Gaussian NB (good choice)\n    \u2502   \u2514\u2500 No \u2192 Consider other models\n    \u2514\u2500 Are features binary (0/1)?\n        \u2514\u2500 Yes \u2192 Bernoulli NB\n```\n\n### Comparison: Naive Bayes vs Other Classifiers\n\n| Criterion | Naive Bayes | Logistic Regression | Decision Trees | SVM |\n|-----------|-------------|---------------------|----------------|-----|\n| **Training speed** | \u2705 Very fast | \u2705 Fast | \u2705 Fast | \u26a0\ufe0f Slow |\n| **Prediction speed** | \u2705 Very fast | \u2705 Very fast | \u2705 Fast | \u26a0\ufe0f Slow |\n| **Text classification** | \u2705 Excellent | \u2705 Good | \u26a0\ufe0f Poor | \u2705 Good |\n| **Small datasets** | \u2705 Excellent | \u26a0\ufe0f Moderate | \u26a0\ufe0f Overfits | \u2705 Good |\n| **Correlated features** | \u274c Poor | \u2705 Good | \u2705 Good | \u2705 Good |\n| **Interpretability** | \u2705 Good | \u2705 Excellent | \u2705 Excellent | \u274c Poor |\n| **Probability calibration** | \u274c Poor | \u2705 Good | \u26a0\ufe0f Moderate | \u274c Poor |\n| **Feature interactions** | \u274c Cannot learn | \u274c Manual only | \u2705 Automatic | \u26a0\ufe0f Kernel only |\n\n### Choosing the Right Naive Bayes Variant\n\n| Variant | Feature Type | Use Case | Example |\n|---------|--------------|----------|---------|\n| **Gaussian NB** | Continuous (real numbers) | General classification | Iris flowers, medical measurements |\n| **Multinomial NB** | Counts/frequencies | Text classification | Word counts, TF-IDF |\n| **Bernoulli NB** | Binary (0/1) | Binary features | Word presence (not count) |\n| **Complement NB** | Counts (imbalanced) | Imbalanced text data | Rare category detection |\n\n### Real-World Applications Where Naive Bayes Excels:\n\n1. **Spam Detection**: High-dimensional word features, need fast prediction, works great!\n2. **Sentiment Analysis**: Positive/negative classification from text reviews\n3. **Document Categorization**: News articles into topics (sports, politics, tech)\n4. **Medical Diagnosis**: Symptoms as features, diseases as classes\n5. **Recommendation Systems**: \"Users who liked X also liked Y\" patterns\n6. **Real-time Classification**: When latency matters (milliseconds prediction time)\n\n### The Bottom Line:\n\n**Choose Naive Bayes when:**\n- Text classification or high-dimensional sparse data\n- Need fast training and prediction\n- Have limited training data\n- Want a simple, interpretable baseline\n\n**Consider alternatives when:**\n- Features are highly correlated\n- Need to capture feature interactions\n- Require well-calibrated probabilities\n- Decision boundary is highly non-linear"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode: Gaussian Naive Bayes\n",
    "\n",
    "```\n",
    "TRAINING:\n",
    "1. For each class y in classes:\n",
    "   a. Calculate prior: P(y) = count(y) / total_samples\n",
    "   b. For each feature i:\n",
    "      - Calculate mean: \u03bc_yi = mean of feature i where class = y\n",
    "      - Calculate variance: \u03c3\u00b2_yi = variance of feature i where class = y\n",
    "\n",
    "PREDICTION:\n",
    "1. For each class y:\n",
    "   a. Start with log_prob = log(P(y))  # log prior\n",
    "   b. For each feature i:\n",
    "      - Add log(P(x_i|y)) using Gaussian PDF\n",
    "   c. Store total log_prob for class y\n",
    "2. Return class with highest log probability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Pseudocode: Multinomial Naive Bayes\n\n```\n# Multinomial Naive Bayes \u2014 For Text/Count Data\n# Inputs\n# X \u2190 document-term matrix (N documents \u00d7 V vocabulary)\n# y \u2190 class labels\n# \u03b1 \u2190 Laplace smoothing parameter (default: 1)\n# X_query \u2190 documents to classify\n\n# ----- fit -----\nclasses \u2190 unique(y)\nV \u2190 number_of_columns(X)        # vocabulary size\n\nFOR each class c in classes DO\n    # Prior probability\n    prior[c] \u2190 count(y == c) / N\n    \n    # Get all documents of class c\n    X_c \u2190 X[y == c]\n    \n    # Count total words per feature in class c\n    feature_counts[c] \u2190 sum(X_c, axis=0)    # shape: (V,)\n    total_count[c] \u2190 sum(feature_counts[c])\n    \n    # Apply Laplace smoothing\n    # P(word_i | class c) = (count_i + \u03b1) / (total + \u03b1 \u00d7 V)\n    log_prob[c] \u2190 log((feature_counts[c] + \u03b1) / (total_count[c] + \u03b1 \u00d7 V))\nEND FOR\n\n# ----- predict -----\nFOR each document d in X_query DO\n    FOR each class c in classes DO\n        # Log posterior = log prior + sum of (word_count \u00d7 log_prob)\n        score[c] \u2190 log(prior[c]) + dot(d, log_prob[c])\n    END FOR\n    prediction[d] \u2190 argmax(score)\nEND FOR\n\nRETURN predictions\n```\n\n**Key Differences from Gaussian NB:**\n\n| Aspect | Gaussian NB | Multinomial NB |\n|--------|-------------|----------------|\n| **Feature type** | Continuous | Counts/frequencies |\n| **Distribution** | Gaussian (\u03bc, \u03c3\u00b2) | Multinomial |\n| **Parameters stored** | Mean, variance per feature | Log probability per feature |\n| **Smoothing** | var_smoothing (numerical stability) | \u03b1 (Laplace, prevents zero prob) |\n| **Likelihood** | Gaussian PDF | Word count \u00d7 log probability |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint 1: Test Your Understanding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 1\n\nWhat does the \"naive\" assumption in Naive Bayes refer to?\n\nA) Features are assumed to be independent given the class label  \nB) Features are assumed to be identically distributed across all classes  \nC) Each feature contributes equally to the classification decision  \nD) The prior probabilities are assumed to be equal for all classes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: A) Features are assumed to be independent given the class label**\n\nThe \"naive\" assumption refers to **conditional independence**: given the class label, all features are assumed to be independent of each other. Mathematically: P(X|y) = \u220fP(x\u1d62|y). This allows us to compute P(X|y) by simply multiplying individual feature probabilities.\n\n**Example with numbers:**\n\nFor spam classification with features [contains \"free\", contains \"money\"]:\n- **With independence assumption**: P(\"free\", \"money\" | spam) = P(\"free\" | spam) \u00d7 P(\"money\" | spam) = 0.8 \u00d7 0.6 = 0.48\n- **Without assumption**: Would need P(\"free\", \"money\" | spam) directly from data, requiring exponentially more samples\n\nThis simplification reduces parameters from O(2\u207f) to O(n) for n binary features!\n\n**Why other answers are incorrect:**\n\n- **B) Features are assumed to be identically distributed across all classes**: This is incorrect. Naive Bayes explicitly models *different* distributions for each class - that's the whole point. Example: P(\"free\" | spam) = 0.8 but P(\"free\" | not_spam) = 0.1. The mean and variance (Gaussian NB) or word frequencies (Multinomial NB) are computed separately for each class.\n\n- **C) Each feature contributes equally to the classification decision**: This is incorrect. Features have very different contributions. Example: If P(\"free\" | spam) = 0.8 and P(\"free\" | not_spam) = 0.1, the word \"free\" strongly indicates spam (ratio 8:1). But if P(\"the\" | spam) = 0.9 and P(\"the\" | not_spam) = 0.85, \"the\" barely helps (ratio ~1:1).\n\n- **D) The prior probabilities are assumed to be equal for all classes**: This is incorrect. Naive Bayes explicitly computes priors from training data. Example: If 30% of training emails are spam, P(spam) = 0.3 and P(not_spam) = 0.7. These priors directly influence predictions.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Gaussian Naive Bayes Implementation\n",
    "\n",
    "Let's implement Gaussian Naive Bayes step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB as SklearnGaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB as SklearnMultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 2D classification dataset for visualization\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Calculate Class Statistics\n",
    "\n",
    "In this exercise, you'll implement methods to calculate the **prior probabilities** and **class statistics** (mean and variance) needed for Gaussian Naive Bayes.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Calculate the prior probability for each class\n",
    "2. Calculate the mean of each feature for each class\n",
    "3. Calculate the variance of each feature for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, default=1e-9\n",
    "        Portion of the largest variance of all features added to variances\n",
    "        for numerical stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None      # Prior probabilities for each class\n",
    "        self.theta_ = None       # Mean of each feature per class\n",
    "        self.var_ = None         # Variance of each feature per class\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the prior probability of each class.\n",
    "        \n",
    "        Prior P(y) = count(y) / total_samples\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        priors : array of shape (n_classes,)\n",
    "            Prior probability for each class.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the prior probability for each class\n",
    "        # Hint: For each class, divide the count of samples in that class\n",
    "        # by the total number of samples\n",
    "        \n",
    "        priors = None  # Replace with your implementation\n",
    "        \n",
    "        return priors\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate mean and variance of each feature for each class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        theta : array of shape (n_classes, n_features)\n",
    "            Mean of each feature per class.\n",
    "        var : array of shape (n_classes, n_features)\n",
    "            Variance of each feature per class.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # TODO: For each class, calculate the mean and variance of each feature\n",
    "        # Hint: Filter X to only include samples of each class, then compute statistics\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Get samples belonging to class c\n",
    "            X_c = None  # TODO: Filter X for samples where y == c\n",
    "            \n",
    "            # Calculate mean for each feature\n",
    "            theta[idx, :] = None  # TODO: Calculate mean along axis 0\n",
    "            \n",
    "            # Calculate variance for each feature\n",
    "            var[idx, :] = None  # TODO: Calculate variance along axis 0\n",
    "        \n",
    "        return theta, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 1\n",
    "\n",
    "Run this cell to verify your implementation of priors and class statistics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the priors and class statistics calculation\n",
    "gnb_test = GaussianNaiveBayes()\n",
    "gnb_test.classes_ = np.unique(y_train)\n",
    "\n",
    "# Test priors\n",
    "priors = gnb_test._calculate_priors(y_train)\n",
    "print(\"Prior Probabilities:\")\n",
    "if priors is not None:\n",
    "    for i, c in enumerate(gnb_test.classes_):\n",
    "        print(f\"  P(y={c}) = {priors[i]:.4f}\")\n",
    "    \n",
    "    # Verify priors sum to 1\n",
    "    assert np.isclose(priors.sum(), 1.0), \"Priors should sum to 1!\"\n",
    "    print(\"\\n\u2713 Priors sum to 1.0\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test class statistics\n",
    "theta, var = gnb_test._calculate_class_statistics(X_train, y_train)\n",
    "print(\"Class Statistics:\")\n",
    "if theta is not None and var is not None:\n",
    "    for i, c in enumerate(gnb_test.classes_):\n",
    "        print(f\"\\nClass {c}:\")\n",
    "        print(f\"  Mean (\u03b8): {theta[i]}\")\n",
    "        print(f\"  Variance (\u03c3\u00b2): {var[i]}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert theta.shape == (len(gnb_test.classes_), X_train.shape[1]), \"Theta shape incorrect!\"\n",
    "    assert var.shape == (len(gnb_test.classes_), X_train.shape[1]), \"Variance shape incorrect!\"\n",
    "    print(\"\\n\u2713 Class statistics shapes are correct\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">\ud83d\udca1 Click here for Exercise 1 Solution</summary>\n",
    "\n",
    "```python\n",
    "def _calculate_priors(self, y):\n",
    "    priors = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    return priors\n",
    "\n",
    "def _calculate_class_statistics(self, X, y):\n",
    "    n_features = X.shape[1]\n",
    "    n_classes = len(self.classes_)\n",
    "    \n",
    "    theta = np.zeros((n_classes, n_features))\n",
    "    var = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    for idx, c in enumerate(self.classes_):\n",
    "        # Get samples belonging to class c\n",
    "        X_c = X[y == c]\n",
    "        \n",
    "        # Calculate mean for each feature\n",
    "        theta[idx, :] = X_c.mean(axis=0)\n",
    "        \n",
    "        # Calculate variance for each feature\n",
    "        var[idx, :] = X_c.var(axis=0)\n",
    "    \n",
    "    return theta, var\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Priors**: For each class, we count how many samples belong to that class and divide by total samples\n",
    "- **Mean (\u03b8)**: Average value of each feature for samples in each class\n",
    "- **Variance (\u03c3\u00b2)**: Spread of each feature for samples in each class\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Calculate Gaussian Log-Likelihood\n",
    "\n",
    "Now implement the method to calculate the **log-likelihood** of observing features given a class, using the Gaussian probability density function.\n",
    "\n",
    "**Formula:**\n",
    "$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"Calculate prior probabilities.\"\"\"\n",
    "        priors = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "        return priors\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"Calculate mean and variance for each class.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            theta[idx, :] = X_c.mean(axis=0)\n",
    "            var[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return theta, var\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood of X for each class using Gaussian PDF.\n",
    "        \n",
    "        Log P(x_i|y) = -0.5 * log(2\u03c0 * \u03c3\u00b2) - (x_i - \u03bc)\u00b2 / (2\u03c3\u00b2)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : array of shape (n_samples, n_classes)\n",
    "            Log-likelihood for each sample and each class.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        log_likelihood = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        # TODO: Calculate log-likelihood for each class\n",
    "        # For each class:\n",
    "        # 1. Calculate the log of the Gaussian PDF for each feature\n",
    "        # 2. Sum across features (naive assumption - features are independent)\n",
    "        \n",
    "        for idx in range(n_classes):\n",
    "            # Get mean and variance for this class\n",
    "            mean = self.theta_[idx]  # shape: (n_features,)\n",
    "            var = self.var_[idx]     # shape: (n_features,)\n",
    "            \n",
    "            # TODO: Calculate log-likelihood using Gaussian PDF formula\n",
    "            # Hint: Use np.log for logarithm, np.pi for \u03c0\n",
    "            # The formula is: -0.5 * log(2\u03c0 * \u03c3\u00b2) - (x - \u03bc)\u00b2 / (2\u03c3\u00b2)\n",
    "            # Sum across features for each sample\n",
    "            \n",
    "            log_likelihood[:, idx] = None  # Replace with your implementation\n",
    "        \n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 2\n",
    "\n",
    "Run this cell to verify your log-likelihood implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test log-likelihood calculation\n",
    "gnb_test = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb_test.classes_ = np.unique(y_train)\n",
    "gnb_test.theta_, gnb_test.var_ = gnb_test._calculate_class_statistics(X_train, y_train)\n",
    "\n",
    "# Add smoothing to variance\n",
    "gnb_test.var_ = gnb_test.var_ + gnb_test.var_smoothing\n",
    "\n",
    "# Calculate log-likelihood for test samples\n",
    "log_likelihood = gnb_test._calculate_log_likelihood(X_test[:5])\n",
    "\n",
    "print(\"Log-Likelihood for first 5 test samples:\")\n",
    "if log_likelihood is not None and not np.any(log_likelihood == None):\n",
    "    print(f\"Shape: {log_likelihood.shape}\")\n",
    "    print(f\"\\nLog-likelihood values:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  Sample {i}: Class 0 = {log_likelihood[i, 0]:.4f}, Class 1 = {log_likelihood[i, 1]:.4f}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert log_likelihood.shape == (5, 2), \"Log-likelihood shape incorrect!\"\n",
    "    # Verify no NaN or Inf values\n",
    "    assert not np.any(np.isnan(log_likelihood)), \"Log-likelihood contains NaN!\"\n",
    "    assert not np.any(np.isinf(log_likelihood)), \"Log-likelihood contains Inf!\"\n",
    "    print(\"\\n\u2713 Log-likelihood implementation looks correct\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint 2: Test Your Understanding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2\n\nWhy do we use log probabilities instead of raw probabilities in Naive Bayes?\n\nA) Log transformation normalizes the feature distributions to be Gaussian  \nB) Logarithms convert the product of probabilities into a sum, preventing numerical underflow  \nC) Log probabilities allow the model to handle negative feature values  \nD) Using logs reduces the computational complexity from O(n\u00b2) to O(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) Logarithms convert the product of probabilities into a sum, preventing numerical underflow**\n\nWhen multiplying many small probabilities (like P(x\u2081|y) \u00d7 P(x\u2082|y) \u00d7 ... \u00d7 P(x\u2099|y)), the result can become astronomically small, causing numerical underflow to zero.\n\n**Example with numbers:**\n\nConsider classifying a document with 100 words, each with P(word|class) \u2248 0.01:\n- **Raw probabilities**: 0.01\u00b9\u2070\u2070 = 10\u207b\u00b2\u2070\u2070 \u2192 **Underflows to 0!**\n- **Log probabilities**: 100 \u00d7 log(0.01) = 100 \u00d7 (-4.6) = -460 \u2192 **Computable!**\n\nThe classification comparison still works:\n```\nClass A: log(0.01) \u00d7 100 = -460\nClass B: log(0.02) \u00d7 100 = -340  \u2190 Winner (less negative)\n```\n\nUsing log probabilities:\n- Converts multiplication to addition: log(a \u00d7 b) = log(a) + log(b)\n- Keeps values in a manageable numerical range (e.g., -500 instead of 10\u207b\u00b2\u2070\u2070)\n- Preserves the relative ordering needed for classification (log is monotonic)\n\n**Why other answers are incorrect:**\n\n- **A) Log transformation normalizes the feature distributions to be Gaussian**: This is incorrect. Log transformation of probabilities has nothing to do with making features Gaussian. Example: If P(word|spam) follows any distribution, taking log just changes scale, not shape. Log transformations of *features* (not probabilities) can sometimes help with skewed data, but that's a different concept.\n\n- **C) Log probabilities allow the model to handle negative feature values**: This is incorrect. Log probabilities are about the probability values (which are always positive: 0 < p < 1), not about handling negative features. Example: Gaussian NB naturally handles x = -5 because it uses (x - \u03bc)\u00b2 in the PDF. Multinomial NB requires non-negative counts regardless of whether logs are used.\n\n- **D) Using logs reduces the computational complexity from O(n\u00b2) to O(n)**: This is incorrect. The computational complexity remains O(n) for n features with or without logs. We still compute n terms and sum them. Log is applied element-wise: log(p\u2081) + log(p\u2082) + ... + log(p\u2099) is still O(n) operations.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">\ud83d\udca1 Click here for Exercise 2 Solution</summary>\n",
    "\n",
    "```python\n",
    "def _calculate_log_likelihood(self, X):\n",
    "    n_samples = X.shape[0]\n",
    "    n_classes = len(self.classes_)\n",
    "    \n",
    "    log_likelihood = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for idx in range(n_classes):\n",
    "        mean = self.theta_[idx]\n",
    "        var = self.var_[idx]\n",
    "        \n",
    "        # Log of Gaussian PDF: -0.5 * log(2\u03c0 * \u03c3\u00b2) - (x - \u03bc)\u00b2 / (2\u03c3\u00b2)\n",
    "        # Sum across features (naive assumption)\n",
    "        log_likelihood[:, idx] = np.sum(\n",
    "            -0.5 * np.log(2 * np.pi * var) - ((X - mean) ** 2) / (2 * var),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return log_likelihood\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- We compute the log of the Gaussian PDF for each feature\n",
    "- The naive assumption allows us to sum log-probabilities across features\n",
    "- Broadcasting handles the vectorized computation efficiently\n",
    "- `axis=1` sums across features for each sample\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Complete the Gaussian Naive Bayes Classifier\n",
    "\n",
    "Now implement the complete `fit` and `predict` methods to finish the Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"Calculate prior probabilities.\"\"\"\n",
    "        return np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"Calculate mean and variance for each class.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            theta[idx, :] = X_c.mean(axis=0)\n",
    "            var[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return theta, var\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"Calculate log-likelihood using Gaussian PDF.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        log_likelihood = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for idx in range(n_classes):\n",
    "            mean = self.theta_[idx]\n",
    "            var = self.var_[idx]\n",
    "            log_likelihood[:, idx] = np.sum(\n",
    "                -0.5 * np.log(2 * np.pi * var) - ((X - mean) ** 2) / (2 * var),\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the fit method\n",
    "        # 1. Store unique classes\n",
    "        # 2. Calculate prior probabilities\n",
    "        # 3. Calculate class statistics (mean and variance)\n",
    "        # 4. Apply variance smoothing for numerical stability\n",
    "        \n",
    "        # Store unique classes\n",
    "        self.classes_ = None  # TODO\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors_ = None  # TODO\n",
    "        \n",
    "        # Calculate class statistics\n",
    "        self.theta_, self.var_ = None, None  # TODO\n",
    "        \n",
    "        # Apply variance smoothing\n",
    "        # TODO: Add var_smoothing to variance to prevent division by zero\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the predict method\n",
    "        # 1. Calculate log priors\n",
    "        # 2. Calculate log likelihoods\n",
    "        # 3. Combine: log_posterior \u221d log_prior + log_likelihood\n",
    "        # 4. Return the class with highest log posterior for each sample\n",
    "        \n",
    "        # Calculate log priors (same for all samples)\n",
    "        log_priors = None  # TODO: Use np.log on priors\n",
    "        \n",
    "        # Calculate log likelihoods\n",
    "        log_likelihood = None  # TODO\n",
    "        \n",
    "        # Combine log prior and log likelihood\n",
    "        log_posterior = None  # TODO: Add log_priors to log_likelihood\n",
    "        \n",
    "        # Return class with highest log posterior\n",
    "        return None  # TODO: Use self.classes_ and np.argmax\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : array of shape (n_samples, n_classes)\n",
    "            Probability of each class for each sample.\n",
    "        \"\"\"\n",
    "        log_priors = np.log(self.priors_)\n",
    "        log_likelihood = self._calculate_log_likelihood(X)\n",
    "        log_posterior = log_priors + log_likelihood\n",
    "        \n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        # Subtract max for numerical stability\n",
    "        log_posterior = log_posterior - np.max(log_posterior, axis=1, keepdims=True)\n",
    "        posterior = np.exp(log_posterior)\n",
    "        return posterior / posterior.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 3\n",
    "\n",
    "Run this cell to verify your complete Gaussian Naive Bayes implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete implementation\n",
    "gnb = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "if y_pred is not None:\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    sklearn_gnb = SklearnGaussianNB(var_smoothing=1e-9)\n",
    "    sklearn_gnb.fit(X_train, y_train)\n",
    "    sklearn_pred = sklearn_gnb.predict(X_test)\n",
    "    sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "    \n",
    "    print(f\"\\nScikit-learn GaussianNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "    \n",
    "    if np.isclose(accuracy, sklearn_accuracy, atol=0.01):\n",
    "        print(\"\\n\u2713 Your implementation matches scikit-learn!\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0 Accuracy differs from sklearn by {abs(accuracy - sklearn_accuracy):.4f}\")\n",
    "else:\n",
    "    print(\"Prediction not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">\ud83d\udca1 Click here for Exercise 3 Solution</summary>\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    # Store unique classes\n",
    "    self.classes_ = np.unique(y)\n",
    "    \n",
    "    # Calculate priors\n",
    "    self.priors_ = self._calculate_priors(y)\n",
    "    \n",
    "    # Calculate class statistics\n",
    "    self.theta_, self.var_ = self._calculate_class_statistics(X, y)\n",
    "    \n",
    "    # Apply variance smoothing for numerical stability\n",
    "    self.var_ = self.var_ + self.var_smoothing\n",
    "    \n",
    "    return self\n",
    "\n",
    "def predict(self, X):\n",
    "    # Calculate log priors\n",
    "    log_priors = np.log(self.priors_)\n",
    "    \n",
    "    # Calculate log likelihoods\n",
    "    log_likelihood = self._calculate_log_likelihood(X)\n",
    "    \n",
    "    # Combine: log_posterior \u221d log_prior + log_likelihood\n",
    "    log_posterior = log_priors + log_likelihood\n",
    "    \n",
    "    # Return class with highest log posterior\n",
    "    return self.classes_[np.argmax(log_posterior, axis=1)]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **fit**: Stores classes, computes priors, means, variances, and adds smoothing\n",
    "- **predict**: Computes log posterior = log prior + log likelihood, returns argmax class\n",
    "- Using log probabilities avoids numerical underflow from multiplying small numbers\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classifier.\n",
    "    \"\"\"\n",
    "    h = 0.02  # Step size\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', \n",
    "                label='Class 0', edgecolors='k', alpha=0.7)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', \n",
    "                label='Class 1', edgecolors='k', alpha=0.7)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for our implementation\n",
    "if y_pred is not None:\n",
    "    plot_decision_boundary(gnb, X_train, y_train, \n",
    "                          \"Gaussian Naive Bayes Decision Boundary (Our Implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Impact of Outliers on Gaussian Naive Bayes\n\nGaussian Naive Bayes estimates the mean (\u03bc) and variance (\u03c3\u00b2) of each feature for each class. Since these statistics are sensitive to extreme values, **outliers can significantly distort the model's decision boundary**.\n\nLet's visualize how outliers affect Gaussian NB:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate the impact of outliers on Gaussian Naive Bayes\nnp.random.seed(42)\n\n# Create clean data\nn_samples = 100\nX_clean_0 = np.random.randn(n_samples, 2) + np.array([-2, -2])\nX_clean_1 = np.random.randn(n_samples, 2) + np.array([2, 2])\nX_clean = np.vstack([X_clean_0, X_clean_1])\ny_clean = np.array([0] * n_samples + [1] * n_samples)\n\n# Create data with outliers (add extreme points to class 0)\nX_with_outliers = X_clean.copy()\noutliers = np.array([[8, 8], [9, 7], [7, 9]])  # Extreme outliers in class 0\nX_with_outliers = np.vstack([X_with_outliers, outliers])\ny_with_outliers = np.append(y_clean, [0, 0, 0])\n\n# Train models\ngnb_clean = GaussianNaiveBayes(var_smoothing=1e-9)\ngnb_clean.fit(X_clean, y_clean)\n\ngnb_outliers = GaussianNaiveBayes(var_smoothing=1e-9)\ngnb_outliers.fit(X_with_outliers, y_with_outliers)\n\n# Visualization function for comparison\ndef plot_gnb_comparison(X1, y1, model1, title1, X2, y2, model2, title2):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    for ax, X, y, model, title in [(axes[0], X1, y1, model1, title1), \n                                    (axes[1], X2, y2, model2, title2)]:\n        h = 0.1\n        x_min, x_max = -6, 12\n        y_min, y_max = -6, 12\n        \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n        \n        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        \n        ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n        ax.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', \n                   label='Class 0', edgecolors='k', alpha=0.7)\n        ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', \n                   label='Class 1', edgecolors='k', alpha=0.7)\n        \n        # Mark outliers\n        if title == title2:\n            ax.scatter(outliers[:, 0], outliers[:, 1], c='blue', s=200, \n                      marker='*', edgecolors='yellow', linewidths=2, label='Outliers')\n        \n        ax.set_xlabel('Feature 1')\n        ax.set_ylabel('Feature 2')\n        ax.set_title(title)\n        ax.legend()\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_gnb_comparison(X_clean, y_clean, gnb_clean, 'Clean Data (No Outliers)',\n                   X_with_outliers, y_with_outliers, gnb_outliers, 'Data with Outliers')\n\n# Print statistics comparison\nprint(\"Class 0 Statistics Comparison:\")\nprint(f\"  Without outliers - Mean: {gnb_clean.theta_[0]}, Var: {gnb_clean.var_[0]}\")\nprint(f\"  With outliers    - Mean: {gnb_outliers.theta_[0]}, Var: {gnb_outliers.var_[0]}\")\nprint(f\"\\nNotice how outliers shift the mean and inflate the variance of Class 0!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Observations on Outliers\n\n**Effects of outliers on Gaussian NB:**\n\n1. **Mean distortion**: Outliers pull the class mean toward them, shifting the decision boundary\n2. **Variance inflation**: Outliers increase the variance estimate, making the Gaussian distribution \"wider\"\n3. **Decision boundary shift**: The combined effect can cause significant misclassification of normal points\n\n**Mitigation strategies:**\n\n| Strategy | Description |\n|----------|-------------|\n| **Outlier removal** | Remove points beyond k standard deviations |\n| **Robust statistics** | Use median and MAD instead of mean and variance |\n| **Feature transformation** | Apply log transform or winsorization |\n| **Different model** | Consider models less sensitive to outliers |\n\n> **Note**: Gaussian NB is particularly vulnerable because both mean and variance are affected. Compare this to k-NN, where only nearby points influence predictions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint 3: Test Your Understanding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 3\n\nWhat is the purpose of `var_smoothing` in Gaussian Naive Bayes?\n\nA) To add regularization that prevents overfitting to the training data  \nB) To ensure numerical stability when variance is very small or zero  \nC) To standardize features to have unit variance before training  \nD) To control the trade-off between model complexity and generalization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) To ensure numerical stability when variance is very small or zero**\n\n`var_smoothing` adds a small value (typically 1e-9) to the computed variance of each feature. In the Gaussian PDF formula, we divide by variance (\u03c3\u00b2). If variance is zero or very close to zero, this causes division by zero or numerical overflow.\n\n**Example with numbers:**\n\nConsider a feature where all samples in class \"spam\" have the same value:\n```\nFeature \"email_length\" for spam class: [100, 100, 100, 100, 100]\nComputed variance: \u03c3\u00b2 = 0\n\nWithout smoothing:\nP(x=105|spam) = 1/(\u221a(2\u03c0\u00d70)) \u00d7 exp(-(105-100)\u00b2/(2\u00d70)) = 1/0 \u00d7 exp(-\u221e) \u2192 ERROR!\n\nWith var_smoothing = 1e-9:\n\u03c3\u00b2 = 0 + 1e-9 = 1e-9\nP(x=105|spam) = 1/(\u221a(2\u03c0\u00d71e-9)) \u00d7 exp(-(105-100)\u00b2/(2\u00d71e-9)) \u2248 0 (very small but computable)\n```\n\n**Why other answers are incorrect:**\n\n- **A) To add regularization that prevents overfitting to the training data**: While smoothing can have a mild regularization effect, this is not its primary purpose. The default value 1e-9 is far too small to meaningfully regularize. Example: Adding 0.000000001 to a variance of 2.5 doesn't change predictions. Contrast with Laplace smoothing (\u03b1=1) in Multinomial NB, which explicitly regularizes.\n\n- **C) To standardize features to have unit variance before training**: This is incorrect. var_smoothing does not standardize features. Example: If feature has variance 100, var_smoothing adds 1e-9, giving 100.000000001 - not unit variance! StandardScaler (subtracting mean, dividing by std) standardizes features, which is a separate preprocessing step.\n\n- **D) To control the trade-off between model complexity and generalization**: This describes regularization hyperparameters like \u03b1 in Multinomial NB. var_smoothing's default (1e-9) is chosen for numerical stability only. Example: Changing var_smoothing from 1e-9 to 1e-8 has negligible effect on predictions - it's not a tuning knob.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Choice Questions: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Multinomial Naive Bayes for Text Classification\n",
    "\n",
    "Now let's implement **Multinomial Naive Bayes**, which is commonly used for text classification with word count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Example\n",
    "\n",
    "We'll classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews dataset\n",
    "reviews = [\n",
    "    \"This movie was fantastic and amazing\",\n",
    "    \"Great film with excellent acting\",\n",
    "    \"Wonderful story and brilliant performance\",\n",
    "    \"I loved this movie so much\",\n",
    "    \"Best movie I have ever seen\",\n",
    "    \"Outstanding cinematography and plot\",\n",
    "    \"Terrible movie waste of time\",\n",
    "    \"Awful film with bad acting\",\n",
    "    \"Boring and disappointing story\",\n",
    "    \"I hated this movie completely\",\n",
    "    \"Worst movie ever made\",\n",
    "    \"Poor direction and terrible script\",\n",
    "    \"Amazing performances by all actors\",\n",
    "    \"A masterpiece of modern cinema\",\n",
    "    \"Dreadful experience awful waste\",\n",
    "    \"Horrible plot and bad dialogue\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    reviews, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_text_train)}\")\n",
    "print(f\"Test samples: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_text_train).toarray()\n",
    "X_test_bow = vectorizer.transform(X_text_test).toarray()\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nVocabulary: {vectorizer.get_feature_names_out()}\")\n",
    "print(f\"\\nBag-of-words shape: {X_train_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Implement Multinomial Naive Bayes\n",
    "\n",
    "Implement the Multinomial Naive Bayes classifier with **Laplace smoothing**.\n",
    "\n",
    "**Formula for feature likelihood:**\n",
    "$$P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{y,i}$ = count of feature $i$ in class $y$\n",
    "- $N_y$ = total count of all features in class $y$\n",
    "- $\\alpha$ = smoothing parameter (usually 1 for Laplace smoothing)\n",
    "- $n$ = number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes classifier for text classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, default=1.0\n",
    "        Laplace smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.feature_log_prob_ = None  # Log probability of features given class\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Multinomial Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data (word counts).\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors_ = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "        \n",
    "        # Calculate feature log probabilities with Laplace smoothing\n",
    "        self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # TODO: Calculate P(x_i|y) for each feature and class using Laplace smoothing\n",
    "        # Formula: P(x_i|y) = (N_yi + alpha) / (N_y + alpha * n_features)\n",
    "        # Then take log for numerical stability\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Get samples belonging to class c\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # TODO: Calculate N_yi (sum of feature i across all samples in class c)\n",
    "            feature_counts = None  # Sum along axis 0\n",
    "            \n",
    "            # TODO: Calculate N_y (total count of all features in class c)\n",
    "            total_count = None  # Sum of all feature counts\n",
    "            \n",
    "            # TODO: Apply Laplace smoothing and calculate log probabilities\n",
    "            # P(x_i|y) = (feature_counts + alpha) / (total_count + alpha * n_features)\n",
    "            self.feature_log_prob_[idx, :] = None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate log posterior for each class\n",
    "        # log_posterior = log_prior + sum(x_i * log_P(x_i|y))\n",
    "        \n",
    "        log_priors = np.log(self.priors_)\n",
    "        \n",
    "        # TODO: Calculate log likelihood using feature counts and log probabilities\n",
    "        # Hint: Use matrix multiplication X @ self.feature_log_prob_.T\n",
    "        log_likelihood = None\n",
    "        \n",
    "        # TODO: Calculate log posterior\n",
    "        log_posterior = None\n",
    "        \n",
    "        # Return class with highest log posterior\n",
    "        return self.classes_[np.argmax(log_posterior, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multinomial Naive Bayes\n",
    "mnb = MultinomialNaiveBayes(alpha=1.0)\n",
    "mnb.fit(X_train_bow, np.array(y_text_train))\n",
    "\n",
    "# Make predictions\n",
    "y_text_pred = mnb.predict(X_test_bow)\n",
    "\n",
    "if y_text_pred is not None:\n",
    "    accuracy = accuracy_score(y_text_test, y_text_pred)\n",
    "    print(f\"Multinomial Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nPredictions vs Actual:\")\n",
    "    for review, actual, pred in zip(X_text_test, y_text_test, y_text_pred):\n",
    "        sentiment_actual = \"Positive\" if actual == 1 else \"Negative\"\n",
    "        sentiment_pred = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        match = \"\u2713\" if actual == pred else \"\u2717\"\n",
    "        print(f\"  {match} '{review[:40]}...' - Actual: {sentiment_actual}, Predicted: {sentiment_pred}\")\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    sklearn_mnb = SklearnMultinomialNB(alpha=1.0)\n",
    "    sklearn_mnb.fit(X_train_bow, np.array(y_text_train))\n",
    "    sklearn_pred = sklearn_mnb.predict(X_test_bow)\n",
    "    sklearn_accuracy = accuracy_score(y_text_test, sklearn_pred)\n",
    "    \n",
    "    print(f\"\\nScikit-learn MultinomialNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "    \n",
    "    if np.allclose(y_text_pred, sklearn_pred):\n",
    "        print(\"\\n\u2713 Your implementation matches scikit-learn!\")\n",
    "else:\n",
    "    print(\"Prediction not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint 4: Test Your Understanding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 4\n\nWhat problem does Laplace smoothing (alpha) solve in Multinomial Naive Bayes?\n\nA) It handles the case where a word appears in test data but not in any training documents  \nB) It prevents zero probabilities when a word never appears in documents of a particular class  \nC) It removes stop words that appear too frequently across all documents  \nD) It corrects for the different document lengths in the training corpus"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) It prevents zero probabilities when a word never appears in documents of a particular class**\n\nWithout smoothing, if a word never appears in training documents of a particular class, P(word|class) = 0. When we multiply probabilities, any zero makes the entire product zero.\n\n**Example with numbers:**\n\nConsider classifying: \"Free money offer\" with vocabulary [free, money, offer, hello]\n\nTraining data counts:\n```\n           free  money  offer  hello  TOTAL\nspam:        10      8      5      2     25\nnot_spam:     0      1      0     15     16\n```\n\n**Without smoothing (\u03b1=0):**\n```\nP(\"free\"|not_spam) = 0/16 = 0\nP(\"money\"|not_spam) = 1/16 = 0.0625\nP(\"offer\"|not_spam) = 0/16 = 0\n\nP(not_spam|\"Free money offer\") \u221d 0 \u00d7 0.0625 \u00d7 0 = 0  \u2190 Always zero!\n```\n\n**With Laplace smoothing (\u03b1=1):**\n```\nP(\"free\"|not_spam) = (0+1)/(16+4) = 1/20 = 0.05\nP(\"money\"|not_spam) = (1+1)/(16+4) = 2/20 = 0.10\nP(\"offer\"|not_spam) = (0+1)/(16+4) = 1/20 = 0.05\n\nP(not_spam|\"Free money offer\") \u221d 0.05 \u00d7 0.10 \u00d7 0.05 = 0.00025  \u2190 Non-zero!\n```\n\n**Why other answers are incorrect:**\n\n- **A) It handles the case where a word appears in test data but not in any training documents**: This is \"out-of-vocabulary\" (OOV), a different problem. Example: If \"cryptocurrency\" isn't in vocabulary at all, it's simply ignored during prediction. You'd need unknown word tokens or subword models to handle truly unseen words.\n\n- **C) It removes stop words that appear too frequently across all documents**: Incorrect - smoothing doesn't remove anything. Example: The word \"the\" appearing 1000 times still gets counted. Stop word removal is a separate preprocessing step (using nltk.corpus.stopwords or similar).\n\n- **D) It corrects for the different document lengths in the training corpus**: Incorrect - smoothing doesn't normalize by document length. Example: A 1000-word document contributes more counts than a 10-word document. Document length normalization requires dividing by document length or using TF-IDF.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">\ud83d\udca1 Click here for Exercise 4 Solution</summary>\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    self.classes_ = np.unique(y)\n",
    "    n_classes = len(self.classes_)\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Calculate priors\n",
    "    self.priors_ = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    \n",
    "    # Calculate feature log probabilities with Laplace smoothing\n",
    "    self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    for idx, c in enumerate(self.classes_):\n",
    "        X_c = X[y == c]\n",
    "        \n",
    "        # N_yi: sum of feature i across all samples in class c\n",
    "        feature_counts = X_c.sum(axis=0)\n",
    "        \n",
    "        # N_y: total count of all features in class c\n",
    "        total_count = feature_counts.sum()\n",
    "        \n",
    "        # Apply Laplace smoothing and calculate log probabilities\n",
    "        self.feature_log_prob_[idx, :] = np.log(\n",
    "            (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
    "        )\n",
    "    \n",
    "    return self\n",
    "\n",
    "def predict(self, X):\n",
    "    log_priors = np.log(self.priors_)\n",
    "    \n",
    "    # Log likelihood: sum of (x_i * log P(x_i|y))\n",
    "    log_likelihood = X @ self.feature_log_prob_.T\n",
    "    \n",
    "    # Log posterior\n",
    "    log_posterior = log_priors + log_likelihood\n",
    "    \n",
    "    return self.classes_[np.argmax(log_posterior, axis=1)]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Laplace smoothing**: Adds \u03b1 to each count to avoid zero probabilities for unseen words\n",
    "- **Feature counts**: Sum of each word's frequency across all documents in a class\n",
    "- **Log likelihood**: For count data, we multiply log probabilities by word counts\n",
    "- Matrix multiplication `X @ feature_log_prob_.T` efficiently computes the sum\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Choice Questions: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## TF-IDF vs Bag-of-Words for Text Classification\n\nSo far we've used **Bag-of-Words (BoW)** - raw word counts. However, **TF-IDF (Term Frequency-Inverse Document Frequency)** often provides better features for text classification.\n\n### The Problem with Raw Counts\n\nCommon words like \"the\", \"is\", \"and\" appear frequently in all documents but carry little discriminative information. Raw counts give these words high importance.\n\n### TF-IDF Solution\n\nTF-IDF weighs terms by:\n\n$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n\nWhere:\n- **TF(t, d)** = frequency of term t in document d\n- **IDF(t)** = log(N / df(t)) where N is total documents and df(t) is documents containing t\n\n**Key insight**: Words appearing in many documents get lower IDF weights, reducing the influence of common words.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare Bag-of-Words vs TF-IDF for text classification\n# Using a larger dataset to see the difference\n\n# Extended movie reviews for better comparison\nextended_reviews = [\n    \"This movie was fantastic and amazing\",\n    \"Great film with excellent acting\",\n    \"Wonderful story and brilliant performance\",\n    \"I loved this movie so much\",\n    \"Best movie I have ever seen\",\n    \"Outstanding cinematography and plot\",\n    \"Amazing performances by all actors\",\n    \"A masterpiece of modern cinema\",\n    \"Incredible film that I highly recommend\",\n    \"Superb acting and wonderful direction\",\n    \"Terrible movie waste of time\",\n    \"Awful film with bad acting\",\n    \"Boring and disappointing story\",\n    \"I hated this movie completely\",\n    \"Worst movie ever made\",\n    \"Poor direction and terrible script\",\n    \"Dreadful experience awful waste\",\n    \"Horrible plot and bad dialogue\",\n    \"The movie was so boring I fell asleep\",\n    \"Disappointing film with weak characters\"\n]\n\nextended_labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n# Split data\nX_ext_train, X_ext_test, y_ext_train, y_ext_test = train_test_split(\n    extended_reviews, extended_labels, test_size=0.3, random_state=42\n)\n\n# Bag-of-Words\nbow_vectorizer = CountVectorizer()\nX_train_bow_ext = bow_vectorizer.fit_transform(X_ext_train).toarray()\nX_test_bow_ext = bow_vectorizer.transform(X_ext_test).toarray()\n\n# TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_ext_train).toarray()\nX_test_tfidf = tfidf_vectorizer.transform(X_ext_test).toarray()\n\n# Train classifiers\nsklearn_mnb_bow = SklearnMultinomialNB(alpha=1.0)\nsklearn_mnb_bow.fit(X_train_bow_ext, np.array(y_ext_train))\n\nsklearn_mnb_tfidf = SklearnMultinomialNB(alpha=1.0)\nsklearn_mnb_tfidf.fit(X_train_tfidf, np.array(y_ext_train))\n\n# Compare results\nbow_accuracy = accuracy_score(y_ext_test, sklearn_mnb_bow.predict(X_test_bow_ext))\ntfidf_accuracy = accuracy_score(y_ext_test, sklearn_mnb_tfidf.predict(X_test_tfidf))\n\nprint(\"Comparison: Bag-of-Words vs TF-IDF\")\nprint(\"=\" * 50)\nprint(f\"Bag-of-Words Accuracy:  {bow_accuracy:.4f}\")\nprint(f\"TF-IDF Accuracy:        {tfidf_accuracy:.4f}\")\n\n# Visualize feature weights for a sample word\nprint(\"\\nFeature Representation Comparison (sample word: 'movie'):\")\nif 'movie' in bow_vectorizer.vocabulary_:\n    bow_idx = bow_vectorizer.vocabulary_['movie']\n    tfidf_idx = tfidf_vectorizer.vocabulary_['movie']\n    \n    print(f\"  BoW values for first 3 training docs:   {X_train_bow_ext[:3, bow_idx]}\")\n    print(f\"  TF-IDF values for first 3 training docs: {X_train_tfidf[:3, tfidf_idx].round(3)}\")\n    print(\"\\n  Notice: TF-IDF down-weights common words like 'movie' that appear in many documents\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## N-grams: Capturing Word Context\n\nA key limitation of Bag-of-Words is that it treats words independently, losing important context. **N-grams** help capture word sequences and handle negation.\n\n### The Problem: Negation and Context\n\nConsider these reviews:\n- \"This movie is **not good**\" \u2192 Negative sentiment\n- \"This movie is **good**\" \u2192 Positive sentiment\n\nWith unigrams (single words), both contain \"good\" with the same count, making them appear similar!\n\n### N-gram Solution\n\n| N-gram Type | Description | Example: \"not good at all\" |\n|-------------|-------------|----------------------------|\n| **Unigrams (n=1)** | Single words | [\"not\", \"good\", \"at\", \"all\"] |\n| **Bigrams (n=2)** | Word pairs | [\"not good\", \"good at\", \"at all\"] |\n| **Trigrams (n=3)** | Word triples | [\"not good at\", \"good at all\"] |\n\n**Key insight**: \"not good\" as a bigram captures the negation that unigrams miss!\n\n### Example with Sentiment Analysis\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Unigrams only\nvec_uni = CountVectorizer(ngram_range=(1, 1))\n\n# Unigrams + Bigrams\nvec_bi = CountVectorizer(ngram_range=(1, 2))\n\ntext = [\"This movie is not good\"]\nprint(vec_uni.fit_transform(text).toarray())  # [good, is, movie, not, this]\nprint(vec_bi.fit_transform(text).toarray())   # [good, is, is not, movie, movie is, not, not good, this, this movie]\n```\n\n### Trade-offs\n\n| Aspect | Unigrams | Unigrams + Bigrams | Higher N-grams |\n|--------|----------|-------------------|----------------|\n| **Vocabulary size** | V | V + V\u00b2 (approx) | Exponential growth |\n| **Captures negation** | \u274c No | \u2705 Yes | \u2705 Yes |\n| **Sparse features** | Moderate | High | Very high |\n| **Overfitting risk** | Low | Medium | High |\n| **Training data needed** | Less | More | Much more |\n\n### Best Practices for N-grams\n\n1. **Start with (1, 2)**: Unigrams + bigrams is a good default\n2. **Limit vocabulary**: Use `max_features` to cap vocabulary size\n3. **Use with smoothing**: Higher \u03b1 for sparser n-gram features\n4. **Consider TF-IDF**: Helps with n-gram feature weighting\n\n```python\n# Recommended setup for sentiment analysis\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\nvectorizer = TfidfVectorizer(\n    ngram_range=(1, 2),      # Unigrams and bigrams\n    max_features=10000,       # Limit vocabulary\n    min_df=2                  # Ignore very rare terms\n)\n\nmodel = MultinomialNB(alpha=0.1)  # Lower alpha for TF-IDF\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate N-grams effect on sentiment classification\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Reviews with negation - tricky for unigrams!\nnegation_reviews = [\n    \"This movie is good\",\n    \"This movie is not good\", \n    \"I really loved this film\",\n    \"I did not love this film\",\n    \"Great acting and plot\",\n    \"Not great acting at all\",\n    \"The story was amazing\",\n    \"The story was not amazing\"\n]\nnegation_labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n\n# Compare unigrams vs bigrams\nprint(\"N-grams Comparison for Handling Negation\")\nprint(\"=\" * 60)\n\n# Unigrams only\nvec_uni = CountVectorizer(ngram_range=(1, 1))\nX_uni = vec_uni.fit_transform(negation_reviews).toarray()\n\n# Unigrams + Bigrams\nvec_bi = CountVectorizer(ngram_range=(1, 2))\nX_bi = vec_bi.fit_transform(negation_reviews).toarray()\n\nprint(f\"\\nUnigrams vocabulary size: {len(vec_uni.get_feature_names_out())}\")\nprint(f\"Unigrams + Bigrams vocabulary size: {len(vec_bi.get_feature_names_out())}\")\n\n# Show key features\nprint(\"\\n--- Unigram features (sample) ---\")\nuni_features = vec_uni.get_feature_names_out()\nprint(f\"Features: {list(uni_features)}\")\n\nprint(\"\\n--- Bigram features that capture negation ---\")\nbi_features = vec_bi.get_feature_names_out()\nnegation_bigrams = [f for f in bi_features if 'not' in f]\nprint(f\"Negation bigrams: {negation_bigrams}\")\n\n# Train and compare\nfrom sklearn.model_selection import cross_val_score\n\nmnb = SklearnMultinomialNB(alpha=1.0)\n\ncv_uni = cross_val_score(mnb, X_uni, negation_labels, cv=4)\ncv_bi = cross_val_score(mnb, X_bi, negation_labels, cv=4)\n\nprint(f\"\\n--- Cross-validation Accuracy ---\")\nprint(f\"Unigrams only:        {cv_uni.mean():.2f} (\u00b1{cv_uni.std():.2f})\")\nprint(f\"Unigrams + Bigrams:   {cv_bi.mean():.2f} (\u00b1{cv_bi.std():.2f})\")\nprint(\"\\nNote: Bigrams help capture negation patterns like 'not good', 'not great'!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### When to Use TF-IDF vs Bag-of-Words\n\n| Feature | Bag-of-Words | TF-IDF |\n|---------|--------------|--------|\n| **Representation** | Raw word counts | Weighted by term importance |\n| **Common words** | High values | Down-weighted |\n| **Rare but discriminative words** | Low values | Up-weighted |\n| **Best for** | Short texts, when word frequency matters | Longer documents, diverse vocabulary |\n| **Computational cost** | Lower | Slightly higher |\n\n> **Note**: TF-IDF values are continuous, so they work better with models that handle continuous features. For Multinomial NB (which expects counts), you may need to scale TF-IDF values appropriately.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint 5: Test Your Understanding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 5\n\nWhen would you choose Multinomial NB over Gaussian NB?\n\nA) When features represent word frequencies or document-term counts  \nB) When the features have high correlation with each other  \nC) When the dataset has many more samples than features  \nD) When you need well-calibrated probability estimates"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: A) When features represent word frequencies or document-term counts**\n\nMultinomial NB is designed for discrete count data, particularly text classification with bag-of-words features. It models the probability of word occurrences using the multinomial distribution.\n\n**Example with numbers:**\n\n| Dataset | Feature Type | Best Model | Why |\n|---------|--------------|------------|-----|\n| Email spam detection | Word counts: [free:3, money:2, click:1] | **Multinomial NB** | Discrete counts |\n| Iris classification | Measurements: [sepal:5.1cm, petal:1.4cm] | **Gaussian NB** | Continuous values |\n| Sentiment analysis | Word frequencies | **Multinomial NB** | Count data |\n| Medical diagnosis | Lab values: [glucose:95, cholesterol:180] | **Gaussian NB** | Continuous measurements |\n\n**Decision rule:**\n```\nAre your features counts/frequencies (0, 1, 2, 3, ...)?\n\u251c\u2500 Yes \u2192 Multinomial NB\n\u2514\u2500 No \u2192 Are they continuous real numbers?\n        \u251c\u2500 Yes \u2192 Gaussian NB\n        \u2514\u2500 No (binary 0/1) \u2192 Bernoulli NB\n```\n\n**Why other answers are incorrect:**\n\n- **B) When the features have high correlation with each other**: Neither variant handles correlated features well - both assume conditional independence! Example: If \"free\" and \"money\" always appear together, both models treat them as independent. For correlated features, consider PCA first or use models like Random Forests.\n\n- **C) When the dataset has many more samples than features**: The sample-to-feature ratio doesn't determine the choice. Example: Text classification often has vocabulary_size=10,000 features but only 1,000 documents (more features than samples!) - and Multinomial NB still excels. The choice is about feature *type*, not dataset dimensions.\n\n- **D) When you need well-calibrated probability estimates**: Neither variant produces well-calibrated probabilities. Example: Naive Bayes might output P(spam)=0.99 when the true probability is 0.75 (overconfident). For calibrated probabilities, use Platt scaling or isotonic regression post-hoc.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Bias-Variance Tradeoff in Naive Bayes\n\nThe smoothing parameter (\u03b1 in Multinomial NB, var_smoothing in Gaussian NB) controls the **bias-variance tradeoff**:\n\n- **Low smoothing (\u03b1 \u2192 0)**: High variance, low bias\n  - Model closely follows training data\n  - Risk of overfitting, especially with sparse data\n  - Zero probabilities for unseen features\n  \n- **High smoothing (\u03b1 \u2192 \u221e)**: High bias, low variance\n  - Model approaches uniform probabilities\n  - Ignores training data evidence\n  - Underfitting - poor discrimination between classes\n\nLet's visualize this tradeoff:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize bias-variance tradeoff with different smoothing values\nfrom sklearn.model_selection import cross_val_score\n\n# Generate synthetic text-like data for demonstration\nnp.random.seed(42)\n\n# Create a more substantial dataset for meaningful cross-validation\nn_train = 100\nvocab_size = 50\n\n# Simulate word count data\nX_synthetic = np.random.poisson(lam=2, size=(n_train, vocab_size))\n# Add some class-specific signal\nclass_signal = np.zeros((n_train, vocab_size))\nclass_signal[:n_train//2, :10] = np.random.poisson(lam=3, size=(n_train//2, 10))\nclass_signal[n_train//2:, 10:20] = np.random.poisson(lam=3, size=(n_train//2, 10))\nX_synthetic = X_synthetic + class_signal\ny_synthetic = np.array([0] * (n_train//2) + [1] * (n_train//2))\n\n# Test different alpha values\nalphas_bv = np.logspace(-3, 2, 20)  # From 0.001 to 100\nmean_train_scores = []\nmean_cv_scores = []\nstd_cv_scores = []\n\nfor alpha in alphas_bv:\n    model = SklearnMultinomialNB(alpha=alpha)\n    \n    # Training score\n    model.fit(X_synthetic, y_synthetic)\n    train_score = model.score(X_synthetic, y_synthetic)\n    mean_train_scores.append(train_score)\n    \n    # Cross-validation score\n    cv_scores = cross_val_score(model, X_synthetic, y_synthetic, cv=5)\n    mean_cv_scores.append(cv_scores.mean())\n    std_cv_scores.append(cv_scores.std())\n\nmean_cv_scores = np.array(mean_cv_scores)\nstd_cv_scores = np.array(std_cv_scores)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Training vs CV accuracy\nax1 = axes[0]\nax1.semilogx(alphas_bv, mean_train_scores, 'b-', label='Training Accuracy', linewidth=2)\nax1.semilogx(alphas_bv, mean_cv_scores, 'r-', label='CV Accuracy', linewidth=2)\nax1.fill_between(alphas_bv, mean_cv_scores - std_cv_scores, \n                  mean_cv_scores + std_cv_scores, alpha=0.2, color='red')\nax1.axvline(x=alphas_bv[np.argmax(mean_cv_scores)], color='green', linestyle='--', \n             label=f'Best \u03b1 = {alphas_bv[np.argmax(mean_cv_scores)]:.3f}')\nax1.set_xlabel('Smoothing Parameter (\u03b1)', fontsize=12)\nax1.set_ylabel('Accuracy', fontsize=12)\nax1.set_title('Bias-Variance Tradeoff in Multinomial NB', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim([0.4, 1.05])\n\n# Right plot: Gap between training and CV (indicator of overfitting)\nax2 = axes[1]\ngap = np.array(mean_train_scores) - np.array(mean_cv_scores)\nax2.semilogx(alphas_bv, gap, 'purple', linewidth=2)\nax2.fill_between(alphas_bv, 0, gap, alpha=0.3, color='purple')\nax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax2.set_xlabel('Smoothing Parameter (\u03b1)', fontsize=12)\nax2.set_ylabel('Train - CV Accuracy (Overfitting Gap)', fontsize=12)\nax2.set_title('Overfitting Indicator', fontsize=14)\nax2.grid(True, alpha=0.3)\n\n# Add annotations\nax2.annotate('High Variance\\n(Overfitting)', xy=(0.005, 0.15), fontsize=10, ha='center')\nax2.annotate('High Bias\\n(Underfitting)', xy=(20, 0.02), fontsize=10, ha='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Optimal \u03b1 (highest CV accuracy): {alphas_bv[np.argmax(mean_cv_scores)]:.4f}\")\nprint(f\"Best CV Accuracy: {max(mean_cv_scores):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpreting the Bias-Variance Plot\n\n**Left plot (Training vs CV Accuracy):**\n- **Small \u03b1**: High training accuracy but lower CV accuracy \u2192 overfitting\n- **Large \u03b1**: Both accuracies drop \u2192 underfitting  \n- **Optimal \u03b1**: Where CV accuracy is maximized (green line)\n\n**Right plot (Overfitting Gap):**\n- Large gap = high variance (overfitting)\n- Near-zero gap with low accuracy = high bias (underfitting)\n- Sweet spot: small gap with high overall accuracy\n\n**Practical advice:**\n1. Use cross-validation to find optimal \u03b1\n2. Default \u03b1=1.0 works well in most cases\n3. Smaller \u03b1 for large vocabularies, larger \u03b1 for small datasets",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Checkpoint 6: Test Your Understanding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 6\n\nWhat happens if we increase the smoothing parameter \u03b1 in Multinomial NB from 1.0 to 10.0?\n\nA) The model gives more weight to words that appear frequently in the training data  \nB) The model makes feature probabilities more uniform, reducing the influence of observed word counts  \nC) The model becomes more sensitive to rare words in the vocabulary  \nD) The model's training time increases significantly due to more complex calculations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) The model makes feature probabilities more uniform, reducing the influence of observed word counts**\n\nAs \u03b1 increases, the smoothing formula P(word|class) = (count + \u03b1) / (total + \u03b1 \u00d7 vocab_size) causes all feature probabilities to move closer to uniform (1/vocab_size).\n\n**Example with numbers:**\n\nWord \"free\" appears 10 times in spam class (total words in spam = 100, vocab_size = 1000):\n\n| \u03b1 | P(\"free\"\\|spam) | Effect |\n|---|----------------|--------|\n| 0 | 10/100 = **0.100** | Pure observed frequency |\n| 1 | (10+1)/(100+1000) = 11/1100 = **0.010** | Smoothed |\n| 10 | (10+10)/(100+10000) = 20/10100 = **0.002** | More uniform |\n| 100 | (10+100)/(100+100000) = 110/100100 = **0.0011** | Nearly uniform (\u22481/1000) |\n\nNotice: As \u03b1 increases, P(\"free\"|spam) approaches 1/1000 = 0.001 (uniform probability).\n\n**Bias-Variance Tradeoff:**\n- **\u03b1 small**: Low bias, high variance (fits training data closely, may overfit)\n- **\u03b1 large**: High bias, low variance (ignores data, may underfit)\n\n**Why other answers are incorrect:**\n\n- **A) The model gives more weight to words that appear frequently**: This is **backwards**! Larger \u03b1 *reduces* the relative weight of observed frequencies. Example from table above: The ratio between a word appearing 10 times vs 0 times:\n  - \u03b1=0: 10/100 vs 0/100 = infinite ratio\n  - \u03b1=1: 11/1100 vs 1/1100 = 11:1 ratio  \n  - \u03b1=100: 110/100100 vs 100/100100 = 1.1:1 ratio (nearly equal!)\n\n- **C) The model becomes more sensitive to rare words**: **Opposite is true**. Higher \u03b1 washes out rare words. Example: A discriminative rare word appearing once gets drowned by the \u03b1 term when \u03b1 is large.\n\n- **D) The model's training time increases significantly**: Incorrect - \u03b1 doesn't affect computational complexity. The formula (count + \u03b1)/(total + \u03b1 \u00d7 vocab_size) takes the same O(1) time regardless of \u03b1's value. Training time depends on dataset size, not \u03b1.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Effect of Smoothing Parameter\n",
    "\n",
    "Let's visualize how the smoothing parameter affects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    mnb_test = MultinomialNaiveBayes(alpha=alpha)\n",
    "    mnb_test.fit(X_train_bow, np.array(y_text_train))\n",
    "    \n",
    "    train_pred = mnb_test.predict(X_train_bow)\n",
    "    test_pred = mnb_test.predict(X_test_bow)\n",
    "    \n",
    "    if train_pred is not None and test_pred is not None:\n",
    "        train_accuracies.append(accuracy_score(y_text_train, train_pred))\n",
    "        test_accuracies.append(accuracy_score(y_text_test, test_pred))\n",
    "\n",
    "if train_accuracies and test_accuracies:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(alphas, train_accuracies, 'bo-', label='Training Accuracy', markersize=8)\n",
    "    plt.plot(alphas, test_accuracies, 'rs-', label='Test Accuracy', markersize=8)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Alpha (Smoothing Parameter)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Effect of Laplace Smoothing on Multinomial NB')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete Exercise 4 to see the smoothing effect visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Applying to Real Dataset - Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Features: {iris.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train our Gaussian NB\n",
    "gnb_iris = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb_iris.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "# Predict\n",
    "y_iris_pred = gnb_iris.predict(X_iris_test)\n",
    "\n",
    "if y_iris_pred is not None:\n",
    "    print(\"Gaussian Naive Bayes on Iris Dataset\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_iris_test, y_iris_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_iris_test, y_iris_pred, target_names=iris.target_names))\n",
    "    \n",
    "    # Confusion Matrix visualization\n",
    "    cm = confusion_matrix(y_iris_test, y_iris_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Iris Classification')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(iris.target_names))\n",
    "    plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, iris.target_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete Exercise 3 to see Iris classification results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Feature Engineering\n",
    "- **Gaussian NB**: Works best when features approximately follow normal distribution\n",
    "- **Multinomial NB**: Best for count data (text); consider TF-IDF for better results\n",
    "\n",
    "### 2. Choosing Smoothing Parameters\n",
    "- **var_smoothing** (Gaussian): Start with 1e-9, increase if numerical issues occur\n",
    "- **alpha** (Multinomial): Use cross-validation to find optimal value; 1.0 is a good default\n",
    "\n",
    "### 3. When Naive Bayes Shines\n",
    "- Text classification (spam, sentiment, categorization)\n",
    "- High-dimensional data with many features\n",
    "- When you need a quick baseline model\n",
    "- When training data is limited\n",
    "\n",
    "### 4. When to Consider Alternatives\n",
    "- When features are highly correlated\n",
    "- When decision boundaries are complex\n",
    "- When probability estimates need to be well-calibrated\n",
    "\n",
    "### 5. Common Mistakes to Avoid\n",
    "- Forgetting to use log probabilities \u2192 numerical underflow\n",
    "- Using Multinomial NB with negative feature values\n",
    "- Not applying smoothing \u2192 zero probability issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Bayes Theorem Foundation**: How to use $P(y|X) \\propto P(X|y)P(y)$ for classification\n",
    "\n",
    "2. **Gaussian Naive Bayes**: \n",
    "   - Assumes continuous features follow Gaussian distributions\n",
    "   - Computes mean and variance per feature per class\n",
    "   - Uses variance smoothing for numerical stability\n",
    "\n",
    "3. **Multinomial Naive Bayes**:\n",
    "   - Best for count/frequency data (text classification)\n",
    "   - Uses Laplace smoothing to handle zero counts\n",
    "   - Feature probability: $P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha n}$\n",
    "\n",
    "4. **Numerical Stability**:\n",
    "   - Always use log probabilities to avoid underflow\n",
    "   - Convert multiplication to addition: $\\log(ab) = \\log(a) + \\log(b)$\n",
    "\n",
    "5. **The Naive Assumption**:\n",
    "   - Features are conditionally independent given the class\n",
    "   - This simplification makes computation tractable\n",
    "   - Often works well despite being unrealistic\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Naive Bayes is fast, simple, and effective for many tasks\n",
    "- Choose the right variant based on your data type\n",
    "- Smoothing parameters control the bias-variance tradeoff\n",
    "- Log probabilities are essential for numerical stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}