{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Naive%20Bayes/Naive%20Bayes%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Hands-On Lab\n",
    "\n",
    "In this hands-on lab, you will implement the Naive Bayes classifier from scratch, gaining a deep understanding of probabilistic classification. You'll work with both **Gaussian Naive Bayes** for continuous features and **Multinomial Naive Bayes** for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand Bayes Theorem**: Apply the fundamental formula for probabilistic inference\n",
    "2. **Implement Gaussian Naive Bayes**: Build a classifier for continuous features from scratch\n",
    "3. **Implement Multinomial Naive Bayes**: Create a text classifier using bag-of-words representation\n",
    "4. **Handle numerical stability**: Use log probabilities to avoid numerical underflow\n",
    "5. **Apply Laplace smoothing**: Prevent zero probability issues in classification\n",
    "6. **Visualize decision boundaries**: Understand how Naive Bayes separates classes\n",
    "7. **Compare with scikit-learn**: Validate your implementation against the library version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Overview\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes Theorem**:\n",
    "\n",
    "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
    "\n",
    "Where:\n",
    "- $P(y|X)$ is the **posterior probability** - probability of class $y$ given features $X$\n",
    "- $P(X|y)$ is the **likelihood** - probability of features $X$ given class $y$\n",
    "- $P(y)$ is the **prior probability** - probability of class $y$ before seeing the data\n",
    "- $P(X)$ is the **evidence** - probability of the features (normalizing constant)\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "The \"naive\" in Naive Bayes comes from the **conditional independence assumption**: given the class label, all features are assumed to be independent of each other.\n",
    "\n",
    "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y) = \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "This simplification makes the algorithm computationally efficient and surprisingly effective in practice.\n",
    "\n",
    "### Classification Decision\n",
    "\n",
    "To classify a new sample, we compute:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "Since $P(X)$ is constant for all classes, we can ignore it for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "For **continuous features**, we assume each feature follows a Gaussian (normal) distribution within each class:\n",
    "\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_{y,i}$ is the mean of feature $i$ for class $y$\n",
    "- $\\sigma_{y,i}^2$ is the variance of feature $i$ for class $y$\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "\n",
    "For **count data** (like word frequencies in text), we use the multinomial distribution:\n",
    "\n",
    "$$P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{y,i}$ is the count of feature $i$ in class $y$\n",
    "- $N_y$ is the total count of all features in class $y$\n",
    "- $\\alpha$ is the Laplace smoothing parameter\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Stability: Log Probabilities\n",
    "\n",
    "Multiplying many small probabilities can lead to **numerical underflow**. To avoid this, we work with **log probabilities**:\n",
    "\n",
    "$$\\log P(y|X) \\propto \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i|y)$$\n",
    "\n",
    "For Gaussian likelihood, the log probability becomes:\n",
    "\n",
    "$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Naive Bayes\n",
    "\n",
    "| Strengths | Limitations |\n",
    "|-----------|-------------|\n",
    "| Very fast training and prediction | Assumes feature independence (rarely true) |\n",
    "| Works well with high-dimensional data | Cannot learn feature interactions |\n",
    "| Performs well with small training sets | Continuous features may not follow Gaussian |\n",
    "| Handles missing values naturally | Probability estimates can be poor |\n",
    "| Resistant to irrelevant features | Sensitive to feature scaling (Gaussian NB) |\n",
    "| Excellent for text classification | May be outperformed by other algorithms |\n",
    "\n",
    "### Best Use Cases\n",
    "\n",
    "- **Text classification**: Spam filtering, sentiment analysis, document categorization\n",
    "- **Medical diagnosis**: When features are conditionally independent given disease\n",
    "- **Real-time prediction**: When speed is critical\n",
    "- **Baseline model**: Quick benchmark before trying complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode: Gaussian Naive Bayes\n",
    "\n",
    "```\n",
    "TRAINING:\n",
    "1. For each class y in classes:\n",
    "   a. Calculate prior: P(y) = count(y) / total_samples\n",
    "   b. For each feature i:\n",
    "      - Calculate mean: Œº_yi = mean of feature i where class = y\n",
    "      - Calculate variance: œÉ¬≤_yi = variance of feature i where class = y\n",
    "\n",
    "PREDICTION:\n",
    "1. For each class y:\n",
    "   a. Start with log_prob = log(P(y))  # log prior\n",
    "   b. For each feature i:\n",
    "      - Add log(P(x_i|y)) using Gaussian PDF\n",
    "   c. Store total log_prob for class y\n",
    "2. Return class with highest log probability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Gaussian Naive Bayes Implementation\n",
    "\n",
    "Let's implement Gaussian Naive Bayes step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB as SklearnGaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB as SklearnMultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 2D classification dataset for visualization\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Calculate Class Statistics\n",
    "\n",
    "In this exercise, you'll implement methods to calculate the **prior probabilities** and **class statistics** (mean and variance) needed for Gaussian Naive Bayes.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Calculate the prior probability for each class\n",
    "2. Calculate the mean of each feature for each class\n",
    "3. Calculate the variance of each feature for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, default=1e-9\n",
    "        Portion of the largest variance of all features added to variances\n",
    "        for numerical stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None      # Prior probabilities for each class\n",
    "        self.theta_ = None       # Mean of each feature per class\n",
    "        self.var_ = None         # Variance of each feature per class\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the prior probability of each class.\n",
    "        \n",
    "        Prior P(y) = count(y) / total_samples\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        priors : array of shape (n_classes,)\n",
    "            Prior probability for each class.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the prior probability for each class\n",
    "        # Hint: For each class, divide the count of samples in that class\n",
    "        # by the total number of samples\n",
    "        \n",
    "        priors = None  # Replace with your implementation\n",
    "        \n",
    "        return priors\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate mean and variance of each feature for each class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        theta : array of shape (n_classes, n_features)\n",
    "            Mean of each feature per class.\n",
    "        var : array of shape (n_classes, n_features)\n",
    "            Variance of each feature per class.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # TODO: For each class, calculate the mean and variance of each feature\n",
    "        # Hint: Filter X to only include samples of each class, then compute statistics\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Get samples belonging to class c\n",
    "            X_c = None  # TODO: Filter X for samples where y == c\n",
    "            \n",
    "            # Calculate mean for each feature\n",
    "            theta[idx, :] = None  # TODO: Calculate mean along axis 0\n",
    "            \n",
    "            # Calculate variance for each feature\n",
    "            var[idx, :] = None  # TODO: Calculate variance along axis 0\n",
    "        \n",
    "        return theta, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 1\n",
    "\n",
    "Run this cell to verify your implementation of priors and class statistics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the priors and class statistics calculation\n",
    "gnb_test = GaussianNaiveBayes()\n",
    "gnb_test.classes_ = np.unique(y_train)\n",
    "\n",
    "# Test priors\n",
    "priors = gnb_test._calculate_priors(y_train)\n",
    "print(\"Prior Probabilities:\")\n",
    "if priors is not None:\n",
    "    for i, c in enumerate(gnb_test.classes_):\n",
    "        print(f\"  P(y={c}) = {priors[i]:.4f}\")\n",
    "    \n",
    "    # Verify priors sum to 1\n",
    "    assert np.isclose(priors.sum(), 1.0), \"Priors should sum to 1!\"\n",
    "    print(\"\\n‚úì Priors sum to 1.0\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test class statistics\n",
    "theta, var = gnb_test._calculate_class_statistics(X_train, y_train)\n",
    "print(\"Class Statistics:\")\n",
    "if theta is not None and var is not None:\n",
    "    for i, c in enumerate(gnb_test.classes_):\n",
    "        print(f\"\\nClass {c}:\")\n",
    "        print(f\"  Mean (Œ∏): {theta[i]}\")\n",
    "        print(f\"  Variance (œÉ¬≤): {var[i]}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert theta.shape == (len(gnb_test.classes_), X_train.shape[1]), \"Theta shape incorrect!\"\n",
    "    assert var.shape == (len(gnb_test.classes_), X_train.shape[1]), \"Variance shape incorrect!\"\n",
    "    print(\"\\n‚úì Class statistics shapes are correct\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 1 Solution</summary>\n",
    "\n",
    "```python\n",
    "def _calculate_priors(self, y):\n",
    "    priors = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    return priors\n",
    "\n",
    "def _calculate_class_statistics(self, X, y):\n",
    "    n_features = X.shape[1]\n",
    "    n_classes = len(self.classes_)\n",
    "    \n",
    "    theta = np.zeros((n_classes, n_features))\n",
    "    var = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    for idx, c in enumerate(self.classes_):\n",
    "        # Get samples belonging to class c\n",
    "        X_c = X[y == c]\n",
    "        \n",
    "        # Calculate mean for each feature\n",
    "        theta[idx, :] = X_c.mean(axis=0)\n",
    "        \n",
    "        # Calculate variance for each feature\n",
    "        var[idx, :] = X_c.var(axis=0)\n",
    "    \n",
    "    return theta, var\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Priors**: For each class, we count how many samples belong to that class and divide by total samples\n",
    "- **Mean (Œ∏)**: Average value of each feature for samples in each class\n",
    "- **Variance (œÉ¬≤)**: Spread of each feature for samples in each class\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Calculate Gaussian Log-Likelihood\n",
    "\n",
    "Now implement the method to calculate the **log-likelihood** of observing features given a class, using the Gaussian probability density function.\n",
    "\n",
    "**Formula:**\n",
    "$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"Calculate prior probabilities.\"\"\"\n",
    "        priors = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "        return priors\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"Calculate mean and variance for each class.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            theta[idx, :] = X_c.mean(axis=0)\n",
    "            var[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return theta, var\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood of X for each class using Gaussian PDF.\n",
    "        \n",
    "        Log P(x_i|y) = -0.5 * log(2œÄ * œÉ¬≤) - (x_i - Œº)¬≤ / (2œÉ¬≤)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : array of shape (n_samples, n_classes)\n",
    "            Log-likelihood for each sample and each class.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        log_likelihood = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        # TODO: Calculate log-likelihood for each class\n",
    "        # For each class:\n",
    "        # 1. Calculate the log of the Gaussian PDF for each feature\n",
    "        # 2. Sum across features (naive assumption - features are independent)\n",
    "        \n",
    "        for idx in range(n_classes):\n",
    "            # Get mean and variance for this class\n",
    "            mean = self.theta_[idx]  # shape: (n_features,)\n",
    "            var = self.var_[idx]     # shape: (n_features,)\n",
    "            \n",
    "            # TODO: Calculate log-likelihood using Gaussian PDF formula\n",
    "            # Hint: Use np.log for logarithm, np.pi for œÄ\n",
    "            # The formula is: -0.5 * log(2œÄ * œÉ¬≤) - (x - Œº)¬≤ / (2œÉ¬≤)\n",
    "            # Sum across features for each sample\n",
    "            \n",
    "            log_likelihood[:, idx] = None  # Replace with your implementation\n",
    "        \n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 2\n",
    "\n",
    "Run this cell to verify your log-likelihood implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test log-likelihood calculation\n",
    "gnb_test = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb_test.classes_ = np.unique(y_train)\n",
    "gnb_test.theta_, gnb_test.var_ = gnb_test._calculate_class_statistics(X_train, y_train)\n",
    "\n",
    "# Add smoothing to variance\n",
    "gnb_test.var_ = gnb_test.var_ + gnb_test.var_smoothing\n",
    "\n",
    "# Calculate log-likelihood for test samples\n",
    "log_likelihood = gnb_test._calculate_log_likelihood(X_test[:5])\n",
    "\n",
    "print(\"Log-Likelihood for first 5 test samples:\")\n",
    "if log_likelihood is not None and not np.any(log_likelihood == None):\n",
    "    print(f\"Shape: {log_likelihood.shape}\")\n",
    "    print(f\"\\nLog-likelihood values:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  Sample {i}: Class 0 = {log_likelihood[i, 0]:.4f}, Class 1 = {log_likelihood[i, 1]:.4f}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert log_likelihood.shape == (5, 2), \"Log-likelihood shape incorrect!\"\n",
    "    # Verify no NaN or Inf values\n",
    "    assert not np.any(np.isnan(log_likelihood)), \"Log-likelihood contains NaN!\"\n",
    "    assert not np.any(np.isinf(log_likelihood)), \"Log-likelihood contains Inf!\"\n",
    "    print(\"\\n‚úì Log-likelihood implementation looks correct\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 2 Solution</summary>\n",
    "\n",
    "```python\n",
    "def _calculate_log_likelihood(self, X):\n",
    "    n_samples = X.shape[0]\n",
    "    n_classes = len(self.classes_)\n",
    "    \n",
    "    log_likelihood = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for idx in range(n_classes):\n",
    "        mean = self.theta_[idx]\n",
    "        var = self.var_[idx]\n",
    "        \n",
    "        # Log of Gaussian PDF: -0.5 * log(2œÄ * œÉ¬≤) - (x - Œº)¬≤ / (2œÉ¬≤)\n",
    "        # Sum across features (naive assumption)\n",
    "        log_likelihood[:, idx] = np.sum(\n",
    "            -0.5 * np.log(2 * np.pi * var) - ((X - mean) ** 2) / (2 * var),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return log_likelihood\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- We compute the log of the Gaussian PDF for each feature\n",
    "- The naive assumption allows us to sum log-probabilities across features\n",
    "- Broadcasting handles the vectorized computation efficiently\n",
    "- `axis=1` sums across features for each sample\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Complete the Gaussian Naive Bayes Classifier\n",
    "\n",
    "Now implement the complete `fit` and `predict` methods to finish the Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"Calculate prior probabilities.\"\"\"\n",
    "        return np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"Calculate mean and variance for each class.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            theta[idx, :] = X_c.mean(axis=0)\n",
    "            var[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return theta, var\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"Calculate log-likelihood using Gaussian PDF.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        log_likelihood = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for idx in range(n_classes):\n",
    "            mean = self.theta_[idx]\n",
    "            var = self.var_[idx]\n",
    "            log_likelihood[:, idx] = np.sum(\n",
    "                -0.5 * np.log(2 * np.pi * var) - ((X - mean) ** 2) / (2 * var),\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the fit method\n",
    "        # 1. Store unique classes\n",
    "        # 2. Calculate prior probabilities\n",
    "        # 3. Calculate class statistics (mean and variance)\n",
    "        # 4. Apply variance smoothing for numerical stability\n",
    "        \n",
    "        # Store unique classes\n",
    "        self.classes_ = None  # TODO\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors_ = None  # TODO\n",
    "        \n",
    "        # Calculate class statistics\n",
    "        self.theta_, self.var_ = None, None  # TODO\n",
    "        \n",
    "        # Apply variance smoothing\n",
    "        # TODO: Add var_smoothing to variance to prevent division by zero\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the predict method\n",
    "        # 1. Calculate log priors\n",
    "        # 2. Calculate log likelihoods\n",
    "        # 3. Combine: log_posterior ‚àù log_prior + log_likelihood\n",
    "        # 4. Return the class with highest log posterior for each sample\n",
    "        \n",
    "        # Calculate log priors (same for all samples)\n",
    "        log_priors = None  # TODO: Use np.log on priors\n",
    "        \n",
    "        # Calculate log likelihoods\n",
    "        log_likelihood = None  # TODO\n",
    "        \n",
    "        # Combine log prior and log likelihood\n",
    "        log_posterior = None  # TODO: Add log_priors to log_likelihood\n",
    "        \n",
    "        # Return class with highest log posterior\n",
    "        return None  # TODO: Use self.classes_ and np.argmax\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : array of shape (n_samples, n_classes)\n",
    "            Probability of each class for each sample.\n",
    "        \"\"\"\n",
    "        log_priors = np.log(self.priors_)\n",
    "        log_likelihood = self._calculate_log_likelihood(X)\n",
    "        log_posterior = log_priors + log_likelihood\n",
    "        \n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        # Subtract max for numerical stability\n",
    "        log_posterior = log_posterior - np.max(log_posterior, axis=1, keepdims=True)\n",
    "        posterior = np.exp(log_posterior)\n",
    "        return posterior / posterior.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 3\n",
    "\n",
    "Run this cell to verify your complete Gaussian Naive Bayes implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete implementation\n",
    "gnb = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "if y_pred is not None:\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    sklearn_gnb = SklearnGaussianNB(var_smoothing=1e-9)\n",
    "    sklearn_gnb.fit(X_train, y_train)\n",
    "    sklearn_pred = sklearn_gnb.predict(X_test)\n",
    "    sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "    \n",
    "    print(f\"\\nScikit-learn GaussianNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "    \n",
    "    if np.isclose(accuracy, sklearn_accuracy, atol=0.01):\n",
    "        print(\"\\n‚úì Your implementation matches scikit-learn!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö† Accuracy differs from sklearn by {abs(accuracy - sklearn_accuracy):.4f}\")\n",
    "else:\n",
    "    print(\"Prediction not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 3 Solution</summary>\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    # Store unique classes\n",
    "    self.classes_ = np.unique(y)\n",
    "    \n",
    "    # Calculate priors\n",
    "    self.priors_ = self._calculate_priors(y)\n",
    "    \n",
    "    # Calculate class statistics\n",
    "    self.theta_, self.var_ = self._calculate_class_statistics(X, y)\n",
    "    \n",
    "    # Apply variance smoothing for numerical stability\n",
    "    self.var_ = self.var_ + self.var_smoothing\n",
    "    \n",
    "    return self\n",
    "\n",
    "def predict(self, X):\n",
    "    # Calculate log priors\n",
    "    log_priors = np.log(self.priors_)\n",
    "    \n",
    "    # Calculate log likelihoods\n",
    "    log_likelihood = self._calculate_log_likelihood(X)\n",
    "    \n",
    "    # Combine: log_posterior ‚àù log_prior + log_likelihood\n",
    "    log_posterior = log_priors + log_likelihood\n",
    "    \n",
    "    # Return class with highest log posterior\n",
    "    return self.classes_[np.argmax(log_posterior, axis=1)]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **fit**: Stores classes, computes priors, means, variances, and adds smoothing\n",
    "- **predict**: Computes log posterior = log prior + log likelihood, returns argmax class\n",
    "- Using log probabilities avoids numerical underflow from multiplying small numbers\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classifier.\n",
    "    \"\"\"\n",
    "    h = 0.02  # Step size\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', \n",
    "                label='Class 0', edgecolors='k', alpha=0.7)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', \n",
    "                label='Class 1', edgecolors='k', alpha=0.7)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for our implementation\n",
    "if y_pred is not None:\n",
    "    plot_decision_boundary(gnb, X_train, y_train, \n",
    "                          \"Gaussian Naive Bayes Decision Boundary (Our Implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Choice Questions: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What does the \"naive\" assumption in Naive Bayes refer to?\n",
    "\n",
    "A) The algorithm is simple and basic  \n",
    "B) Features are assumed to be independent given the class label  \n",
    "C) The algorithm ignores prior probabilities  \n",
    "D) Only one feature is used for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n",
    "\n",
    "**Answer: B) Features are assumed to be independent given the class label**\n",
    "\n",
    "The \"naive\" assumption means that given the class label, all features are conditionally independent of each other. Mathematically: P(X|y) = ‚àèP(x·µ¢|y). This assumption is rarely true in practice, but Naive Bayes often works well despite this simplification. This allows us to compute P(X|y) by simply multiplying individual feature probabilities, making the algorithm computationally efficient.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Why do we use log probabilities instead of raw probabilities in Naive Bayes?\n",
    "\n",
    "A) To make the algorithm faster  \n",
    "B) To convert multiplication to addition  \n",
    "C) To avoid numerical underflow when multiplying many small probabilities  \n",
    "D) Both B and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n",
    "\n",
    "**Answer: D) Both B and C**\n",
    "\n",
    "Using log probabilities serves two important purposes:\n",
    "\n",
    "1. **Numerical stability**: When multiplying many small probabilities (like P(x‚ÇÅ|y) √ó P(x‚ÇÇ|y) √ó ... √ó P(x‚Çô|y)), the result can become so small that it underflows to zero. Log probabilities avoid this issue.\n",
    "\n",
    "2. **Computational convenience**: log(a √ó b) = log(a) + log(b), so multiplication becomes addition, which is computationally more efficient and numerically stable.\n",
    "\n",
    "For example, instead of computing P(y)‚àèP(x·µ¢|y), we compute log(P(y)) + Œ£log(P(x·µ¢|y)).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is the purpose of `var_smoothing` in Gaussian Naive Bayes?\n",
    "\n",
    "A) To increase model accuracy  \n",
    "B) To prevent division by zero when variance is very small  \n",
    "C) To reduce overfitting  \n",
    "D) To normalize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n",
    "\n",
    "**Answer: B) To prevent division by zero when variance is very small**\n",
    "\n",
    "`var_smoothing` adds a small value to the variance of each feature to ensure numerical stability. In the Gaussian PDF formula, we divide by variance (œÉ¬≤). If variance is zero or very close to zero (which can happen if all samples of a class have the same feature value), this would cause division by zero or numerical instability.\n",
    "\n",
    "By adding `var_smoothing` (typically 1e-9), we ensure the variance is never zero:\n",
    "```\n",
    "var = var + var_smoothing\n",
    "```\n",
    "\n",
    "This is similar to Laplace smoothing in Multinomial NB but for continuous features.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Multinomial Naive Bayes for Text Classification\n",
    "\n",
    "Now let's implement **Multinomial Naive Bayes**, which is commonly used for text classification with word count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Example\n",
    "\n",
    "We'll classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews dataset\n",
    "reviews = [\n",
    "    \"This movie was fantastic and amazing\",\n",
    "    \"Great film with excellent acting\",\n",
    "    \"Wonderful story and brilliant performance\",\n",
    "    \"I loved this movie so much\",\n",
    "    \"Best movie I have ever seen\",\n",
    "    \"Outstanding cinematography and plot\",\n",
    "    \"Terrible movie waste of time\",\n",
    "    \"Awful film with bad acting\",\n",
    "    \"Boring and disappointing story\",\n",
    "    \"I hated this movie completely\",\n",
    "    \"Worst movie ever made\",\n",
    "    \"Poor direction and terrible script\",\n",
    "    \"Amazing performances by all actors\",\n",
    "    \"A masterpiece of modern cinema\",\n",
    "    \"Dreadful experience awful waste\",\n",
    "    \"Horrible plot and bad dialogue\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    reviews, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_text_train)}\")\n",
    "print(f\"Test samples: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_text_train).toarray()\n",
    "X_test_bow = vectorizer.transform(X_text_test).toarray()\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nVocabulary: {vectorizer.get_feature_names_out()}\")\n",
    "print(f\"\\nBag-of-words shape: {X_train_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Implement Multinomial Naive Bayes\n",
    "\n",
    "Implement the Multinomial Naive Bayes classifier with **Laplace smoothing**.\n",
    "\n",
    "**Formula for feature likelihood:**\n",
    "$$P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{y,i}$ = count of feature $i$ in class $y$\n",
    "- $N_y$ = total count of all features in class $y$\n",
    "- $\\alpha$ = smoothing parameter (usually 1 for Laplace smoothing)\n",
    "- $n$ = number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes classifier for text classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, default=1.0\n",
    "        Laplace smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.feature_log_prob_ = None  # Log probability of features given class\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Multinomial Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data (word counts).\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors_ = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "        \n",
    "        # Calculate feature log probabilities with Laplace smoothing\n",
    "        self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # TODO: Calculate P(x_i|y) for each feature and class using Laplace smoothing\n",
    "        # Formula: P(x_i|y) = (N_yi + alpha) / (N_y + alpha * n_features)\n",
    "        # Then take log for numerical stability\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Get samples belonging to class c\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # TODO: Calculate N_yi (sum of feature i across all samples in class c)\n",
    "            feature_counts = None  # Sum along axis 0\n",
    "            \n",
    "            # TODO: Calculate N_y (total count of all features in class c)\n",
    "            total_count = None  # Sum of all feature counts\n",
    "            \n",
    "            # TODO: Apply Laplace smoothing and calculate log probabilities\n",
    "            # P(x_i|y) = (feature_counts + alpha) / (total_count + alpha * n_features)\n",
    "            self.feature_log_prob_[idx, :] = None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate log posterior for each class\n",
    "        # log_posterior = log_prior + sum(x_i * log_P(x_i|y))\n",
    "        \n",
    "        log_priors = np.log(self.priors_)\n",
    "        \n",
    "        # TODO: Calculate log likelihood using feature counts and log probabilities\n",
    "        # Hint: Use matrix multiplication X @ self.feature_log_prob_.T\n",
    "        log_likelihood = None\n",
    "        \n",
    "        # TODO: Calculate log posterior\n",
    "        log_posterior = None\n",
    "        \n",
    "        # Return class with highest log posterior\n",
    "        return self.classes_[np.argmax(log_posterior, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multinomial Naive Bayes\n",
    "mnb = MultinomialNaiveBayes(alpha=1.0)\n",
    "mnb.fit(X_train_bow, np.array(y_text_train))\n",
    "\n",
    "# Make predictions\n",
    "y_text_pred = mnb.predict(X_test_bow)\n",
    "\n",
    "if y_text_pred is not None:\n",
    "    accuracy = accuracy_score(y_text_test, y_text_pred)\n",
    "    print(f\"Multinomial Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nPredictions vs Actual:\")\n",
    "    for review, actual, pred in zip(X_text_test, y_text_test, y_text_pred):\n",
    "        sentiment_actual = \"Positive\" if actual == 1 else \"Negative\"\n",
    "        sentiment_pred = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        match = \"‚úì\" if actual == pred else \"‚úó\"\n",
    "        print(f\"  {match} '{review[:40]}...' - Actual: {sentiment_actual}, Predicted: {sentiment_pred}\")\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    sklearn_mnb = SklearnMultinomialNB(alpha=1.0)\n",
    "    sklearn_mnb.fit(X_train_bow, np.array(y_text_train))\n",
    "    sklearn_pred = sklearn_mnb.predict(X_test_bow)\n",
    "    sklearn_accuracy = accuracy_score(y_text_test, sklearn_pred)\n",
    "    \n",
    "    print(f\"\\nScikit-learn MultinomialNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "    \n",
    "    if np.allclose(y_text_pred, sklearn_pred):\n",
    "        print(\"\\n‚úì Your implementation matches scikit-learn!\")\n",
    "else:\n",
    "    print(\"Prediction not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 4 Solution</summary>\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    self.classes_ = np.unique(y)\n",
    "    n_classes = len(self.classes_)\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Calculate priors\n",
    "    self.priors_ = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    \n",
    "    # Calculate feature log probabilities with Laplace smoothing\n",
    "    self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    for idx, c in enumerate(self.classes_):\n",
    "        X_c = X[y == c]\n",
    "        \n",
    "        # N_yi: sum of feature i across all samples in class c\n",
    "        feature_counts = X_c.sum(axis=0)\n",
    "        \n",
    "        # N_y: total count of all features in class c\n",
    "        total_count = feature_counts.sum()\n",
    "        \n",
    "        # Apply Laplace smoothing and calculate log probabilities\n",
    "        self.feature_log_prob_[idx, :] = np.log(\n",
    "            (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
    "        )\n",
    "    \n",
    "    return self\n",
    "\n",
    "def predict(self, X):\n",
    "    log_priors = np.log(self.priors_)\n",
    "    \n",
    "    # Log likelihood: sum of (x_i * log P(x_i|y))\n",
    "    log_likelihood = X @ self.feature_log_prob_.T\n",
    "    \n",
    "    # Log posterior\n",
    "    log_posterior = log_priors + log_likelihood\n",
    "    \n",
    "    return self.classes_[np.argmax(log_posterior, axis=1)]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Laplace smoothing**: Adds Œ± to each count to avoid zero probabilities for unseen words\n",
    "- **Feature counts**: Sum of each word's frequency across all documents in a class\n",
    "- **Log likelihood**: For count data, we multiply log probabilities by word counts\n",
    "- Matrix multiplication `X @ feature_log_prob_.T` efficiently computes the sum\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Choice Questions: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What problem does Laplace smoothing (alpha) solve in Multinomial Naive Bayes?\n",
    "\n",
    "A) It speeds up training  \n",
    "B) It prevents zero probabilities for unseen words  \n",
    "C) It normalizes the feature values  \n",
    "D) It reduces the vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n",
    "\n",
    "**Answer: B) It prevents zero probabilities for unseen words**\n",
    "\n",
    "Without smoothing, if a word never appears in training documents of a particular class, P(word|class) = 0. When we multiply probabilities, any zero makes the entire product zero, causing misclassification.\n",
    "\n",
    "Laplace smoothing adds Œ± (usually 1) to each word count:\n",
    "```\n",
    "P(word|class) = (count + Œ±) / (total + Œ± √ó vocabulary_size)\n",
    "```\n",
    "\n",
    "This ensures no probability is ever zero, while still giving higher probabilities to more frequent words.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "When would you choose Multinomial NB over Gaussian NB?\n",
    "\n",
    "A) When features are continuous and normally distributed  \n",
    "B) When working with count/frequency data like word occurrences  \n",
    "C) When you have very few training samples  \n",
    "D) When features have high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n",
    "\n",
    "**Answer: B) When working with count/frequency data like word occurrences**\n",
    "\n",
    "- **Multinomial NB**: Best for discrete count data, especially text classification with bag-of-words or TF-IDF features. Assumes features represent counts from a multinomial distribution.\n",
    "\n",
    "- **Gaussian NB**: Best for continuous features that follow a Gaussian (normal) distribution. Used for general classification with continuous data.\n",
    "\n",
    "Examples:\n",
    "- Document classification ‚Üí Multinomial NB\n",
    "- Spam detection (word counts) ‚Üí Multinomial NB  \n",
    "- Iris flower classification (petal measurements) ‚Üí Gaussian NB\n",
    "- Sensor data classification ‚Üí Gaussian NB\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What happens if we increase the smoothing parameter Œ± in Multinomial NB?\n",
    "\n",
    "A) The model becomes more confident in its predictions  \n",
    "B) Feature probabilities become more uniform across classes  \n",
    "C) Training becomes faster  \n",
    "D) The vocabulary size decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n",
    "\n",
    "**Answer: B) Feature probabilities become more uniform across classes**\n",
    "\n",
    "As Œ± increases:\n",
    "- All feature probabilities move closer to uniform distribution (1/vocabulary_size)\n",
    "- The model relies less on observed data and more on the prior\n",
    "- This increases **bias** but reduces **variance** (bias-variance tradeoff)\n",
    "\n",
    "Example:\n",
    "- Œ± = 0: P(word|class) purely based on training counts (high variance)\n",
    "- Œ± = 1: Standard Laplace smoothing (balanced)\n",
    "- Œ± >> 1: Probabilities approach uniform, model ignores data (high bias)\n",
    "\n",
    "This is similar to regularization in other models.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Effect of Smoothing Parameter\n",
    "\n",
    "Let's visualize how the smoothing parameter affects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    mnb_test = MultinomialNaiveBayes(alpha=alpha)\n",
    "    mnb_test.fit(X_train_bow, np.array(y_text_train))\n",
    "    \n",
    "    train_pred = mnb_test.predict(X_train_bow)\n",
    "    test_pred = mnb_test.predict(X_test_bow)\n",
    "    \n",
    "    if train_pred is not None and test_pred is not None:\n",
    "        train_accuracies.append(accuracy_score(y_text_train, train_pred))\n",
    "        test_accuracies.append(accuracy_score(y_text_test, test_pred))\n",
    "\n",
    "if train_accuracies and test_accuracies:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(alphas, train_accuracies, 'bo-', label='Training Accuracy', markersize=8)\n",
    "    plt.plot(alphas, test_accuracies, 'rs-', label='Test Accuracy', markersize=8)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Alpha (Smoothing Parameter)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Effect of Laplace Smoothing on Multinomial NB')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete Exercise 4 to see the smoothing effect visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Applying to Real Dataset - Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Features: {iris.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train our Gaussian NB\n",
    "gnb_iris = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb_iris.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "# Predict\n",
    "y_iris_pred = gnb_iris.predict(X_iris_test)\n",
    "\n",
    "if y_iris_pred is not None:\n",
    "    print(\"Gaussian Naive Bayes on Iris Dataset\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_iris_test, y_iris_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_iris_test, y_iris_pred, target_names=iris.target_names))\n",
    "    \n",
    "    # Confusion Matrix visualization\n",
    "    cm = confusion_matrix(y_iris_test, y_iris_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Iris Classification')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(iris.target_names))\n",
    "    plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, iris.target_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete Exercise 3 to see Iris classification results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Feature Engineering\n",
    "- **Gaussian NB**: Works best when features approximately follow normal distribution\n",
    "- **Multinomial NB**: Best for count data (text); consider TF-IDF for better results\n",
    "\n",
    "### 2. Choosing Smoothing Parameters\n",
    "- **var_smoothing** (Gaussian): Start with 1e-9, increase if numerical issues occur\n",
    "- **alpha** (Multinomial): Use cross-validation to find optimal value; 1.0 is a good default\n",
    "\n",
    "### 3. When Naive Bayes Shines\n",
    "- Text classification (spam, sentiment, categorization)\n",
    "- High-dimensional data with many features\n",
    "- When you need a quick baseline model\n",
    "- When training data is limited\n",
    "\n",
    "### 4. When to Consider Alternatives\n",
    "- When features are highly correlated\n",
    "- When decision boundaries are complex\n",
    "- When probability estimates need to be well-calibrated\n",
    "\n",
    "### 5. Common Mistakes to Avoid\n",
    "- Forgetting to use log probabilities ‚Üí numerical underflow\n",
    "- Using Multinomial NB with negative feature values\n",
    "- Not applying smoothing ‚Üí zero probability issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Bayes Theorem Foundation**: How to use $P(y|X) \\propto P(X|y)P(y)$ for classification\n",
    "\n",
    "2. **Gaussian Naive Bayes**: \n",
    "   - Assumes continuous features follow Gaussian distributions\n",
    "   - Computes mean and variance per feature per class\n",
    "   - Uses variance smoothing for numerical stability\n",
    "\n",
    "3. **Multinomial Naive Bayes**:\n",
    "   - Best for count/frequency data (text classification)\n",
    "   - Uses Laplace smoothing to handle zero counts\n",
    "   - Feature probability: $P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha n}$\n",
    "\n",
    "4. **Numerical Stability**:\n",
    "   - Always use log probabilities to avoid underflow\n",
    "   - Convert multiplication to addition: $\\log(ab) = \\log(a) + \\log(b)$\n",
    "\n",
    "5. **The Naive Assumption**:\n",
    "   - Features are conditionally independent given the class\n",
    "   - This simplification makes computation tractable\n",
    "   - Often works well despite being unrealistic\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Naive Bayes is fast, simple, and effective for many tasks\n",
    "- Choose the right variant based on your data type\n",
    "- Smoothing parameters control the bias-variance tradeoff\n",
    "- Log probabilities are essential for numerical stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
