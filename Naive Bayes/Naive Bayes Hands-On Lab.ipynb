{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Naive%20Bayes/Naive%20Bayes%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Hands-On Lab\n",
    "\n",
    "In this hands-on lab, you will implement the Naive Bayes classifier from scratch, gaining a deep understanding of probabilistic classification. You'll work with both **Gaussian Naive Bayes** for continuous features and **Multinomial Naive Bayes** for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand Bayes Theorem**: Apply the fundamental formula for probabilistic inference\n",
    "2. **Implement Gaussian Naive Bayes**: Build a classifier for continuous features from scratch\n",
    "3. **Implement Multinomial Naive Bayes**: Create a text classifier using bag-of-words representation\n",
    "4. **Handle numerical stability**: Use log probabilities to avoid numerical underflow\n",
    "5. **Apply Laplace smoothing**: Prevent zero probability issues in classification\n",
    "6. **Visualize decision boundaries**: Understand how Naive Bayes separates classes\n",
    "7. **Compare with scikit-learn**: Validate your implementation against the library version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Overview\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes Theorem**:\n",
    "\n",
    "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
    "\n",
    "Where:\n",
    "- $P(y|X)$ is the **posterior probability** - probability of class $y$ given features $X$\n",
    "- $P(X|y)$ is the **likelihood** - probability of features $X$ given class $y$\n",
    "- $P(y)$ is the **prior probability** - probability of class $y$ before seeing the data\n",
    "- $P(X)$ is the **evidence** - probability of the features (normalizing constant)\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "The \"naive\" in Naive Bayes comes from the **conditional independence assumption**: given the class label, all features are assumed to be independent of each other.\n",
    "\n",
    "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y) = \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "This simplification makes the algorithm computationally efficient and surprisingly effective in practice.\n",
    "\n",
    "### Classification Decision\n",
    "\n",
    "To classify a new sample, we compute:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "Since $P(X)$ is constant for all classes, we can ignore it for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "For **continuous features**, we assume each feature follows a Gaussian (normal) distribution within each class:\n",
    "\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_{y,i}$ is the mean of feature $i$ for class $y$\n",
    "- $\\sigma_{y,i}^2$ is the variance of feature $i$ for class $y$\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "\n",
    "For **count data** (like word frequencies in text), we use the multinomial distribution:\n",
    "\n",
    "$$P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{y,i}$ is the count of feature $i$ in class $y$\n",
    "- $N_y$ is the total count of all features in class $y$\n",
    "- $\\alpha$ is the Laplace smoothing parameter\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Stability: Log Probabilities\n",
    "\n",
    "Multiplying many small probabilities can lead to **numerical underflow**. To avoid this, we work with **log probabilities**:\n",
    "\n",
    "$$\\log P(y|X) \\propto \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i|y)$$\n",
    "\n",
    "For Gaussian likelihood, the log probability becomes:\n",
    "\n",
    "$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Naive Bayes\n",
    "\n",
    "| Strengths | Limitations |\n",
    "|-----------|-------------|\n",
    "| Very fast training and prediction | Assumes feature independence (rarely true) |\n",
    "| Works well with high-dimensional data | Cannot learn feature interactions |\n",
    "| Performs well with small training sets | Continuous features may not follow Gaussian |\n",
    "| Handles missing values naturally | Probability estimates can be poor |\n",
    "| Resistant to irrelevant features | Sensitive to feature scaling (Gaussian NB) |\n",
    "| Excellent for text classification | May be outperformed by other algorithms |\n",
    "\n",
    "### Best Use Cases\n",
    "\n",
    "- **Text classification**: Spam filtering, sentiment analysis, document categorization\n",
    "- **Medical diagnosis**: When features are conditionally independent given disease\n",
    "- **Real-time prediction**: When speed is critical\n",
    "- **Baseline model**: Quick benchmark before trying complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode: Gaussian Naive Bayes\n",
    "\n",
    "```\n",
    "TRAINING:\n",
    "1. For each class y in classes:\n",
    "   a. Calculate prior: P(y) = count(y) / total_samples\n",
    "   b. For each feature i:\n",
    "      - Calculate mean: Œº_yi = mean of feature i where class = y\n",
    "      - Calculate variance: œÉ¬≤_yi = variance of feature i where class = y\n",
    "\n",
    "PREDICTION:\n",
    "1. For each class y:\n",
    "   a. Start with log_prob = log(P(y))  # log prior\n",
    "   b. For each feature i:\n",
    "      - Add log(P(x_i|y)) using Gaussian PDF\n",
    "   c. Store total log_prob for class y\n",
    "2. Return class with highest log probability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Gaussian Naive Bayes Implementation\n",
    "\n",
    "Let's implement Gaussian Naive Bayes step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB as SklearnGaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB as SklearnMultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 2D classification dataset for visualization\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Calculate Class Statistics\n",
    "\n",
    "In this exercise, you'll implement methods to calculate the **prior probabilities** and **class statistics** (mean and variance) needed for Gaussian Naive Bayes.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Calculate the prior probability for each class\n",
    "2. Calculate the mean of each feature for each class\n",
    "3. Calculate the variance of each feature for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, default=1e-9\n",
    "        Portion of the largest variance of all features added to variances\n",
    "        for numerical stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None      # Prior probabilities for each class\n",
    "        self.theta_ = None       # Mean of each feature per class\n",
    "        self.var_ = None         # Variance of each feature per class\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the prior probability of each class.\n",
    "        \n",
    "        Prior P(y) = count(y) / total_samples\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        priors : array of shape (n_classes,)\n",
    "            Prior probability for each class.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the prior probability for each class\n",
    "        # Hint: For each class, divide the count of samples in that class\n",
    "        # by the total number of samples\n",
    "        \n",
    "        priors = None  # Replace with your implementation\n",
    "        \n",
    "        return priors\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate mean and variance of each feature for each class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        theta : array of shape (n_classes, n_features)\n",
    "            Mean of each feature per class.\n",
    "        var : array of shape (n_classes, n_features)\n",
    "            Variance of each feature per class.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # TODO: For each class, calculate the mean and variance of each feature\n",
    "        # Hint: Filter X to only include samples of each class, then compute statistics\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Get samples belonging to class c\n",
    "            X_c = None  # TODO: Filter X for samples where y == c\n",
    "            \n",
    "            # Calculate mean for each feature\n",
    "            theta[idx, :] = None  # TODO: Calculate mean along axis 0\n",
    "            \n",
    "            # Calculate variance for each feature\n",
    "            var[idx, :] = None  # TODO: Calculate variance along axis 0\n",
    "        \n",
    "        return theta, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 1\n",
    "\n",
    "Run this cell to verify your implementation of priors and class statistics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the priors and class statistics calculation\n",
    "gnb_test = GaussianNaiveBayes()\n",
    "gnb_test.classes_ = np.unique(y_train)\n",
    "\n",
    "# Test priors\n",
    "priors = gnb_test._calculate_priors(y_train)\n",
    "print(\"Prior Probabilities:\")\n",
    "if priors is not None:\n",
    "    for i, c in enumerate(gnb_test.classes_):\n",
    "        print(f\"  P(y={c}) = {priors[i]:.4f}\")\n",
    "    \n",
    "    # Verify priors sum to 1\n",
    "    assert np.isclose(priors.sum(), 1.0), \"Priors should sum to 1!\"\n",
    "    print(\"\\n‚úì Priors sum to 1.0\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test class statistics\n",
    "theta, var = gnb_test._calculate_class_statistics(X_train, y_train)\n",
    "print(\"Class Statistics:\")\n",
    "if theta is not None and var is not None:\n",
    "    for i, c in enumerate(gnb_test.classes_):\n",
    "        print(f\"\\nClass {c}:\")\n",
    "        print(f\"  Mean (Œ∏): {theta[i]}\")\n",
    "        print(f\"  Variance (œÉ¬≤): {var[i]}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert theta.shape == (len(gnb_test.classes_), X_train.shape[1]), \"Theta shape incorrect!\"\n",
    "    assert var.shape == (len(gnb_test.classes_), X_train.shape[1]), \"Variance shape incorrect!\"\n",
    "    print(\"\\n‚úì Class statistics shapes are correct\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 1 Solution</summary>\n",
    "\n",
    "```python\n",
    "def _calculate_priors(self, y):\n",
    "    priors = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    return priors\n",
    "\n",
    "def _calculate_class_statistics(self, X, y):\n",
    "    n_features = X.shape[1]\n",
    "    n_classes = len(self.classes_)\n",
    "    \n",
    "    theta = np.zeros((n_classes, n_features))\n",
    "    var = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    for idx, c in enumerate(self.classes_):\n",
    "        # Get samples belonging to class c\n",
    "        X_c = X[y == c]\n",
    "        \n",
    "        # Calculate mean for each feature\n",
    "        theta[idx, :] = X_c.mean(axis=0)\n",
    "        \n",
    "        # Calculate variance for each feature\n",
    "        var[idx, :] = X_c.var(axis=0)\n",
    "    \n",
    "    return theta, var\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Priors**: For each class, we count how many samples belong to that class and divide by total samples\n",
    "- **Mean (Œ∏)**: Average value of each feature for samples in each class\n",
    "- **Variance (œÉ¬≤)**: Spread of each feature for samples in each class\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Calculate Gaussian Log-Likelihood\n",
    "\n",
    "Now implement the method to calculate the **log-likelihood** of observing features given a class, using the Gaussian probability density function.\n",
    "\n",
    "**Formula:**\n",
    "$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"Calculate prior probabilities.\"\"\"\n",
    "        priors = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "        return priors\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"Calculate mean and variance for each class.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            theta[idx, :] = X_c.mean(axis=0)\n",
    "            var[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return theta, var\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood of X for each class using Gaussian PDF.\n",
    "        \n",
    "        Log P(x_i|y) = -0.5 * log(2œÄ * œÉ¬≤) - (x_i - Œº)¬≤ / (2œÉ¬≤)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : array of shape (n_samples, n_classes)\n",
    "            Log-likelihood for each sample and each class.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        log_likelihood = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        # TODO: Calculate log-likelihood for each class\n",
    "        # For each class:\n",
    "        # 1. Calculate the log of the Gaussian PDF for each feature\n",
    "        # 2. Sum across features (naive assumption - features are independent)\n",
    "        \n",
    "        for idx in range(n_classes):\n",
    "            # Get mean and variance for this class\n",
    "            mean = self.theta_[idx]  # shape: (n_features,)\n",
    "            var = self.var_[idx]     # shape: (n_features,)\n",
    "            \n",
    "            # TODO: Calculate log-likelihood using Gaussian PDF formula\n",
    "            # Hint: Use np.log for logarithm, np.pi for œÄ\n",
    "            # The formula is: -0.5 * log(2œÄ * œÉ¬≤) - (x - Œº)¬≤ / (2œÉ¬≤)\n",
    "            # Sum across features for each sample\n",
    "            \n",
    "            log_likelihood[:, idx] = None  # Replace with your implementation\n",
    "        \n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 2\n",
    "\n",
    "Run this cell to verify your log-likelihood implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test log-likelihood calculation\n",
    "gnb_test = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb_test.classes_ = np.unique(y_train)\n",
    "gnb_test.theta_, gnb_test.var_ = gnb_test._calculate_class_statistics(X_train, y_train)\n",
    "\n",
    "# Add smoothing to variance\n",
    "gnb_test.var_ = gnb_test.var_ + gnb_test.var_smoothing\n",
    "\n",
    "# Calculate log-likelihood for test samples\n",
    "log_likelihood = gnb_test._calculate_log_likelihood(X_test[:5])\n",
    "\n",
    "print(\"Log-Likelihood for first 5 test samples:\")\n",
    "if log_likelihood is not None and not np.any(log_likelihood == None):\n",
    "    print(f\"Shape: {log_likelihood.shape}\")\n",
    "    print(f\"\\nLog-likelihood values:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  Sample {i}: Class 0 = {log_likelihood[i, 0]:.4f}, Class 1 = {log_likelihood[i, 1]:.4f}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert log_likelihood.shape == (5, 2), \"Log-likelihood shape incorrect!\"\n",
    "    # Verify no NaN or Inf values\n",
    "    assert not np.any(np.isnan(log_likelihood)), \"Log-likelihood contains NaN!\"\n",
    "    assert not np.any(np.isinf(log_likelihood)), \"Log-likelihood contains Inf!\"\n",
    "    print(\"\\n‚úì Log-likelihood implementation looks correct\")\n",
    "else:\n",
    "    print(\"  Not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 2 Solution</summary>\n",
    "\n",
    "```python\n",
    "def _calculate_log_likelihood(self, X):\n",
    "    n_samples = X.shape[0]\n",
    "    n_classes = len(self.classes_)\n",
    "    \n",
    "    log_likelihood = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for idx in range(n_classes):\n",
    "        mean = self.theta_[idx]\n",
    "        var = self.var_[idx]\n",
    "        \n",
    "        # Log of Gaussian PDF: -0.5 * log(2œÄ * œÉ¬≤) - (x - Œº)¬≤ / (2œÉ¬≤)\n",
    "        # Sum across features (naive assumption)\n",
    "        log_likelihood[:, idx] = np.sum(\n",
    "            -0.5 * np.log(2 * np.pi * var) - ((X - mean) ** 2) / (2 * var),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return log_likelihood\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- We compute the log of the Gaussian PDF for each feature\n",
    "- The naive assumption allows us to sum log-probabilities across features\n",
    "- Broadcasting handles the vectorized computation efficiently\n",
    "- `axis=1` sums across features for each sample\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Complete the Gaussian Naive Bayes Classifier\n",
    "\n",
    "Now implement the complete `fit` and `predict` methods to finish the Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    \n",
    "    def _calculate_priors(self, y):\n",
    "        \"\"\"Calculate prior probabilities.\"\"\"\n",
    "        return np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    \n",
    "    def _calculate_class_statistics(self, X, y):\n",
    "        \"\"\"Calculate mean and variance for each class.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        theta = np.zeros((n_classes, n_features))\n",
    "        var = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            theta[idx, :] = X_c.mean(axis=0)\n",
    "            var[idx, :] = X_c.var(axis=0)\n",
    "        \n",
    "        return theta, var\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"Calculate log-likelihood using Gaussian PDF.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        log_likelihood = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for idx in range(n_classes):\n",
    "            mean = self.theta_[idx]\n",
    "            var = self.var_[idx]\n",
    "            log_likelihood[:, idx] = np.sum(\n",
    "                -0.5 * np.log(2 * np.pi * var) - ((X - mean) ** 2) / (2 * var),\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the fit method\n",
    "        # 1. Store unique classes\n",
    "        # 2. Calculate prior probabilities\n",
    "        # 3. Calculate class statistics (mean and variance)\n",
    "        # 4. Apply variance smoothing for numerical stability\n",
    "        \n",
    "        # Store unique classes\n",
    "        self.classes_ = None  # TODO\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors_ = None  # TODO\n",
    "        \n",
    "        # Calculate class statistics\n",
    "        self.theta_, self.var_ = None, None  # TODO\n",
    "        \n",
    "        # Apply variance smoothing\n",
    "        # TODO: Add var_smoothing to variance to prevent division by zero\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the predict method\n",
    "        # 1. Calculate log priors\n",
    "        # 2. Calculate log likelihoods\n",
    "        # 3. Combine: log_posterior ‚àù log_prior + log_likelihood\n",
    "        # 4. Return the class with highest log posterior for each sample\n",
    "        \n",
    "        # Calculate log priors (same for all samples)\n",
    "        log_priors = None  # TODO: Use np.log on priors\n",
    "        \n",
    "        # Calculate log likelihoods\n",
    "        log_likelihood = None  # TODO\n",
    "        \n",
    "        # Combine log prior and log likelihood\n",
    "        log_posterior = None  # TODO: Add log_priors to log_likelihood\n",
    "        \n",
    "        # Return class with highest log posterior\n",
    "        return None  # TODO: Use self.classes_ and np.argmax\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : array of shape (n_samples, n_classes)\n",
    "            Probability of each class for each sample.\n",
    "        \"\"\"\n",
    "        log_priors = np.log(self.priors_)\n",
    "        log_likelihood = self._calculate_log_likelihood(X)\n",
    "        log_posterior = log_priors + log_likelihood\n",
    "        \n",
    "        # Convert log probabilities to probabilities using softmax\n",
    "        # Subtract max for numerical stability\n",
    "        log_posterior = log_posterior - np.max(log_posterior, axis=1, keepdims=True)\n",
    "        posterior = np.exp(log_posterior)\n",
    "        return posterior / posterior.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 3\n",
    "\n",
    "Run this cell to verify your complete Gaussian Naive Bayes implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete implementation\n",
    "gnb = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "if y_pred is not None:\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    sklearn_gnb = SklearnGaussianNB(var_smoothing=1e-9)\n",
    "    sklearn_gnb.fit(X_train, y_train)\n",
    "    sklearn_pred = sklearn_gnb.predict(X_test)\n",
    "    sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "    \n",
    "    print(f\"\\nScikit-learn GaussianNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "    \n",
    "    if np.isclose(accuracy, sklearn_accuracy, atol=0.01):\n",
    "        print(\"\\n‚úì Your implementation matches scikit-learn!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö† Accuracy differs from sklearn by {abs(accuracy - sklearn_accuracy):.4f}\")\n",
    "else:\n",
    "    print(\"Prediction not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 3 Solution</summary>\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    # Store unique classes\n",
    "    self.classes_ = np.unique(y)\n",
    "    \n",
    "    # Calculate priors\n",
    "    self.priors_ = self._calculate_priors(y)\n",
    "    \n",
    "    # Calculate class statistics\n",
    "    self.theta_, self.var_ = self._calculate_class_statistics(X, y)\n",
    "    \n",
    "    # Apply variance smoothing for numerical stability\n",
    "    self.var_ = self.var_ + self.var_smoothing\n",
    "    \n",
    "    return self\n",
    "\n",
    "def predict(self, X):\n",
    "    # Calculate log priors\n",
    "    log_priors = np.log(self.priors_)\n",
    "    \n",
    "    # Calculate log likelihoods\n",
    "    log_likelihood = self._calculate_log_likelihood(X)\n",
    "    \n",
    "    # Combine: log_posterior ‚àù log_prior + log_likelihood\n",
    "    log_posterior = log_priors + log_likelihood\n",
    "    \n",
    "    # Return class with highest log posterior\n",
    "    return self.classes_[np.argmax(log_posterior, axis=1)]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **fit**: Stores classes, computes priors, means, variances, and adds smoothing\n",
    "- **predict**: Computes log posterior = log prior + log likelihood, returns argmax class\n",
    "- Using log probabilities avoids numerical underflow from multiplying small numbers\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classifier.\n",
    "    \"\"\"\n",
    "    h = 0.02  # Step size\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', \n",
    "                label='Class 0', edgecolors='k', alpha=0.7)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', \n",
    "                label='Class 1', edgecolors='k', alpha=0.7)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for our implementation\n",
    "if y_pred is not None:\n",
    "    plot_decision_boundary(gnb, X_train, y_train, \n",
    "                          \"Gaussian Naive Bayes Decision Boundary (Our Implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Impact of Outliers on Gaussian Naive Bayes\n\nGaussian Naive Bayes estimates the mean (Œº) and variance (œÉ¬≤) of each feature for each class. Since these statistics are sensitive to extreme values, **outliers can significantly distort the model's decision boundary**.\n\nLet's visualize how outliers affect Gaussian NB:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate the impact of outliers on Gaussian Naive Bayes\nnp.random.seed(42)\n\n# Create clean data\nn_samples = 100\nX_clean_0 = np.random.randn(n_samples, 2) + np.array([-2, -2])\nX_clean_1 = np.random.randn(n_samples, 2) + np.array([2, 2])\nX_clean = np.vstack([X_clean_0, X_clean_1])\ny_clean = np.array([0] * n_samples + [1] * n_samples)\n\n# Create data with outliers (add extreme points to class 0)\nX_with_outliers = X_clean.copy()\noutliers = np.array([[8, 8], [9, 7], [7, 9]])  # Extreme outliers in class 0\nX_with_outliers = np.vstack([X_with_outliers, outliers])\ny_with_outliers = np.append(y_clean, [0, 0, 0])\n\n# Train models\ngnb_clean = GaussianNaiveBayes(var_smoothing=1e-9)\ngnb_clean.fit(X_clean, y_clean)\n\ngnb_outliers = GaussianNaiveBayes(var_smoothing=1e-9)\ngnb_outliers.fit(X_with_outliers, y_with_outliers)\n\n# Visualization function for comparison\ndef plot_gnb_comparison(X1, y1, model1, title1, X2, y2, model2, title2):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    for ax, X, y, model, title in [(axes[0], X1, y1, model1, title1), \n                                    (axes[1], X2, y2, model2, title2)]:\n        h = 0.1\n        x_min, x_max = -6, 12\n        y_min, y_max = -6, 12\n        \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n        \n        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        \n        ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n        ax.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', \n                   label='Class 0', edgecolors='k', alpha=0.7)\n        ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', \n                   label='Class 1', edgecolors='k', alpha=0.7)\n        \n        # Mark outliers\n        if title == title2:\n            ax.scatter(outliers[:, 0], outliers[:, 1], c='blue', s=200, \n                      marker='*', edgecolors='yellow', linewidths=2, label='Outliers')\n        \n        ax.set_xlabel('Feature 1')\n        ax.set_ylabel('Feature 2')\n        ax.set_title(title)\n        ax.legend()\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_gnb_comparison(X_clean, y_clean, gnb_clean, 'Clean Data (No Outliers)',\n                   X_with_outliers, y_with_outliers, gnb_outliers, 'Data with Outliers')\n\n# Print statistics comparison\nprint(\"Class 0 Statistics Comparison:\")\nprint(f\"  Without outliers - Mean: {gnb_clean.theta_[0]}, Var: {gnb_clean.var_[0]}\")\nprint(f\"  With outliers    - Mean: {gnb_outliers.theta_[0]}, Var: {gnb_outliers.var_[0]}\")\nprint(f\"\\nNotice how outliers shift the mean and inflate the variance of Class 0!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Observations on Outliers\n\n**Effects of outliers on Gaussian NB:**\n\n1. **Mean distortion**: Outliers pull the class mean toward them, shifting the decision boundary\n2. **Variance inflation**: Outliers increase the variance estimate, making the Gaussian distribution \"wider\"\n3. **Decision boundary shift**: The combined effect can cause significant misclassification of normal points\n\n**Mitigation strategies:**\n\n| Strategy | Description |\n|----------|-------------|\n| **Outlier removal** | Remove points beyond k standard deviations |\n| **Robust statistics** | Use median and MAD instead of mean and variance |\n| **Feature transformation** | Apply log transform or winsorization |\n| **Different model** | Consider models less sensitive to outliers |\n\n> **Note**: Gaussian NB is particularly vulnerable because both mean and variance are affected. Compare this to k-NN, where only nearby points influence predictions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Choice Questions: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 1\n\nWhat does the \"naive\" assumption in Naive Bayes refer to?\n\nA) Features are assumed to be independent given the class label  \nB) Features are assumed to be identically distributed across all classes  \nC) Each feature contributes equally to the classification decision  \nD) The prior probabilities are assumed to be equal for all classes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: A) Features are assumed to be independent given the class label**\n\nThe \"naive\" assumption refers to **conditional independence**: given the class label, all features are assumed to be independent of each other. Mathematically: P(X|y) = ‚àèP(x·µ¢|y). This allows us to compute P(X|y) by simply multiplying individual feature probabilities.\n\n**Why other answers are incorrect:**\n\n- **B) Features are assumed to be identically distributed across all classes**: This is incorrect. Naive Bayes explicitly models *different* distributions for each class - that's the whole point. The mean and variance (Gaussian NB) or word frequencies (Multinomial NB) are computed separately for each class, allowing the model to distinguish between classes.\n\n- **C) Each feature contributes equally to the classification decision**: This is incorrect. Features can have very different contributions depending on their discriminative power. A feature with large differences in mean/variance between classes will influence the decision more than one with similar statistics across classes.\n\n- **D) The prior probabilities are assumed to be equal for all classes**: This is incorrect. Naive Bayes explicitly computes and uses prior probabilities P(y) from the training data. Class imbalance is handled through these priors, giving more weight to more frequent classes.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2\n\nWhy do we use log probabilities instead of raw probabilities in Naive Bayes?\n\nA) Log transformation normalizes the feature distributions to be Gaussian  \nB) Logarithms convert the product of probabilities into a sum, preventing numerical underflow  \nC) Log probabilities allow the model to handle negative feature values  \nD) Using logs reduces the computational complexity from O(n¬≤) to O(n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) Logarithms convert the product of probabilities into a sum, preventing numerical underflow**\n\nWhen multiplying many small probabilities (like P(x‚ÇÅ|y) √ó P(x‚ÇÇ|y) √ó ... √ó P(x‚Çô|y)), the result can become astronomically small (e.g., 10‚Åª¬≥‚Å∞‚Å∞), causing numerical underflow to zero. Using log probabilities:\n- Converts multiplication to addition: log(a √ó b) = log(a) + log(b)\n- Keeps values in a manageable numerical range\n- Preserves the relative ordering needed for classification (log is monotonic)\n\n**Why other answers are incorrect:**\n\n- **A) Log transformation normalizes the feature distributions to be Gaussian**: This is incorrect. Log transformation of probabilities has nothing to do with making features Gaussian. Log transformations of *features* (not probabilities) can sometimes help with skewed data, but that's a different concept. The log transformation here is applied to the computed probabilities, not the input features.\n\n- **C) Log probabilities allow the model to handle negative feature values**: This is incorrect. Log probabilities are about the probability values (which are always positive), not about handling negative features. Gaussian NB can handle negative features naturally, while Multinomial NB requires non-negative counts regardless of whether logs are used.\n\n- **D) Using logs reduces the computational complexity from O(n¬≤) to O(n)**: This is incorrect. The computational complexity remains the same - O(n) for n features. The log transformation is applied element-wise and doesn't change the algorithmic complexity. It's purely a numerical stability technique, not an optimization.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 3\n\nWhat is the purpose of `var_smoothing` in Gaussian Naive Bayes?\n\nA) To add regularization that prevents overfitting to the training data  \nB) To ensure numerical stability when variance is very small or zero  \nC) To standardize features to have unit variance before training  \nD) To control the trade-off between model complexity and generalization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) To ensure numerical stability when variance is very small or zero**\n\n`var_smoothing` adds a small value (typically 1e-9) to the computed variance of each feature. In the Gaussian PDF formula, we divide by variance (œÉ¬≤). If variance is zero or very close to zero (which can happen if all samples of a class have the same feature value), this causes division by zero or numerical overflow.\n\n**Why other answers are incorrect:**\n\n- **A) To add regularization that prevents overfitting to the training data**: While smoothing can have a mild regularization effect, this is not its primary purpose in Gaussian NB. True regularization would systematically bias the model toward simpler solutions. var_smoothing is primarily a numerical fix - the value 1e-9 is too small to meaningfully regularize. Contrast with Laplace smoothing (alpha) in Multinomial NB, which does serve as explicit regularization.\n\n- **C) To standardize features to have unit variance before training**: This is incorrect. var_smoothing does not standardize or normalize features - it modifies the *computed* variance by adding a small constant, not scaling features. Feature standardization would involve transforming X to have zero mean and unit variance, which is a preprocessing step separate from var_smoothing.\n\n- **D) To control the trade-off between model complexity and generalization**: This describes hyperparameter tuning for regularization (like the alpha parameter in Multinomial NB). var_smoothing's default value (1e-9) is chosen for numerical stability, not as a tunable complexity control. While you *can* increase it for smoothing effects, this is not its designed purpose.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Multinomial Naive Bayes for Text Classification\n",
    "\n",
    "Now let's implement **Multinomial Naive Bayes**, which is commonly used for text classification with word count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Example\n",
    "\n",
    "We'll classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews dataset\n",
    "reviews = [\n",
    "    \"This movie was fantastic and amazing\",\n",
    "    \"Great film with excellent acting\",\n",
    "    \"Wonderful story and brilliant performance\",\n",
    "    \"I loved this movie so much\",\n",
    "    \"Best movie I have ever seen\",\n",
    "    \"Outstanding cinematography and plot\",\n",
    "    \"Terrible movie waste of time\",\n",
    "    \"Awful film with bad acting\",\n",
    "    \"Boring and disappointing story\",\n",
    "    \"I hated this movie completely\",\n",
    "    \"Worst movie ever made\",\n",
    "    \"Poor direction and terrible script\",\n",
    "    \"Amazing performances by all actors\",\n",
    "    \"A masterpiece of modern cinema\",\n",
    "    \"Dreadful experience awful waste\",\n",
    "    \"Horrible plot and bad dialogue\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    reviews, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_text_train)}\")\n",
    "print(f\"Test samples: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_text_train).toarray()\n",
    "X_test_bow = vectorizer.transform(X_text_test).toarray()\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nVocabulary: {vectorizer.get_feature_names_out()}\")\n",
    "print(f\"\\nBag-of-words shape: {X_train_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Implement Multinomial Naive Bayes\n",
    "\n",
    "Implement the Multinomial Naive Bayes classifier with **Laplace smoothing**.\n",
    "\n",
    "**Formula for feature likelihood:**\n",
    "$$P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "Where:\n",
    "- $N_{y,i}$ = count of feature $i$ in class $y$\n",
    "- $N_y$ = total count of all features in class $y$\n",
    "- $\\alpha$ = smoothing parameter (usually 1 for Laplace smoothing)\n",
    "- $n$ = number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes classifier for text classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, default=1.0\n",
    "        Laplace smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.feature_log_prob_ = None  # Log probability of features given class\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Multinomial Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data (word counts).\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors_ = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "        \n",
    "        # Calculate feature log probabilities with Laplace smoothing\n",
    "        self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # TODO: Calculate P(x_i|y) for each feature and class using Laplace smoothing\n",
    "        # Formula: P(x_i|y) = (N_yi + alpha) / (N_y + alpha * n_features)\n",
    "        # Then take log for numerical stability\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # Get samples belonging to class c\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # TODO: Calculate N_yi (sum of feature i across all samples in class c)\n",
    "            feature_counts = None  # Sum along axis 0\n",
    "            \n",
    "            # TODO: Calculate N_y (total count of all features in class c)\n",
    "            total_count = None  # Sum of all feature counts\n",
    "            \n",
    "            # TODO: Apply Laplace smoothing and calculate log probabilities\n",
    "            # P(x_i|y) = (feature_counts + alpha) / (total_count + alpha * n_features)\n",
    "            self.feature_log_prob_[idx, :] = None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate log posterior for each class\n",
    "        # log_posterior = log_prior + sum(x_i * log_P(x_i|y))\n",
    "        \n",
    "        log_priors = np.log(self.priors_)\n",
    "        \n",
    "        # TODO: Calculate log likelihood using feature counts and log probabilities\n",
    "        # Hint: Use matrix multiplication X @ self.feature_log_prob_.T\n",
    "        log_likelihood = None\n",
    "        \n",
    "        # TODO: Calculate log posterior\n",
    "        log_posterior = None\n",
    "        \n",
    "        # Return class with highest log posterior\n",
    "        return self.classes_[np.argmax(log_posterior, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multinomial Naive Bayes\n",
    "mnb = MultinomialNaiveBayes(alpha=1.0)\n",
    "mnb.fit(X_train_bow, np.array(y_text_train))\n",
    "\n",
    "# Make predictions\n",
    "y_text_pred = mnb.predict(X_test_bow)\n",
    "\n",
    "if y_text_pred is not None:\n",
    "    accuracy = accuracy_score(y_text_test, y_text_pred)\n",
    "    print(f\"Multinomial Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nPredictions vs Actual:\")\n",
    "    for review, actual, pred in zip(X_text_test, y_text_test, y_text_pred):\n",
    "        sentiment_actual = \"Positive\" if actual == 1 else \"Negative\"\n",
    "        sentiment_pred = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        match = \"‚úì\" if actual == pred else \"‚úó\"\n",
    "        print(f\"  {match} '{review[:40]}...' - Actual: {sentiment_actual}, Predicted: {sentiment_pred}\")\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    sklearn_mnb = SklearnMultinomialNB(alpha=1.0)\n",
    "    sklearn_mnb.fit(X_train_bow, np.array(y_text_train))\n",
    "    sklearn_pred = sklearn_mnb.predict(X_test_bow)\n",
    "    sklearn_accuracy = accuracy_score(y_text_test, sklearn_pred)\n",
    "    \n",
    "    print(f\"\\nScikit-learn MultinomialNB Accuracy: {sklearn_accuracy:.4f}\")\n",
    "    \n",
    "    if np.allclose(y_text_pred, sklearn_pred):\n",
    "        print(\"\\n‚úì Your implementation matches scikit-learn!\")\n",
    "else:\n",
    "    print(\"Prediction not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer; font-weight: bold;\">üí° Click here for Exercise 4 Solution</summary>\n",
    "\n",
    "```python\n",
    "def fit(self, X, y):\n",
    "    self.classes_ = np.unique(y)\n",
    "    n_classes = len(self.classes_)\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Calculate priors\n",
    "    self.priors_ = np.array([np.sum(y == c) / len(y) for c in self.classes_])\n",
    "    \n",
    "    # Calculate feature log probabilities with Laplace smoothing\n",
    "    self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    for idx, c in enumerate(self.classes_):\n",
    "        X_c = X[y == c]\n",
    "        \n",
    "        # N_yi: sum of feature i across all samples in class c\n",
    "        feature_counts = X_c.sum(axis=0)\n",
    "        \n",
    "        # N_y: total count of all features in class c\n",
    "        total_count = feature_counts.sum()\n",
    "        \n",
    "        # Apply Laplace smoothing and calculate log probabilities\n",
    "        self.feature_log_prob_[idx, :] = np.log(\n",
    "            (feature_counts + self.alpha) / (total_count + self.alpha * n_features)\n",
    "        )\n",
    "    \n",
    "    return self\n",
    "\n",
    "def predict(self, X):\n",
    "    log_priors = np.log(self.priors_)\n",
    "    \n",
    "    # Log likelihood: sum of (x_i * log P(x_i|y))\n",
    "    log_likelihood = X @ self.feature_log_prob_.T\n",
    "    \n",
    "    # Log posterior\n",
    "    log_posterior = log_priors + log_likelihood\n",
    "    \n",
    "    return self.classes_[np.argmax(log_posterior, axis=1)]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Laplace smoothing**: Adds Œ± to each count to avoid zero probabilities for unseen words\n",
    "- **Feature counts**: Sum of each word's frequency across all documents in a class\n",
    "- **Log likelihood**: For count data, we multiply log probabilities by word counts\n",
    "- Matrix multiplication `X @ feature_log_prob_.T` efficiently computes the sum\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiple Choice Questions: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 4\n\nWhat problem does Laplace smoothing (alpha) solve in Multinomial Naive Bayes?\n\nA) It handles the case where a word appears in test data but not in any training documents  \nB) It prevents zero probabilities when a word never appears in documents of a particular class  \nC) It removes stop words that appear too frequently across all documents  \nD) It corrects for the different document lengths in the training corpus"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) It prevents zero probabilities when a word never appears in documents of a particular class**\n\nWithout smoothing, if a word never appears in training documents of a particular class, P(word|class) = 0. When we multiply probabilities, any zero makes the entire product zero, causing that class to be immediately ruled out regardless of other evidence. Laplace smoothing adds Œ± (usually 1) to each word count:\n\n```\nP(word|class) = (count + Œ±) / (total + Œ± √ó vocabulary_size)\n```\n\n**Why other answers are incorrect:**\n\n- **A) It handles the case where a word appears in test data but not in any training documents**: This is a different problem called \"out-of-vocabulary\" (OOV). Laplace smoothing doesn't solve this - if a word isn't in the vocabulary at all, it simply gets ignored during prediction. You would need techniques like unknown word tokens or subword models to handle truly unseen words.\n\n- **C) It removes stop words that appear too frequently across all documents**: This is incorrect. Laplace smoothing doesn't remove any words; it adjusts probability calculations. Stop word removal is a separate preprocessing step. In fact, smoothing *increases* the relative probability of rare words compared to frequent ones, the opposite of what removing frequent words would do.\n\n- **D) It corrects for the different document lengths in the training corpus**: This is incorrect. Document length normalization is a separate concern, often handled by techniques like TF-IDF or dividing by document length. Laplace smoothing operates on word counts aggregated by class, not individual document lengths. The formula doesn't account for document length variation.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 5\n\nWhen would you choose Multinomial NB over Gaussian NB?\n\nA) When features represent word frequencies or document-term counts  \nB) When the features have high correlation with each other  \nC) When the dataset has many more samples than features  \nD) When you need well-calibrated probability estimates"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: A) When features represent word frequencies or document-term counts**\n\nMultinomial NB is designed for discrete count data, particularly text classification with bag-of-words features. It models the probability of word occurrences using the multinomial distribution.\n\n| Scenario | Best Choice |\n|----------|-------------|\n| Document/spam classification | Multinomial NB |\n| Sentiment analysis (word counts) | Multinomial NB |\n| Iris flower classification | Gaussian NB |\n| Sensor readings | Gaussian NB |\n\n**Why other answers are incorrect:**\n\n- **B) When the features have high correlation with each other**: This is incorrect. Neither variant of Naive Bayes handles correlated features well - both assume conditional independence. If your features are highly correlated, consider PCA for dimensionality reduction, or use models that can capture feature interactions (like Random Forests or neural networks).\n\n- **C) When the dataset has many more samples than features**: This is incorrect. The sample-to-feature ratio doesn't determine the choice between Multinomial and Gaussian NB. In fact, text classification (where Multinomial NB excels) often has *more features than samples* (high-dimensional vocabulary). The choice depends on the *nature* of the features (counts vs. continuous), not the dataset dimensions.\n\n- **D) When you need well-calibrated probability estimates**: This is incorrect. Neither Naive Bayes variant produces well-calibrated probabilities. Due to the independence assumption, Naive Bayes tends to produce overconfident predictions (probabilities pushed toward 0 or 1). For calibrated probabilities, apply Platt scaling or isotonic regression post-hoc, or use models like logistic regression.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 6\n\nWhat happens if we increase the smoothing parameter Œ± in Multinomial NB from 1.0 to 10.0?\n\nA) The model gives more weight to words that appear frequently in the training data  \nB) The model makes feature probabilities more uniform, reducing the influence of observed word counts  \nC) The model becomes more sensitive to rare words in the vocabulary  \nD) The model's training time increases significantly due to more complex calculations"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## TF-IDF vs Bag-of-Words for Text Classification\n\nSo far we've used **Bag-of-Words (BoW)** - raw word counts. However, **TF-IDF (Term Frequency-Inverse Document Frequency)** often provides better features for text classification.\n\n### The Problem with Raw Counts\n\nCommon words like \"the\", \"is\", \"and\" appear frequently in all documents but carry little discriminative information. Raw counts give these words high importance.\n\n### TF-IDF Solution\n\nTF-IDF weighs terms by:\n\n$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n\nWhere:\n- **TF(t, d)** = frequency of term t in document d\n- **IDF(t)** = log(N / df(t)) where N is total documents and df(t) is documents containing t\n\n**Key insight**: Words appearing in many documents get lower IDF weights, reducing the influence of common words.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare Bag-of-Words vs TF-IDF for text classification\n# Using a larger dataset to see the difference\n\n# Extended movie reviews for better comparison\nextended_reviews = [\n    \"This movie was fantastic and amazing\",\n    \"Great film with excellent acting\",\n    \"Wonderful story and brilliant performance\",\n    \"I loved this movie so much\",\n    \"Best movie I have ever seen\",\n    \"Outstanding cinematography and plot\",\n    \"Amazing performances by all actors\",\n    \"A masterpiece of modern cinema\",\n    \"Incredible film that I highly recommend\",\n    \"Superb acting and wonderful direction\",\n    \"Terrible movie waste of time\",\n    \"Awful film with bad acting\",\n    \"Boring and disappointing story\",\n    \"I hated this movie completely\",\n    \"Worst movie ever made\",\n    \"Poor direction and terrible script\",\n    \"Dreadful experience awful waste\",\n    \"Horrible plot and bad dialogue\",\n    \"The movie was so boring I fell asleep\",\n    \"Disappointing film with weak characters\"\n]\n\nextended_labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n# Split data\nX_ext_train, X_ext_test, y_ext_train, y_ext_test = train_test_split(\n    extended_reviews, extended_labels, test_size=0.3, random_state=42\n)\n\n# Bag-of-Words\nbow_vectorizer = CountVectorizer()\nX_train_bow_ext = bow_vectorizer.fit_transform(X_ext_train).toarray()\nX_test_bow_ext = bow_vectorizer.transform(X_ext_test).toarray()\n\n# TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_ext_train).toarray()\nX_test_tfidf = tfidf_vectorizer.transform(X_ext_test).toarray()\n\n# Train classifiers\nsklearn_mnb_bow = SklearnMultinomialNB(alpha=1.0)\nsklearn_mnb_bow.fit(X_train_bow_ext, np.array(y_ext_train))\n\nsklearn_mnb_tfidf = SklearnMultinomialNB(alpha=1.0)\nsklearn_mnb_tfidf.fit(X_train_tfidf, np.array(y_ext_train))\n\n# Compare results\nbow_accuracy = accuracy_score(y_ext_test, sklearn_mnb_bow.predict(X_test_bow_ext))\ntfidf_accuracy = accuracy_score(y_ext_test, sklearn_mnb_tfidf.predict(X_test_tfidf))\n\nprint(\"Comparison: Bag-of-Words vs TF-IDF\")\nprint(\"=\" * 50)\nprint(f\"Bag-of-Words Accuracy:  {bow_accuracy:.4f}\")\nprint(f\"TF-IDF Accuracy:        {tfidf_accuracy:.4f}\")\n\n# Visualize feature weights for a sample word\nprint(\"\\nFeature Representation Comparison (sample word: 'movie'):\")\nif 'movie' in bow_vectorizer.vocabulary_:\n    bow_idx = bow_vectorizer.vocabulary_['movie']\n    tfidf_idx = tfidf_vectorizer.vocabulary_['movie']\n    \n    print(f\"  BoW values for first 3 training docs:   {X_train_bow_ext[:3, bow_idx]}\")\n    print(f\"  TF-IDF values for first 3 training docs: {X_train_tfidf[:3, tfidf_idx].round(3)}\")\n    print(\"\\n  Notice: TF-IDF down-weights common words like 'movie' that appear in many documents\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### When to Use TF-IDF vs Bag-of-Words\n\n| Feature | Bag-of-Words | TF-IDF |\n|---------|--------------|--------|\n| **Representation** | Raw word counts | Weighted by term importance |\n| **Common words** | High values | Down-weighted |\n| **Rare but discriminative words** | Low values | Up-weighted |\n| **Best for** | Short texts, when word frequency matters | Longer documents, diverse vocabulary |\n| **Computational cost** | Lower | Slightly higher |\n\n> **Note**: TF-IDF values are continuous, so they work better with models that handle continuous features. For Multinomial NB (which expects counts), you may need to scale TF-IDF values appropriately.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Bias-Variance Tradeoff in Naive Bayes\n\nThe smoothing parameter (Œ± in Multinomial NB, var_smoothing in Gaussian NB) controls the **bias-variance tradeoff**:\n\n- **Low smoothing (Œ± ‚Üí 0)**: High variance, low bias\n  - Model closely follows training data\n  - Risk of overfitting, especially with sparse data\n  - Zero probabilities for unseen features\n  \n- **High smoothing (Œ± ‚Üí ‚àû)**: High bias, low variance\n  - Model approaches uniform probabilities\n  - Ignores training data evidence\n  - Underfitting - poor discrimination between classes\n\nLet's visualize this tradeoff:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize bias-variance tradeoff with different smoothing values\nfrom sklearn.model_selection import cross_val_score\n\n# Generate synthetic text-like data for demonstration\nnp.random.seed(42)\n\n# Create a more substantial dataset for meaningful cross-validation\nn_train = 100\nvocab_size = 50\n\n# Simulate word count data\nX_synthetic = np.random.poisson(lam=2, size=(n_train, vocab_size))\n# Add some class-specific signal\nclass_signal = np.zeros((n_train, vocab_size))\nclass_signal[:n_train//2, :10] = np.random.poisson(lam=3, size=(n_train//2, 10))\nclass_signal[n_train//2:, 10:20] = np.random.poisson(lam=3, size=(n_train//2, 10))\nX_synthetic = X_synthetic + class_signal\ny_synthetic = np.array([0] * (n_train//2) + [1] * (n_train//2))\n\n# Test different alpha values\nalphas_bv = np.logspace(-3, 2, 20)  # From 0.001 to 100\nmean_train_scores = []\nmean_cv_scores = []\nstd_cv_scores = []\n\nfor alpha in alphas_bv:\n    model = SklearnMultinomialNB(alpha=alpha)\n    \n    # Training score\n    model.fit(X_synthetic, y_synthetic)\n    train_score = model.score(X_synthetic, y_synthetic)\n    mean_train_scores.append(train_score)\n    \n    # Cross-validation score\n    cv_scores = cross_val_score(model, X_synthetic, y_synthetic, cv=5)\n    mean_cv_scores.append(cv_scores.mean())\n    std_cv_scores.append(cv_scores.std())\n\nmean_cv_scores = np.array(mean_cv_scores)\nstd_cv_scores = np.array(std_cv_scores)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Training vs CV accuracy\nax1 = axes[0]\nax1.semilogx(alphas_bv, mean_train_scores, 'b-', label='Training Accuracy', linewidth=2)\nax1.semilogx(alphas_bv, mean_cv_scores, 'r-', label='CV Accuracy', linewidth=2)\nax1.fill_between(alphas_bv, mean_cv_scores - std_cv_scores, \n                  mean_cv_scores + std_cv_scores, alpha=0.2, color='red')\nax1.axvline(x=alphas_bv[np.argmax(mean_cv_scores)], color='green', linestyle='--', \n             label=f'Best Œ± = {alphas_bv[np.argmax(mean_cv_scores)]:.3f}')\nax1.set_xlabel('Smoothing Parameter (Œ±)', fontsize=12)\nax1.set_ylabel('Accuracy', fontsize=12)\nax1.set_title('Bias-Variance Tradeoff in Multinomial NB', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim([0.4, 1.05])\n\n# Right plot: Gap between training and CV (indicator of overfitting)\nax2 = axes[1]\ngap = np.array(mean_train_scores) - np.array(mean_cv_scores)\nax2.semilogx(alphas_bv, gap, 'purple', linewidth=2)\nax2.fill_between(alphas_bv, 0, gap, alpha=0.3, color='purple')\nax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax2.set_xlabel('Smoothing Parameter (Œ±)', fontsize=12)\nax2.set_ylabel('Train - CV Accuracy (Overfitting Gap)', fontsize=12)\nax2.set_title('Overfitting Indicator', fontsize=14)\nax2.grid(True, alpha=0.3)\n\n# Add annotations\nax2.annotate('High Variance\\n(Overfitting)', xy=(0.005, 0.15), fontsize=10, ha='center')\nax2.annotate('High Bias\\n(Underfitting)', xy=(20, 0.02), fontsize=10, ha='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Optimal Œ± (highest CV accuracy): {alphas_bv[np.argmax(mean_cv_scores)]:.4f}\")\nprint(f\"Best CV Accuracy: {max(mean_cv_scores):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpreting the Bias-Variance Plot\n\n**Left plot (Training vs CV Accuracy):**\n- **Small Œ±**: High training accuracy but lower CV accuracy ‚Üí overfitting\n- **Large Œ±**: Both accuracies drop ‚Üí underfitting  \n- **Optimal Œ±**: Where CV accuracy is maximized (green line)\n\n**Right plot (Overfitting Gap):**\n- Large gap = high variance (overfitting)\n- Near-zero gap with low accuracy = high bias (underfitting)\n- Sweet spot: small gap with high overall accuracy\n\n**Practical advice:**\n1. Use cross-validation to find optimal Œ±\n2. Default Œ±=1.0 works well in most cases\n3. Smaller Œ± for large vocabularies, larger Œ± for small datasets",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary style=\"cursor: pointer; font-weight: bold;\">Click here for Answer</summary>\n\n**Answer: B) The model makes feature probabilities more uniform, reducing the influence of observed word counts**\n\nAs Œ± increases, the smoothing formula P(word|class) = (count + Œ±) / (total + Œ± √ó vocab_size) causes:\n- All feature probabilities to move closer to uniform (1/vocab_size)\n- The model relies less on observed training data\n- This increases **bias** but reduces **variance** (bias-variance tradeoff)\n\nExample: If a word appears 10 times in class A and 0 times in class B:\n- Œ± = 1: Strong preference for class A\n- Œ± = 100: Weak preference for class A (probabilities nearly equal)\n\n**Why other answers are incorrect:**\n\n- **A) The model gives more weight to words that appear frequently in the training data**: This is backwards. Higher Œ± actually *reduces* the relative weight of observed frequencies. With small Œ±, a word appearing 100 times has much higher probability than one appearing 10 times. With large Œ±, this difference shrinks. The formula shows: as Œ± ‚Üí ‚àû, all words approach equal probability.\n\n- **C) The model becomes more sensitive to rare words in the vocabulary**: This is incorrect. Higher Œ± makes the model *less* sensitive to all word frequencies, including rare ones. Rare words that might distinguish classes get \"washed out\" when Œ± is large. Lower Œ± values actually make the model more sensitive to rare but discriminative words.\n\n- **D) The model's training time increases significantly due to more complex calculations**: This is incorrect. The smoothing parameter doesn't affect computational complexity at all - it's just a constant added to counts. Training time depends on dataset size and vocabulary size, not Œ±. The formula remains O(n) regardless of Œ±'s value.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Effect of Smoothing Parameter\n",
    "\n",
    "Let's visualize how the smoothing parameter affects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    mnb_test = MultinomialNaiveBayes(alpha=alpha)\n",
    "    mnb_test.fit(X_train_bow, np.array(y_text_train))\n",
    "    \n",
    "    train_pred = mnb_test.predict(X_train_bow)\n",
    "    test_pred = mnb_test.predict(X_test_bow)\n",
    "    \n",
    "    if train_pred is not None and test_pred is not None:\n",
    "        train_accuracies.append(accuracy_score(y_text_train, train_pred))\n",
    "        test_accuracies.append(accuracy_score(y_text_test, test_pred))\n",
    "\n",
    "if train_accuracies and test_accuracies:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(alphas, train_accuracies, 'bo-', label='Training Accuracy', markersize=8)\n",
    "    plt.plot(alphas, test_accuracies, 'rs-', label='Test Accuracy', markersize=8)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Alpha (Smoothing Parameter)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Effect of Laplace Smoothing on Multinomial NB')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete Exercise 4 to see the smoothing effect visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Applying to Real Dataset - Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Features: {iris.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train our Gaussian NB\n",
    "gnb_iris = GaussianNaiveBayes(var_smoothing=1e-9)\n",
    "gnb_iris.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "# Predict\n",
    "y_iris_pred = gnb_iris.predict(X_iris_test)\n",
    "\n",
    "if y_iris_pred is not None:\n",
    "    print(\"Gaussian Naive Bayes on Iris Dataset\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_iris_test, y_iris_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_iris_test, y_iris_pred, target_names=iris.target_names))\n",
    "    \n",
    "    # Confusion Matrix visualization\n",
    "    cm = confusion_matrix(y_iris_test, y_iris_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Iris Classification')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(iris.target_names))\n",
    "    plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, iris.target_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complete Exercise 3 to see Iris classification results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Feature Engineering\n",
    "- **Gaussian NB**: Works best when features approximately follow normal distribution\n",
    "- **Multinomial NB**: Best for count data (text); consider TF-IDF for better results\n",
    "\n",
    "### 2. Choosing Smoothing Parameters\n",
    "- **var_smoothing** (Gaussian): Start with 1e-9, increase if numerical issues occur\n",
    "- **alpha** (Multinomial): Use cross-validation to find optimal value; 1.0 is a good default\n",
    "\n",
    "### 3. When Naive Bayes Shines\n",
    "- Text classification (spam, sentiment, categorization)\n",
    "- High-dimensional data with many features\n",
    "- When you need a quick baseline model\n",
    "- When training data is limited\n",
    "\n",
    "### 4. When to Consider Alternatives\n",
    "- When features are highly correlated\n",
    "- When decision boundaries are complex\n",
    "- When probability estimates need to be well-calibrated\n",
    "\n",
    "### 5. Common Mistakes to Avoid\n",
    "- Forgetting to use log probabilities ‚Üí numerical underflow\n",
    "- Using Multinomial NB with negative feature values\n",
    "- Not applying smoothing ‚Üí zero probability issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Bayes Theorem Foundation**: How to use $P(y|X) \\propto P(X|y)P(y)$ for classification\n",
    "\n",
    "2. **Gaussian Naive Bayes**: \n",
    "   - Assumes continuous features follow Gaussian distributions\n",
    "   - Computes mean and variance per feature per class\n",
    "   - Uses variance smoothing for numerical stability\n",
    "\n",
    "3. **Multinomial Naive Bayes**:\n",
    "   - Best for count/frequency data (text classification)\n",
    "   - Uses Laplace smoothing to handle zero counts\n",
    "   - Feature probability: $P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha n}$\n",
    "\n",
    "4. **Numerical Stability**:\n",
    "   - Always use log probabilities to avoid underflow\n",
    "   - Convert multiplication to addition: $\\log(ab) = \\log(a) + \\log(b)$\n",
    "\n",
    "5. **The Naive Assumption**:\n",
    "   - Features are conditionally independent given the class\n",
    "   - This simplification makes computation tractable\n",
    "   - Often works well despite being unrealistic\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Naive Bayes is fast, simple, and effective for many tasks\n",
    "- Choose the right variant based on your data type\n",
    "- Smoothing parameters control the bias-variance tradeoff\n",
    "- Log probabilities are essential for numerical stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}