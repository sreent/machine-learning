{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20(KNN)%20Classification/Case%20Study%3A%20K-Nearest%20Neighbours%20Classification%20with%20Wine%20Dataset%20(UCI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519b78c4",
      "metadata": {
        "id": "519b78c4"
      },
      "source": [
        "# Case Study: KNN Classification with Wine Dataset (UCI)\n",
        "\n",
        "K‑Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that classifies new examples based on similarity to known examples. In this case study, we’ll step through a practical example using the **Wine recognition** dataset (from the UCI Machine Learning Repository) to illustrate key concepts and best practices of KNN classification. This dataset contains chemical analysis results for wines from three cultivars (classes), with 13 continuous features (e.g. alcohol content, acidity, magnesium, phenols, color intensity, etc.). We simulate the scenario of predicting a wine’s cultivar from its chemical properties, akin to a chemist identifying origin by lab measurements.\n",
        "\n",
        "## What we’ll cover\n",
        "- **Data exploration and preparation:** Understanding feature scales and splitting data into training, validation, and test sets.  \n",
        "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance.  \n",
        "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
        "- **Distance metric considerations:** How the choice of distance measure can affect KNN.  \n",
        "- **Model evaluation:** Evaluating the final model on a test set to ensure it generalizes well to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b168f8f4",
      "metadata": {
        "id": "b168f8f4"
      },
      "source": [
        "## Real‑World Applications: Where This Is Useful (Concrete Ops)\n",
        "- **Authenticity & origin (PDO/PGI):** Check that a lot labeled “Cultivar A / Region X” matches historical chemical profiles; flag likely mislabels or adulteration.  \n",
        "- **Supplier & intake QA:** Compare incoming lots against past lots of the same cultivar to catch off‑spec deliveries early, saving tank space and time.  \n",
        "- **Process monitoring & early warnings:** Periodic lab panels classified against expected states; abnormal neighbors trigger investigation of contamination or process drift.\n",
        "- **Counterfeit screening:** Rapidly triage shipments before expensive sensory panels or full mass‑spec profiling.  \n",
        "\n",
        "## Why KNN specifically\n",
        "- **Small/medium data with strong locality:** Wine labs typically have hundreds or thousands of historical lots—KNN thrives here without heavy parametric assumptions.  \n",
        "- **Example‑based explanation:** You can justify a prediction by saying, “8/10 nearest wines were Cultivar 2 with similar magnesium, phenolics, and color intensity.”  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab88f59c",
      "metadata": {
        "id": "ab88f59c"
      },
      "source": [
        "## Exploring the Dataset\n",
        "Before diving into modeling, let's load the dataset and examine its features. The dataset has 178 samples, each with 13 features. The target `class` is an integer (0, 1, or 2) representing the wine cultivar.\n",
        "\n",
        "**Typical feature ranges (intuition):**  \n",
        "- Alcohol ~ 11–15  \n",
        "- Malic acid ~ 0.7–6  \n",
        "- Alcalinity of ash ~ 10–30  \n",
        "- Magnesium ~ 70–160  \n",
        "- Color intensity ~ 1–13  \n",
        "- Proline ~ 280–1700  \n",
        "\n",
        "Large differences in magnitude (e.g., *Proline* in hundreds vs *Malic acid* single digits) motivate **scaling** before using distance-based models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a85ef4",
      "metadata": {
        "id": "a2a85ef4"
      },
      "outputs": [],
      "source": [
        "# Imports and data loading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, f1_score,\n",
        "    precision_recall_fscore_support, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from collections import Counter\n",
        "\n",
        "# Load the wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create DataFrame for exploration\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['class'] = y\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2847407",
      "metadata": {
        "id": "f2847407"
      },
      "source": [
        "Let’s examine the class distribution and basic feature statistics, then split the data into training, validation, and test sets using a 60/20/20 stratified split. This ensures each set has similar class proportions.\n",
        "\n",
        "> **Question**: Why do we set aside a separate test dataset *after* choosing a model’s parameters?\n",
        ">  \n",
        "> A) To calibrate the model’s probability outputs.  \n",
        ">\n",
        "> B) To have an unbiased measure of final model performance on unseen data.  \n",
        ">\n",
        "> C) To use it for training if the model underfits.  \n",
        ">\n",
        "> D) To perform cross-validation more effectively.\n",
        "\n",
        "Holding out a test set is standard to avoid overfitting and obtain an unbiased estimate of performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine class distribution\n",
        "print(\"Class distribution:\\n\", df['class'].value_counts().sort_index(), \"\\n\")\n",
        "\n",
        "# Descriptive statistics of features\n",
        "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
        "\n",
        "# Stratified split into train, validation, and test sets (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "1SfshufVixc7"
      },
      "id": "1SfshufVixc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These figures match the scikit-learn load_wine description (178 samples, 13 features, 3 classes, with per-class counts)."
      ],
      "metadata": {
        "id": "ccokg4IsGYB8"
      },
      "id": "ccokg4IsGYB8"
    },
    {
      "cell_type": "markdown",
      "id": "813c67d3",
      "metadata": {
        "id": "813c67d3"
      },
      "source": [
        "## Effect of Feature Scaling on KNN\n",
        "\n",
        "KNN uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The tiny demo below illustrates how a difference in *Proline* (hundreds) can swamp a difference in *Malic acid* (tenths). Therefore, scaling features to comparable ranges is critical for KNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c14c88",
      "metadata": {
        "id": "73c14c88"
      },
      "outputs": [],
      "source": [
        "# Demonstrate distance dominance (hypothetical differences)\n",
        "from math import sqrt\n",
        "\n",
        "delta_proline_large = 100.0\n",
        "delta_malic_small = 0.5\n",
        "\n",
        "d1 = sqrt(delta_proline_large**2 + 0.0**2)\n",
        "d2 = sqrt(0.0**2 + delta_malic_small**2)\n",
        "\n",
        "print(\"Distance if only Proline differs by +100:\", round(d1, 3))\n",
        "print(\"Distance if only Malic differs by +0.5  :\", round(d2, 3))\n",
        "print(\"Ratio (Proline / Malic):\", round(d1 / d2, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d936dd00",
      "metadata": {
        "id": "d936dd00"
      },
      "source": [
        "Next, we train a baseline KNN model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. Note that we scale using parameters learned from the training set only to avoid leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline without scaling\n",
        "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "raw_val_acc = accuracy_score(y_val, knn_raw.predict(X_val))\n",
        "\n",
        "# Baseline with scaling (fit on train only)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "scaled_val_acc = accuracy_score(y_val, knn_scaled.predict(X_val_scaled))\n",
        "\n",
        "print(f\"Validation accuracy without scaling: {raw_val_acc:.3f}\")\n",
        "print(f\"Validation accuracy with scaling:  {scaled_val_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "ldQ8a-dFjJqk"
      },
      "id": "ldQ8a-dFjJqk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b9863c1c",
      "metadata": {
        "id": "b9863c1c"
      },
      "source": [
        "The scaled model often performs dramatically better because each feature contributes fairly to distance computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8322849",
      "metadata": {
        "id": "d8322849"
      },
      "source": [
        "**t‑SNE visualization of the wine dataset after feature scaling.**  \n",
        "t‑SNE is fit on the **full dataset** purely for visualization. It preserves local structure but should **not** be used for tuning or evaluation. This does **not** leak information into the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_scaled_full = StandardScaler().fit_transform(X)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled_full)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel(\"$X_1$ (t-SNE)\")\n",
        "plt.ylabel(\"$X_2$ (t-SNE)\")\n",
        "plt.axis([-20, 20, -20, 20])\n",
        "plt.title(\"Wine dataset — t-SNE (Scaled Features)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZVTuE_9qjlJN"
      },
      "id": "ZVTuE_9qjlJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "85ab069b",
      "metadata": {
        "id": "85ab069b"
      },
      "source": [
        "## Distance Metric Considerations\n",
        "Choosing a distance metric is itself a hyperparameter. For continuous features, **Euclidean (L2)** is the default and measures straight-line distance; **Manhattan (L1)** sums absolute differences and can be more robust to outliers. For text documents or high‑dimensional sparse data, **cosine similarity/distance** (which measures the angle between vectors) is often more meaningful than raw Euclidean distance. In practice, treat the metric as something to tune by validation.\n",
        "\n",
        "> **Question**: Your features are continuous lab measurements (e.g., alcohol, magnesium, color intensity). If some features can have occasional outliers, which distance measure is more robust in KNN, and why?\n",
        ">\n",
        "> A Euclidean (L2) — squares differences, more sensitive to outliers\n",
        ">\n",
        "> B Manhattan (L1) — sums absolute differences, less sensitive to outliers\n",
        ">\n",
        "> C Hamming — appropriate for binary features only\n",
        ">\n",
        "> D No difference — both behave the same on outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the effect of different metrics and K values, we can do a small grid of {Euclidean, Manhattan} × {3, 5, 7, 9} on the validation set. This isn’t exhaustive but shows that **distance metric** is a tunable choice.\n"
      ],
      "metadata": {
        "id": "SijCu2MEmRXk"
      },
      "id": "SijCu2MEmRXk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbd1a3b",
      "metadata": {
        "id": "1fbd1a3b"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "k_values = [3, 5, 7, 9]\n",
        "rows = []\n",
        "\n",
        "for metric, k in product(metrics, k_values):\n",
        "    mdl = KNeighborsClassifier(n_neighbors=k, metric=metric).fit(X_train_scaled, y_train)\n",
        "    acc = accuracy_score(y_val, mdl.predict(X_val_scaled))\n",
        "    rows.append((metric, k, acc))\n",
        "\n",
        "grid_df = pd.DataFrame(rows, columns=['metric', 'k', 'val_accuracy']) \\\n",
        "          .pivot(index='metric', columns='k', values='val_accuracy')\n",
        "grid_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick best (metric, k) by validation accuracy only (no weights)\n",
        "grid_long = (\n",
        "    grid_df.stack()                 # -> Series with MultiIndex (metric, k)\n",
        "           .rename('val_accuracy')\n",
        "           .reset_index()           # -> columns: ['metric', 'k', 'val_accuracy']\n",
        ")\n",
        "\n",
        "# Tie-breaker: prefer smaller k, then 'euclidean' over 'manhattan'\n",
        "grid_long['tie_metric_rank'] = grid_long['metric'].map({'euclidean': 0, 'manhattan': 1})\n",
        "\n",
        "best_row = (\n",
        "    grid_long.sort_values(\n",
        "        ['val_accuracy', 'k', 'tie_metric_rank'],\n",
        "        ascending=[False, True, True]\n",
        "    )\n",
        "    .iloc[0]\n",
        ")\n",
        "\n",
        "chosen_metric = best_row['metric']\n",
        "chosen_k      = int(best_row['k'])\n",
        "best_val      = float(best_row['val_accuracy'])\n",
        "\n",
        "print(\"Decision log — chosen params (metric & k only):\")\n",
        "print({\"metric\": chosen_metric, \"n_neighbors\": chosen_k, \"val_accuracy\": round(best_val, 3)})\n",
        "\n",
        "# (Optional) sanity check with chosen params on validation data\n",
        "_knn = KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric).fit(X_train_scaled, y_train)\n",
        "val_acc_check = accuracy_score(y_val, _knn.predict(X_val_scaled))\n",
        "print(\"Validation accuracy (chosen metric & k):\", round(val_acc_check, 3))\n"
      ],
      "metadata": {
        "id": "2WpqG8z5E_eM"
      },
      "id": "2WpqG8z5E_eM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4157e321",
      "metadata": {
        "id": "4157e321"
      },
      "source": [
        "## Choosing K: Bias–Variance Trade‑off\n",
        "A small K (e.g., K=1) is highly flexible and fits training data very closely—**high variance** and potential overfitting. A very large K (approaching the size of the training set) averages over many neighbors—**high bias** and potential underfitting. We sweep K from 1 to 20 and plot training vs. validation accuracy to pick the best K by validation performance.\n",
        "\n",
        "> **Question**: K=1 gives 100% train accuracy but only 88% validation. What should you do?\n",
        ">  \n",
        "> A) Increase K to reduce variance (overfitting).  \n",
        ">\n",
        "> B) Decrease K further  \n",
        ">\n",
        "> C) Change to Manhattan distance  \n",
        ">\n",
        "> D) Add more features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a985951",
      "metadata": {
        "id": "3a985951"
      },
      "outputs": [],
      "source": [
        "train_acc, val_acc = [], []\n",
        "k_sweep = range(1, 21)\n",
        "\n",
        "for k in k_sweep:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_acc.append(accuracy_score(y_train, model.predict(X_train_scaled)))\n",
        "    val_acc.append(accuracy_score(y_val, model.predict(X_val_scaled)))\n",
        "\n",
        "# Best K by validation\n",
        "best_k = int(np.argmax(val_acc) + 1)\n",
        "best_val = max(val_acc)\n",
        "max_gap = np.max(np.array(train_acc) - np.array(val_acc))\n",
        "\n",
        "print(\"Best K (validation):\", best_k, \"Validation accuracy:\", round(best_val, 3))\n",
        "print(\"Max (train - validation) gap across K:\", round(max_gap, 3))\n",
        "\n",
        "# Plot train vs validation accuracy vs K\n",
        "plt.figure()\n",
        "plt.scatter(list(k_sweep), train_acc, label='Train Accuracy')\n",
        "plt.scatter(list(k_sweep), val_acc, label='Validation Accuracy')\n",
        "plt.axvline(best_k, linestyle='--', label=f'Best K={best_k}')\n",
        "plt.axis([0, 20, 0.8, 1.05])\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e431ac",
      "metadata": {
        "id": "90e431ac"
      },
      "source": [
        "## Model Evaluation on Test Set\n",
        "With the chosen hyperparameters, we refit KNN (within a pipeline to avoid leakage) on the combined training + validation data and evaluate performance on the **held-out test** set. This provides an unbiased estimate of real-world performance.\n",
        "\n",
        "> **Question**: Test accuracy is slightly worse than validation. Before deployment, what’s the right approach?\n",
        ">  \n",
        "> A Check if the gap is variance/representativeness (e.g. re-run the split or consider temporal splits) and confirm test performance meets requirements.\n",
        ">\n",
        "> B Re-tune hyperparameters on the test set.  \n",
        ">\n",
        "> C Ignore the difference and deploy immediately.  \n",
        ">\n",
        "> D Conclude there was leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea0ff29",
      "metadata": {
        "id": "5ea0ff29"
      },
      "outputs": [],
      "source": [
        "# Combine training and validation sets for final training\n",
        "X_train_all = np.vstack([X_train, X_val])\n",
        "y_train_all = np.hstack([y_train, y_val])\n",
        "\n",
        "# Build pipeline (scaler + KNN) with chosen hyperparameters — no weights\n",
        "final_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric))\n",
        "])\n",
        "\n",
        "final_pipe.fit(X_train_all, y_train_all)\n",
        "\n",
        "# Predict on test set\n",
        "test_pred = final_pipe.predict(X_test)\n",
        "test_acc  = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Test accuracy:\", round(test_acc, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9cb57ed",
      "metadata": {
        "id": "e9cb57ed"
      },
      "source": [
        "Beyond accuracy, we examine **balanced accuracy** (accounts for class imbalance) and **macro F1** (averages F1 across classes), print a **classification report**, show a **per-class table**, and plot both the raw and normalized confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report and per-class metrics\n",
        "print(\"\\nClassification report (test):\")\n",
        "print(classification_report(y_test, test_pred, digits=3, target_names=[str(c) for c in np.unique(y)]))\n",
        "\n",
        "labels = list(np.unique(y))\n",
        "prec, rec, f1, sup = precision_recall_fscore_support(y_test, test_pred, labels=labels)\n",
        "\n",
        "per_class_df = pd.DataFrame({\n",
        "    'precision': prec,\n",
        "    'recall': rec,\n",
        "    'f1': f1,\n",
        "    'support': sup\n",
        "}, index=labels)\n",
        "display(per_class_df)\n",
        "\n",
        "# Balanced accuracy & macro F1\n",
        "print(\"Balanced accuracy (test):\", round(balanced_accuracy_score(y_test, test_pred), 3))\n",
        "print(\"Macro F1 (test):         \", round(f1_score(y_test, test_pred, average='macro'), 3))\n",
        "\n",
        "# Confusion matrices: raw and normalized\n",
        "cm_raw = confusion_matrix(y_test, test_pred, labels=labels)\n",
        "cm_norm = confusion_matrix(y_test, test_pred, labels=labels, normalize='true')\n",
        "\n",
        "# Raw confusion matrix heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm_raw, cmap='Blues')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Confusion Matrix (Raw)\")\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        ax.text(j, i, cm_raw[i, j], ha='center', va='center', color='black')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Normalized confusion matrix heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm_norm, vmin=0, vmax=1, cmap='Blues')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Confusion Matrix (Normalized)\")\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha='center', va='center', color='black')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7CIOzVT8oJS2"
      },
      "id": "7CIOzVT8oJS2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9feef4b9",
      "metadata": {
        "id": "9feef4b9"
      },
      "source": [
        "## Limitations (Current Scope) & What’s Next\n",
        "This notebook uses a **single hold‑out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k‑fold cross‑validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute‑force neighbor search (`algorithm='brute'`) and didn’t explore scalability techniques like KD‑trees, Ball Trees, or approximate nearest neighbor libraries (e.g. FAISS, HNSW). These become important when your archive grows to millions of rows or requires low‑latency predictions. Finally, we didn’t address class imbalance or cost‑sensitive KNN; these are natural extensions for more advanced courses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10c90ca2",
      "metadata": {
        "id": "10c90ca2"
      },
      "source": [
        "## Conclusion\n",
        "- **Scaling** prevents large‑range features from dominating distance computations.  \n",
        "- **Tuning K** via validation balances bias and variance; a very small K overfits, a very large K underfits.  \n",
        "- **Distance metric and K** are hyperparameters; small grids reveal significant differences.  \n",
        "- KNN remains a powerful, intuitive baseline—use it to build intuition about distance and similarity before advancing to more sophisticated models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PF4cv9GyDwsf"
      },
      "id": "PF4cv9GyDwsf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}