{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20(KNN)%20Classification/Case%20Study%3A%20K-Nearest%20Neighbours%20Classification%20with%20Wine%20Dataset%20(UCI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519b78c4",
      "metadata": {
        "id": "519b78c4"
      },
      "source": [
        "# Case Study: KNN Classification with Wine Dataset (UCI)\n",
        "\n",
        "K‑Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that classifies new examples based on similarity to known examples. In this case study, we’ll step through a practical example using the **Wine recognition** dataset (from the UCI Machine Learning Repository) to illustrate key concepts and best practices of KNN classification. This dataset contains chemical analysis results for wines from three cultivars (classes), with 13 continuous features (e.g. alcohol content, acidity, magnesium, phenols, color intensity, etc.). We simulate the scenario of predicting a wine’s cultivar from its chemical properties, akin to a chemist identifying origin by lab measurements.\n",
        "\n",
        "## What we’ll cover\n",
        "- **Data exploration and preparation:** Understanding feature scales and splitting data into training, validation, and test sets.  \n",
        "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance.  \n",
        "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
        "- **Distance metric considerations:** How the choice of distance measure can affect KNN.  \n",
        "- **Model evaluation:** Evaluating the final model on a test set to ensure it generalizes well to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b168f8f4",
      "metadata": {
        "id": "b168f8f4"
      },
      "source": [
        "## Real‑World Applications: Where This Is Useful (Concrete Ops)\n",
        "- **Authenticity & origin (PDO/PGI):** Check that a lot labeled “Cultivar A / Region X” matches historical chemical profiles; flag likely mislabels or adulteration.  \n",
        "- **Supplier & intake QA:** Compare incoming lots against past lots of the same cultivar to catch off‑spec deliveries early, saving tank space and time.  \n",
        "- **Process monitoring & early warnings:** Periodic lab panels classified against expected states; abnormal neighbors trigger investigation of contamination or process drift.  \n",
        "- **Blending & barrel allocation:** Suggest a likely cultivar or sub‑style for a new lot by finding the nearest historical lots; helps select compatible barrels, yeasts, or blending partners.  \n",
        "- **Counterfeit screening:** Rapidly triage shipments before expensive sensory panels or full mass‑spec profiling.  \n",
        "- **Sensory proxy:** When trained on batches with known sensory ratings, nearest batches help apprentices understand the drivers behind various wine styles.\n",
        "\n",
        "## Why KNN specifically\n",
        "- **Small/medium data with strong locality:** Wine labs typically have hundreds or thousands of historical lots—KNN thrives here without heavy parametric assumptions.  \n",
        "- **Example‑based explanation:** You can justify a prediction by saying, “8/10 nearest wines were Cultivar 2 with similar magnesium, phenolics, and color intensity.”  \n",
        "- **Fast iteration:** KNN has minimal “training” time—just store the data; new lots can be added easily.  \n",
        "- **Non‑linear decision boundaries:** KNN naturally captures cluster structures without complex feature engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab88f59c",
      "metadata": {
        "id": "ab88f59c"
      },
      "source": [
        "## Exploring the Dataset\n",
        "Before diving into modeling, let's load the dataset and examine its features. The dataset has 178 samples, each with 13 features. The target `class` is an integer (0, 1, or 2) representing the wine cultivar.\n",
        "\n",
        "**Typical feature ranges (intuition):**  \n",
        "- Alcohol ~ 11–15  \n",
        "- Malic acid ~ 0.7–6  \n",
        "- Alcalinity of ash ~ 10–30  \n",
        "- Magnesium ~ 70–160  \n",
        "- Color intensity ~ 1–13  \n",
        "- Proline ~ 280–1700  \n",
        "\n",
        "Large differences in magnitude (e.g., *Proline* in hundreds vs *Malic acid* single digits) motivate **scaling** before using distance-based models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a85ef4",
      "metadata": {
        "id": "a2a85ef4"
      },
      "outputs": [],
      "source": [
        "# Imports and data loading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, f1_score,\n",
        "    precision_recall_fscore_support, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from collections import Counter\n",
        "\n",
        "# Load the wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create DataFrame for exploration\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['class'] = y\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2847407",
      "metadata": {
        "id": "f2847407"
      },
      "source": [
        "Let’s examine the class distribution and basic feature statistics, then split the data into training, validation, and test sets using a 60/20/20 stratified split. This ensures each set has similar class proportions.\n",
        "\n",
        "> **Question**: Why do we set aside a separate test dataset *after* choosing a model’s parameters?\n",
        ">  \n",
        "> A) To calibrate the model’s probability outputs.  \n",
        ">\n",
        "> B) To have an unbiased measure of final model performance on unseen data.  \n",
        ">\n",
        "> C) To use it for training if the model underfits.  \n",
        ">\n",
        "> D) To perform cross-validation more effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine class distribution\n",
        "print(\"Class distribution:\\n\", df['class'].value_counts().sort_index(), \"\\n\")\n",
        "\n",
        "# Descriptive statistics of features\n",
        "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
        "\n",
        "# Stratified split into train, validation, and test sets (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "1SfshufVixc7"
      },
      "id": "1SfshufVixc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "813c67d3",
      "metadata": {
        "id": "813c67d3"
      },
      "source": [
        "## Effect of Feature Scaling on KNN\n",
        "\n",
        "KNN uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The tiny demo below illustrates how a difference in *Proline* (hundreds) can swamp a difference in *Malic acid* (tenths). Therefore, scaling features to comparable ranges is critical for KNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c14c88",
      "metadata": {
        "id": "73c14c88"
      },
      "outputs": [],
      "source": [
        "# Demonstrate distance dominance (hypothetical differences)\n",
        "from math import sqrt\n",
        "\n",
        "delta_proline_large = 100.0\n",
        "delta_malic_small = 0.5\n",
        "\n",
        "d1 = sqrt(delta_proline_large**2 + 0.0**2)\n",
        "d2 = sqrt(0.0**2 + delta_malic_small**2)\n",
        "\n",
        "print(\"Distance if only Proline differs by +100:\", round(d1, 3))\n",
        "print(\"Distance if only Malic differs by +0.5  :\", round(d2, 3))\n",
        "print(\"Ratio (Proline / Malic):\", round(d1 / d2, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d936dd00",
      "metadata": {
        "id": "d936dd00"
      },
      "source": [
        "Next, we train a baseline KNN model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. Note that we scale using parameters learned from the training set only to avoid leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline without scaling\n",
        "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "raw_val_acc = accuracy_score(y_val, knn_raw.predict(X_val))\n",
        "\n",
        "# Baseline with scaling (fit on train only)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "scaled_val_acc = accuracy_score(y_val, knn_scaled.predict(X_val_scaled))\n",
        "\n",
        "print(f\"Validation accuracy without scaling: {raw_val_acc:.3f}\")\n",
        "print(f\"Validation accuracy with scaling:  {scaled_val_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "ldQ8a-dFjJqk"
      },
      "id": "ldQ8a-dFjJqk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b9863c1c",
      "metadata": {
        "id": "b9863c1c"
      },
      "source": [
        "The scaled model often performs dramatically better because each feature contributes fairly to distance computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Leakage Demo\n",
        "\n",
        "If you scale using **all** the data (train plus validation) before splitting, you leak information from the validation set into training. Here’s how leakage can inflate validation accuracy:\n"
      ],
      "metadata": {
        "id": "nS5unaQ3jRO8"
      },
      "id": "nS5unaQ3jRO8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e707a8f1",
      "metadata": {
        "id": "e707a8f1"
      },
      "outputs": [],
      "source": [
        "# WRONG scaling: fit on train+val (leaks information)\n",
        "leaky_scaler = StandardScaler().fit(np.vstack([X_train, X_val]))\n",
        "X_train_leaky = leaky_scaler.transform(X_train)\n",
        "X_val_leaky   = leaky_scaler.transform(X_val)\n",
        "\n",
        "leaky_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "leaky_knn.fit(X_train_leaky, y_train)\n",
        "leaky_val_acc = accuracy_score(y_val, leaky_knn.predict(X_val_leaky))\n",
        "\n",
        "print(\"Leaky validation accuracy (wrong):\", round(leaky_val_acc, 3))\n",
        "print(\"Correct scaled validation accuracy:\", round(scaled_val_acc, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8322849",
      "metadata": {
        "id": "d8322849"
      },
      "source": [
        "**t‑SNE visualization of the wine dataset after feature scaling.**  \n",
        "t‑SNE is fit on the **full dataset** purely for visualization. It preserves local structure but should **not** be used for tuning or evaluation. This does **not** leak information into the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_scaled_full = StandardScaler().fit_transform(X)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled_full)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel(\"$X_1$ (t-SNE)\")\n",
        "plt.ylabel(\"$X_2$ (t-SNE)\")\n",
        "plt.axis([-20, 20, -20, 20])\n",
        "plt.title(\"Wine dataset — t-SNE (Scaled Features)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZVTuE_9qjlJN"
      },
      "id": "ZVTuE_9qjlJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "85ab069b",
      "metadata": {
        "id": "85ab069b"
      },
      "source": [
        "## Distance Metric Considerations\n",
        "Choosing a distance metric is itself a hyperparameter. For continuous features, **Euclidean (L2)** is the default and measures straight-line distance; **Manhattan (L1)** sums absolute differences and can be more robust to outliers. For text documents or high‑dimensional sparse data, **cosine similarity/distance** (which measures the angle between vectors) is often more meaningful than raw Euclidean distance. In practice, treat the metric as something to tune by validation.\n",
        "\n",
        "> **Question**: Suppose you are using KNN to classify text documents represented by word counts (high-dimensional, sparse features). Which distance measure would most likely be more appropriate than Euclidean for this task, and why? distance & why?\n",
        ">\n",
        "> A Cosine similarity/distance  \n",
        ">\n",
        "> B Manhattan distance  \n",
        ">\n",
        "> C Hamming distance  \n",
        ">\n",
        "> D No difference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the effect of different metrics and K values, we can do a small grid of {Euclidean, Manhattan} × {3, 5, 7, 9} on the validation set. This isn’t exhaustive but shows that **distance metric** is a tunable choice.\n"
      ],
      "metadata": {
        "id": "SijCu2MEmRXk"
      },
      "id": "SijCu2MEmRXk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbd1a3b",
      "metadata": {
        "id": "1fbd1a3b"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "k_values = [3, 5, 7, 9]\n",
        "rows = []\n",
        "\n",
        "for metric, k in product(metrics, k_values):\n",
        "    mdl = KNeighborsClassifier(n_neighbors=k, metric=metric).fit(X_train_scaled, y_train)\n",
        "    acc = accuracy_score(y_val, mdl.predict(X_val_scaled))\n",
        "    rows.append((metric, k, acc))\n",
        "\n",
        "grid_df = pd.DataFrame(rows, columns=['metric', 'k', 'val_accuracy']) \\\n",
        "          .pivot(index='metric', columns='k', values='val_accuracy')\n",
        "grid_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4157e321",
      "metadata": {
        "id": "4157e321"
      },
      "source": [
        "## Choosing K: Bias–Variance Trade‑off\n",
        "A small K (e.g., K=1) is highly flexible and fits training data very closely—**high variance** and potential overfitting. A very large K (approaching the size of the training set) averages over many neighbors—**high bias** and potential underfitting. We sweep K from 1 to 20 and plot training vs. validation accuracy to pick the best K by validation performance.\n",
        "\n",
        "> **Question**: K=1 gives 100% train accuracy but only 88% validation. What should you do?\n",
        ">  \n",
        "> A) Increase K to reduce variance (overfitting).  \n",
        ">\n",
        "> B) Decrease K further  \n",
        ">\n",
        "> C) Change to Manhattan distance  \n",
        ">\n",
        "> D) Add more features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a985951",
      "metadata": {
        "id": "3a985951"
      },
      "outputs": [],
      "source": [
        "train_acc, val_acc = [], []\n",
        "k_sweep = range(1, 21)\n",
        "\n",
        "for k in k_sweep:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_acc.append(accuracy_score(y_train, model.predict(X_train_scaled)))\n",
        "    val_acc.append(accuracy_score(y_val, model.predict(X_val_scaled)))\n",
        "\n",
        "# Best K by validation\n",
        "best_k = int(np.argmax(val_acc) + 1)\n",
        "best_val = max(val_acc)\n",
        "max_gap = np.max(np.array(train_acc) - np.array(val_acc))\n",
        "\n",
        "print(\"Best K (validation):\", best_k, \"Validation accuracy:\", round(best_val, 3))\n",
        "print(\"Max (train - validation) gap across K:\", round(max_gap, 3))\n",
        "\n",
        "# Plot train vs validation accuracy vs K\n",
        "plt.figure()\n",
        "plt.scatter(list(k_sweep), train_acc, label='Train Accuracy')\n",
        "plt.scatter(list(k_sweep), val_acc, label='Validation Accuracy')\n",
        "plt.axvline(best_k, linestyle='--', label=f'Best K={best_k}')\n",
        "plt.axis([0, 20, 0.8, 1.05])\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d064d9e",
      "metadata": {
        "id": "2d064d9e"
      },
      "source": [
        "### Distance-Weighted Voting vs. Uniform Voting\n",
        "By default, KNN assigns equal weight to all K neighbors. Alternatively, you can weigh each neighbor by the inverse of its distance (`weights='distance'`), so closer neighbors matter more. This often helps when data has clusters of varying density.\n",
        "\n",
        "**Tie Policy:** scikit-learn does not expose an explicit tie-breaking policy; ties depend on internal neighbor order. To reduce ties, prefer **odd K** or use distance weighting, which often makes ties less likely.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404f8213",
      "metadata": {
        "id": "404f8213"
      },
      "outputs": [],
      "source": [
        "# Uniform voting (best_k from validation sweep)\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=best_k, weights='uniform').fit(X_train_scaled, y_train)\n",
        "uniform_val_acc = accuracy_score(y_val, knn_uniform.predict(X_val_scaled))\n",
        "\n",
        "# Distance-weighted voting\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=best_k, weights='distance').fit(X_train_scaled, y_train)\n",
        "distance_val_acc = accuracy_score(y_val, knn_distance.predict(X_val_scaled))\n",
        "\n",
        "print(f'Validation acc (uniform weights @K={best_k}): {uniform_val_acc:.3f}')\n",
        "print(f'Validation acc (distance weights @K={best_k}): {distance_val_acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c851943d",
      "metadata": {
        "id": "c851943d"
      },
      "source": [
        "## Abstain Rule (Coverage vs. Accuracy)\n",
        "In quality‑assurance settings, we might prefer the classifier to **abstain** from labeling a new sample when it is far from all known samples. One simple strategy uses the **average distance to the K nearest neighbors** as a confidence proxy: if the average distance is too large, abstain (i.e. defer to a human or more expensive test). The plot below shows the trade‑off between **coverage** (fraction of samples automatically classified) and **accuracy on the kept cases** for various distance thresholds.\n",
        "\n",
        "Pick a point based on your cost trade‑offs—for example, require ≥0.98 accuracy on kept cases even if coverage drops to ~0.7.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be3464b3",
      "metadata": {
        "id": "be3464b3"
      },
      "outputs": [],
      "source": [
        "# Compute average distance to K neighbors (validation set)\n",
        "mdl = KNeighborsClassifier(n_neighbors=best_k).fit(X_train_scaled, y_train)\n",
        "distances, _ = mdl.kneighbors(X_val_scaled, n_neighbors=best_k, return_distance=True)\n",
        "avg_kdist = distances.mean(axis=1)\n",
        "\n",
        "def evaluate_with_abstain(th):\n",
        "    keep = avg_kdist <= th\n",
        "    if keep.sum() == 0:\n",
        "        return {\"threshold\": float(th), \"coverage\": 0.0, \"acc_on_kept\": None}\n",
        "    yhat = mdl.predict(X_val_scaled[keep])\n",
        "    return {\"threshold\": float(th), \"coverage\": float(keep.mean()),\n",
        "            \"acc_on_kept\": float(accuracy_score(y_val[keep], yhat))}\n",
        "\n",
        "# Print a few sample thresholds\n",
        "for th in np.quantile(avg_kdist, [0.5, 0.7, 0.9]):\n",
        "    print(evaluate_with_abstain(th))\n",
        "\n",
        "# Plot coverage vs accuracy on kept cases across many thresholds\n",
        "ths = np.quantile(avg_kdist, np.linspace(0.1, 0.95, 15))\n",
        "coverage_vals, accuracy_vals = [], []\n",
        "for th in ths:\n",
        "    result = evaluate_with_abstain(th)\n",
        "    coverage_vals.append(result['coverage'])\n",
        "    accuracy_vals.append(result['acc_on_kept'])\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(coverage_vals, accuracy_vals, marker='o')\n",
        "plt.xlabel(\"Coverage (fraction auto-classified)\")\n",
        "plt.ylabel(\"Accuracy on kept cases\")\n",
        "plt.title(\"Abstain policy frontier (validation)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b175a3cd",
      "metadata": {
        "id": "b175a3cd"
      },
      "source": [
        "## Selecting the Final Hyperparameters\n",
        "We now choose the **metric**, **K**, and **weights** using evidence from the validation set. The logic is:\n",
        "\n",
        "1. Pick the `(metric, K)` pair from our mini grid with the highest validation accuracy.  \n",
        "2. Compare `weights='uniform'` vs `weights='distance'` for that pair to pick the better.  \n",
        "3. Document the decision and use these hyperparameters for the final model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: choose metric & k from the mini grid with highest validation accuracy\n",
        "try:\n",
        "    best_cell = grid_df.stack().astype(float).idxmax()  # (metric, k)\n",
        "    chosen_metric, chosen_k = best_cell[0], int(best_cell[1])\n",
        "except Exception:\n",
        "    chosen_metric, chosen_k = 'euclidean', int(best_k)\n",
        "\n",
        "# Step 2: compare weights for chosen_metric and chosen_k\n",
        "mdl_uniform  = KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric, weights='uniform')\n",
        "mdl_distance = KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric, weights='distance')\n",
        "mdl_uniform.fit(X_train_scaled, y_train)\n",
        "mdl_distance.fit(X_train_scaled, y_train)\n",
        "\n",
        "val_uniform  = accuracy_score(y_val, mdl_uniform.predict(X_val_scaled))\n",
        "val_distance = accuracy_score(y_val, mdl_distance.predict(X_val_scaled))\n",
        "\n",
        "chosen_weights = 'distance' if val_distance > val_uniform else 'uniform'\n",
        "\n",
        "print(\"Decision log — chosen params:\")\n",
        "print({\"metric\": chosen_metric, \"n_neighbors\": chosen_k, \"weights\": chosen_weights})\n"
      ],
      "metadata": {
        "id": "4rP1O5hgnrfo"
      },
      "id": "4rP1O5hgnrfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "90e431ac",
      "metadata": {
        "id": "90e431ac"
      },
      "source": [
        "## Model Evaluation on Test Set\n",
        "With the chosen hyperparameters, we refit KNN (within a pipeline to avoid leakage) on the combined training + validation data and evaluate performance on the **held-out test** set. This provides an unbiased estimate of real-world performance.\n",
        "\n",
        "> **Question**: Test accuracy is slightly worse than validation. Before deployment, what’s the right approach?\n",
        ">  \n",
        "> A Check if the gap is variance/representativeness (e.g. re-run the split or consider temporal splits) and confirm test performance meets requirements.\n",
        ">\n",
        "> B Re-tune hyperparameters on the test set.  \n",
        ">\n",
        "> C Ignore the difference and deploy immediately.  \n",
        ">\n",
        "> D Conclude there was leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea0ff29",
      "metadata": {
        "id": "5ea0ff29"
      },
      "outputs": [],
      "source": [
        "# Combine training and validation sets for final training\n",
        "X_train_all = np.vstack([X_train, X_val])\n",
        "y_train_all = np.hstack([y_train, y_val])\n",
        "\n",
        "# Build pipeline (scaler + KNN) with chosen hyperparameters\n",
        "final_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric, weights=chosen_weights))\n",
        "])\n",
        "final_pipe.fit(X_train_all, y_train_all)\n",
        "\n",
        "# Predict on test set\n",
        "test_pred = final_pipe.predict(X_test)\n",
        "test_acc  = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Test accuracy:\", round(test_acc, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9cb57ed",
      "metadata": {
        "id": "e9cb57ed"
      },
      "source": [
        "Beyond accuracy, we examine **balanced accuracy** (accounts for class imbalance) and **macro F1** (averages F1 across classes), print a **classification report**, show a **per-class table**, and plot both the raw and normalized confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report and per-class metrics\n",
        "print(\"\\nClassification report (test):\")\n",
        "print(classification_report(y_test, test_pred, digits=3, target_names=[str(c) for c in np.unique(y)]))\n",
        "\n",
        "labels = list(np.unique(y))\n",
        "prec, rec, f1, sup = precision_recall_fscore_support(y_test, test_pred, labels=labels)\n",
        "\n",
        "per_class_df = pd.DataFrame({\n",
        "    'precision': prec,\n",
        "    'recall': rec,\n",
        "    'f1': f1,\n",
        "    'support': sup\n",
        "}, index=labels)\n",
        "display(per_class_df)\n",
        "\n",
        "# Balanced accuracy & macro F1\n",
        "print(\"Balanced accuracy (test):\", round(balanced_accuracy_score(y_test, test_pred), 3))\n",
        "print(\"Macro F1 (test):         \", round(f1_score(y_test, test_pred, average='macro'), 3))\n",
        "\n",
        "# Confusion matrices: raw and normalized\n",
        "cm_raw = confusion_matrix(y_test, test_pred, labels=labels)\n",
        "cm_norm = confusion_matrix(y_test, test_pred, labels=labels, normalize='true')\n",
        "\n",
        "# Raw confusion matrix heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm_raw, cmap='Blues')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Confusion Matrix (Raw)\")\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        ax.text(j, i, cm_raw[i, j], ha='center', va='center', color='black')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Normalized confusion matrix heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm_norm, vmin=0, vmax=1, cmap='Blues')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Confusion Matrix (Normalized)\")\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha='center', va='center', color='black')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7CIOzVT8oJS2"
      },
      "id": "7CIOzVT8oJS2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0ee65e30",
      "metadata": {
        "id": "0ee65e30"
      },
      "source": [
        "## Nearest-Neighbors Sanity Probe\n",
        "To interpret KNN decisions, we can list the indices, classes, and distances of the K nearest training samples for a few randomly chosen test samples. The vote tally makes it clear why a certain class was predicted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51bff000",
      "metadata": {
        "id": "51bff000"
      },
      "outputs": [],
      "source": [
        "# Compute scaled versions for neighbor analysis\n",
        "scaler_fitted = final_pipe.named_steps['scaler']\n",
        "knn_fitted    = final_pipe.named_steps['knn']\n",
        "X_train_all_scaled = scaler_fitted.transform(X_train_all)\n",
        "X_test_scaled = scaler_fitted.transform(X_test)\n",
        "\n",
        "def show_neighbors_with_vote(i):\n",
        "    x = X_test_scaled[i]\n",
        "    d = pairwise_distances(X_train_all_scaled, [x]).ravel()\n",
        "    idx = np.argsort(d)[:knn_fitted.n_neighbors]\n",
        "    votes = Counter(y_train_all[idx])\n",
        "    print(f\"— Test idx {i}: true={y_test[i]}, pred={test_pred[i]}, votes={dict(votes)}\")\n",
        "    print(\"  neighbors (train_index | class | distance):\")\n",
        "    for j in idx:\n",
        "        print(f\"    {j:3d} | {y_train_all[j]} | {d[j]:.3f}\")\n",
        "\n",
        "# Show neighbors for three random test points\n",
        "for i in np.random.RandomState(42).choice(len(X_test), size=3, replace=False):\n",
        "    show_neighbors_with_vote(int(i))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9feef4b9",
      "metadata": {
        "id": "9feef4b9"
      },
      "source": [
        "## Limitations (Current Scope) & What’s Next\n",
        "This notebook uses a **single hold‑out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k‑fold cross‑validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute‑force neighbor search (`algorithm='brute'`) and didn’t explore scalability techniques like KD‑trees, Ball Trees, or approximate nearest neighbor libraries (e.g. FAISS, HNSW). These become important when your archive grows to millions of rows or requires low‑latency predictions. Finally, we didn’t address class imbalance or cost‑sensitive KNN; these are natural extensions for more advanced courses.\n",
        "\n",
        "Use this foundational KNN workflow as a baseline. As you advance, you’ll learn to replace simple hold‑out validation with cross‑validation, integrate model pipelines into production, and explore neighbor indexing structures to scale your models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10c90ca2",
      "metadata": {
        "id": "10c90ca2"
      },
      "source": [
        "## Conclusion\n",
        "- **Scaling** prevents large‑range features from dominating distance computations.  \n",
        "- **Tuning K** via validation balances bias and variance; a very small K overfits, a very large K underfits.  \n",
        "- **Distance metric and weight** choices are hyperparameters; small grids reveal significant differences.  \n",
        "- **Abstain rules** are valuable in quality‑assurance workflows, trading coverage for reliability.  \n",
        "- **Pipeline and test evaluation** ensure leak‑free preprocessing and unbiased performance estimates.  \n",
        "- KNN remains a powerful, intuitive baseline—use it to build intuition about distance and similarity before advancing to more sophisticated models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}