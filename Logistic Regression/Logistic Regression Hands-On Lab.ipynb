{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Logistic%20Regression/Logistic%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Hands-On Lab\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand** how logistic regression models binary classification problems using the sigmoid function\n",
    "2. **Implement** a custom logistic regression classifier using gradient descent\n",
    "3. **Apply** logistic regression to real-world classification datasets\n",
    "4. **Evaluate** model performance using accuracy, precision, recall, and F1-score\n",
    "5. **Optimize** hyperparameters using K-fold cross-validation\n",
    "6. **Visualize** decision boundaries and probability distributions\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "**Logistic Regression** is a classification algorithm that models the probability of a binary outcome:\n",
    "\n",
    "$$P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w}) = \\frac{1}{1 + e^{-\\vec{x}^T \\times \\vec{w}}}$$\n",
    "\n",
    "Where:\n",
    "- $\\vec{x}$ is the input feature vector\n",
    "- $\\vec{w}$ is the weight vector\n",
    "- $\\sigma$ is the sigmoid (logistic) function\n",
    "\n",
    "**Loss Function** (Negative Log-Likelihood):\n",
    "\n",
    "$$J(\\vec{w}) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\log p^{(i)} + (1-y^{(i)}) \\log(1-p^{(i)}) \\right]$$\n",
    "\n",
    "**Gradient:**\n",
    "\n",
    "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y})$$\n",
    "\n",
    "**Gradient Descent Update:**\n",
    "\n",
    "$$\\vec{w} = \\vec{w} - \\alpha \\nabla_{\\vec{w}} J$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "The **learning rate** $\\alpha$ controls how much we adjust weights at each iteration:\n",
    "\n",
    "- **Too small**: Slow convergence, may take many iterations\n",
    "- **Too large**: May overshoot the minimum, fail to converge\n",
    "- **Just right**: Converges efficiently to the optimum\n",
    "\n",
    "Typical values: $\\alpha \\in [0.001, 0.1]$ for normalized features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode for Logistic Regression\n",
    "\n",
    "```\n",
    "# Logistic Regression \u2014 Gradient Descent on NLL\n",
    "# Inputs\n",
    "# data \u2190 (X, y) with y \u2208 {0,1}\n",
    "# \u03b7 \u2190 learning rate\n",
    "# max_iter \u2190 maximum iterations\n",
    "# tol \u2190 stop when ||\u2207L(w)|| \u2264 tol\n",
    "# X_query \u2190 examples to predict\n",
    "\n",
    "# ----- fit -----\n",
    "\u03a6 \u2190 concat_column(ones(N), X)      # design matrix with bias\n",
    "w \u2190 zeros(columns(\u03a6))               # initialize\n",
    "\n",
    "# NLL: L(w) = - \u03a3 [ y log p + (1\u2212y) log(1\u2212p) ], p = \u03c3(\u03a6w)\n",
    "FOR t = 1 TO max_iter DO\n",
    "    z \u2190 \u03a6 \u00b7 w\n",
    "    p \u2190 1 / (1 + exp(\u2212z))           # sigmoid\n",
    "    g \u2190 transpose(\u03a6) \u00b7 (p \u2212 y)      # \u2207L(w)\n",
    "    IF norm(g) \u2264 tol THEN BREAK\n",
    "    w \u2190 w \u2212 \u03b7 \u00b7 g                   # GD step\n",
    "END FOR\n",
    "\n",
    "# ----- predict -----\n",
    "\u03a6* \u2190 concat_column(ones(|X_query|), X_query)\n",
    "p* \u2190 1 / (1 + exp(\u2212\u03a6* \u00b7 w))\n",
    "\u0177 \u2190 1 if p* \u2265 0.5 else 0\n",
    "RETURN p*, \u0177\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from scipy.special import expit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MyLogisticRegression\n",
    "\n",
    "Now you'll implement a custom logistic regression classifier. The class follows the scikit-learn API with `fit`, `predict`, and `predict_proba` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Custom Logistic Regression classifier using gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Step size for gradient descent updates\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    tol : float, default=1e-6\n",
    "        Tolerance for gradient norm to declare convergence\n",
    "    random_state : int, default=None\n",
    "        Random seed for weight initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6, random_state=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the logistic regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values (0 or 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # TODO: Create design matrix Phi by adding bias column to X\n",
    "        # Hint: Phi = np.c_[np.ones(len(X)), X]\n",
    "        Phi = np.c_[np.ones(len(X)), X]\n",
    "        \n",
    "        # TODO: Initialize weights with small random values\n",
    "        # Hint: Use np.random.seed() and np.random.randn()\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        self.weights_ = np.random.randn(Phi.shape[1]) * 0.01\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.loss_history_ = []\n",
    "        \n",
    "        # Gradient descent loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            # TODO: Compute scores (z = Phi @ weights)\n",
    "            scores = Phi @ self.weights_\n",
    "            \n",
    "            # TODO: Apply sigmoid to get probabilities\n",
    "            # Hint: Use expit() from scipy.special\n",
    "            probabilities = expit(scores)\n",
    "            \n",
    "            # TODO: Compute NLL loss\n",
    "            # NLL = -\u03a3[y*log(p) + (1-y)*log(1-p)]\n",
    "            # Use epsilon = 1e-15 for numerical stability\n",
    "            epsilon = 1e-15\n",
    "            p_safe = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "            nll = -np.sum(y * np.log(p_safe) + (1 - y) * np.log(1 - p_safe))\n",
    "            self.loss_history_.append(nll)\n",
    "            \n",
    "            # TODO: Compute gradient: \u2207L = \u03a6^T (p - y)\n",
    "            gradient = Phi.T @ (probabilities - y)\n",
    "            \n",
    "            # TODO: Check convergence (if gradient norm < tolerance, break)\n",
    "            if np.linalg.norm(gradient) < self.tol:\n",
    "                break\n",
    "            \n",
    "            # TODO: Update weights: w = w - learning_rate * gradient\n",
    "            self.weights_ -= self.learning_rate * gradient\n",
    "        \n",
    "        self.n_iter_ = iteration + 1\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        proba : array, shape (n_samples, 2)\n",
    "            Probabilities for each class [P(y=0), P(y=1)]\n",
    "        \"\"\"\n",
    "        # TODO: Create design matrix for X\n",
    "        Phi = np.c_[np.ones(len(X)), X]\n",
    "        \n",
    "        # TODO: Compute scores\n",
    "        scores = Phi @ self.weights_\n",
    "        \n",
    "        # TODO: Apply sigmoid to get P(y=1|X)\n",
    "        p1 = expit(scores)\n",
    "        \n",
    "        # Return probabilities for both classes\n",
    "        return np.column_stack([1 - p1, p1])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Predicted class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        # TODO: Get probabilities and threshold at 0.5\n",
    "        # Hint: Use predict_proba and check if P(y=1) >= 0.5\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Simple Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple test data\n",
    "np.random.seed(42)\n",
    "X_simple = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "y_simple = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Fit model\n",
    "model_simple = MyLogisticRegression(learning_rate=0.1, max_iter=1000)\n",
    "model_simple.fit(X_simple, y_simple)\n",
    "\n",
    "# Predict\n",
    "y_pred_simple = model_simple.predict(X_simple)\n",
    "print(\"True labels:\", y_simple)\n",
    "print(\"Predictions:\", y_pred_simple)\n",
    "print(\"Accuracy:\", accuracy_score(y_simple, y_pred_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Binary Classification Data\n",
    "\n",
    "We'll use the same data generation approach from the lecture slides (slide 26)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two-class data\n",
    "m = 100  # samples per class\n",
    "n = 2    # features\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Class 0: centered around (1.5, -1.5)\n",
    "class_0 = np.hstack((\n",
    "    1.5 + np.random.randn(m, 1),\n",
    "    -1.5 + np.random.randn(m, 1)\n",
    "))\n",
    "\n",
    "# Class 1: centered around (-1.5, 1.5)\n",
    "class_1 = np.hstack((\n",
    "    -1.5 + np.random.randn(m, 1),\n",
    "    1.5 + np.random.randn(m, 1)\n",
    "))\n",
    "\n",
    "# Combine\n",
    "X = np.vstack((class_0, class_1))\n",
    "y = np.concatenate([np.zeros(m), np.ones(m)])\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Class 0', edgecolors='k', s=50)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='skyblue', label='Class 1', edgecolors='k', s=50)\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Binary Classification Dataset', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = MyLogisticRegression(learning_rate=0.1, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training completed in {model.n_iter_} iterations\")\n",
    "print(f\"Final weights: {model.weights_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.loss_history_, linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Negative Log-Likelihood', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Show some predictions\n",
    "print(\"\\nSample predictions (first 10):\")\n",
    "for i in range(min(10, len(y_test))):\n",
    "    print(f\"True: {int(y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i])}, \"\n",
    "          f\"Predicted: {y_pred[i]}, \"\n",
    "          f\"P(y=1): {y_proba[i, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\n[TN  FP]\")\n",
    "print(\"[FN  TP]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', fontsize=20)\n",
    "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Calculate metrics manually\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                        np.linspace(x2_min, x2_max, 200))\n",
    "\n",
    "# Predict probabilities for mesh\n",
    "X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "probs_mesh = model.predict_proba(X_mesh)[:, 1].reshape(xx1.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
    "plt.colorbar(label='P(y=1|x,w)')\n",
    "plt.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "            c='orange', label='Train Class 0', edgecolors='k', s=50, marker='o')\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "            c='skyblue', label='Train Class 1', edgecolors='k', s=50, marker='o')\n",
    "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], \n",
    "            c='orange', label='Test Class 0', edgecolors='k', s=100, marker='s')\n",
    "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], \n",
    "            c='skyblue', label='Test Class 1', edgecolors='k', s=100, marker='s')\n",
    "\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Decision Boundary', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model_lr = MyLogisticRegression(learning_rate=lr, max_iter=1000, random_state=42)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    accuracy_lr = accuracy_score(y_test, model_lr.predict(X_test))\n",
    "    results[lr] = {\n",
    "        'model': model_lr,\n",
    "        'accuracy': accuracy_lr,\n",
    "        'n_iter': model_lr.n_iter_,\n",
    "        'final_loss': model_lr.loss_history_[-1]\n",
    "    }\n",
    "    print(f\"Learning Rate={lr}: Accuracy={accuracy_lr:.4f}, \"\n",
    "          f\"Iterations={model_lr.n_iter_}, Final Loss={model_lr.loss_history_[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for lr in learning_rates:\n",
    "    plt.plot(results[lr]['model'].loss_history_, label=f'\u03b1={lr}', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Negative Log-Likelihood', fontsize=12)\n",
    "plt.title('Training Loss for Different Learning Rates', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "Let's use K-fold cross-validation to get a more reliable estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use training data for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    MyLogisticRegression(learning_rate=0.1, max_iter=1000, random_state=42),\n",
    "    X_train, y_train, cv=kf, scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features for Non-Linear Decision Boundaries\n",
    "\n",
    "Now let's work with a more complex dataset that requires non-linear decision boundaries. We'll use the mixture dataset from the lecture slides and apply polynomial features to capture the non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Mixture Dataset from Google Drive\n",
    "\n",
    "This dataset contains two classes with non-linear separation (as shown in the lecture slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the mixture dataset from Google Drive\n",
    "# File ID: 1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT\n",
    "# Direct download URL\n",
    "url = 'https://drive.google.com/uc?id=1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT'\n",
    "\n",
    "# Load data\n",
    "df_mixture = pd.read_csv(url)\n",
    "print(f\"Mixture dataset shape: {df_mixture.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_mixture.head())\n",
    "print(f\"\\nColumn names: {df_mixture.columns.tolist()}\")\n",
    "print(f\"Class distribution:\\n{df_mixture.iloc[:, -1].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Mixture Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "# Assuming last column is the label, and first columns are features\n",
    "X_mixture = df_mixture.iloc[:, :-1].values\n",
    "y_mixture = df_mixture.iloc[:, -1].values\n",
    "\n",
    "print(f\"Features shape: {X_mixture.shape}\")\n",
    "print(f\"Labels shape: {y_mixture.shape}\")\n",
    "print(f\"Unique labels: {np.unique(y_mixture)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Mixture Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_mixture[y_mixture == 0, 0], X_mixture[y_mixture == 0, 1], \n",
    "            c='orange', label='Class 0', edgecolors='k', s=50, alpha=0.7)\n",
    "plt.scatter(X_mixture[y_mixture == 1, 0], X_mixture[y_mixture == 1, 1], \n",
    "            c='skyblue', label='Class 1', edgecolors='k', s=50, alpha=0.7)\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Mixture Dataset (Non-Linear Boundary)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Mixture Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_mix_train, X_mix_test, y_mix_train, y_mix_test = train_test_split(\n",
    "    X_mixture, y_mixture, test_size=0.3, random_state=42, stratify=y_mixture)\n",
    "\n",
    "print(f\"Mixture training set: {X_mix_train.shape[0]} samples\")\n",
    "print(f\"Mixture test set: {X_mix_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Polynomial Features to Mixture Data\n",
    "\n",
    "Let's test different polynomial degrees to find the best model for this non-linear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different polynomial degrees\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "poly_results = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_mix_train_poly = poly.fit_transform(X_mix_train)\n",
    "    X_mix_test_poly = poly.transform(X_mix_test)\n",
    "    \n",
    "    # Train model with smaller learning rate for higher dimensions\n",
    "    lr = 0.01 if degree <= 2 else 0.001\n",
    "    model_poly = MyLogisticRegression(learning_rate=lr, max_iter=3000, random_state=42)\n",
    "    model_poly.fit(X_mix_train_poly, y_mix_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_mix_pred_poly = model_poly.predict(X_mix_test_poly)\n",
    "    accuracy_poly = accuracy_score(y_mix_test, y_mix_pred_poly)\n",
    "    \n",
    "    poly_results[degree] = {\n",
    "        'poly': poly,\n",
    "        'model': model_poly,\n",
    "        'accuracy': accuracy_poly,\n",
    "        'n_features': X_mix_train_poly.shape[1]\n",
    "    }\n",
    "    \n",
    "    print(f\"Degree={degree}: Features={X_mix_train_poly.shape[1]}, \"\n",
    "          f\"Accuracy={accuracy_poly:.4f}, Iterations={model_poly.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Polynomial Decision Boundaries on Mixture Data\n",
    "\n",
    "Notice how higher-degree polynomials can capture more complex, non-linear boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplot grid based on number of degrees\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Get data ranges\n",
    "x1_min_mix, x1_max_mix = X_mixture[:, 0].min() - 0.5, X_mixture[:, 0].max() + 0.5\n",
    "x2_min_mix, x2_max_mix = X_mixture[:, 1].min() - 0.5, X_mixture[:, 1].max() + 0.5\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get polynomial transformer and model\n",
    "    poly = poly_results[degree]['poly']\n",
    "    model_poly = poly_results[degree]['model']\n",
    "    \n",
    "    # Create mesh\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min_mix, x1_max_mix, 200),\n",
    "                            np.linspace(x2_min_mix, x2_max_mix, 200))\n",
    "    X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    X_mesh_poly = poly.transform(X_mesh)\n",
    "    probs_mesh = model_poly.predict_proba(X_mesh_poly)[:, 1].reshape(xx1.shape)\n",
    "    \n",
    "    # Plot contours and decision boundary\n",
    "    ax.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
    "    ax.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2.5)\n",
    "    \n",
    "    # Plot training data\n",
    "    ax.scatter(X_mix_train[y_mix_train == 0, 0], X_mix_train[y_mix_train == 0, 1],\n",
    "                c='orange', edgecolors='k', s=40, alpha=0.7, label='Class 0 (train)')\n",
    "    ax.scatter(X_mix_train[y_mix_train == 1, 0], X_mix_train[y_mix_train == 1, 1],\n",
    "                c='skyblue', edgecolors='k', s=40, alpha=0.7, label='Class 1 (train)')\n",
    "    \n",
    "    # Plot test data with different marker\n",
    "    ax.scatter(X_mix_test[y_mix_test == 0, 0], X_mix_test[y_mix_test == 0, 1],\n",
    "                c='orange', edgecolors='k', s=80, marker='s', alpha=0.9, label='Class 0 (test)')\n",
    "    ax.scatter(X_mix_test[y_mix_test == 1, 0], X_mix_test[y_mix_test == 1, 1],\n",
    "                c='skyblue', edgecolors='k', s=80, marker='s', alpha=0.9, label='Class 1 (test)')\n",
    "    \n",
    "    ax.set_title(f'Degree={degree}, Features={poly_results[degree][\"n_features\"]}, '\\\n",
    "                     f'Acc={poly_results[degree][\"accuracy\"]:.3f}', fontsize=12)\n",
    "    ax.set_xlabel('$x_1$', fontsize=11)\n",
    "    ax.set_ylabel('$x_2$', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=9, loc='best')\n",
    "\n",
    "# Hide the last subplot if we have fewer than 6 degrees\n",
    "if len(degrees) < 6:\n",
    "    axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Polynomial Degrees\n",
    "\n",
    "Observe the following:\n",
    "- **Degree 1 (Linear)**: Cannot capture the non-linear boundary, lower accuracy\n",
    "- **Degree 2 (Quadratic)**: Begins to capture curvature in the decision boundary\n",
    "- **Degree 3-4**: Better fit for complex boundaries\n",
    "- **Degree 5+**: Risk of overfitting - may fit training noise rather than true pattern\n",
    "\n",
    "**Key insight**: The mixture dataset requires polynomial features because the classes are not linearly separable. This demonstrates why feature engineering (like polynomial features) is important for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sklearn model\n",
    "sklearn_model = SklearnLogisticRegression(penalty=None, max_iter=1000, random_state=42)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare\n",
    "our_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "sklearn_accuracy = sklearn_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Our model accuracy:      {our_accuracy:.4f}\")\n",
    "print(f\"sklearn model accuracy:  {sklearn_accuracy:.4f}\")\n",
    "print(f\"\\nOur weights:     {model.weights_}\")\n",
    "print(f\"sklearn weights: {np.concatenate([sklearn_model.intercept_, sklearn_model.coef_[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Choice Questions\n",
    "\n",
    "Test your understanding of logistic regression!\n",
    "\n",
    "### Question 1: What does the sigmoid function do?\n",
    "\n",
    "A) Maps any real number to [0, 1]  \n",
    "B) Maps probabilities to real numbers  \n",
    "C) Computes the gradient  \n",
    "D) Normalizes features  \n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: A**\n",
    "\n",
    "The sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ maps any real-valued input to a value between 0 and 1, which can be interpreted as a probability.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What is the loss function for logistic regression?\n",
    "\n",
    "A) Sum of Squared Errors (SSE)  \n",
    "B) Mean Squared Error (MSE)  \n",
    "C) Negative Log-Likelihood (NLL) / Binary Cross-Entropy  \n",
    "D) Absolute Error  \n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: C**\n",
    "\n",
    "Logistic regression uses the Negative Log-Likelihood (also called binary cross-entropy):\n",
    "$$J(\\vec{w}) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\log p^{(i)} + (1-y^{(i)}) \\log(1-p^{(i)}) \\right]$$\n",
    "\n",
    "This loss function is appropriate for classification because it measures how well predicted probabilities match the actual labels.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What is the gradient for logistic regression?\n",
    "\n",
    "A) $\\nabla J = \\Phi^T (\\vec{y} - \\vec{p})$  \n",
    "B) $\\nabla J = \\Phi^T (\\vec{p} - \\vec{y})$  \n",
    "C) $\\nabla J = -2\\Phi^T (\\vec{y} - \\vec{p})$  \n",
    "D) $\\nabla J = \\Phi (\\vec{p} - \\vec{y})$  \n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "The gradient of the NLL loss with respect to weights is:\n",
    "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y})$$\n",
    "\n",
    "Where $\\vec{p}$ are the predicted probabilities and $\\vec{y}$ are the true labels.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What does the decision boundary represent?\n",
    "\n",
    "A) Where P(y=1|x) = 1.0  \n",
    "B) Where P(y=1|x) = 0.5  \n",
    "C) Where P(y=1|x) = 0.0  \n",
    "D) The maximum probability  \n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "The decision boundary is where $P(y=1|\\vec{x}, \\vec{w}) = 0.5$, which occurs when $\\vec{x}^T \\times \\vec{w} = 0$. Points on one side of this boundary are classified as class 1, and points on the other side as class 0.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: When should we use higher learning rates?\n",
    "\n",
    "A) Always, to converge faster  \n",
    "B) When features are normalized  \n",
    "C) When the model is overfitting  \n",
    "D) Never, always use small learning rates  \n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "Higher learning rates (e.g., 0.1) can be used when features are normalized (e.g., using StandardScaler) because the gradients are on a similar scale. Without normalization, features with different scales can cause unstable updates, requiring smaller learning rates.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: What metric is most important for spam detection?\n",
    "\n",
    "A) Accuracy  \n",
    "B) Precision (minimize false positives)  \n",
    "C) Recall (minimize false negatives)  \n",
    "D) F1-score  \n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B - Precision**\n",
    "\n",
    "For spam detection, **precision** is typically more important because:\n",
    "- **False positives** (legitimate emails marked as spam) are very costly - users might miss important emails\n",
    "- **False negatives** (spam in inbox) are annoying but less costly\n",
    "\n",
    "However, this depends on the specific application requirements. Some systems might prioritize recall to catch all spam, even if it means occasionally flagging legitimate emails.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Feature Scaling\n",
    "Always normalize/standardize features when using gradient descent:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### 2. Learning Rate Selection\n",
    "- Start with \u03b1 = 0.01 for normalized features\n",
    "- If loss increases or oscillates: reduce \u03b1\n",
    "- If convergence is too slow: increase \u03b1\n",
    "- Monitor loss curve to diagnose\n",
    "\n",
    "### 3. Handling Class Imbalance\n",
    "- Use stratified splits: `train_test_split(..., stratify=y)`\n",
    "- Consider weighted loss or resampling\n",
    "- Focus on precision/recall instead of accuracy\n",
    "\n",
    "### 4. Convergence\n",
    "- Set reasonable `max_iter` (e.g., 1000-10000)\n",
    "- Use `tol` to stop early when gradient is small\n",
    "- Check if loss is still decreasing\n",
    "\n",
    "### 5. Multiclass Classification\n",
    "For more than 2 classes, use:\n",
    "- One-vs-Rest (OvR): Train C binary classifiers\n",
    "- Softmax regression (multinomial logistic regression)\n",
    "\n",
    "### 6. Regularization\n",
    "To prevent overfitting:\n",
    "- L2 regularization: Add $\\lambda ||\\vec{w}||^2$ to loss\n",
    "- L1 regularization: Add $\\lambda ||\\vec{w}||_1$ to loss\n",
    "\n",
    "### 7. Evaluation\n",
    "- Use cross-validation for small datasets\n",
    "- Report multiple metrics: accuracy, precision, recall, F1\n",
    "- Visualize confusion matrix\n",
    "- Plot ROC curve and PR curve for threshold selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "1. \u2705 Implemented a custom **Logistic Regression** classifier from scratch\n",
    "2. \u2705 Understood the **sigmoid function** and how it models probabilities\n",
    "3. \u2705 Applied **gradient descent** to minimize the negative log-likelihood loss\n",
    "4. \u2705 Experimented with different **learning rates** and observed their effects\n",
    "5. \u2705 Used **K-fold cross-validation** to evaluate model performance\n",
    "6. \u2705 Applied **polynomial features** to model non-linear decision boundaries\n",
    "7. \u2705 Evaluated models using **accuracy, precision, recall, and F1-score**\n",
    "8. \u2705 Visualized **decision boundaries** and probability distributions\n",
    "9. \u2705 Compared your implementation with scikit-learn\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Logistic regression models $P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w})$\n",
    "- The loss function is the negative log-likelihood (binary cross-entropy)\n",
    "- The gradient is $\\nabla J = \\Phi^T (\\vec{p} - \\vec{y})$\n",
    "- Learning rate must be tuned carefully\n",
    "- Feature scaling improves convergence\n",
    "- Different applications require different metric priorities (precision vs recall)\n",
    "- Cross-validation provides more reliable performance estimates\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try logistic regression on real-world datasets (e.g., breast cancer, iris)\n",
    "- Implement multiclass classification using One-vs-Rest\n",
    "- Add L2 regularization to prevent overfitting\n",
    "- Experiment with different optimization algorithms (SGD, Adam)\n",
    "- Compare with other classifiers (SVM, Decision Trees, Neural Networks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}