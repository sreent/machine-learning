{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Logistic%20Regression/Logistic%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0oFpqjhJNk"
      },
      "source": [
        "# Logistic Regression: Hands-On Lab\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. **Understand** how logistic regression models binary classification problems using the sigmoid function\n",
        "2. **Implement** a custom logistic regression classifier using gradient descent\n",
        "3. **Apply** logistic regression to real-world classification datasets\n",
        "4. **Evaluate** model performance using accuracy, precision, recall, and F1-score\n",
        "5. **Optimize** hyperparameters using K-fold cross-validation\n",
        "6. **Visualize** decision boundaries and probability distributions\n",
        "\n",
        "## Algorithm Overview\n",
        "\n",
        "**Logistic Regression** is a classification algorithm that models the probability of a binary outcome:\n",
        "\n",
        "$$P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w}) = \\frac{1}{1 + e^{-\\vec{x}^T \\times \\vec{w}}}$$\n",
        "\n",
        "Where:\n",
        "- $\\vec{x}$ is the input feature vector\n",
        "- $\\vec{w}$ is the weight vector\n",
        "- $\\sigma$ is the sigmoid (logistic) function\n",
        "\n",
        "**Loss Function** (Negative Log-Likelihood):\n",
        "\n",
        "$$J(\\vec{w}) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\log p^{(i)} + (1-y^{(i)}) \\log(1-p^{(i)}) \\right]$$\n",
        "\n",
        "**Gradient:**\n",
        "\n",
        "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y})$$\n",
        "\n",
        "**Gradient Descent Update:**\n",
        "\n",
        "$$\\vec{w} = \\vec{w} - \\alpha \\nabla_{\\vec{w}} J$$\n",
        "\n",
        "Where $\\alpha$ is the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "### Why These Components?\n",
        "\n",
        "**Why sigmoid?** The sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is used because:\n",
        "- It maps any real number to (0,1), perfect for probabilities\n",
        "- It's differentiable everywhere (needed for gradient descent)\n",
        "- It has nice mathematical properties: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$\n",
        "\n",
        "**Why NLL loss?** The negative log-likelihood is the natural loss function for probabilistic classification because it directly measures how well our predicted probabilities match the true labels. Maximizing likelihood = minimizing NLL.\n",
        "\n",
        "**Why gradient descent?** Unlike linear regression, logistic regression has no closed-form solution. Gradient descent iteratively finds the optimal weights by following the direction that most reduces the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmU3lSIbhJNk"
      },
      "source": [
        "## When to Use Logistic Regression\n",
        "\n",
        "Logistic regression is a fundamental classification algorithm with specific strengths and limitations. Understanding when to use it is crucial for effective model selection.\n",
        "\n",
        "### ‚úÖ Use Logistic Regression When:\n",
        "\n",
        "**1. Binary Classification with Linearly Separable Features**\n",
        "- Classes can be separated by a linear decision boundary (or made separable with feature transformations)\n",
        "- Examples: spam detection, medical diagnosis (disease/no disease), customer churn prediction\n",
        "\n",
        "**2. Need Probabilistic Outputs**\n",
        "- When you need P(y=1|x) probabilities, not just class predictions\n",
        "- Critical for risk assessment, confidence scoring, or threshold tuning\n",
        "- Example: \"This email has 87% probability of being spam\"\n",
        "\n",
        "**3. Interpretable Coefficients Required**\n",
        "- Each feature has a clear weight showing its contribution\n",
        "- Positive coefficient ‚Üí feature increases probability of class 1\n",
        "- Negative coefficient ‚Üí feature decreases probability of class 1\n",
        "- Essential in healthcare, finance, and regulated industries\n",
        "\n",
        "**4. Small to Medium Datasets**\n",
        "- Works well with datasets from hundreds to hundreds of thousands of samples\n",
        "- Efficient training with gradient descent or closed-form solutions\n",
        "- Lower computational cost than complex models\n",
        "\n",
        "**5. Baseline Model for Comparison**\n",
        "- Start with logistic regression as a simple, interpretable baseline\n",
        "- Compare more complex models against it to justify added complexity\n",
        "\n",
        "### ‚ùå Don't Use Logistic Regression When:\n",
        "\n",
        "**1. Highly Non-Linear Decision Boundaries**\n",
        "- If classes require complex, curved boundaries that can't be approximated with polynomial features\n",
        "- **Better alternatives**: Kernel SVM, Decision Trees, Random Forests, Neural Networks\n",
        "\n",
        "**2. Many Categorical Features**\n",
        "- Logistic regression struggles with high-cardinality categorical features\n",
        "- One-hot encoding creates many sparse features\n",
        "- **Better alternatives**: Tree-based methods (Random Forest, XGBoost, LightGBM)\n",
        "\n",
        "**3. Very Large Datasets with Many Features**\n",
        "- Standard gradient descent can be slow for millions of samples\n",
        "- **Better alternatives**: SGD-based approaches, Neural Networks with mini-batch training\n",
        "\n",
        "**4. Multiclass Classification (Without Extensions)**\n",
        "- Standard logistic regression is binary; needs One-vs-Rest or multinomial extensions\n",
        "- **Better alternatives**: Softmax regression, tree-based methods, neural networks\n",
        "\n",
        "**5. Need Feature Interactions Without Manual Engineering**\n",
        "- Logistic regression requires explicit polynomial features for interactions\n",
        "- **Better alternatives**: Decision Trees (automatically find interactions), Neural Networks\n",
        "\n",
        "### Quick Comparison: Logistic Regression vs Other Classifiers\n",
        "\n",
        "| Criterion | Logistic Regression | Decision Trees | SVM (RBF) | Neural Networks |\n",
        "|-----------|-------------------|----------------|-----------|-----------------|\n",
        "| **Linear boundaries** | ‚úÖ Excellent | ‚ùå Weak | ‚úÖ Good | ‚úÖ Good |\n",
        "| **Non-linear boundaries** | ‚ö†Ô∏è Manual features | ‚úÖ Excellent | ‚úÖ Excellent | ‚úÖ Excellent |\n",
        "| **Interpretability** | ‚úÖ Excellent | ‚úÖ Good | ‚ùå Poor | ‚ùå Very Poor |\n",
        "| **Probabilistic output** | ‚úÖ Natural | ‚ö†Ô∏è Approximation | ‚ö†Ô∏è Calibration needed | ‚úÖ Softmax |\n",
        "| **Training speed** | ‚úÖ Fast | ‚úÖ Fast | ‚ö†Ô∏è Moderate | ‚ùå Slow |\n",
        "| **Small datasets** | ‚úÖ Excellent | ‚úÖ Good | ‚úÖ Excellent | ‚ùå Overfits |\n",
        "| **Large datasets** | ‚úÖ Good | ‚úÖ Excellent | ‚ö†Ô∏è Slow | ‚úÖ Excellent |\n",
        "| **Categorical features** | ‚ö†Ô∏è One-hot | ‚úÖ Native | ‚ö†Ô∏è One-hot | ‚ö†Ô∏è Embedding |\n",
        "\n",
        "### Real-World Applications Where Logistic Regression Excels:\n",
        "\n",
        "1. **Medical Diagnosis**: Predicting disease presence based on symptoms and test results\n",
        "2. **Credit Scoring**: Assessing loan default risk with interpretable coefficients for regulators\n",
        "3. **Marketing**: Predicting customer purchase probability or email click-through rates\n",
        "4. **Fraud Detection**: Identifying fraudulent transactions (when combined with good features)\n",
        "5. **Customer Churn**: Predicting which customers will leave a service\n",
        "6. **A/B Testing**: Analyzing treatment effects in experiments\n",
        "\n",
        "### The Bottom Line:\n",
        "\n",
        "**Logistic regression is your go-to algorithm when:**\n",
        "- You need a simple, fast, interpretable binary classifier\n",
        "- Decision boundaries are approximately linear (or can be made so with feature engineering)\n",
        "- You need calibrated probability estimates\n",
        "- You're establishing a baseline before trying complex models\n",
        "\n",
        "**Consider alternatives when:**\n",
        "- Decision boundaries are highly non-linear and feature engineering is impractical\n",
        "- You have massive datasets and need maximum predictive power\n",
        "- Interpretability is not a requirement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHax-5phhJNk"
      },
      "source": [
        "## Pseudocode for Logistic Regression\n",
        "\n",
        "```\n",
        "# Logistic Regression ‚Äî Gradient Descent on NLL\n",
        "# Inputs\n",
        "# data ‚Üê (X, y) with y ‚àà {0,1}\n",
        "# Œ∑ ‚Üê learning rate\n",
        "# max_iter ‚Üê maximum iterations\n",
        "# tol ‚Üê stop when ||‚àáL(w)|| ‚â§ tol\n",
        "# X_query ‚Üê examples to predict\n",
        "\n",
        "# ----- fit -----\n",
        "Œ¶ ‚Üê concat_column(ones(N), X)      # design matrix with bias\n",
        "w ‚Üê zeros(columns(Œ¶))               # initialize\n",
        "\n",
        "# NLL: L(w) = - Œ£ [ y log p + (1‚àíy) log(1‚àíp) ], p = œÉ(Œ¶w)\n",
        "FOR t = 1 TO max_iter DO\n",
        "    z ‚Üê Œ¶ ¬∑ w\n",
        "    p ‚Üê 1 / (1 + exp(‚àíz))           # sigmoid\n",
        "    g ‚Üê transpose(Œ¶) ¬∑ (p ‚àí y)      # ‚àáL(w)\n",
        "    IF norm(g) ‚â§ tol THEN BREAK\n",
        "    w ‚Üê w ‚àí Œ∑ ¬∑ g                   # GD step\n",
        "END FOR\n",
        "\n",
        "# ----- predict -----\n",
        "Œ¶* ‚Üê concat_column(ones(|X_query|), X_query)\n",
        "p* ‚Üê 1 / (1 + exp(‚àíŒ¶* ¬∑ w))\n",
        "≈∑ ‚Üê 1 if p* ‚â• 0.5 else 0\n",
        "RETURN p*, ≈∑\n",
        "```\n",
        "\n",
        "**Note:** The **design matrix** Œ¶ is the feature matrix X with a column of 1s prepended for the bias term (intercept). This allows us to include the bias in the weight vector w, simplifying the math: instead of computing $w_0 + x_1w_1 + x_2w_2$, we compute $\\vec{\\phi}^T \\cdot \\vec{w}$ where $\\vec{\\phi} = [1, x_1, x_2]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3D2YZ9hJNl"
      },
      "source": [
        "## Learning Rate\n",
        "\n",
        "Now that we've seen the algorithm, let's understand the **learning rate** $\\alpha$ - a critical hyperparameter that controls how much we adjust weights at each iteration:\n",
        "\n",
        "- **Too small**: Slow convergence, may take many iterations\n",
        "- **Too large**: May overshoot the minimum, fail to converge\n",
        "- **Just right**: Converges efficiently to the optimum\n",
        "\n",
        "Typical values: $\\alpha \\in [0.001, 0.1]$ for normalized features.\n",
        "\n",
        "**Why does feature scaling affect learning rate?** If features have different scales (e.g., $x_1 \\in [0,1]$ and $x_2 \\in [0,10000]$), the gradient components will have very different magnitudes. A learning rate suitable for $x_2$ might be too large for $x_1$, causing oscillation. Feature scaling ensures all gradients are on similar scales, allowing one learning rate to work well for all features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7790CdbhJNl"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56w1BrM8hJNl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
        "from scipy.special import expit  # expit is the numerically stable sigmoid function: œÉ(z) = 1/(1+e^(-z))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v75-Pg3qhJNl"
      },
      "source": [
        "## Exercise 1: Implement Sigmoid and Prediction\n",
        "\n",
        "Welcome to the hands-on implementation! We'll build the MyLogisticRegression class in **three independent exercises** to help you test and debug each component.\n",
        "\n",
        "**In this exercise, you'll implement:**\n",
        "- `_sigmoid()`: The sigmoid activation function\n",
        "- `predict_proba()`: Probability prediction method\n",
        "\n",
        "**Why separate exercises?**\n",
        "- Test each component immediately\n",
        "- No cascading failures\n",
        "- Build confidence step-by-step\n",
        "- Easy to debug if something goes wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myf0moYPhJNl"
      },
      "outputs": [],
      "source": [
        "class MyLogisticRegression(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Custom Logistic Regression classifier using gradient descent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    learning_rate : float, default=0.01\n",
        "        Step size for gradient descent updates\n",
        "    max_iter : int, default=1000\n",
        "        Maximum number of iterations\n",
        "    tol : float, default=1e-6\n",
        "        Tolerance for gradient norm to declare convergence\n",
        "    random_state : int, default=None\n",
        "        Random seed for weight initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6, random_state=None):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute the sigmoid function: œÉ(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : array-like\n",
        "            Input values (scores)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sigmoid : array-like\n",
        "            Sigmoid outputs in range (0, 1)\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        We use scipy.special.expit for numerical stability instead of 1/(1+exp(-z))\n",
        "        \"\"\"\n",
        "        # TODO: Implement sigmoid function using expit from scipy.special\n",
        "        # Hint: expit(z) computes 1/(1+exp(-z)) in a numerically stable way\n",
        "        return None\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Samples\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        proba : array, shape (n_samples, 2)\n",
        "            Probabilities for each class [P(y=0), P(y=1)]\n",
        "        \"\"\"\n",
        "        # TODO: Create design matrix Phi by adding column of 1s to X for bias\n",
        "        # Hint: Use np.c_[np.ones(X.shape[0]), X]\n",
        "        Phi = None\n",
        "\n",
        "        # TODO: Compute scores z = Phi @ weights\n",
        "        scores = None\n",
        "\n",
        "        # TODO: Apply sigmoid to get P(y=1|X) using your _sigmoid method\n",
        "        p1 = None\n",
        "\n",
        "        # Return probabilities for both classes [P(y=0), P(y=1)]\n",
        "        return np.column_stack([1 - p1, p1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK47qNDzhJNl"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EXERCISE 1 VERIFICATION: Testing Sigmoid and Prediction\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create a test instance\n",
        "model_ex1 = MyLogisticRegression()\n",
        "\n",
        "# Provide pre-trained weights for testing\n",
        "# These weights were learned from a simple XOR-like problem\n",
        "model_ex1.weights_ = np.array([0.5, 1.2, -0.8])\n",
        "\n",
        "# Test data: 4 samples, 2 features\n",
        "X_test_ex1 = np.array([\n",
        "    [0.0, 0.0],\n",
        "    [1.0, 1.0],\n",
        "    [1.0, 0.0],\n",
        "    [0.0, 1.0]\n",
        "])\n",
        "\n",
        "print(\"\\n1. Testing _sigmoid function:\")\n",
        "print(\"-\" * 70)\n",
        "test_scores = np.array([-2, -1, 0, 1, 2])\n",
        "sigmoid_output = model_ex1._sigmoid(test_scores)\n",
        "print(f\"Input scores:  {test_scores}\")\n",
        "print(f\"Sigmoid output: {sigmoid_output}\")\n",
        "print(f\"Expected:       [0.119  0.269  0.500  0.731  0.881] (approximately)\")\n",
        "\n",
        "# Verify sigmoid properties\n",
        "print(f\"\\n‚úì All values in (0,1)? {np.all((sigmoid_output > 0) & (sigmoid_output < 1))}\")\n",
        "print(f\"‚úì sigmoid(0) ‚âà 0.5? {np.abs(model_ex1._sigmoid(0) - 0.5) < 0.001}\")\n",
        "print(f\"‚úì sigmoid(-z) = 1 - sigmoid(z)? {np.allclose(model_ex1._sigmoid(-test_scores), 1 - model_ex1._sigmoid(test_scores))}\")\n",
        "\n",
        "print(\"\\n2. Testing predict_proba function:\")\n",
        "print(\"-\" * 70)\n",
        "probabilities = model_ex1.predict_proba(X_test_ex1)\n",
        "print(f\"Test samples:\\n{X_test_ex1}\\n\")\n",
        "print(f\"Predicted probabilities [P(y=0), P(y=1)]:\")\n",
        "for i, (x, probs) in enumerate(zip(X_test_ex1, probabilities)):\n",
        "    print(f\"  Sample {i} {x}: P(y=0)={probs[0]:.3f}, P(y=1)={probs[1]:.3f}\")\n",
        "\n",
        "print(\"\\nExpected probabilities (approximately):\")\n",
        "print(\"  Sample 0 [0. 0.]: P(y=0)=0.378, P(y=1)=0.622\")\n",
        "print(\"  Sample 1 [1. 1.]: P(y=0)=0.452, P(y=1)=0.548\")\n",
        "print(\"  Sample 2 [1. 0.]: P(y=0)=0.142, P(y=1)=0.858\")\n",
        "print(\"  Sample 3 [0. 1.]: P(y=0)=0.669, P(y=1)=0.331\")\n",
        "\n",
        "# Verify probability properties\n",
        "print(\"\\n‚úì Probabilities sum to 1? \", np.allclose(probabilities.sum(axis=1), 1.0))\n",
        "print(f\"‚úì All probabilities in [0,1]? {np.all((probabilities >= 0) & (probabilities <= 1))}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"If your outputs match the expected values, proceed to Exercise 2!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Checkpoint Question 1: What does the sigmoid function do?\n",
        "\n",
        "A) Maps any real-valued number to the range zero to one, which can be interpreted as a probability for binary classification tasks\n",
        "\n",
        "B) Maps probability values between zero and one to their corresponding unbounded real-valued logit scores on the entire number line\n",
        "\n",
        "C) Computes the gradient of the negative log-likelihood loss function with respect to the model weights during gradient descent optimization\n",
        "\n",
        "D) Normalizes input features to have zero mean and unit variance by subtracting mean and dividing by standard deviation\n",
        "\n",
        "<details>\n",
        "<summary>Click to see answer</summary>\n",
        "\n",
        "**Answer: A**\n",
        "\n",
        "**Key Insight:** The sigmoid function œÉ(z) = 1/(1 + e^(-z)) is the bridge between linear combinations of features (which can be any real number) and probabilities (which must be between 0 and 1). It enables logistic regression to output valid probability estimates for classification.\n",
        "\n",
        "**Detailed Explanation:**\n",
        "\n",
        "The sigmoid function maps any real-valued input to a value between 0 and 1, which can be interpreted as a probability. For example:\n",
        "- œÉ(-5) ‚âà 0.007 (very low probability)\n",
        "- œÉ(0) = 0.5 (neutral/boundary)\n",
        "- œÉ(5) ‚âà 0.993 (very high probability)\n",
        "\n",
        "This is essential for logistic regression because:\n",
        "1. The linear combination x^T¬∑w can be any real number\n",
        "2. We need P(y=1|x) which must be between 0 and 1\n",
        "3. The sigmoid provides this transformation smoothly\n",
        "\n",
        "**Why other answers are incorrect:**\n",
        "\n",
        "- **B is FALSE**: This describes the inverse of the sigmoid (logit function: logit(p) = log(p/(1-p))), which maps probabilities (0,1) back to real numbers (-‚àû,+‚àû). The logit is used in deriving logistic regression but is not what the sigmoid does.\n",
        "- **C is FALSE**: The sigmoid is used in computing predictions (forward pass), not gradients. The gradient is computed using the derivative of the loss function: ‚àáJ = Œ¶^T(p - y), where p comes from applying sigmoid but the gradient computation is a separate step.\n",
        "- **D is FALSE**: This describes feature standardization (StandardScaler in scikit-learn), which is a preprocessing step unrelated to the sigmoid function. Standardization is applied to input features before training, while sigmoid is applied to the model's output scores.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "yBJ414shhJNl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Cc4czRhJNm"
      },
      "source": [
        "## Exercise 2: Implement Gradient Computation\n",
        "\n",
        "Excellent! You now have working sigmoid and prediction functions. Let's implement the **gradient computation** - the core of how the model learns.\n",
        "\n",
        "**What you'll implement:**\n",
        "- `_compute_gradient()`: Computes ‚àáJ = Œ¶·µÄ (p - y)\n",
        "\n",
        "**Why this matters:**\n",
        "The gradient tells us the direction and magnitude to adjust each weight to reduce the loss. Without a correct gradient, the model can't learn!\n",
        "\n",
        "**Testing approach:**\n",
        "We'll test the gradient computation with known values before using it in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMKA3cHYhJNm"
      },
      "outputs": [],
      "source": [
        "# Add gradient computation method to MyLogisticRegression\n",
        "def _compute_gradient(self, Phi, y, probabilities):\n",
        "    \"\"\"\n",
        "    Compute the gradient of NLL loss with respect to weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Phi : array-like, shape (n_samples, n_features + 1)\n",
        "        Design matrix (X with bias column)\n",
        "    y : array-like, shape (n_samples,)\n",
        "        True labels (0 or 1)\n",
        "    probabilities : array-like, shape (n_samples,)\n",
        "        Predicted probabilities P(y=1|X)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    gradient : array, shape (n_features + 1,)\n",
        "        Gradient vector ‚àáJ = Œ¶·µÄ (p - y)\n",
        "    \"\"\"\n",
        "    # TODO: Compute gradient using the formula: Œ¶·µÄ (p - y)\n",
        "    # Hint: Use @ operator or np.dot() for matrix multiplication\n",
        "    # Shape check: Phi is (N, D), (p-y) is (N,), result should be (D,)\n",
        "    gradient = None\n",
        "    return gradient\n",
        "\n",
        "# Add the method to the class\n",
        "MyLogisticRegression._compute_gradient = _compute_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Checkpoint Question 2: What is the gradient for logistic regression?\n",
        "\n",
        "A) The gradient is computed as the design matrix transpose times the difference between true labels and predictions: Œ¶·µÄ (y - p)\n",
        "\n",
        "B) The gradient is computed as the design matrix transpose times the difference between predictions and true labels: Œ¶·µÄ (p - y)\n",
        "\n",
        "C) The gradient is negative two times the design matrix transpose times the difference between true labels and predictions: -2Œ¶·µÄ (y - p)\n",
        "\n",
        "D) The gradient is computed as the design matrix times the difference between predictions and true labels without transposition: Œ¶ (p - y)\n",
        "\n",
        "<details>\n",
        "<summary>Click to see answer</summary>\n",
        "\n",
        "**Answer: B**\n",
        "\n",
        "**Key Insight:** The gradient ‚àáJ = Œ¶·µÄ(p - y) tells us how to adjust each weight to reduce loss. When predictions p are too high (p > y), the gradient is positive, so gradient descent subtracts from weights, reducing future predictions. When p is too low (p < y), the gradient is negative, so we add to weights, increasing predictions.\n",
        "\n",
        "**Detailed Explanation:**\n",
        "\n",
        "The gradient of the negative log-likelihood loss with respect to weights is:\n",
        "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y})$$\n",
        "\n",
        "Where:\n",
        "- Œ¶ is the design matrix (N √ó D) with a column of 1s for bias\n",
        "- p is the vector of predicted probabilities (N √ó 1)\n",
        "- y is the vector of true labels (N √ó 1)\n",
        "- The result is a D-dimensional vector showing how much to change each weight\n",
        "\n",
        "**Example with numbers:**\n",
        "If p = [0.9, 0.2, 0.7] and y = [1, 0, 1]:\n",
        "- (p - y) = [-0.1, 0.2, -0.3]\n",
        "- Then Œ¶·µÄ multiplies these errors by each feature's values\n",
        "- Features that are active when p > y will have positive gradients (decrease weight)\n",
        "- Features that are active when p < y will have negative gradients (increase weight)\n",
        "\n",
        "**Gradient descent update:**\n",
        "w_new = w_old - Œ± ¬∑ ‚àáJ = w_old - Œ± ¬∑ Œ¶·µÄ(p - y)\n",
        "\n",
        "**Why other answers are incorrect:**\n",
        "\n",
        "- **A is FALSE**: Œ¶·µÄ(y - p) has the wrong sign. This would be the gradient for maximizing the likelihood instead of minimizing the negative log-likelihood. Using this would cause gradient *ascent* instead of descent, making the loss increase.\n",
        "- **C is FALSE**: The -2 coefficient appears in the gradient of Mean Squared Error (MSE), not NLL. For MSE, the gradient is ‚àáJ = -2Œ¶·µÄ(y - ≈∑) because d/dw[(y-≈∑)¬≤] = 2(y-≈∑)¬∑(-d≈∑/dw). This is not applicable to logistic regression's loss function.\n",
        "- **D is FALSE**: Missing the transpose means the dimensions don't match. Œ¶ is (N √ó D) and (p - y) is (N √ó 1), so Œ¶(p - y) would be (N √ó 1), not a proper gradient vector of size (D √ó 1). We need Œ¶·µÄ to get the correct (D √ó 1) shape.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "d8mcVNOChJNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb22pfXshJNm"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EXERCISE 2 VERIFICATION: Testing Gradient Computation\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test gradient computation with known values\n",
        "Phi_test = np.array([\n",
        "    [1.0, 0.5, -0.3],\n",
        "    [1.0, -0.2, 0.8],\n",
        "    [1.0, 0.9, 0.1],\n",
        "    [1.0, -0.6, -0.5]\n",
        "])\n",
        "\n",
        "y_test = np.array([0, 1, 1, 0])\n",
        "p_test = np.array([0.3, 0.7, 0.8, 0.2])\n",
        "\n",
        "model_ex2 = MyLogisticRegression()\n",
        "gradient = model_ex2._compute_gradient(Phi_test, y_test, p_test)\n",
        "\n",
        "print(\"\\nTest setup:\")\n",
        "print(f\"Design matrix Phi shape: {Phi_test.shape}\")\n",
        "print(f\"True labels y: {y_test}\")\n",
        "print(f\"Predicted probabilities p: {p_test}\")\n",
        "\n",
        "print(f\"\\nComputed gradient: {gradient}\")\n",
        "print(f\"Expected gradient: [ 0.2  -0.02  0.16 ] (approximately)\")\n",
        "\n",
        "# Verify gradient properties\n",
        "print(\"\\n‚úì Gradient shape correct?\", gradient.shape == (3,))\n",
        "print(f\"‚úì Gradient values reasonable? {np.all(np.abs(gradient) < 10)}\")\n",
        "\n",
        "# Test on actual data\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Testing gradient on real data:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Use a small sample\n",
        "np.random.seed(42)\n",
        "X_small = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
        "y_small = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Create design matrix\n",
        "Phi_small = np.c_[np.ones(X_small.shape[0]), X_small]\n",
        "\n",
        "# Initialize some weights\n",
        "weights_test = np.array([0.1, 0.2, -0.1])\n",
        "\n",
        "# Compute probabilities\n",
        "scores = Phi_small @ weights_test\n",
        "probs = expit(scores)\n",
        "\n",
        "# Compute gradient\n",
        "grad = model_ex2._compute_gradient(Phi_small, y_small, probs)\n",
        "\n",
        "print(f\"Weights: {weights_test}\")\n",
        "print(f\"Computed probabilities: {probs}\")\n",
        "print(f\"Gradient: {grad}\")\n",
        "\n",
        "print(\"\\nGradient interpretation:\")\n",
        "print(f\"  - Bias gradient: {grad[0]:.4f} ({'increase' if grad[0] > 0 else 'decrease'} intercept)\")\n",
        "print(f\"  - Feature 1 gradient: {grad[1]:.4f} ({'increase' if grad[1] > 0 else 'decrease'} weight)\")\n",
        "print(f\"  - Feature 2 gradient: {grad[2]:.4f} ({'increase' if grad[2] > 0 else 'decrease'} weight)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"If your gradient computation works, proceed to Exercise 3!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poYdX3DWhJNm"
      },
      "source": [
        "## Exercise 3: Implement Full Training Loop\n",
        "\n",
        "Excellent! You now have working components:\n",
        "- ‚úÖ Sigmoid function\n",
        "- ‚úÖ Probability prediction  \n",
        "- ‚úÖ Gradient computation\n",
        "\n",
        "Now let's put it all together and implement the **complete training loop** with gradient descent!\n",
        "\n",
        "**What you'll implement:**\n",
        "- `fit()`: The main training method using gradient descent\n",
        "- `predict()`: Convert probabilities to class labels (threshold at 0.5)\n",
        "\n",
        "**What happens in training:**\n",
        "1. Initialize weights randomly\n",
        "2. For each iteration:\n",
        "   - Compute predictions\n",
        "   - Calculate loss (Negative Log-Likelihood)\n",
        "   - Compute gradient\n",
        "   - Update weights: w = w - Œ±‚àáJ\n",
        "   - Check convergence\n",
        "\n",
        "**Testing:**\n",
        "We'll train on a simple dataset and verify the model learns correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Checkpoint Question 3: What is the loss function for logistic regression?\n",
        "\n",
        "A) Sum of Squared Errors averaged over all training samples, measuring the squared difference between predictions and true labels continuously\n",
        "\n",
        "B) Mean Squared Error computed between predicted probability values and true binary labels, penalizing errors proportional to their squared magnitude\n",
        "\n",
        "C) Negative Log-Likelihood also known as Binary Cross-Entropy loss, penalizing confident wrong predictions more heavily than uncertain ones\n",
        "\n",
        "D) Mean Absolute Error summed across all predictions and labels, measuring the average absolute deviation from correct classification labels\n",
        "\n",
        "<details>\n",
        "<summary>Click to see answer</summary>\n",
        "\n",
        "**Answer: C**\n",
        "\n",
        "**Key Insight:** Logistic regression uses Negative Log-Likelihood (NLL) because it's derived from maximum likelihood estimation for Bernoulli distributions. It heavily penalizes confident wrong predictions (e.g., predicting p=0.95 when y=0) while being lenient on uncertain predictions near 0.5.\n",
        "\n",
        "**Detailed Explanation:**\n",
        "\n",
        "The NLL loss for logistic regression is:\n",
        "$$J(\\vec{w}) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\log p^{(i)} + (1-y^{(i)}) \\log(1-p^{(i)}) \\right]$$\n",
        "\n",
        "This loss function has important properties:\n",
        "- When y=1 and p‚Üí1: loss ‚Üí 0 (correct confident prediction)\n",
        "- When y=1 and p‚Üí0: loss ‚Üí ‚àû (wrong confident prediction, heavily penalized)\n",
        "- When y=0 and p‚Üí0: loss ‚Üí 0 (correct confident prediction)\n",
        "- When y=0 and p‚Üí1: loss ‚Üí ‚àû (wrong confident prediction, heavily penalized)\n",
        "\n",
        "**Example with numbers:**\n",
        "- True label y=1, predicted p=0.9: loss = -log(0.9) = 0.105 (small)\n",
        "- True label y=1, predicted p=0.1: loss = -log(0.1) = 2.303 (large!)\n",
        "- True label y=1, predicted p=0.5: loss = -log(0.5) = 0.693 (moderate)\n",
        "\n",
        "**Why other answers are incorrect:**\n",
        "\n",
        "- **A is FALSE**: Sum of Squared Errors (SSE) is used for linear regression: Œ£(y - ≈∑)¬≤. It treats errors linearly and doesn't account for the probabilistic nature of classification. SSE would penalize p=0.4 vs y=0 equally to p=0.6 vs y=0, which doesn't reflect the classification problem structure.\n",
        "- **B is FALSE**: Mean Squared Error (MSE = 1/N Œ£(y - p)¬≤) is also for regression. While it could theoretically be used for classification, it doesn't have the proper probabilistic interpretation and doesn't penalize confident mistakes as heavily as NLL.\n",
        "- **D is FALSE**: Mean Absolute Error (MAE = Œ£|y - p|) is also a regression loss. It treats all errors equally regardless of confidence, unlike NLL which heavily penalizes confident wrong predictions.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "UkEJd0EuhJNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yprw4GJjhJNm"
      },
      "outputs": [],
      "source": [
        "# Add fit and predict methods to complete the class\n",
        "def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the logistic regression model using gradient descent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training data\n",
        "    y : array-like, shape (n_samples,)\n",
        "        Target values (0 or 1)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : object\n",
        "        Returns self for method chaining\n",
        "    \"\"\"\n",
        "    # TODO: Create design matrix Phi by adding bias column to X\n",
        "    # Hint: Use np.c_[np.ones(X.shape[0]), X]\n",
        "    Phi = None\n",
        "\n",
        "    # TODO: Initialize weights with small random values\n",
        "    # Hint: Use np.random.randn() and scale by 0.01\n",
        "    if self.random_state is not None:\n",
        "        np.random.seed(self.random_state)\n",
        "    self.weights_ = None\n",
        "\n",
        "    # Initialize loss history\n",
        "    self.loss_history_ = []\n",
        "\n",
        "    # Gradient descent loop\n",
        "    for iteration in range(self.max_iter):\n",
        "        # TODO: Compute probabilities using your _sigmoid method\n",
        "        # Step 1: Compute scores (z = Phi @ weights)\n",
        "        scores = None\n",
        "        # Step 2: Apply sigmoid\n",
        "        probabilities = None\n",
        "\n",
        "        # TODO: Compute NLL loss: -Œ£[y*log(p) + (1-y)*log(1-p)]\n",
        "        # Use epsilon for numerical stability\n",
        "        epsilon = 1e-15\n",
        "        p_safe = np.clip(probabilities, epsilon, 1 - epsilon)\n",
        "        nll = None\n",
        "        self.loss_history_.append(nll)\n",
        "\n",
        "        # TODO: Compute gradient using your _compute_gradient method\n",
        "        gradient = None\n",
        "\n",
        "        # TODO: Check convergence - if gradient norm < tolerance, break\n",
        "        if None:  # Replace with convergence check\n",
        "            break\n",
        "\n",
        "        # TODO: Update weights using gradient descent: w = w - learning_rate * gradient\n",
        "        pass\n",
        "\n",
        "    self.n_iter_ = iteration + 1\n",
        "    return self\n",
        "\n",
        "def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predict class labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Samples\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_pred : array, shape (n_samples,)\n",
        "        Predicted class labels (0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Get probabilities using predict_proba and threshold at 0.5\n",
        "    # Hint: predict_proba returns shape (n_samples, 2), we want column 1\n",
        "    proba = None\n",
        "    return (proba[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "# Add methods to the class\n",
        "MyLogisticRegression.fit = fit\n",
        "MyLogisticRegression.predict = predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdsXM7AohJNm"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EXERCISE 3 VERIFICATION: Testing Complete Training\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test on simple dataset\n",
        "np.random.seed(42)\n",
        "X_simple = np.array([\n",
        "    [0, 0], [0, 1], [1, 0], [1, 1],\n",
        "    [0, 0.5], [0.5, 0], [1, 0.5], [0.5, 1]\n",
        "])\n",
        "y_simple = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n",
        "\n",
        "print(\"\\n1. Training on simple dataset:\")\n",
        "print(\"-\" * 70)\n",
        "model_ex3 = MyLogisticRegression(learning_rate=0.1, max_iter=1000, random_state=42)\n",
        "model_ex3.fit(X_simple, y_simple)\n",
        "\n",
        "print(f\"‚úì Training completed in {model_ex3.n_iter_} iterations\")\n",
        "print(f\"‚úì Final weights: {model_ex3.weights_}\")\n",
        "print(f\"‚úì Initial loss: {model_ex3.loss_history_[0]:.4f}\")\n",
        "print(f\"‚úì Final loss: {model_ex3.loss_history_[-1]:.4f}\")\n",
        "print(f\"‚úì Loss decreased? {model_ex3.loss_history_[-1] < model_ex3.loss_history_[0]}\")\n",
        "\n",
        "print(\"\\n2. Testing predictions:\")\n",
        "print(\"-\" * 70)\n",
        "y_pred = model_ex3.predict(X_simple)\n",
        "accuracy = accuracy_score(y_simple, y_pred)\n",
        "\n",
        "print(f\"True labels:      {y_simple}\")\n",
        "print(f\"Predicted labels: {y_pred}\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "print(\"\\n3. Testing probabilities:\")\n",
        "print(\"-\" * 70)\n",
        "y_proba = model_ex3.predict_proba(X_simple)\n",
        "print(\"Sample predictions:\")\n",
        "for i in range(len(X_simple)):\n",
        "    print(f\"  {X_simple[i]} -> True: {y_simple[i]}, \"\n",
        "          f\"Pred: {y_pred[i]}, P(y=1): {y_proba[i, 1]:.3f}\")\n",
        "\n",
        "print(\"\\n4. Visualizing loss convergence:\")\n",
        "print(\"-\" * 70)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(model_ex3.loss_history_, linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('NLL Loss', fontsize=12)\n",
        "plt.title('Training Loss - Exercise 3 Verification', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUCCESS! Your complete implementation is working!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüéâ You can now proceed to apply your model to the full dataset!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DanKQLQFhJNm"
      },
      "source": [
        "## Test on Simple Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9kBh4YOhJNm"
      },
      "outputs": [],
      "source": [
        "# Generate simple test data\n",
        "np.random.seed(42)\n",
        "X_simple = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
        "y_simple = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Fit model\n",
        "model_simple = MyLogisticRegression(learning_rate=0.1, max_iter=1000)\n",
        "model_simple.fit(X_simple, y_simple)\n",
        "\n",
        "# Predict\n",
        "y_pred_simple = model_simple.predict(X_simple)\n",
        "print(\"True labels:\", y_simple)\n",
        "print(\"Predictions:\", y_pred_simple)\n",
        "print(\"Accuracy:\", accuracy_score(y_simple, y_pred_simple))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKYQA5XthJNn"
      },
      "source": [
        "## Generate Synthetic Binary Classification Data\n",
        "\n",
        "We'll use the same data generation approach from the lecture slides (slide 26)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnYVyotbhJNn"
      },
      "outputs": [],
      "source": [
        "# Generate two-class data\n",
        "m = 100  # samples per class\n",
        "n = 2    # features\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Class 0: centered around (1.5, -1.5)\n",
        "class_0 = np.hstack((\n",
        "    1.5 + np.random.randn(m, 1),\n",
        "    -1.5 + np.random.randn(m, 1)\n",
        "))\n",
        "\n",
        "# Class 1: centered around (-1.5, 1.5)\n",
        "class_1 = np.hstack((\n",
        "    -1.5 + np.random.randn(m, 1),\n",
        "    1.5 + np.random.randn(m, 1)\n",
        "))\n",
        "\n",
        "# Combine\n",
        "X = np.vstack((class_0, class_1))\n",
        "y = np.concatenate([np.zeros(m), np.ones(m)])\n",
        "\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L59_A3pxhJNn"
      },
      "source": [
        "## Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abEp0uQJhJNn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Class 0', edgecolors='k', s=50)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='skyblue', label='Class 1', edgecolors='k', s=50)\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Binary Classification Dataset', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdVC_qpGhJNn"
      },
      "source": [
        "## Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KlTSTSthJNn"
      },
      "outputs": [],
      "source": [
        "# Split data (70% train, 30% test)\n",
        "# stratify=y ensures both classes are equally represented in train/test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0--tsSLhJNn"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O5DNZLwhJNn"
      },
      "outputs": [],
      "source": [
        "# Create and train model\n",
        "model = MyLogisticRegression(learning_rate=0.1, max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Training completed in {model.n_iter_} iterations\")\n",
        "print(f\"Final weights: {model.weights_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Learned Model\n",
        "\n",
        "Let's interpret what the model has learned. The weights tell us how each feature affects the classification decision."
      ],
      "metadata": {
        "id": "4goGZm66hJNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print learned weights\n",
        "print(\"Learned Weights:\")\n",
        "print(f\"  Intercept (bias): {model.weights_[0]:.4f}\")\n",
        "print(f\"  Weight for x‚ÇÅ:    {model.weights_[1]:.4f}\")\n",
        "print(f\"  Weight for x‚ÇÇ:    {model.weights_[2]:.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Positive weight ‚Üí increasing this feature increases P(y=1)\")\n",
        "print(\"- Negative weight ‚Üí increasing this feature decreases P(y=1)\")\n",
        "print(\"- Larger magnitude ‚Üí stronger influence on classification\")\n",
        "\n",
        "print(\"\\nDecision Boundary:\")\n",
        "print(f\"The decision boundary is the line where x^T¬∑w = 0\")\n",
        "print(f\"For our model: {model.weights_[0]:.4f} + {model.weights_[1]:.4f}*x‚ÇÅ + {model.weights_[2]:.4f}*x‚ÇÇ = 0\")\n",
        "print(f\"Solving for x‚ÇÇ: x‚ÇÇ = {-model.weights_[0]/model.weights_[2]:.4f} + {-model.weights_[1]/model.weights_[2]:.4f}*x‚ÇÅ\")"
      ],
      "metadata": {
        "id": "mPd9GJM6hJNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrufMSmXhJNn"
      },
      "source": [
        "## Visualize Training Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Read the Loss Curve\n",
        "\n",
        "The loss curve provides important diagnostic information about your training process:\n",
        "\n",
        "- **Still decreasing steadily** ‚Üí Model hasn't converged yet. Increase `max_iter` or increase learning rate\n",
        "- **Oscillating or increasing** ‚Üí Learning rate is too large. Decrease `learning_rate` (e.g., from 0.1 to 0.01)\n",
        "- **Flattened/plateaued** ‚Üí Converged successfully! The gradient is near zero and the model has found a minimum\n",
        "- **Decreasing very slowly** ‚Üí Learning rate might be too small. Try increasing it for faster convergence\n",
        "\n",
        "**Our curve above shows:** Rapid decrease then flattening = healthy convergence pattern!"
      ],
      "metadata": {
        "id": "I5fIlAqwhJNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAxJ2-GDhJNn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model.loss_history_, linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Negative Log-Likelihood', fontsize=12)\n",
        "plt.title('Training Loss Over Time', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34C_RHxQhJNn"
      },
      "source": [
        "## Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeVfIZk1hJNn"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Show some predictions\n",
        "print(\"\\nSample predictions (first 10):\")\n",
        "for i in range(min(10, len(y_test))):\n",
        "    print(f\"True: {int(y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i])}, \"\n",
        "          f\"Predicted: {y_pred[i]}, \"\n",
        "          f\"P(y=1): {y_proba[i, 1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sQCRgmxhJNo"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Checkpoint Question 4: What metric is most important for spam detection?\n",
        "\n",
        "A) Overall accuracy measured across both spam and legitimate emails, providing a single comprehensive metric for model performance evaluation\n",
        "\n",
        "B) Precision to minimize false positives and avoid blocking legitimate emails, which is more costly than letting spam through\n",
        "\n",
        "C) Recall to minimize false negatives and catch all possible spam messages, ensuring comprehensive spam filtering without misses\n",
        "\n",
        "D) F1-score to balance both precision and recall equally well, giving equal weight to false positives and negatives\n",
        "\n",
        "<details>\n",
        "<summary>Click to see answer</summary>\n",
        "\n",
        "**Answer: B**\n",
        "\n",
        "**Key Insight:** In spam detection, false positives (legitimate email marked as spam) are typically more costly than false negatives (spam in inbox) because users may miss important messages. Therefore, precision (minimizing false positives) is usually prioritized over recall, though the exact balance depends on the specific application context.\n",
        "\n",
        "**Detailed Explanation:**\n",
        "\n",
        "**Precision** = TP / (TP + FP) = \"Of all emails marked as spam, how many were actually spam?\"\n",
        "\n",
        "For spam detection, **precision** is typically most important because:\n",
        "- **False positives are very costly**: Missing an important work email, job offer, or password reset could have serious consequences\n",
        "- **False negatives are annoying but manageable**: A few spam emails in the inbox are tolerable\n",
        "\n",
        "**Example with numbers:**\n",
        "```\n",
        "Confusion Matrix:\n",
        "                Predicted Spam    Predicted Legitimate\n",
        "Actual Spam           950              50 (FN)\n",
        "Actual Legit           10             990 (TN)\n",
        "\n",
        "Precision = 950/(950+10) = 0.99 (99% of spam predictions are correct)\n",
        "Recall = 950/(950+50) = 0.95 (caught 95% of actual spam)\n",
        "```\n",
        "\n",
        "Only 10 legitimate emails were incorrectly blocked - acceptable trade-off.\n",
        "\n",
        "**Context matters:**\n",
        "- **Consumer email (Gmail)**: High precision preferred (don't block important mail)\n",
        "- **Corporate email filter**: Might balance precision/recall more equally\n",
        "- **High-security environment**: Might even prioritize recall (catch all threats)\n",
        "\n",
        "**Real-world approaches:**\n",
        "- Most spam filters use a confidence threshold above 0.5 (e.g., 0.7 or 0.8)\n",
        "- This increases precision at the cost of recall\n",
        "- Example: Only mark as spam if P(spam) > 0.8\n",
        "\n",
        "**Why other answers are incorrect:**\n",
        "\n",
        "- **A is FALSE**: Accuracy can be very misleading with imbalanced data. If 95% of emails are legitimate, a naive classifier that always predicts \"not spam\" would achieve 95% accuracy while catching zero spam! Accuracy doesn't capture the asymmetric cost of errors in spam detection. We need to specifically minimize false positives, not just maximize overall correctness.\n",
        "- **C is FALSE**: Prioritizing recall would mean flagging more aggressively to catch all spam, but this would result in many false positives (legitimate emails blocked). While catching 100% of spam sounds good, blocking 50 legitimate emails to catch 10 more spam messages is usually not worth it. Recall is important but typically secondary to precision.\n",
        "- **D is FALSE**: F1-score gives equal weight to precision and recall (F1 = 2¬∑(P¬∑R)/(P+R)), but in spam detection, precision and recall do NOT have equal importance. The asymmetric cost structure (false positives >> false negatives) means we should prioritize precision. F1-score is useful when costs are balanced, but that's not the case here.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "S8v24Z_LhJNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8a8ozC2hJNo"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\n[TN  FP]\")\n",
        "print(\"[FN  TP]\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i, j], ha='center', va='center', fontsize=20)\n",
        "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
        "plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Checkpoint Question 5: What does the decision boundary represent?\n",
        "\n",
        "A) The set of points where the model predicts maximum probability P(y=1|x) equals one point zero for class one predictions\n",
        "\n",
        "B) The set of points where the model predicts equal probability P(y=1|x) equals zero point five, the classification threshold\n",
        "\n",
        "C) The set of points where the model predicts minimum probability P(y=1|x) equals zero for class zero predictions exclusively\n",
        "\n",
        "D) The region of maximum classification confidence where the model achieves its highest accuracy on training data samples predominantly\n",
        "\n",
        "<details>\n",
        "<summary>Click to see answer</summary>\n",
        "\n",
        "**Answer: B**\n",
        "\n",
        "**Key Insight:** The decision boundary is the geometric surface where P(y=1|x) = 0.5, which occurs when x^T¬∑w = 0. Points on one side have P(y=1|x) > 0.5 (classified as class 1), while points on the other side have P(y=1|x) < 0.5 (classified as class 0). For 2D features, this is a line; for 3D, a plane; for higher dimensions, a hyperplane.\n",
        "\n",
        "**Detailed Explanation:**\n",
        "\n",
        "The decision boundary is where the model is maximally uncertain - exactly at the threshold between the two classes.\n",
        "\n",
        "Mathematically:\n",
        "- Decision boundary: {x : œÉ(x^T¬∑w) = 0.5}\n",
        "- Since œÉ(0) = 0.5, this simplifies to: {x : x^T¬∑w = 0}\n",
        "- For 2 features: w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ = 0\n",
        "\n",
        "**Example with numbers:**\n",
        "If weights are w = [1, 2, -3]:\n",
        "- Decision boundary: 1 + 2x‚ÇÅ - 3x‚ÇÇ = 0\n",
        "- Solving for x‚ÇÇ: x‚ÇÇ = (1 + 2x‚ÇÅ)/3\n",
        "- Points above this line: predicted as class 1\n",
        "- Points below this line: predicted as class 0\n",
        "\n",
        "**Visualizing predictions:**\n",
        "- At x = [0, 0]: x^T¬∑w = 1 > 0, so p = œÉ(1) = 0.73 ‚Üí class 1\n",
        "- At x = [-1, 0]: x^T¬∑w = -1 < 0, so p = œÉ(-1) = 0.27 ‚Üí class 0\n",
        "- At x = [-0.5, 0]: x^T¬∑w = 0, so p = œÉ(0) = 0.50 ‚Üí on boundary\n",
        "\n",
        "**Why other answers are incorrect:**\n",
        "\n",
        "- **A is FALSE**: P(y=1|x) = 1.0 would require x^T¬∑w = +‚àû, which never happens in practice with finite feature values. Points very far from the boundary have high confidence (e.g., p = 0.99) but never reach exactly 1.0. The decision boundary is at p = 0.5, not p = 1.0.\n",
        "- **C is FALSE**: Similarly, P(y=1|x) = 0 would require x^T¬∑w = -‚àû. The decision boundary is at the threshold (0.5), not at the extremes (0 or 1). Points with p ‚âà 0 are far from the boundary, not on it.\n",
        "- **D is FALSE**: The decision boundary is actually where the model has *minimum* confidence (p = 0.5), not maximum. Points far from the boundary have high confidence (p close to 0 or 1), while points near the boundary have low confidence (p near 0.5). Also, the decision boundary is defined by the learned weights, not by training accuracy.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "zLgs2NiYhJNr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0LYH9s9hJNr"
      },
      "source": [
        "## Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUoO0ZAghJNr"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
        "\n",
        "# Calculate metrics manually\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nPrecision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZBa2P4VhJNr"
      },
      "source": [
        "## Visualize Decision Boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Checkpoint Question 6: When should we use higher learning rates?\n",
        "\n",
        "A) Always use high learning rates to achieve faster convergence to the optimum solution in all training scenarios regardless of data\n",
        "\n",
        "B) When features are normalized to have similar scales and distributions, ensuring gradients have comparable magnitudes across all feature dimensions\n",
        "\n",
        "C) When the model is overfitting to the training data, use higher rates to reduce model complexity and regularize the learning\n",
        "\n",
        "D) Never use high learning rates because they always cause divergence problems and prevent the model from converging to any solution\n",
        "\n",
        "<details>\n",
        "<summary>Click to see answer</summary>\n",
        "\n",
        "**Answer: B**\n",
        "\n",
        "**Key Insight:** Feature normalization (e.g., StandardScaler) ensures all gradients have similar magnitudes, allowing a single learning rate to work well for all features. Without normalization, features with large scales dominate gradients, requiring a small learning rate that makes other features learn too slowly.\n",
        "\n",
        "**Detailed Explanation:**\n",
        "\n",
        "Higher learning rates (e.g., Œ± = 0.1 instead of 0.01) can be used when features are normalized because:\n",
        "\n",
        "1. **Unnormalized features cause problems:**\n",
        "   - Feature x‚ÇÅ ‚àà [0, 1] and x‚ÇÇ ‚àà [0, 10000]\n",
        "   - Gradient for w‚ÇÇ will be ~10,000√ó larger than gradient for w‚ÇÅ\n",
        "   - Large Œ± works for x‚ÇÅ but causes oscillation/divergence for x‚ÇÇ\n",
        "   - Small Œ± works for x‚ÇÇ but makes x‚ÇÅ learn extremely slowly\n",
        "\n",
        "2. **Normalized features enable higher learning rates:**\n",
        "   - After StandardScaler: all features have mean=0, std=1\n",
        "   - All gradients have similar magnitudes\n",
        "   - Can use Œ± = 0.1 or higher safely\n",
        "   - Faster convergence without instability\n",
        "\n",
        "**Example with numbers:**\n",
        "```\n",
        "# Unnormalized\n",
        "Feature 1: [1, 2, 3] ‚Üí gradient ‚âà 2.5\n",
        "Feature 2: [1000, 2000, 3000] ‚Üí gradient ‚âà 2500\n",
        "Ratio: 1000:1 (need very small Œ±)\n",
        "\n",
        "# After StandardScaler\n",
        "Feature 1: [-1, 0, 1] ‚Üí gradient ‚âà 2.5\n",
        "Feature 2: [-1, 0, 1] ‚Üí gradient ‚âà 2.5\n",
        "Ratio: 1:1 (can use larger Œ±)\n",
        "```\n",
        "\n",
        "**Recommended learning rates:**\n",
        "- Normalized features: Œ± ‚àà [0.01, 0.5]\n",
        "- Unnormalized features: Œ± ‚àà [0.0001, 0.01]\n",
        "\n",
        "**Why other answers are incorrect:**\n",
        "\n",
        "- **A is FALSE**: \"Always\" is too strong. Very high learning rates (e.g., Œ± = 10) can cause divergence even with normalized features. The gradient might overshoot the minimum, causing the loss to oscillate or increase. The optimal learning rate depends on the problem, dataset size, and loss surface curvature.\n",
        "- **C is FALSE**: Overfitting is addressed through regularization (L1/L2 penalties), early stopping, or getting more training data - not by changing the learning rate. The learning rate controls convergence speed and stability, not model complexity. A higher learning rate might even worsen overfitting by causing instability.\n",
        "- **D is FALSE**: \"Never\" and \"always\" are both too absolute. Higher learning rates are perfectly fine with proper feature scaling and can significantly speed up training. For example, Œ± = 0.1 often works well for normalized features and converges faster than Œ± = 0.001. The key is matching the learning rate to the feature scales.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "5Qcanc1OhJNr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_gWhl3ohJNr"
      },
      "outputs": [],
      "source": [
        "# Create mesh\n",
        "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                        np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "# Predict probabilities for mesh\n",
        "X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "probs_mesh = model.predict_proba(X_mesh)[:, 1].reshape(xx1.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
        "plt.colorbar(label='P(y=1|x,w)')\n",
        "plt.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2)\n",
        "\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='orange', label='Train Class 0', edgecolors='k', s=50, marker='o')\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='skyblue', label='Train Class 1', edgecolors='k', s=50, marker='o')\n",
        "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n",
        "            c='orange', label='Test Class 0', edgecolors='k', s=100, marker='s')\n",
        "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n",
        "            c='skyblue', label='Test Class 1', edgecolors='k', s=100, marker='s')\n",
        "\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Decision Boundary', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLq1nXHyhJNs"
      },
      "source": [
        "## Experiment with Different Learning Rates\n",
        "\n",
        "We've seen how our model performs with $\\alpha = 0.1$. But how does the choice of learning rate affect convergence speed and final accuracy? Let's systematically compare different learning rates to understand this critical hyperparameter and observe the convergence patterns we discussed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8IUImoyhJNs"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model_lr = MyLogisticRegression(learning_rate=lr, max_iter=1000, random_state=42)\n",
        "    model_lr.fit(X_train, y_train)\n",
        "    accuracy_lr = accuracy_score(y_test, model_lr.predict(X_test))\n",
        "    results[lr] = {\n",
        "        'model': model_lr,\n",
        "        'accuracy': accuracy_lr,\n",
        "        'n_iter': model_lr.n_iter_,\n",
        "        'final_loss': model_lr.loss_history_[-1]\n",
        "    }\n",
        "    print(f\"Learning Rate={lr}: Accuracy={accuracy_lr:.4f}, \"\n",
        "          f\"Iterations={model_lr.n_iter_}, Final Loss={model_lr.loss_history_[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-9FD0n_hJNs"
      },
      "source": [
        "### Compare Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zksS1l_PhJNs"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "for lr in learning_rates:\n",
        "    plt.plot(results[lr]['model'].loss_history_, label=f'Œ±={lr}', linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Negative Log-Likelihood', fontsize=12)\n",
        "plt.title('Training Loss for Different Learning Rates', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPgBDj8DhJNs"
      },
      "source": [
        "## K-Fold Cross-Validation\n",
        "\n",
        "So far we've evaluated our model using a single train/test split. However, our performance estimate might be sensitive to this particular split - we might have gotten \"lucky\" or \"unlucky\" with which samples ended up in the test set. K-fold cross-validation provides a more robust performance estimate by testing on multiple different splits of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Cross-Validation?\n",
        "\n",
        "So far we've evaluated our model using a single train/test split. However, this approach has limitations:\n",
        "\n",
        "**Problem with single split:**\n",
        "- Performance estimate depends heavily on *which* samples ended up in the test set\n",
        "- Might get \"lucky\" with an easy test set (overestimate performance)\n",
        "- Might get \"unlucky\" with a hard test set (underestimate performance)\n",
        "- Uses less data for training (70% in our case)\n",
        "\n",
        "**K-Fold Cross-Validation solves this:**\n",
        "1. **Split data into K folds** (e.g., K=5): divide training data into 5 equal parts\n",
        "2. **Train K times**: Each time, use K-1 folds for training, 1 fold for validation\n",
        "3. **Get K performance scores**: Each fold serves as validation set once\n",
        "4. **Average the scores**: More robust estimate with confidence interval\n",
        "\n",
        "**Benefits:**\n",
        "- ‚úÖ More reliable performance estimate (average of K runs)\n",
        "- ‚úÖ Confidence interval showing variance (e.g., 0.92 ¬± 0.03)\n",
        "- ‚úÖ Uses all training data (every sample is validated once)\n",
        "- ‚úÖ Detects if model is unstable across different data splits\n",
        "\n",
        "**Visual representation of 5-fold CV:**\n",
        "```\n",
        "Fold 1: [Test][Train][Train][Train][Train] ‚Üí Score 1\n",
        "Fold 2: [Train][Test][Train][Train][Train] ‚Üí Score 2\n",
        "Fold 3: [Train][Train][Test][Train][Train] ‚Üí Score 3\n",
        "Fold 4: [Train][Train][Train][Test][Train] ‚Üí Score 4\n",
        "Fold 5: [Train][Train][Train][Train][Test] ‚Üí Score 5\n",
        "                                             ‚Üì\n",
        "                        Final: Mean ¬± Std of 5 scores\n",
        "```\n",
        "\n",
        "**Important:** We only use cross-validation on the training set. The held-out test set remains completely untouched for final evaluation."
      ],
      "metadata": {
        "id": "8I1v_qiIhJNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è OPTIONAL/ADVANCED: Polynomial Features for Non-Linear Decision Boundaries\n",
        "\n",
        "**‚ö†Ô∏è Note:** This section covers **feature engineering** (creating polynomial features), not core logistic regression. It demonstrates how to handle non-linear decision boundaries when the basic logistic regression model with original features isn't sufficient.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to transform features to capture non-linear relationships\n",
        "- When polynomial features are needed (non-linearly separable data)\n",
        "- Trade-offs between model complexity and performance\n",
        "\n",
        "**Core logistic regression concepts are complete** - this is an extension showing how to adapt the algorithm for more complex datasets.\n",
        "\n",
        "Now let's work with a more complex dataset that requires non-linear decision boundaries. We'll use the mixture dataset from the lecture slides and apply polynomial features to capture the non-linear patterns."
      ],
      "metadata": {
        "id": "aHILHmI1hqcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVN3X9oQhJNs"
      },
      "outputs": [],
      "source": [
        "# Use training data for cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(\n",
        "    MyLogisticRegression(learning_rate=0.1, max_iter=1000, random_state=42),\n",
        "    X_train, y_train, cv=kf, scoring='accuracy'\n",
        ")\n",
        "\n",
        "print(f\"Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAKqekV3hJNs"
      },
      "source": [
        "## Polynomial Features for Non-Linear Decision Boundaries\n",
        "\n",
        "Now let's work with a more complex dataset that requires non-linear decision boundaries. We'll use the mixture dataset from the lecture slides and apply polynomial features to capture the non-linear patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4bJ9edehJNs"
      },
      "source": [
        "### Load Mixture Dataset from Google Drive\n",
        "\n",
        "This dataset contains two classes with non-linear separation (as shown in the lecture slides)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx5oOKsWhJNs"
      },
      "outputs": [],
      "source": [
        "# Download the mixture dataset from Google Drive\n",
        "# File ID: 1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT\n",
        "# Direct download URL\n",
        "url = 'https://drive.google.com/uc?id=1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT'\n",
        "\n",
        "# Load data\n",
        "df_mixture = pd.read_csv(url)\n",
        "print(f\"Mixture dataset shape: {df_mixture.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df_mixture.head())\n",
        "print(f\"\\nColumn names: {df_mixture.columns.tolist()}\")\n",
        "print(f\"Class distribution:\\n{df_mixture.iloc[:, -1].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6StE4V0hJNs"
      },
      "source": [
        "### Prepare Mixture Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKuWP-UhhJNs"
      },
      "outputs": [],
      "source": [
        "# Extract features and labels\n",
        "# Assuming last column is the label, and first columns are features\n",
        "X_mixture = df_mixture.iloc[:, :-1].values\n",
        "y_mixture = df_mixture.iloc[:, -1].values\n",
        "\n",
        "print(f\"Features shape: {X_mixture.shape}\")\n",
        "print(f\"Labels shape: {y_mixture.shape}\")\n",
        "print(f\"Unique labels: {np.unique(y_mixture)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObRTVqa9hJNs"
      },
      "source": [
        "### Visualize Mixture Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQa2tSDHhJNs"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_mixture[y_mixture == 0, 0], X_mixture[y_mixture == 0, 1],\n",
        "            c='orange', label='Class 0', edgecolors='k', s=50, alpha=0.7)\n",
        "plt.scatter(X_mixture[y_mixture == 1, 0], X_mixture[y_mixture == 1, 1],\n",
        "            c='skyblue', label='Class 1', edgecolors='k', s=50, alpha=0.7)\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Mixture Dataset (Non-Linear Boundary)', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWYd_0ZQhJNs"
      },
      "source": [
        "### Split Mixture Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt5U1VmHhJNs"
      },
      "outputs": [],
      "source": [
        "# Split into train and test sets\n",
        "X_mix_train, X_mix_test, y_mix_train, y_mix_test = train_test_split(\n",
        "    X_mixture, y_mixture, test_size=0.3, random_state=42, stratify=y_mixture)\n",
        "\n",
        "print(f\"Mixture training set: {X_mix_train.shape[0]} samples\")\n",
        "print(f\"Mixture test set: {X_mix_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4NE1UcbhJNs"
      },
      "source": [
        "### Apply Polynomial Features to Mixture Data\n",
        "\n",
        "Let's test different polynomial degrees to find the best model for this non-linear dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScMS_YNlhJNs"
      },
      "outputs": [],
      "source": [
        "# Test different polynomial degrees\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "poly_results = {}\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_mix_train_poly = poly.fit_transform(X_mix_train)\n",
        "    X_mix_test_poly = poly.transform(X_mix_test)\n",
        "\n",
        "    # Train model with smaller learning rate for higher dimensions\n",
        "    lr = 0.01 if degree <= 2 else 0.001\n",
        "    model_poly = MyLogisticRegression(learning_rate=lr, max_iter=3000, random_state=42)\n",
        "    model_poly.fit(X_mix_train_poly, y_mix_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_mix_pred_poly = model_poly.predict(X_mix_test_poly)\n",
        "    accuracy_poly = accuracy_score(y_mix_test, y_mix_pred_poly)\n",
        "\n",
        "    poly_results[degree] = {\n",
        "        'poly': poly,\n",
        "        'model': model_poly,\n",
        "        'accuracy': accuracy_poly,\n",
        "        'n_features': X_mix_train_poly.shape[1]\n",
        "    }\n",
        "\n",
        "    print(f\"Degree={degree}: Features={X_mix_train_poly.shape[1]}, \"\n",
        "          f\"Accuracy={accuracy_poly:.4f}, Iterations={model_poly.n_iter_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apzMorv0hJNs"
      },
      "source": [
        "### Visualize Polynomial Decision Boundaries on Mixture Data\n",
        "\n",
        "Notice how higher-degree polynomials can capture more complex, non-linear boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuXdgkxChJNs"
      },
      "outputs": [],
      "source": [
        "# Create subplot grid based on number of degrees\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Get data ranges\n",
        "x1_min_mix, x1_max_mix = X_mixture[:, 0].min() - 0.5, X_mixture[:, 0].max() + 0.5\n",
        "x2_min_mix, x2_max_mix = X_mixture[:, 1].min() - 0.5, X_mixture[:, 1].max() + 0.5\n",
        "\n",
        "for idx, degree in enumerate(degrees):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Get polynomial transformer and model\n",
        "    poly = poly_results[degree]['poly']\n",
        "    model_poly = poly_results[degree]['model']\n",
        "\n",
        "    # Create mesh\n",
        "    xx1, xx2 = np.meshgrid(np.linspace(x1_min_mix, x1_max_mix, 200),\n",
        "                            np.linspace(x2_min_mix, x2_max_mix, 200))\n",
        "    X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "    X_mesh_poly = poly.transform(X_mesh)\n",
        "    probs_mesh = model_poly.predict_proba(X_mesh_poly)[:, 1].reshape(xx1.shape)\n",
        "\n",
        "    # Plot contours and decision boundary\n",
        "    ax.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
        "    ax.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2.5)\n",
        "\n",
        "    # Plot training data\n",
        "    ax.scatter(X_mix_train[y_mix_train == 0, 0], X_mix_train[y_mix_train == 0, 1],\n",
        "                c='orange', edgecolors='k', s=40, alpha=0.7, label='Class 0 (train)')\n",
        "    ax.scatter(X_mix_train[y_mix_train == 1, 0], X_mix_train[y_mix_train == 1, 1],\n",
        "                c='skyblue', edgecolors='k', s=40, alpha=0.7, label='Class 1 (train)')\n",
        "\n",
        "    # Plot test data with different marker\n",
        "    ax.scatter(X_mix_test[y_mix_test == 0, 0], X_mix_test[y_mix_test == 0, 1],\n",
        "                c='orange', edgecolors='k', s=80, marker='s', alpha=0.9, label='Class 0 (test)')\n",
        "    ax.scatter(X_mix_test[y_mix_test == 1, 0], X_mix_test[y_mix_test == 1, 1],\n",
        "                c='skyblue', edgecolors='k', s=80, marker='s', alpha=0.9, label='Class 1 (test)')\n",
        "\n",
        "    ax.set_title(f'Degree={degree}, Features={poly_results[degree][\"n_features\"]}, '\\\n",
        "                     f'Acc={poly_results[degree][\"accuracy\"]:.3f}', fontsize=12)\n",
        "    ax.set_xlabel('$x_1$', fontsize=11)\n",
        "    ax.set_ylabel('$x_2$', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    if idx == 0:\n",
        "        ax.legend(fontsize=9, loc='best')\n",
        "\n",
        "# Hide the last subplot if we have fewer than 6 degrees\n",
        "if len(degrees) < 6:\n",
        "    axes[5].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lm7401PhJNt"
      },
      "source": [
        "### Analysis of Polynomial Degrees\n",
        "\n",
        "Observe the following:\n",
        "- **Degree 1 (Linear)**: Cannot capture the non-linear boundary, lower accuracy\n",
        "- **Degree 2 (Quadratic)**: Begins to capture curvature in the decision boundary\n",
        "- **Degree 3-4**: Better fit for complex boundaries\n",
        "- **Degree 5+**: Risk of overfitting - may fit training noise rather than true pattern\n",
        "\n",
        "**Key insight**: The mixture dataset requires polynomial features because the classes are not linearly separable. This demonstrates why feature engineering (like polynomial features) is important for logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGfB0dUAhJNt"
      },
      "source": [
        "## Comparison with scikit-learn\n",
        "\n",
        "Now that we've implemented logistic regression and explored its behavior with different datasets, hyperparameters, and feature transformations, let's validate our implementation by comparing it with sklearn's professional implementation on our original synthetic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Wj1BCOhJNt"
      },
      "outputs": [],
      "source": [
        "# Train sklearn model\n",
        "sklearn_model = SklearnLogisticRegression(penalty=None, max_iter=1000, random_state=42)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "\n",
        "# Compare\n",
        "our_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "sklearn_accuracy = sklearn_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"Our model accuracy:      {our_accuracy:.4f}\")\n",
        "print(f\"sklearn model accuracy:  {sklearn_accuracy:.4f}\")\n",
        "print(f\"\\nOur weights:     {model.weights_}\")\n",
        "print(f\"sklearn weights: {np.concatenate([sklearn_model.intercept_, sklearn_model.coef_[0]])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er89QK1hhJNt"
      },
      "source": [
        "## Best Practices and Tips\n",
        "\n",
        "### 1. Feature Scaling\n",
        "Always normalize/standardize features when using gradient descent:\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "### 2. Learning Rate Selection\n",
        "- Start with Œ± = 0.01 for normalized features\n",
        "- If loss increases or oscillates: reduce Œ±\n",
        "- If convergence is too slow: increase Œ±\n",
        "- Monitor loss curve to diagnose\n",
        "\n",
        "### 3. Handling Class Imbalance\n",
        "- Use stratified splits: `train_test_split(..., stratify=y)`\n",
        "- Consider weighted loss or resampling\n",
        "- Focus on precision/recall instead of accuracy\n",
        "\n",
        "### 4. Convergence\n",
        "- Set reasonable `max_iter` (e.g., 1000-10000)\n",
        "- Use `tol` to stop early when gradient is small\n",
        "- Check if loss is still decreasing\n",
        "\n",
        "### 5. Multiclass Classification\n",
        "For more than 2 classes, use:\n",
        "- One-vs-Rest (OvR): Train C binary classifiers\n",
        "- Softmax regression (multinomial logistic regression)\n",
        "\n",
        "### 6. Regularization\n",
        "To prevent overfitting:\n",
        "- L2 regularization: Add $\\lambda ||\\vec{w}||^2$ to loss\n",
        "- L1 regularization: Add $\\lambda ||\\vec{w}||_1$ to loss\n",
        "\n",
        "### 7. Evaluation\n",
        "- Use cross-validation for small datasets\n",
        "- Report multiple metrics: accuracy, precision, recall, F1\n",
        "- Visualize confusion matrix\n",
        "- Plot ROC curve and PR curve for threshold selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZakB_08hJNt"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you:\n",
        "\n",
        "1. ‚úÖ Implemented a custom **Logistic Regression** classifier from scratch\n",
        "2. ‚úÖ Understood the **sigmoid function** and how it models probabilities\n",
        "3. ‚úÖ Applied **gradient descent** to minimize the negative log-likelihood loss\n",
        "4. ‚úÖ Experimented with different **learning rates** and observed their effects\n",
        "5. ‚úÖ Used **K-fold cross-validation** to evaluate model performance\n",
        "6. ‚úÖ Applied **polynomial features** to model non-linear decision boundaries\n",
        "7. ‚úÖ Evaluated models using **accuracy, precision, recall, and F1-score**\n",
        "8. ‚úÖ Visualized **decision boundaries** and probability distributions\n",
        "9. ‚úÖ Compared your implementation with scikit-learn\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- Logistic regression models $P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w})$\n",
        "- The loss function is the negative log-likelihood (binary cross-entropy)\n",
        "- The gradient is $\\nabla J = \\Phi^T (\\vec{p} - \\vec{y})$\n",
        "- Learning rate must be tuned carefully\n",
        "- Feature scaling improves convergence\n",
        "- Different applications require different metric priorities (precision vs recall)\n",
        "- Cross-validation provides more reliable performance estimates\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try logistic regression on real-world datasets (e.g., breast cancer, iris)\n",
        "- Implement multiclass classification using One-vs-Rest\n",
        "- Add L2 regularization to prevent overfitting\n",
        "- Experiment with different optimization algorithms (SGD, Adam)\n",
        "- Compare with other classifiers (SVM, Decision Trees, Neural Networks)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}