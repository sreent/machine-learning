{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Logistic%20Regression/Logistic%20Regression%20Code%20Walk%20Through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Code Walk Through\n",
    "\n",
    "This notebook provides a step-by-step computational walkthrough of **Logistic Regression** using gradient descent optimization.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How the **sigmoid function** maps scores to probabilities\n",
    "- How **gradient descent** optimizes the negative log-likelihood (NLL) loss\n",
    "- How to compute gradients for logistic regression\n",
    "- How the decision boundary evolves during training\n",
    "- How to visualize convergence and final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit  # Numerically stable sigmoid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Binary Classification Data\n",
    "\n",
    "We'll create a simple 2D dataset with two classes, following the pattern from the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two-class data\n",
    "m = 100  # samples per class\n",
    "n = 2    # features\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Class 0: centered around (1.5, -1.5)\n",
    "class_0 = np.hstack((\n",
    "    1.5 + np.random.randn(m, 1),\n",
    "    -1.5 + np.random.randn(m, 1)\n",
    "))\n",
    "\n",
    "# Class 1: centered around (-1.5, 1.5)\n",
    "class_1 = np.hstack((\n",
    "    -1.5 + np.random.randn(m, 1),\n",
    "    1.5 + np.random.randn(m, 1)\n",
    "))\n",
    "\n",
    "# Combine into training set\n",
    "X_train = np.vstack((class_0, class_1))  # shape (2m, 2)\n",
    "y_train = np.concatenate([np.zeros(m), np.ones(m)])  # shape (2m,)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "            c='orange', label='Class 0', edgecolors='k', s=50)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "            c='skyblue', label='Class 1', edgecolors='k', s=50)\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Binary Classification Data', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding the Sigmoid Function\n",
    "\n",
    "The **sigmoid (logistic) function** maps any real-valued score to a probability between 0 and 1:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where $z = \\vec{x}^T \\times \\vec{w}$ is the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid function\n",
    "z_values = np.linspace(-6, 6, 200)\n",
    "sigmoid_values = expit(z_values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z_values, sigmoid_values, linewidth=2, color='purple')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', label='Decision threshold (0.5)')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.scatter([0], [0.5], color='red', s=100, zorder=5, label='Score = 0')\n",
    "plt.xlabel('Score $(\\\\vec{x}^T \\\\times \\\\vec{w})$', fontsize=12)\n",
    "plt.ylabel('$\\\\sigma(\\\\vec{x}^T \\\\times \\\\vec{w})$', fontsize=12)\n",
    "plt.title('Sigmoid Function', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key sigmoid values:\")\n",
    "for z in [-6, -2, 0, 2, 6]:\n",
    "    print(f\"  σ({z:2.0f}) = {expit(z):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Design Matrix\n",
    "\n",
    "Add a bias column (intercept) to the feature matrix:\n",
    "\n",
    "$$\\Phi = [\\vec{1} \\quad X]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design matrix: add bias column\n",
    "Phi = np.c_[np.ones(len(X_train)), X_train]\n",
    "print(f\"Design matrix Φ shape: {Phi.shape}\")\n",
    "print(f\"\\nFirst 5 rows of Φ:\")\n",
    "print(Phi[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Weights\n",
    "\n",
    "Start with small random weights near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights = np.random.randn(Phi.shape[1]) * 0.01\n",
    "print(f\"Initial weights: {weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step-by-Step: One Gradient Descent Iteration\n",
    "\n",
    "Let's walk through a single iteration of gradient descent:\n",
    "\n",
    "### Step 1: Compute scores (logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Phi @ weights\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"First 5 scores: {scores[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply sigmoid to get probabilities\n",
    "\n",
    "$$P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w}) = \\frac{1}{1 + e^{-\\vec{x}^T \\times \\vec{w}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = expit(scores)\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"First 5 probabilities: {probabilities[:5]}\")\n",
    "print(f\"All probabilities in [0,1]: {np.all((probabilities >= 0) & (probabilities <= 1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute errors (residuals)\n",
    "\n",
    "$$\\text{errors} = \\vec{p} - \\vec{y}$$\n",
    "\n",
    "Where $\\vec{p}$ are the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = probabilities - y_train\n",
    "print(f\"Errors shape: {errors.shape}\")\n",
    "print(f\"First 5 errors: {errors[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute Negative Log-Likelihood (NLL) Loss\n",
    "\n",
    "$$J(\\vec{w}) = -\\sum_{i=1}^{N} \\left[ y^{(i)} \\log p^{(i)} + (1-y^{(i)}) \\log(1-p^{(i)}) \\right]$$\n",
    "\n",
    "This is also known as **binary cross-entropy loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute NLL (with numerical stability)\n",
    "epsilon = 1e-15\n",
    "p_safe = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "nll = -np.sum(y_train * np.log(p_safe) + (1 - y_train) * np.log(1 - p_safe))\n",
    "print(f\"Negative Log-Likelihood (NLL): {nll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compute Gradient\n",
    "\n",
    "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = Phi.T @ errors\n",
    "print(f\"Gradient shape: {gradient.shape}\")\n",
    "print(f\"Gradient: {gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Update Weights\n",
    "\n",
    "$$\\vec{w}_{\\text{new}} = \\vec{w}_{\\text{old}} - \\alpha \\nabla_{\\vec{w}} J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "weights_new = weights - learning_rate * gradient\n",
    "print(f\"Old weights: {weights}\")\n",
    "print(f\"New weights: {weights_new}\")\n",
    "print(f\"Weight change: {weights_new - weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Training Loop\n",
    "\n",
    "Now let's run the complete gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset weights\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(Phi.shape[1]) * 0.01\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Storage for tracking\n",
    "loss_history = []\n",
    "weight_history = [weights.copy()]\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Forward pass\n",
    "    scores = Phi @ weights\n",
    "    probabilities = expit(scores)\n",
    "    \n",
    "    # Compute loss\n",
    "    p_safe = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "    nll = -np.sum(y_train * np.log(p_safe) + (1 - y_train) * np.log(1 - p_safe))\n",
    "    loss_history.append(nll)\n",
    "    \n",
    "    # Compute gradient\n",
    "    errors = probabilities - y_train\n",
    "    gradient = Phi.T @ errors\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.linalg.norm(gradient) < tolerance:\n",
    "        print(f\"Converged at iteration {iteration}\")\n",
    "        break\n",
    "    \n",
    "    # Update weights\n",
    "    weights = weights - learning_rate * gradient\n",
    "    weight_history.append(weights.copy())\n",
    "    \n",
    "    # Print progress\n",
    "    if (iteration + 1) % 100 == 0 or iteration == 0:\n",
    "        print(f\"Iteration {iteration+1:4d}: NLL = {nll:10.4f}, ||gradient|| = {np.linalg.norm(gradient):.6f}\")\n",
    "\n",
    "print(f\"\\nFinal weights: {weights}\")\n",
    "print(f\"Final NLL: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Negative Log-Likelihood', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Decision Boundary\n",
    "\n",
    "The decision boundary is where $P(y=1|\\vec{x}, \\vec{w}) = 0.5$, which occurs when $\\vec{x}^T \\times \\vec{w} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh for visualization\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                        np.linspace(x2_min, x2_max, 200))\n",
    "\n",
    "# Compute probabilities for the mesh\n",
    "Phi_mesh = np.c_[np.ones(xx1.ravel().shape[0]), xx1.ravel(), xx2.ravel()]\n",
    "probs_mesh = expit(Phi_mesh @ weights)\n",
    "probs_mesh = probs_mesh.reshape(xx1.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
    "plt.colorbar(label='P(y=1|x,w)')\n",
    "plt.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2, linestyles='dashed')\n",
    "\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "            c='orange', label='Class 0', edgecolors='k', s=50)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "            c='skyblue', label='Class 1', edgecolors='k', s=50)\n",
    "\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Logistic Regression Decision Boundary', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_proba = expit(Phi @ weights)\n",
    "\n",
    "# Predict classes (threshold = 0.5)\n",
    "y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train sklearn model\n",
    "sklearn_model = LogisticRegression(penalty=None, max_iter=1000)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare weights\n",
    "print(\"Our weights:     \", weights)\n",
    "print(\"sklearn weights: \", np.concatenate([sklearn_model.intercept_, sklearn_model.coef_[0]]))\n",
    "\n",
    "# Compare accuracy\n",
    "sklearn_accuracy = sklearn_model.score(X_train, y_train)\n",
    "print(f\"\\nOur accuracy:      {accuracy:.4f}\")\n",
    "print(f\"sklearn accuracy:  {sklearn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this walkthrough, we:\n",
    "\n",
    "1. Generated synthetic binary classification data\n",
    "2. Understood the **sigmoid function** that maps scores to probabilities\n",
    "3. Implemented **gradient descent** to minimize the negative log-likelihood loss\n",
    "4. Visualized the **decision boundary** learned by the model\n",
    "5. Compared our implementation with scikit-learn\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Logistic regression models $P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w})$\n",
    "- The loss function is the negative log-likelihood (binary cross-entropy)\n",
    "- Gradient descent iteratively adjusts weights to minimize loss\n",
    "- The decision boundary is linear: $\\vec{x}^T \\times \\vec{w} = 0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
