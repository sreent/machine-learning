{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyOWUEeQqEJ5Fmm0eQmx8dC6",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Neural%20Networks/Deep%20Neural%20Networks%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "source": "# **Hands-On Lab: Deep Neural Networks**\n\n## **1. Introduction**\n\nWelcome to this hands-on lab on **Deep Neural Networks (DNNs)**! In this notebook, you'll learn how to build, train, and optimize neural networks by following a systematic approach.\n\n### **Learning Objectives**\n\nBy the end of this lab, you will be able to:\n- Apply the **universal machine learning workflow** to real datasets\n- Build DNNs for **binary classification** (IMDB sentiment analysis) and **multi-class classification** (Fashion MNIST)\n- Understand and apply **regularization techniques** (Dropout, L2) to combat overfitting\n- Use **Keras Tuner's Hyperband** for automated hyperparameter optimization\n- Evaluate models using appropriate metrics (**accuracy**, **F1 score**, **confusion matrix**)\n\n### **The Universal ML Workflow**\n\nWe'll follow these 8 steps throughout this lab:\n\n| Step | Description | Covered In |\n|------|-------------|------------|\n| 1 | Define problem & gather data | Sections 2.1, 3.1 |\n| 2 | Choose metrics of success | Sections 2.2, 3.1 |\n| 3 | Choose evaluation protocol | Sections 2.2, 3.1 |\n| 4 | Prepare data | Sections 2.3, 3.1 |\n| 5 | Build a baseline model | Sections 2.4, 3.2 |\n| 6 | Scale up → intentionally overfit | Sections 2.5, 3.3 |\n| 7 | Regularize | Sections 2.6, 3.4 |\n| 8 | Hyperparameter tuning | Sections 2.7, 3.5 |\n\n### **Notebook Outline**\n\n1. **Introduction & Setup**\n2. **Binary Classification: IMDB Sentiment Analysis** (with TF-IDF and F1 score)\n3. **Multi-Class Classification: Fashion MNIST** (10 clothing categories)\n4. **Key Takeaways & Further Improvements**\n5. **Appendix: EarlyStopping Deep Dive**\n\nLet's begin!",
   "metadata": {
    "id": "8_lYvUKUGJc8"
   }
  },
  {
   "cell_type": "code",
   "source": "#############################################\n# 1.1 SETUP & IMPORTS\n#############################################\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Keras Tuner version:\", kt.__version__)\n\n# Utility function for plotting training history\ndef plot_history(history, title=''):\n    \"\"\"\n    Plot training/validation loss & accuracy from a Keras history object.\n    Helps visualize overfitting (gap between train and val curves).\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    ax1.plot(history.history['loss'], label='Train Loss')\n    ax1.plot(history.history['val_loss'], label='Val Loss')\n    ax1.set_title(f'{title} - Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    ax2.plot(history.history['accuracy'], label='Train Accuracy')\n    ax2.plot(history.history['val_accuracy'], label='Val Accuracy')\n    ax2.set_title(f'{title} - Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\nprint(\"\\nSetup complete! Ready to begin.\")",
   "metadata": {
    "id": "fK-V3O9VGgjv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **2. Binary Classification: IMDB Sentiment Analysis**\n\nIn this section, we'll classify movie reviews as **positive** or **negative** using the IMDB dataset.\n\n### **Why IMDB?**\n- It's a classic benchmark for **sentiment analysis**\n- Contains 50,000 movie reviews (25,000 train, 25,000 test)\n- Binary labels: positive (1) or negative (0)\n\n### **Our Approach: TF-IDF Text Representation**\n\nInstead of using raw word indices, we'll convert text to **TF-IDF (Term Frequency-Inverse Document Frequency)** vectors:\n\n- **Term Frequency (TF)**: How often a word appears in a document\n- **Inverse Document Frequency (IDF)**: Downweights common words (like \"the\", \"is\")\n- **TF-IDF = TF × IDF**: Highlights words that are important to a specific document but rare overall\n\nThis gives us a fixed-size numerical vector for each review that we can feed into a neural network.\n\n---\n\n## 2.1 Step 1: Define Problem & Load Data",
   "metadata": {
    "id": "IlO4NP1fGn3Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################\n",
    "# 2.1 IMDB DATA\n",
    "###########################################################\n",
    "num_words = 10000\n",
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = keras.datasets.imdb.load_data(num_words=num_words)\n",
    "\n",
    "print(\"IMDB train samples:\", len(x_train_raw))\n",
    "print(\"IMDB test samples:\", len(x_test_raw))\n",
    "\n",
    "unique, counts = np.unique(y_train_raw, return_counts=True)\n",
    "print(\"IMDB train label distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "classes = dict(zip(unique, counts))\n",
    "print(\"Naive Baseline (Accuracy):\", max([classes[0], classes[1]]) / (classes[0] + classes[1]))"
   ],
   "metadata": {
    "id": "bFraJ0M_G8Xd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.2 Steps 2-3: Metrics & Evaluation Protocol\n\n### **Metrics We'll Use**\n\n| Metric | Why Use It? |\n|--------|-------------|\n| **Accuracy** | Overall correctness - easy to interpret |\n| **F1 Score** | Balances precision and recall - better for imbalanced data |\n| **Confusion Matrix** | Shows true/false positives and negatives |\n\n### **Evaluation Protocol**\n\nWe'll use a **train/validation/test split**:\n- **Training set (80%)**: Used to train the model\n- **Validation set (20% of train)**: Used to tune hyperparameters and monitor overfitting\n- **Test set**: Held out for final evaluation only\n\nThis prevents **data leakage** and gives us an honest estimate of model performance.",
   "metadata": {
    "id": "MMEqX4vyG-h9"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3 Step 4: Data Preparation (TF-IDF Encoding)",
   "metadata": {
    "id": "OPYuuJP0HMu-"
   }
  },
  {
   "cell_type": "markdown",
   "source": "The Keras IMDB dataset comes as integer-encoded sequences. We need to:\n\n1. **Decode** integers back to text strings\n2. **Vectorize** using TF-IDF to create fixed-size feature vectors\n3. **Split** into train/validation sets\n4. **Compute class weights** to handle any class imbalance",
   "metadata": {
    "id": "qoqq5w58HJb4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################\n",
    "# 2.4 BAG-OF-WORDS / TF-IDF FOR IMDB\n",
    "###########################################################\n",
    "# Retrieve word index from Keras\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Reverse mapping (integer -> word)\n",
    "reverse_word_index = {v: k for (k, v) in word_index.items()}\n",
    "# Indices 0..3 are special tokens in Keras IMDB\n",
    "def decode_review(int_seq):\n",
    "    return \" \".join([reverse_word_index.get(i-3, \"?\") for i in int_seq])\n",
    "\n",
    "# Convert integer sequences to raw text\n",
    "train_texts = [\" \".join([reverse_word_index.get(i-3, \"?\") for i in seq]) for seq in x_train_raw]\n",
    "test_texts  = [\" \".join([reverse_word_index.get(i-3, \"?\") for i in seq]) for seq in x_test_raw]\n",
    "\n",
    "# Use TfidfVectorizer or CountVectorizer\n",
    "# vectorizer = CountVectorizer(max_features=num_words, stop_words='english')\n",
    "vectorizer = TfidfVectorizer(max_features=num_words, stop_words='english')\n",
    "\n",
    "X_train_sparse = vectorizer.fit_transform(train_texts)\n",
    "X_test_sparse  = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert sparse -> dense (can be memory-heavy; consider partial approaches for large data)\n",
    "X_train = X_train_sparse.toarray()\n",
    "X_test  = X_test_sparse.toarray()\n",
    "\n",
    "y_train = y_train_raw\n",
    "y_test  = y_test_raw\n",
    "\n",
    "print(\"TF-IDF train shape:\", X_train.shape)\n",
    "print(\"TF-IDF test shape: \", X_test.shape)\n",
    "\n",
    "# Class weighting\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "neg_count, pos_count = counts[0], counts[1]\n",
    "total_count = neg_count + pos_count\n",
    "weight_for_0 = (1.0 / neg_count) * (total_count / 2.0)\n",
    "weight_for_1 = (1.0 / pos_count) * (total_count / 2.0)\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "print(\"IMDB class weights:\", class_weights)"
   ],
   "metadata": {
    "id": "GwF2wn2uGPp2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split out some of the training set for validation\n",
    "X_train_imdb, X_val_imdb, y_train_imdb, y_val_imdb = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Train samples after splitting:\", X_train_imdb.shape[0])\n",
    "print(\"Validation samples:\", X_val_imdb.shape[0])"
   ],
   "metadata": {
    "id": "dn-6TsI7HWQ0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.4 Step 5: Baseline Model (Single-Layer Perceptron)\n\nOur baseline is a **Single-Layer Perceptron (SLP)** - the simplest possible neural network:\n- Just one `Dense(1, sigmoid)` layer\n- Learns a linear decision boundary\n- Establishes a performance floor to beat\n\n**Why start simple?** A baseline tells us if our problem is learnable and gives us a reference point for improvement.",
   "metadata": {
    "id": "OIQ-g1RGHanx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################\n",
    "# 2.5 BASELINE MODEL\n",
    "###########################################################\n",
    "baseline_model = keras.Sequential([\n",
    "    layers.Dense(1, activation='sigmoid', input_shape=(X_train_imdb.shape[1],))\n",
    "])\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_base = baseline_model.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    validation_data=(X_val_imdb, y_val_imdb),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_base, \"IMDB Baseline (TF-IDF)\")\n",
    "\n",
    "# Evaluate on test\n",
    "test_loss_base, test_acc_base = baseline_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Baseline Test Accuracy: {test_acc_base:.4f}\")\n",
    "\n",
    "# Let's also compute F1 on test\n",
    "y_test_preds_base = (baseline_model.predict(X_test) > 0.5).astype(int).ravel()\n",
    "test_f1_base = f1_score(y_test, y_test_preds_base)\n",
    "print(f\"Baseline Test F1 score: {test_f1_base:.4f}\")"
   ],
   "metadata": {
    "id": "yxR4Ef6UHcrx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5 Step 6: Scale Up → Intentionally Overfit\n\nNow we **add capacity** to the model by including hidden layers. The goal is to see if a more complex model can learn better patterns.\n\n**What to expect:**\n- Training accuracy should increase significantly\n- Validation accuracy may plateau or even decrease (overfitting)\n- The gap between train and validation curves indicates overfitting\n\nThis step confirms we have enough model capacity - if we can't overfit, our model might be too simple!",
   "metadata": {
    "id": "UlZcX0IgHh9I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "overfit_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_imdb.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "overfit_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_overfit = overfit_model.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    validation_data=(X_val_imdb, y_val_imdb),\n",
    "    epochs=15,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_overfit, \"IMDB Overfit (TF-IDF)\")\n",
    "\n",
    "# Evaluate on test\n",
    "test_loss_over, test_acc_over = overfit_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Overfit Model Test Accuracy: {test_acc_over:.4f}\")\n",
    "\n",
    "y_test_preds_over = (overfit_model.predict(X_test) > 0.5).astype(int).ravel()\n",
    "test_f1_over = f1_score(y_test, y_test_preds_over)\n",
    "print(f\"Overfit Model Test F1: {test_f1_over:.4f}\")"
   ],
   "metadata": {
    "id": "2k4OWkLbHkbC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.6 Step 7: Regularize (Dropout + L2)\n\nOnce we can overfit, we add **regularization** to improve generalization:\n\n| Technique | How It Works |\n|-----------|--------------|\n| **Dropout** | Randomly \"drops\" neurons during training, preventing co-adaptation |\n| **L2 Regularization** | Penalizes large weights, encouraging simpler models |\n\n**What to expect:**\n- The gap between training and validation curves should shrink\n- Validation performance should improve\n- Training accuracy may decrease slightly (this is okay!)",
   "metadata": {
    "id": "CO7tOD8DHqCS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "reg_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(X_train_imdb.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "reg_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_reg = reg_model.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    validation_data=(X_val_imdb, y_val_imdb),\n",
    "    epochs=15,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_reg, \"IMDB Regularized (TF-IDF)\")\n",
    "\n",
    "# Evaluate test\n",
    "test_loss_reg, test_acc_reg = reg_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Regularized Model Test Accuracy: {test_acc_reg:.4f}\")\n",
    "\n",
    "y_test_preds_reg = (reg_model.predict(X_test) > 0.5).astype(int).ravel()\n",
    "test_f1_reg = f1_score(y_test, y_test_preds_reg)\n",
    "print(f\"Regularized Model Test F1: {test_f1_reg:.4f}\")"
   ],
   "metadata": {
    "id": "DP0oyV-oHt1b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.7 Step 8: Hyperparameter Tuning (Hyperband)\n\nFinally, we use **automated hyperparameter search** to find the best combination of:\n- Number of layers\n- Units per layer\n- Dropout rate\n- L2 regularization strength\n- Learning rate\n\n### **Why Hyperband?**\n\n**Hyperband** is more efficient than random search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations\n\nThis \"early stopping\" approach saves compute time while still exploring widely.",
   "metadata": {
    "id": "ZihWl2_pH3De"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_imdb_model_tuner(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    n_layers = hp.Int('n_layers', 1, 3)\n",
    "    for i in range(n_layers):\n",
    "        units = hp.Choice(f'units_{i}', [64,128,256])\n",
    "        l2_factor = hp.Float(f'l2_{i}', 1e-4, 1e-2, sampling='log')\n",
    "        model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_factor),\n",
    "                               input_shape=(X_train_imdb.shape[1],) if i==0 else ()))\n",
    "        drop = hp.Float(f'drop_{i}', 0.0, 0.5, step=0.1)\n",
    "        model.add(layers.Dropout(drop))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner_imdb = kt.Hyperband(\n",
    "    build_imdb_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory='imdb_tfidf_hyperband',\n",
    "    project_name='imdb_f1_demo'\n",
    ")\n",
    "\n",
    "tuner_imdb.search(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    validation_data=(X_val_imdb, y_val_imdb),\n",
    "    epochs=5,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "best_hp_imdb = tuner_imdb.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparams (IMDB):\", best_hp_imdb.values)\n",
    "\n",
    "best_model_imdb = tuner_imdb.hypermodel.build(best_hp_imdb)\n",
    "history_imdb_tuned = best_model_imdb.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    validation_data=(X_val_imdb, y_val_imdb),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "plot_history(history_imdb_tuned, \"IMDB Tuner (TF-IDF)\")\n",
    "\n",
    "# Evaluate final\n",
    "test_loss_tuned, test_acc_tuned = best_model_imdb.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Tuned Model Test Accuracy: {test_acc_tuned:.4f}\")\n",
    "\n",
    "y_test_preds_tuned = (best_model_imdb.predict(X_test) > 0.5).astype(int).ravel()\n",
    "test_f1_tuned = f1_score(y_test, y_test_preds_tuned)\n",
    "print(f\"Tuned Model Test F1 Score: {test_f1_tuned:.4f}\")"
   ],
   "metadata": {
    "id": "60dg8QwOH6FE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **3. Multi-Class Classification: Fashion MNIST**\n\nNow we tackle a **10-class image classification** problem using Fashion MNIST.\n\n### **Why Fashion MNIST?**\n- Drop-in replacement for classic MNIST (same size: 28×28 grayscale)\n- More challenging than handwritten digits\n- 10 clothing categories to classify\n\n### **The 10 Classes**\n\n| Label | Description |\n|-------|-------------|\n| 0 | T-shirt/top |\n| 1 | Trouser |\n| 2 | Pullover |\n| 3 | Dress |\n| 4 | Coat |\n| 5 | Sandal |\n| 6 | Shirt |\n| 7 | Sneaker |\n| 8 | Bag |\n| 9 | Ankle boot |\n\nWe'll follow the same workflow: **Baseline → Overfit → Regularize → Tune**\n\n---\n\n## 3.1 Steps 1-4: Load, Explore & Prepare Data",
   "metadata": {
    "id": "MKa54FCRILGM"
   }
  },
  {
   "cell_type": "code",
   "source": "#############################################\n# 3.1 LOAD & PREPARE FASHION MNIST\n#############################################\n\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# Load dataset\n(x_train_fm, y_train_fm), (x_test_fm, y_test_fm) = fashion_mnist.load_data()\n\n# Class names for visualization\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nprint(\"Training set shape:\", x_train_fm.shape)\nprint(\"Test set shape:\", x_test_fm.shape)\nprint(\"Number of classes:\", len(class_names))\n\n# Visualize some sample images\nplt.figure(figsize=(12, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(x_train_fm[i], cmap='gray')\n    plt.title(class_names[y_train_fm[i]])\n    plt.axis('off')\nplt.suptitle('Sample Images from Fashion MNIST', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Normalize pixel values to [0, 1]\nx_train_fm = x_train_fm / 255.0\nx_test_fm = x_test_fm / 255.0\n\n# Flatten images: 28x28 -> 784\nX_train_fm_flat = x_train_fm.reshape(-1, 28 * 28)\nX_test_fm_flat = x_test_fm.reshape(-1, 28 * 28)\n\nprint(\"\\nAfter preprocessing:\")\nprint(\"Training features shape:\", X_train_fm_flat.shape)\nprint(\"Test features shape:\", X_test_fm_flat.shape)",
   "metadata": {
    "id": "wXgf20gsH6yi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.2 Step 5: Baseline Model (Single-Layer Softmax)\n\nFor multi-class classification, our baseline uses:\n- **10 output neurons** (one per class)\n- **Softmax activation** (outputs probabilities that sum to 1)\n- **Sparse categorical crossentropy** loss (for integer labels)",
   "metadata": {
    "id": "nVtFz93wISWN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "baseline_fm = keras.Sequential([\n",
    "    layers.Dense(10, activation='softmax', input_shape=(784,))\n",
    "])\n",
    "baseline_fm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_fm_base = baseline_fm.fit(\n",
    "    X_train_fm_flat, y_train_fm,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_fm_base, 'FashionMNIST Baseline')\n",
    "test_loss_fm_base, test_acc_fm_base = baseline_fm.evaluate(X_test_fm_flat, y_test_fm, verbose=0)\n",
    "print(f\"FashionMNIST Baseline Test Accuracy: {test_acc_fm_base:.4f}\")"
   ],
   "metadata": {
    "id": "nMG72rUJIU0n"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.3 Step 6: Scale Up → Intentionally Overfit\n\nAdding hidden layers with ReLU activation to increase model capacity.",
   "metadata": {
    "id": "gXYDEP3LIbKB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "overfit_fm = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "overfit_fm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_fm_over = overfit_fm.fit(\n",
    "    X_train_fm_flat, y_train_fm,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=512,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_fm_over, 'FashionMNIST Overfit')\n",
    "test_loss_fm_over, test_acc_fm_over = overfit_fm.evaluate(X_test_fm_flat, y_test_fm, verbose=0)\n",
    "print(f\"Overfit Model Test Accuracy: {test_acc_fm_over:.4f}\")"
   ],
   "metadata": {
    "id": "ihRSW5taIdcm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.4 Step 7: Regularize (Dropout + L2)\n\nApplying the same regularization techniques to combat overfitting.",
   "metadata": {
    "id": "in50csDHIgsK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "reg_fm = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(784,)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "reg_fm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_fm_reg = reg_fm.fit(\n",
    "    X_train_fm_flat, y_train_fm,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=512,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_fm_reg, 'FashionMNIST Regularized')\n",
    "test_loss_fm_reg, test_acc_fm_reg = reg_fm.evaluate(X_test_fm_flat, y_test_fm, verbose=0)\n",
    "print(f\"FashionMNIST Regularized Test Accuracy: {test_acc_fm_reg:.4f}\")"
   ],
   "metadata": {
    "id": "-e6-_CQaIisA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.5 Step 8: Hyperparameter Tuning (Hyperband)\n\nUsing Keras Tuner to find optimal hyperparameters for Fashion MNIST.",
   "metadata": {
    "id": "APtLPf1gIlwI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_fm_model_tuner(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(784,)))\n",
    "\n",
    "    n_layers = hp.Int('n_layers', 1, 3)\n",
    "    for i in range(n_layers):\n",
    "        units = hp.Choice(f'units_{i}', [128, 256])\n",
    "        l2_factor = hp.Float(f'l2_{i}', 1e-4, 1e-2, sampling='log')\n",
    "        model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_factor)))\n",
    "        drop_rate = hp.Float(f'drop_{i}', 0.0, 0.5, step=0.1)\n",
    "        model.add(layers.Dropout(drop_rate))\n",
    "\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner_fm = kt.Hyperband(\n",
    "    build_fm_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory='fashion_hyperband',\n",
    "    project_name='fm_tfidf_demo'\n",
    ")\n",
    "\n",
    "tuner_fm.search(\n",
    "    X_train_fm_flat, y_train_fm,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "best_hp_fm = tuner_fm.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparams (Fashion MNIST):\", best_hp_fm.values)\n",
    "\n",
    "best_model_fm = tuner_fm.hypermodel.build(best_hp_fm)\n",
    "history_fm_tuned = best_model_fm.fit(\n",
    "    X_train_fm_flat, y_train_fm,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "plot_history(history_fm_tuned, \"FashionMNIST Tuner\")\n",
    "test_loss_fm_tuned, test_acc_fm_tuned = best_model_fm.evaluate(X_test_fm_flat, y_test_fm, verbose=0)\n",
    "print(f\"Tuned Model Test Accuracy: {test_acc_fm_tuned:.4f}\")"
   ],
   "metadata": {
    "id": "AwC4acafIown"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **4. Key Takeaways & Summary**\n\n## What We Learned\n\n### **The Universal ML Workflow Works!**\n\nBy following the systematic 8-step workflow, we achieved strong results on both datasets:\n\n| Dataset | Task | Baseline | After Tuning |\n|---------|------|----------|--------------|\n| IMDB | Binary Classification | ~85% | ~88%+ |\n| Fashion MNIST | 10-Class Classification | ~84% | ~88%+ |\n\n### **Key Insights**\n\n1. **Start Simple**: A baseline model establishes a performance floor and confirms the problem is learnable.\n\n2. **Overfitting is Informative**: If you can't overfit, your model may lack capacity. The ability to overfit means you have room to regularize.\n\n3. **Regularization is Essential**: Dropout and L2 regularization consistently improved generalization on both datasets.\n\n4. **Automated Tuning Saves Time**: Hyperband efficiently explored the hyperparameter space, finding configurations we might not have tried manually.\n\n5. **Metrics Matter**: F1 score provided a more nuanced view of binary classification performance than accuracy alone.\n\n### **Comparing Approaches**\n\n| Aspect | IMDB (Text) | Fashion MNIST (Images) |\n|--------|-------------|------------------------|\n| Input Representation | TF-IDF vectors (10,000 features) | Flattened pixels (784 features) |\n| Output Activation | Sigmoid (binary) | Softmax (multi-class) |\n| Loss Function | Binary Crossentropy | Sparse Categorical Crossentropy |\n| Key Challenge | High-dimensional sparse input | Visual pattern recognition |\n\n## Next Steps\n\nTo further improve these models, consider:\n\n1. **For IMDB**: Use word embeddings (Word2Vec, GloVe) or transformer-based models (BERT)\n2. **For Fashion MNIST**: Use Convolutional Neural Networks (CNNs) which are designed for image data\n3. **Advanced Regularization**: Try Batch Normalization, data augmentation\n4. **Learning Rate Scheduling**: Use callbacks like `ReduceLROnPlateau`",
   "metadata": {
    "id": "ARSDJsH-IxyR"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# **5. Appendix: EarlyStopping Deep Dive**\n\nThis appendix explores **EarlyStopping** - a callback that automatically stops training when validation performance stops improving.\n\n## 5.1 EarlyStopping in Action\n\nLet's see how EarlyStopping works with Fashion MNIST:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.2 EarlyStopping vs. Regularization: Do We Need Both?\n\nA common question: *\"If EarlyStopping prevents overfitting, why bother with Dropout and L2?\"*\n\n### **The Short Answer: They Work Differently**\n\n| Technique | When It Acts | How It Helps |\n|-----------|--------------|--------------|\n| **EarlyStopping** | End of training | Stops training when val loss stops improving |\n| **Dropout/L2** | During training | Shapes the learning trajectory from the start |\n\n### **Why Use Both?**\n\n1. **EarlyStopping is reactive**: It only stops training *after* overfitting has begun\n2. **Regularization is proactive**: It prevents overfitting from happening in the first place\n3. **Combined effect**: Regularization improves the model's learning path, while EarlyStopping ensures we don't train too long\n\n### **Best Practice**\n\nUse **regularization + EarlyStopping** together for optimal results:\n- Regularization finds a better solution\n- EarlyStopping saves training time and prevents late-stage degradation",
   "metadata": {
    "id": "zVIWqV1yI0JX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## **Congratulations!**\n\nYou've completed this hands-on lab on Deep Neural Networks. You now know how to:\n\n- ✅ Apply the universal ML workflow systematically\n- ✅ Build DNNs for binary and multi-class classification\n- ✅ Use TF-IDF for text representation\n- ✅ Combat overfitting with Dropout and L2 regularization\n- ✅ Automate hyperparameter tuning with Keras Tuner's Hyperband\n- ✅ Use EarlyStopping effectively\n\n**Keep experimenting!** Try modifying the architectures, hyperparameters, or even applying these techniques to your own datasets.",
   "metadata": {
    "id": "MOYCgJDfI4G4"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qW-3pVuOI_oK"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}