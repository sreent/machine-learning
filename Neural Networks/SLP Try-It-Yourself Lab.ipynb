{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPbkXaqdMC8Ixyj0u708tt8",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Neural%20Networks/SLP%20Try-It-Yourself%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Try-It-Yourself SLP Lab**\n",
    "\n",
    "## **Contents**\n",
    "\n",
    "1. **IMDB (Binary Classification)**\n",
    "   - 1.1 Data Loading & Bag-of-Words/TF-IDF Encoding  \n",
    "   - 1.2 Build & Train SLP  \n",
    "   - 1.3 Evaluate (Confusion Matrix, Classification Report)  \n",
    "   - 1.4 (Optional) Hyperparameter Tuning  \n",
    "   - 1.5 (Optional) Saving & Loading  \n",
    "\n",
    "2. **CIFAR-10 (Multi-Class Classification)**\n",
    "   - 2.1 Data Loading & Flattening  \n",
    "   - 2.2 Build & Train SLP  \n",
    "   - 2.3 Evaluate (Confusion Matrix, Classification Report)  \n",
    "   - 2.4 Weight Visualization (reshape 32×32×3)  \n",
    "   - 2.5 (Optional) Hyperparameter Tuning  \n",
    "   - 2.6 (Optional) Saving & Loading  \n",
    "\n",
    "**Note**:  \n",
    "A **Single-Layer Perceptron (SLP)** is very limited for these datasets; don’t expect high accuracy. **Our goal** is to learn the fundamentals of:\n",
    "- Data loading & preprocessing  \n",
    "- Model building & training  \n",
    "- Evaluating via accuracy, confusion matrices, classification reports  \n",
    "- Basic weight interpretation (CIFAR-10)\n",
    "\n",
    "Let's get started!\n"
   ],
   "metadata": {
    "id": "nsDXdmmJr-up"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "###########################################################\n",
    "# SETUP & IMPORTS\n",
    "###########################################################\n",
    "# TODO: import necessary libraries here (numpy, matplotlib, tensorflow, etc.)\n",
    "\n",
    "# Example:\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ],
   "metadata": {
    "id": "jTsSG-hZr_Fu"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. IMDB (Binary Classification)**\n",
    "\n",
    "The **IMDB** dataset contains 50,000 movie reviews labeled as **positive (1)** or **negative (0)**. In this lab, we will:\n",
    "\n",
    "1. **Load** the IMDB data from `keras.datasets.imdb`.\n",
    "2. Use a **Bag-of-Words** or **TF-IDF** approach to transform each review into a numeric vector (e.g., using `TfidfVectorizer`).\n",
    "3. **Build** a Single-Layer Perceptron with **1 output neuron** (sigmoid).\n",
    "4. **Train** and **evaluate** the model’s performance (accuracy, confusion matrix, classification report).\n",
    "\n",
    "No weight visualization is done for IMDB, as text doesn’t reshape nicely into a 2D/3D format.\n",
    "\n",
    "### 1.1 Data Loading & Bag-of-Words/TF-IDF Encoding\n"
   ],
   "metadata": {
    "id": "I4LtGVZMr45l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1.1 TODO: Data Loading & Vectorization\n",
    "\n",
    "# Steps:\n",
    "# 1. Load IMDB using keras.datasets.imdb.load_data(num_words=10000)\n",
    "# 2. Decode integer sequences to raw text if you plan to use CountVectorizer/TfidfVectorizer.\n",
    "#    (You'll need to retrieve the word_index, create a reverse mapping, decode each review.)\n",
    "# 3. Use TfidfVectorizer (or CountVectorizer) from sklearn to convert raw text to numeric arrays.\n",
    "#    vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "# 4. Convert the resulting sparse matrix to dense if needed: .toarray()\n",
    "\n",
    "# (x_train_imdb, y_train_imdb), (x_test_imdb, y_test_imdb) = ...\n",
    "# train_texts = ...\n",
    "# test_texts  = ...\n",
    "# vectorizer = ...\n",
    "# X_train_imdb_bow = ...\n",
    "# X_test_imdb_bow  = ...\n"
   ],
   "metadata": {
    "id": "Fj5Ex0RUr153"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Build & Train SLP\n",
    "\n",
    "**Instructions**:\n",
    "- Create `model_imdb = keras.Sequential([...])` with `Dense(1, activation='sigmoid')`.\n",
    "- Compile with `'adam'` and `'binary_crossentropy'`.\n",
    "- Train for ~5 epochs with `validation_split=0.2`.\n"
   ],
   "metadata": {
    "id": "LM9OC89Xx7jR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1.2 TODO: Build & Train SLP\n",
    "\n",
    "# Example:\n",
    "# model_imdb = keras.Sequential([\n",
    "#     layers.Dense(1, activation='sigmoid', input_shape=(X_train_imdb_bow.shape[1],))\n",
    "# ])\n",
    "# model_imdb.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# history_imdb = model_imdb.fit(X_train_imdb_bow, y_train_imdb, ...)\n"
   ],
   "metadata": {
    "id": "wFbfnBwnxnb3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Evaluate (Confusion Matrix, Classification Report)\n",
    "\n",
    "**Instructions**:\n",
    "1. Evaluate on the test set.\n",
    "2. Generate predictions (probabilities), threshold at 0.5 to get binary labels.\n",
    "3. Print confusion matrix & classification report.\n"
   ],
   "metadata": {
    "id": "gVnfh-0Jx37N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1.3 TODO: Evaluate IMDB SLP\n",
    "\n",
    "# test_loss_imdb, test_acc_imdb = model_imdb.evaluate(...)\n",
    "# y_pred_imdb_probs = model_imdb.predict(...)\n",
    "# y_pred_imdb = ...\n",
    "# cm_imdb = confusion_matrix(...)\n",
    "# cr_imdb = classification_report(...)\n",
    "# print(...)\n"
   ],
   "metadata": {
    "id": "h69c6LEaxqsL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (Optional) Plot Training Curves\n",
    "\n",
    "From `history_imdb.history`, you can plot `'loss'`, `'val_loss'`, `'accuracy'`, and `'val_accuracy'`.\n"
   ],
   "metadata": {
    "id": "3G5-cLDpruC3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: plot training curves\n",
    "# plt.figure(...)\n",
    "# plt.plot(...)\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "id": "zqAqtQolrzLI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 (Optional) Hyperparameter Tuning\n",
    "\n",
    "Try changing:\n",
    "- Learning rate (e.g., `learning_rate=0.0005`)\n",
    "- Number of epochs\n",
    "- Batch size\n",
    "\n",
    "Compare results to your baseline SLP.\n"
   ],
   "metadata": {
    "id": "PyJwDEiCsfSd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1.4 (Optional) TODO: Hyperparameter Tuning\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# model_imdb_tuned = ...\n",
    "# model_imdb_tuned.compile(optimizer=Adam(learning_rate=0.0005), ...)\n",
    "# history_imdb_tuned = ...\n",
    "# Evaluate, compare\n"
   ],
   "metadata": {
    "id": "3BWx5R151aEc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 (Optional) Saving & Loading\n",
    "\n",
    "Demonstrate how to save the model to `\"slp_imdb.h5\"` and reload it with `keras.models.load_model()`.\n"
   ],
   "metadata": {
    "id": "hmg5cHMk1geB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1.5 (Optional) TODO: Saving & Loading\n",
    "# model_imdb.save(\"slp_imdb.h5\")\n",
    "# loaded_model_imdb = keras.models.load_model(\"slp_imdb.h5\")\n",
    "# Evaluate loaded_model_imdb on test set\n"
   ],
   "metadata": {
    "id": "w0jfvzHv1jBj"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. CIFAR-10 (Multi-Class Classification)**\n",
    "\n",
    "**Goal**: Classify 32×32 color images into 10 classes.  \n",
    "1. **Load & Flatten** each image to 3,072 features.  \n",
    "2. **Build** an SLP with 10 outputs (softmax).  \n",
    "3. **Train & Evaluate**.  \n",
    "4. **Weight Visualization**: Reshape each output neuron’s weights into (32,32,3) to see the learned “pattern.”  \n",
    "\n",
    "**Note**: Performance is quite low for an SLP on CIFAR-10, but it illustrates the fundamentals.\n",
    "\n",
    "### 2.1 Data Loading & Flattening\n"
   ],
   "metadata": {
    "id": "lyPQx1hysr3t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.1 TODO: CIFAR-10 Loading & Flattening\n",
    "\n",
    "# from tensorflow.keras.datasets import cifar10\n",
    "# (X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = ...\n",
    "# Print shapes, possibly display some images\n",
    "# Scale to [0..1]\n",
    "# Flatten => X_train_cifar.reshape(-1, 32*32*3), etc.\n"
   ],
   "metadata": {
    "id": "rV9FXpIDr0nB"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Build & Train SLP\n",
    "\n",
    "**Instructions**:\n",
    "- Create `Dense(10, activation='softmax')`.\n",
    "- Compile with `optimizer='adam'` and `loss='sparse_categorical_crossentropy'`.\n",
    "- Train for about 5 epochs, `validation_split=0.2`.\n"
   ],
   "metadata": {
    "id": "6mm3H1HSs1h7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.2 TODO: Build & Train SLP for CIFAR-10\n",
    "\n",
    "# model_cifar = keras.Sequential([\n",
    "#     layers.Dense(10, activation='softmax', input_shape=(3072,))\n",
    "# ])\n",
    "# model_cifar.compile(...)\n",
    "# history_cifar = model_cifar.fit(...)\n"
   ],
   "metadata": {
    "id": "KUVt9uixyf-e"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Evaluate (Confusion Matrix, Classification Report)\n",
    "\n",
    "**Instructions**:\n",
    "- Evaluate test accuracy & loss.\n",
    "- Convert probabilities to class predictions via `argmax`.\n",
    "- Print confusion matrix & classification report.\n"
   ],
   "metadata": {
    "id": "cztAMUvHyj85"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.3 TODO: Evaluate CIFAR-10 SLP\n",
    "\n",
    "# test_loss_cifar, test_acc_cifar = model_cifar.evaluate(...)\n",
    "# y_pred_cifar_probs = model_cifar.predict(...)\n",
    "# y_pred_cifar = np.argmax(y_pred_cifar_probs, axis=1)\n",
    "# cm_cifar = confusion_matrix(...)\n",
    "# cr_cifar = classification_report(...)\n",
    "# print(...)\n",
    "\n"
   ],
   "metadata": {
    "id": "iuqHr6wtyisP"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Weight Visualization (reshape 32×32×3)\n",
    "\n",
    "**Instructions**:\n",
    "1. Extract the model’s weight matrix: `weights_cifar = model_cifar.get_weights()[0]` (shape: `(3072, 10)`).\n",
    "2. For each class `i`, reshape `weights_cifar[:, i]` from `(3072,)` => `(32, 32, 3)`.\n",
    "3. Display as an “image.” Optional: normalize for better visibility.\n"
   ],
   "metadata": {
    "id": "8VYwW-bNypCd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.4 TODO: Weight Visualization\n",
    "# weights_cifar = model_cifar.get_weights()[0]  # (3072, 10)\n",
    "# for i in range(10):\n",
    "#     w_i = weights_cifar[:, i]\n",
    "#     w_i_3d = w_i.reshape(32,32,3)\n",
    "#     # optional normalization\n",
    "#     plt.imshow(...)\n",
    "#     plt.title(f\"Class {i}\")\n",
    "#     plt.axis('off')\n",
    "# plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "FhcyyS4nynmj"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 (Optional) Hyperparameter Tuning\n",
    "\n",
    "As before, try adjusting learning rate, epochs, or batch size and compare results.\n",
    "\n"
   ],
   "metadata": {
    "id": "_1qFqyzzyxRC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.5 (Optional) TODO: Hyperparameter Tuning\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# model_cifar_tuned = ...\n",
    "# model_cifar_tuned.compile(optimizer=Adam(learning_rate=...), ...)\n",
    "# history_cifar_tuned = ...\n",
    "# Evaluate & compare\n"
   ],
   "metadata": {
    "id": "3aa5nP_tys6g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 (Optional) Saving & Loading\n",
    "\n",
    "Just like IMDB, we can save the model to `\"slp_cifar.h5\"` and reload it with `keras.models.load_model`.\n"
   ],
   "metadata": {
    "id": "YzRGGH8ky1yo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.6 (Optional) TODO: Saving & Loading\n",
    "# model_cifar.save(\"slp_cifar.h5\")\n",
    "# loaded_model_cifar = keras.models.load_model(\"slp_cifar.h5\")\n",
    "# Evaluate loaded_model_cifar on test data\n"
   ],
   "metadata": {
    "id": "XfLEtQUny0oA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Wrap-Up**\n",
    "\n",
    "1. **IMDB**  \n",
    "   - Binary classification with a bag-of-words or TF-IDF representation.\n",
    "   - SLP performance can be moderate compared to advanced text models (embeddings, RNN, Transformers).\n",
    "\n",
    "2. **CIFAR-10**  \n",
    "   - Flatten 32×32×3 images => 3072 features.\n",
    "   - SLP is purely linear, so ~25–30% accuracy is common, but it demonstrates the fundamentals.\n",
    "   - Weight visualization shows how each class neuron “weights” the input pixels.\n",
    "\n",
    "3. **Hyperparameter Tuning**  \n",
    "   - Adjust learning rate, epochs, batch size.  \n",
    "   - True improvements often require deeper models or specialized architectures (CNNs, etc.).\n",
    "\n",
    "4. **Saving & Loading**  \n",
    "   - Use `model.save(\"...\")` and `keras.models.load_model(\"...\")` for end-to-end workflows.\n",
    "\n",
    "**Next Steps**:\n",
    "- Add hidden layers to see if performance improves.\n",
    "- For IMDB, consider an Embedding layer or RNN-based approaches.\n",
    "- For CIFAR-10, try a basic CNN instead of a flattened input.\n",
    "- Explore callbacks like `EarlyStopping`, advanced regularization, or data augmentation.\n",
    "\n"
   ],
   "metadata": {
    "id": "y-7DOtmIy94U"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vJpeI7m-y7JC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ajN1FTszy8tv"
   }
  }
 ]
}