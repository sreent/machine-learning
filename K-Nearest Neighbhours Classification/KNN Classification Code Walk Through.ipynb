{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20(KNN)%20Classification/Code%20Walk%20Through%3A%20K-Nearest%20Neighbours%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classification: Code Walk Through\n",
    "\n",
    "This notebook walks through the **computational steps** of the K-Nearest Neighbors (KNN) classification algorithm.\n",
    "\n",
    "## What We'll Cover:\n",
    "1. **Visualize the data** - understand the dataset\n",
    "2. **Calculate distances** - measure similarity between points\n",
    "3. **Find K nearest neighbors** - identify closest training points\n",
    "4. **Make prediction** - use majority voting\n",
    "\n",
    "We'll show **both loop versions** (to understand the logic) and **vectorized NumPy versions** (for efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We need:\n",
    "- **NumPy** for numerical operations\n",
    "- **Matplotlib** for visualization\n",
    "- **sklearn.metrics.pairwise_distances** for efficient distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training Data\n",
    "\n",
    "We have:\n",
    "- **10 training points** with **2 features** each\n",
    "- **2 classes**: class 0 and class 1\n",
    "- First 5 points belong to class 0\n",
    "- Last 5 points belong to class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: 10 points with 2 features\n",
    "X_train = np.array( [ [1.536, 3.554],   # Point 0, class 0\n",
    "                      [1.771, 2.783],   # Point 1, class 0\n",
    "                      [2.506, 2.880],   # Point 2, class 0\n",
    "                      [2.652, 4.545],   # Point 3, class 0\n",
    "                      [3.590, 3.784],   # Point 4, class 0\n",
    "                      [1.279, 1.443],   # Point 5, class 1\n",
    "                      [2.000, 2.325],   # Point 6, class 1\n",
    "                      [2.096, 0.583],   # Point 7, class 1\n",
    "                      [2.539, 1.541],   # Point 8, class 1\n",
    "                      [3.251, 0.080] ] ) # Point 9, class 1\n",
    "\n",
    "# Labels: which class each point belongs to (0 or 1)\n",
    "y_train = np.array( [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] )\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)  # (10, 2) = 10 points, 2 features\n",
    "print(\"Labels shape:\", y_train.shape)         # (10,) = 10 labels\n",
    "print(\"\\nFirst few training points:\")\n",
    "print(X_train[:3])\n",
    "print(\"\\nCorresponding labels:\")\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the Data\n",
    "\n",
    "Let's plot our training data to see how it's distributed in 2D space.\n",
    "\n",
    "**Note:** We start by just looking at the data points, **without worrying about their classes yet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter plot of all training points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:,0], X_train[:,1],\n",
    "           c='steelblue', s=100, alpha=0.6,\n",
    "           edgecolors='black', linewidths=1.5)\n",
    "plt.xlabel('Feature 1 ($x_1$)', fontsize=12)\n",
    "plt.ylabel('Feature 2 ($x_2$)', fontsize=12)\n",
    "plt.title('Training Data Visualization', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis([0, 5, 0, 5])\n",
    "plt.show()\n",
    "\n",
    "print(f\"We have {len(X_train)} training points in 2D space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Test Point\n",
    "\n",
    "Now we have a new point **[2.0, 2.0]** that we want to classify.\n",
    "\n",
    "**Question:** Should this point be classified as class 0 or class 1?\n",
    "\n",
    "KNN will answer this by finding the K nearest training points and using **majority voting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test point: a new point we want to classify\n",
    "X_test = np.array([[2.0, 2.0]])\n",
    "\n",
    "print(\"Test point:\", X_test[0])\n",
    "print(\"Shape:\", X_test.shape)  # (1, 2) = 1 point, 2 features\n",
    "\n",
    "# Visualize test point with training data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:,0], X_train[:,1],\n",
    "           c='steelblue', s=100, alpha=0.6,\n",
    "           edgecolors='black', linewidths=1.5,\n",
    "           label='Training points')\n",
    "plt.scatter(X_test[:,0], X_test[:,1],\n",
    "           c='red', s=300, marker='*',\n",
    "           edgecolors='black', linewidths=2,\n",
    "           label='Test point')\n",
    "plt.xlabel('Feature 1 ($x_1$)', fontsize=12)\n",
    "plt.ylabel('Feature 2 ($x_2$)', fontsize=12)\n",
    "plt.title('Test Point to Classify', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis([0, 5, 0, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Distances\n",
    "\n",
    "To find nearest neighbors, we need to calculate the distance from the test point to each training point.\n",
    "\n",
    "We'll use **Euclidean distance**:\n",
    "\n",
    "$$d = \\sqrt{(x_1 - x_1')^2 + (x_2 - x_2')^2}$$\n",
    "\n",
    "### Manual Calculation Example\n",
    "\n",
    "Let's manually calculate the distance from test point **[2.0, 2.0]** to the **first training point [1.536, 3.554]**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation for first training point\n",
    "test_point = X_test[0]       # [2.0, 2.0]\n",
    "first_train_point = X_train[0]  # [1.536, 3.554]\n",
    "\n",
    "print(\"Test point:         \", test_point)\n",
    "print(\"First training point:\", first_train_point)\n",
    "print()\n",
    "\n",
    "# Step 1: Calculate differences\n",
    "diff_1 = test_point[0] - first_train_point[0]\n",
    "diff_2 = test_point[1] - first_train_point[1]\n",
    "print(f\"Step 1 - Differences:\")\n",
    "print(f\"  Feature 1: {test_point[0]:.3f} - {first_train_point[0]:.3f} = {diff_1:.3f}\")\n",
    "print(f\"  Feature 2: {test_point[1]:.3f} - {first_train_point[1]:.3f} = {diff_2:.3f}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Square the differences\n",
    "squared_1 = diff_1 ** 2\n",
    "squared_2 = diff_2 ** 2\n",
    "print(f\"Step 2 - Square the differences:\")\n",
    "print(f\"  ({diff_1:.3f})\u00b2 = {squared_1:.3f}\")\n",
    "print(f\"  ({diff_2:.3f})\u00b2 = {squared_2:.3f}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Sum the squared differences\n",
    "sum_squared = squared_1 + squared_2\n",
    "print(f\"Step 3 - Sum:\")\n",
    "print(f\"  {squared_1:.3f} + {squared_2:.3f} = {sum_squared:.3f}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Take square root\n",
    "distance = np.sqrt(sum_squared)\n",
    "print(f\"Step 4 - Square root:\")\n",
    "print(f\"  \u221a{sum_squared:.3f} = {distance:.3f}\")\n",
    "print()\n",
    "print(f\"Distance from test point to first training point: {distance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using a Loop (Explicit Logic)\n",
    "\n",
    "Now let's calculate distances to **all** training points using a loop.\n",
    "\n",
    "This shows the logic clearly: we go through each training point one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances using a loop\n",
    "distances_loop = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    # Get the training point\n",
    "    train_point = X_train[i]\n",
    "\n",
    "    # Calculate difference for each feature\n",
    "    diff = test_point - train_point\n",
    "\n",
    "    # Square the differences\n",
    "    squared_diff = diff ** 2\n",
    "\n",
    "    # Sum and take square root\n",
    "    distance = np.sqrt(np.sum(squared_diff))\n",
    "\n",
    "    # Store the distance\n",
    "    distances_loop.append(distance)\n",
    "\n",
    "    print(f\"Distance to point {i}: {distance:.4f}\")\n",
    "\n",
    "# Convert to numpy array\n",
    "distances_loop = np.array(distances_loop)\n",
    "print(f\"\\nDistances shape: {distances_loop.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Vectorized NumPy (Broadcasting)\n",
    "\n",
    "Instead of looping, we can use **NumPy broadcasting** to calculate all distances at once!\n",
    "\n",
    "**Key idea:** \n",
    "- When we subtract `X_test` (shape: 1\u00d72) from `X_train` (shape: 10\u00d72), NumPy automatically broadcasts\n",
    "- This creates a 10\u00d72 array of differences - one row for each training point\n",
    "- Then we square, sum across features (axis=1), and take the square root\n",
    "\n",
    "This is **much faster** than loops for large datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances using vectorized NumPy operations\n",
    "# Step 1: Calculate differences (broadcasting automatically expands dimensions)\n",
    "diff = X_test - X_train\n",
    "print(\"Differences shape:\", diff.shape)  # (10, 2) - one row per training point\n",
    "print(\"First few differences:\")\n",
    "print(diff[:3])\n",
    "print()\n",
    "\n",
    "# Step 2: Square the differences\n",
    "squared_diff = diff ** 2\n",
    "print(\"Squared differences shape:\", squared_diff.shape)  # (10, 2)\n",
    "print()\n",
    "\n",
    "# Step 3: Sum across features (axis=1)\n",
    "sum_squared = squared_diff.sum(axis=1)\n",
    "print(\"Sum of squared differences shape:\", sum_squared.shape)  # (10,)\n",
    "print(\"Sum of squared differences:\")\n",
    "print(sum_squared)\n",
    "print()\n",
    "\n",
    "# Step 4: Take square root\n",
    "distances_numpy = np.sqrt(sum_squared)\n",
    "print(\"Distances (NumPy vectorized):\")\n",
    "print(distances_numpy)\n",
    "print()\n",
    "\n",
    "# Or more concisely in one line:\n",
    "distances_numpy_compact = np.sqrt(((X_test - X_train) ** 2).sum(axis=1))\n",
    "print(\"Same result (compact version):\")\n",
    "print(distances_numpy_compact)\n",
    "print()\n",
    "\n",
    "# Verify both approaches give same result\n",
    "print(\"Loop and NumPy results match:\", np.allclose(distances_loop, distances_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Using sklearn's `pairwise_distances`\n",
    "\n",
    "For convenience, sklearn provides `pairwise_distances` which does all of this for us.\n",
    "\n",
    "This is a library function that's highly optimized and can use different distance metrics.\n",
    "\n",
    "**Note:** `pairwise_distances` returns a 2D array (matrix of distances), so we use `.ravel()` to flatten it to 1D."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate distances using pairwise_distances from sklearn\n",
    "distances_2d = pairwise_distances(X_test, X_train)\n",
    "print(\"2D array shape:\", distances_2d.shape)  # (1, 10) = 1 test point, 10 training points\n",
    "print(\"2D array:\")\n",
    "print(distances_2d)\n",
    "print()\n",
    "\n",
    "# Flatten to 1D array using .ravel()\n",
    "distances_sklearn = distances_2d.ravel()\n",
    "print(\"1D array shape:\", distances_sklearn.shape)  # (10,)\n",
    "print(\"1D array:\")\n",
    "print(distances_sklearn)\n",
    "print()\n",
    "\n",
    "# Verify all three approaches give same result\n",
    "print(\"All approaches match:\")\n",
    "print(\"  Loop vs NumPy:\", np.allclose(distances_loop, distances_numpy))",
    "print(\"  NumPy vs sklearn:\", np.allclose(distances_numpy, distances_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Find K Nearest Neighbors\n",
    "\n",
    "Now we have distances to all training points. We need to find the **5 closest points** (K=5).\n",
    "\n",
    "**How do we find them?**\n",
    "We need to:\n",
    "1. Sort the distances from smallest to largest\n",
    "2. Get the **indices** (positions) of the 5 smallest distances\n",
    "\n",
    "### What is `argsort()`?\n",
    "\n",
    "`argsort()` returns the **indices** that would sort an array, not the sorted values themselves.\n",
    "\n",
    "**Example:**\n",
    "- Array: [4.5, 2.1, 7.3, 1.8, 3.2]\n",
    "- `argsort()` returns: [3, 1, 4, 0, 2]\n",
    "- This means: index 3 has the smallest value (1.8), then index 1 (2.1), then index 4 (3.2), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which indices argsort returns\n",
    "sorted_indices = np.argsort(distances_sklearn)\n",
    "print(\"All indices sorted by distance:\")\n",
    "print(sorted_indices)\n",
    "print()\n",
    "\n",
    "# Get the first 5 indices (K=5 nearest neighbors)\n",
    "K = 5\n",
    "nearest_indices = sorted_indices[:K]\n",
    "print(f\"Indices of {K} nearest neighbors:\")\n",
    "print(nearest_indices)\n",
    "print()\n",
    "\n",
    "# Show the actual distances\n",
    "print(f\"Distances to these {K} nearest neighbors:\")\n",
    "for i, idx in enumerate(nearest_indices):\n",
    "    print(f\"  Neighbor {i+1}: point {idx}, distance = {distances_sklearn[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Get Labels of Nearest Neighbors\n",
    "\n",
    "Now we know **which** training points are closest (indices: {}).\n",
    "\n",
    "Let's see what **classes** these neighbors belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels of the K nearest neighbors\n",
    "neighbor_labels = y_train[nearest_indices]\n",
    "\n",
    "print(f\"Labels of {K} nearest neighbors:\")\n",
    "print(neighbor_labels)\n",
    "print()\n",
    "\n",
    "# Show details\n",
    "print(\"Detailed view:\")\n",
    "for i, idx in enumerate(nearest_indices):\n",
    "    print(f\"  Neighbor {i+1}: training point {idx}, class = {y_train[idx]}, distance = {distances_sklearn[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Make Prediction via Majority Voting\n",
    "\n",
    "Now we have the labels: {}\n",
    "\n",
    "**Voting:**\n",
    "- Count how many neighbors belong to each class\n",
    "- The class with the most votes wins!\n",
    "\n",
    "### Approach 1: Manual Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count votes manually\n",
    "count_class_0 = 0\n",
    "count_class_1 = 0\n",
    "\n",
    "for label in neighbor_labels:\n",
    "    if label == 0:\n",
    "        count_class_0 += 1\n",
    "    elif label == 1:\n",
    "        count_class_1 += 1\n",
    "\n",
    "print(\"Vote counts:\")\n",
    "print(f\"  Class 0: {count_class_0} votes\")\n",
    "print(f\"  Class 1: {count_class_1} votes\")\n",
    "print()\n",
    "\n",
    "# Determine winner\n",
    "if count_class_0 > count_class_1:\n",
    "    prediction_manual = 0\n",
    "    print(f\"Winner: Class 0 (with {count_class_0} votes)\")\n",
    "else:\n",
    "    prediction_manual = 1\n",
    "    print(f\"Winner: Class 1 (with {count_class_1} votes)\")\n",
    "\n",
    "print(f\"\\nPredicted class: {prediction_manual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using NumPy's `unique()`\n",
    "\n",
    "`np.unique()` with `return_counts=True` counts occurrences of each unique value.\n",
    "\n",
    "This is more efficient than manual counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count votes using np.unique\n",
    "unique_labels, vote_counts = np.unique(neighbor_labels, return_counts=True)\n",
    "\n",
    "print(\"Unique labels found:\", unique_labels)\n",
    "print(\"Vote counts:\", vote_counts)\n",
    "print()\n",
    "\n",
    "# Show the mapping\n",
    "print(\"Vote summary:\")\n",
    "for label, count in zip(unique_labels, vote_counts):\n",
    "    print(f\"  Class {label}: {count} votes\")\n",
    "print()\n",
    "\n",
    "# Find the class with most votes using argmax\n",
    "winner_index = np.argmax(vote_counts)\n",
    "prediction_numpy = unique_labels[winner_index]\n",
    "\n",
    "print(f\"Winner index: {winner_index}\")\n",
    "print(f\"Predicted class: {prediction_numpy}\")\n",
    "print()\n",
    "\n",
    "# Verify both approaches match\n",
    "print(f\"Manual and NumPy predictions match: {prediction_manual == prediction_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize the Result\n",
    "\n",
    "Let's visualize the test point, its K nearest neighbors, and the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the classification result\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot all training points by class\n",
    "plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1],\n",
    "           c='blue', s=100, alpha=0.3, label='Class 0 (training)', edgecolors='black')\n",
    "plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1],\n",
    "           c='orange', s=100, alpha=0.3, label='Class 1 (training)', edgecolors='black')\n",
    "\n",
    "# Highlight the K nearest neighbors\n",
    "nearest_neighbors_X = X_train[nearest_indices]\n",
    "plt.scatter(nearest_neighbors_X[:,0], nearest_neighbors_X[:,1],\n",
    "           c='green', s=200, marker='s', alpha=0.7,\n",
    "           edgecolors='darkgreen', linewidths=2, label=f'{K} Nearest Neighbors')\n",
    "\n",
    "# Plot test point\n",
    "test_color = 'blue' if prediction_numpy == 0 else 'orange'\n",
    "plt.scatter(X_test[:,0], X_test[:,1],\n",
    "           c=test_color, s=400, marker='*',\n",
    "           edgecolors='black', linewidths=2,\n",
    "           label=f'Test point (predicted: class {prediction_numpy})')\n",
    "\n",
    "plt.xlabel('Feature 1 ($x_1$)', fontsize=12)\n",
    "plt.ylabel('Feature 2 ($x_2$)', fontsize=12)\n",
    "plt.title(f'KNN Classification Result (K={K})', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis([0, 5, 0, 5])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test point {X_test[0]} is classified as class {prediction_numpy}\")\n",
    "print(f\"Based on {K} nearest neighbors voting: {vote_counts[winner_index]} votes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've walked through all the computational steps of KNN Classification:\n",
    "\n",
    "1. \u2705 **Visualized data** - saw training points in 2D space\n",
    "2. \u2705 **Calculated distances** - computed Euclidean distance from test point to each training point\n",
    "3. \u2705 **Found K nearest neighbors** - used `argsort()` to find indices of 5 closest points\n",
    "4. \u2705 **Made prediction** - used majority voting among the K neighbors\n",
    "\n",
    "### Key NumPy Operations Used:\n",
    "\n",
    "- **`pairwise_distances(X_test, X_train)`** - efficiently calculates all distances\n",
    "- **`.ravel()`** - flattens 2D array to 1D\n",
    "- **`np.argsort(distances)`** - returns indices that would sort the array\n",
    "- **`array[indices]`** - fancy indexing to select multiple elements\n",
    "- **`np.unique(labels, return_counts=True)`** - counts occurrences of each value\n",
    "- **`np.argmax(counts)`** - finds index of maximum value\n",
    "\n",
    "### Why Both Approaches?\n",
    "\n",
    "- **Loop versions** help you understand the logic step-by-step\n",
    "- **Vectorized versions** are much faster for large datasets\n",
    "\n",
    "In practice, use vectorized operations, but understanding loops helps you know what's happening under the hood!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNQLvmAeGBonMPiutNnPITO",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}