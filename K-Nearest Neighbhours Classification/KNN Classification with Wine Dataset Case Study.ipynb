{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbhours%20Classification/KNN%20Classification%20with%20Wine%20Dataset%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519b78c4",
      "metadata": {
        "id": "519b78c4"
      },
      "source": [
        "# Case Study: KNN Classification with Wine Dataset (UCI)\n",
        "\n",
        "K‑Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that classifies new examples based on similarity to known examples. In this case study, we’ll step through a practical example using the **Wine recognition** dataset (from the UCI Machine Learning Repository) to illustrate key concepts and best practices of KNN classification. This dataset contains chemical analysis results for wines from three cultivars (classes), with 13 continuous features (e.g. alcohol content, acidity, magnesium, phenols, color intensity, etc.). We simulate the scenario of predicting a wine’s cultivar from its chemical properties, akin to a chemist identifying origin by lab measurements.\n",
        "\n",
        "## What we’ll cover\n",
        "- **Data exploration and preparation:** Understanding feature scales and splitting data into training, validation, and test sets.  \n",
        "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance.  \n",
        "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
        "- **Distance metric considerations:** How the choice of distance measure can affect KNN.  \n",
        "- **Model evaluation:** Evaluating the final model on a test set to ensure it generalizes well to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "di752ya0jq7",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this case study, you will be able to:\n",
        "\n",
        "1. **Explain why feature scaling is critical for KNN** and demonstrate its impact on classification accuracy\n",
        "2. **Apply proper data splitting strategies** (train/validation/test) to avoid data leakage and obtain unbiased performance estimates\n",
        "3. **Tune hyperparameters systematically** by evaluating K values and distance metrics on validation data\n",
        "4. **Interpret the bias-variance tradeoff** in the context of KNN's K parameter and identify signs of overfitting\n",
        "5. **Evaluate classification models comprehensively** using accuracy, balanced accuracy, F1 scores, and confusion matrices\n",
        "6. **Understand when KNN is appropriate** for real-world classification problems and recognize its limitations\n",
        "\n",
        "These skills form the foundation for applying distance-based learning algorithms to practical classification tasks."
      ],
      "metadata": {
        "id": "di752ya0jq7"
      }
    },
    {
      "cell_type": "markdown",
      "id": "ab88f59c",
      "metadata": {
        "id": "ab88f59c"
      },
      "source": [
        "## Exploring the Dataset\n",
        "Before diving into modeling, let's load the dataset and examine its features. The dataset has 178 samples, each with 13 features. The target `class` is an integer (0, 1, or 2) representing the wine cultivar.\n",
        "\n",
        "**Typical feature ranges (intuition):**  \n",
        "- Alcohol ~ 11–15  \n",
        "- Malic acid ~ 0.7–6  \n",
        "- Alcalinity of ash ~ 10–30  \n",
        "- Magnesium ~ 70–160  \n",
        "- Color intensity ~ 1–13  \n",
        "- Proline ~ 280–1700  \n",
        "\n",
        "Large differences in magnitude (e.g., *Proline* in hundreds vs *Malic acid* single digits) motivate **scaling** before using distance-based models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a85ef4",
      "metadata": {
        "id": "a2a85ef4"
      },
      "outputs": [],
      "source": [
        "# Imports and data loading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, f1_score,\n",
        "    precision_recall_fscore_support, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from collections import Counter\n",
        "\n",
        "# Load the wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create DataFrame for exploration\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['class'] = y\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2847407",
      "metadata": {
        "id": "f2847407"
      },
      "source": [
        "Let's examine the class distribution and feature statistics before splitting our data.\n",
        "\n",
        "**Data Splitting Strategy:**  \n",
        "We use a three-way split (60/20/20) to create distinct training, validation, and test sets:\n",
        "- **Training set (60%)**: Used to fit the model (learn patterns from the data)\n",
        "- **Validation set (20%)**: Used to tune hyperparameters (select best K, compare distance metrics, etc.)\n",
        "- **Test set (20%)**: Held out completely until final evaluation to assess real-world performance\n",
        "\n",
        "The split is **stratified** to ensure each subset maintains the same class proportions as the original dataset. This is critical for classification problems to avoid biased performance estimates.\n",
        "\n",
        "> **Question**: After tuning hyperparameters on the validation set, why do we evaluate the final model on a separate test set instead of reporting validation performance?\n",
        ">  \n",
        "> A) To validate that our chosen hyperparameters work well across different random seeds\n",
        ">\n",
        "> B) To get an unbiased estimate of how the model will perform on completely new data in production\n",
        ">\n",
        "> C) To ensure the model complexity matches the data complexity\n",
        ">\n",
        "> D) To verify that feature scaling was applied correctly across all splits\n",
        "\n",
        "The test set provides an honest assessment of generalization performance because it played no role in model selection or hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine class distribution\n",
        "print(\"Class distribution:\\n\", df['class'].value_counts().sort_index(), \"\\n\")\n",
        "\n",
        "# Descriptive statistics of features\n",
        "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
        "\n",
        "# Stratified split into train, validation, and test sets (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "1SfshufVixc7"
      },
      "id": "1SfshufVixc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "813c67d3",
      "metadata": {
        "id": "813c67d3"
      },
      "source": [
        "## Effect of Feature Scaling on KNN\n",
        "\n",
        "KNN uses distance to find nearest neighbors. If features are on vastly different scales, distance calculations will be dominated by the feature with the largest range. For example, in the Wine dataset, Proline ranges from 280–1700 while Malic acid ranges from 0.7–6. Without scaling, differences in Proline will completely overwhelm differences in Malic acid, causing KNN to effectively ignore the smaller-scale features.\n",
        "\n",
        "Let's demonstrate this by training a baseline KNN model with K=5 using both **unscaled** and **scaled** features, then comparing their validation performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d936dd00",
      "metadata": {
        "id": "d936dd00"
      },
      "source": [
        "We fit the scaler using only the training set to avoid data leakage, then transform both train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline without scaling\n",
        "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "raw_val_acc = accuracy_score(y_val, knn_raw.predict(X_val))\n",
        "\n",
        "# Baseline with scaling (fit on train only)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "scaled_val_acc = accuracy_score(y_val, knn_scaled.predict(X_val_scaled))\n",
        "\n",
        "print(f\"Validation accuracy without scaling: {raw_val_acc:.3f}\")\n",
        "print(f\"Validation accuracy with scaling:  {scaled_val_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "ldQ8a-dFjJqk"
      },
      "id": "ldQ8a-dFjJqk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "85ab069b",
      "metadata": {
        "id": "85ab069b"
      },
      "source": [
        "## Distance Metric Considerations\n",
        "\n",
        "The choice of distance metric is a hyperparameter that can significantly impact KNN performance, yet it's often overlooked. Common distance metrics for continuous features include:\n",
        "\n",
        "- **Euclidean (L2)**: Measures straight-line distance; computed as √(Σ(xᵢ - yᵢ)²)\n",
        "- **Manhattan (L1)**: Measures city-block distance; computed as Σ|xᵢ - yᵢ|  \n",
        "- **Minkowski**: Generalizes both L1 and L2 with parameter p (p=1 gives Manhattan, p=2 gives Euclidean)\n",
        "- **Cosine**: Measures angle between vectors; commonly used for text data and high-dimensional sparse features\n",
        "\n",
        "Different metrics can produce different neighbor sets and different predictions, especially when features have varying scales or distributions. The Wine dataset contains chemical measurements that may occasionally include extreme values due to measurement errors, unusual growing conditions, or genuinely exceptional wines.\n",
        "\n",
        "> **Question**: For datasets with continuous features that may contain occasional extreme outliers, which statement about distance metrics in KNN is most accurate?\n",
        ">  \n",
        "> A) Euclidean distance is generally more robust to outliers because squaring differences normalizes their relative impact\n",
        ">\n",
        "> B) Manhattan distance is generally more robust to outliers because it uses absolute differences rather than squared differences\n",
        ">\n",
        "> C) Both metrics are equally affected by outliers once features are properly scaled to the same range\n",
        ">\n",
        "> D) Cosine distance is always the best choice for robustness because it normalizes for vector magnitude\n",
        "\n",
        "The choice of distance metric should be validated empirically on your specific dataset using held-out validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4157e321",
      "metadata": {
        "id": "4157e321"
      },
      "source": [
        "## Choosing K: Bias–Variance Trade‑off\n",
        "\n",
        "The hyperparameter K fundamentally controls how KNN makes predictions and directly impacts model performance:\n",
        "\n",
        "- **Small K (e.g., K=1)**: Each prediction is determined by very few neighbors, creating highly flexible decision boundaries that adapt closely to individual training points\n",
        "- **Large K (e.g., K=50)**: Predictions average over many neighbors, producing smoother decision boundaries that change gradually across the feature space\n",
        "- **Optimal K**: Typically found somewhere in between, balancing the ability to capture genuine patterns while avoiding sensitivity to noise or outliers\n",
        "\n",
        "We'll systematically evaluate K values from 1 to 20, tracking both training and validation accuracy. The gap between these curves reveals how well each K value generalizes to unseen data. Large gaps suggest the model is memorizing training-specific patterns rather than learning generalizable relationships.\n",
        "\n",
        "> **Question**: After training KNN with K=1, you observe 100% training accuracy but only 88% validation accuracy (a 12 percentage point gap). Which approach is most likely to improve validation performance?\n",
        ">  \n",
        "> A) Increase K to create smoother decision boundaries and improve generalization to new data\n",
        ">\n",
        "> B) Keep K=1 but collect more training samples to reduce the performance gap\n",
        ">\n",
        "> C) Keep K=1 but apply more sophisticated feature engineering to capture better patterns  \n",
        ">\n",
        "> D) Switch to weighted KNN with K=1 where closer neighbors have more influence on predictions\n",
        "\n",
        "The train-validation gap is a key diagnostic for detecting when a model is too flexible for the available data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a985951",
      "metadata": {
        "id": "3a985951"
      },
      "outputs": [],
      "source": [
        "train_acc, val_acc = [], []\n",
        "k_sweep = range(1, 21)\n",
        "\n",
        "for k in k_sweep:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_acc.append(accuracy_score(y_train, model.predict(X_train_scaled)))\n",
        "    val_acc.append(accuracy_score(y_val, model.predict(X_val_scaled)))\n",
        "\n",
        "# Best K by validation\n",
        "best_k_idx = int(np.argmax(val_acc))\n",
        "chosen_k = best_k_idx + 1\n",
        "best_val = max(val_acc)\n",
        "max_gap = np.max(np.array(train_acc) - np.array(val_acc))\n",
        "\n",
        "# Use Euclidean distance (default and most commonly used for continuous features)\n",
        "chosen_metric = 'euclidean'\n",
        "\n",
        "print(\"Selected hyperparameters:\")\n",
        "print(f\"  K = {chosen_k}\")\n",
        "print(f\"  Distance metric = {chosen_metric}\")\n",
        "print(f\"  Validation accuracy = {best_val:.3f}\")\n",
        "print(f\"Max (train - validation) gap across K: {max_gap:.3f}\")\n",
        "\n",
        "# Plot train vs validation accuracy vs K\n",
        "plt.figure()\n",
        "plt.scatter(list(k_sweep), train_acc, label='Train Accuracy')\n",
        "plt.scatter(list(k_sweep), val_acc, label='Validation Accuracy')\n",
        "plt.axvline(chosen_k, linestyle='--', label=f'Best K={chosen_k}')\n",
        "plt.axis([0, 20, 0.8, 1.05])\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e431ac",
      "metadata": {
        "id": "90e431ac"
      },
      "source": [
        "## Model Evaluation on Test Set\n",
        "\n",
        "After selecting our hyperparameters using the validation set, we're ready for the final evaluation. At this stage we:\n",
        "\n",
        "1. **Combine training and validation sets**: This provides the model with the maximum available data for learning, since we've already locked in our hyperparameter choices\n",
        "2. **Refit the complete pipeline**: The scaler learns standardization parameters from the combined dataset, and KNN memorizes all combined training examples\n",
        "3. **Evaluate once on the test set**: This held-out data provides our unbiased estimate of real-world performance\n",
        "\n",
        "**Critical principle**: The test set has played zero role in any modeling decisions—no hyperparameter selection, no feature engineering choices, no model architecture decisions. It therefore provides an honest estimate of how the model will perform when deployed on genuinely new data from the same distribution.\n",
        "\n",
        "> **Question**: After final evaluation, your test accuracy (94.4%) is slightly lower than your best validation accuracy (97.2%). Before deployment, which interpretation and next step is most appropriate?\n",
        ">  \n",
        "> A) This small decrease is normal variation; verify that test performance meets your accuracy requirements and document the results\n",
        ">\n",
        "> B) Re-evaluate hyperparameters using the test set to identify values that achieve better performance on this data split\n",
        ">\n",
        "> C) This indicates potential data leakage between validation and test sets; recreate the splits and re-run the experiment  \n",
        ">\n",
        "> D) Average the validation and test accuracies to obtain a more stable estimate of expected production performance\n",
        "\n",
        "Remember: the test set is used exactly once for evaluation. Any optimization based on test results invalidates its role as an unbiased estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea0ff29",
      "metadata": {
        "id": "5ea0ff29"
      },
      "outputs": [],
      "source": [
        "# Combine training and validation sets for final training\n",
        "X_train_all = np.vstack([X_train, X_val])\n",
        "y_train_all = np.hstack([y_train, y_val])\n",
        "\n",
        "# Build pipeline (scaler + KNN) with chosen hyperparameters — no weights\n",
        "final_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric))\n",
        "])\n",
        "\n",
        "final_pipe.fit(X_train_all, y_train_all)\n",
        "\n",
        "# Predict on test set\n",
        "test_pred = final_pipe.predict(X_test)\n",
        "test_acc  = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(\"Test accuracy:\", round(test_acc, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9cb57ed",
      "metadata": {
        "id": "e9cb57ed"
      },
      "source": [
        "Beyond accuracy, we examine **balanced accuracy** (accounts for class imbalance) and **macro F1** (averages F1 across classes), print a **classification report**, show a **per-class table**, and plot both the raw and normalized confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report and per-class metrics\n",
        "print(\"\\nClassification report (test):\")\n",
        "print(classification_report(y_test, test_pred, digits=3, target_names=[str(c) for c in np.unique(y)]))\n",
        "\n",
        "labels = list(np.unique(y))\n",
        "prec, rec, f1, sup = precision_recall_fscore_support(y_test, test_pred, labels=labels)\n",
        "\n",
        "per_class_df = pd.DataFrame({\n",
        "    'precision': prec,\n",
        "    'recall': rec,\n",
        "    'f1': f1,\n",
        "    'support': sup\n",
        "}, index=labels)\n",
        "display(per_class_df)\n",
        "\n",
        "# Balanced accuracy & macro F1\n",
        "print(\"Balanced accuracy (test):\", round(balanced_accuracy_score(y_test, test_pred), 3))\n",
        "print(\"Macro F1 (test):         \", round(f1_score(y_test, test_pred, average='macro'), 3))\n",
        "\n",
        "# Confusion matrices: raw and normalized\n",
        "cm_raw = confusion_matrix(y_test, test_pred, labels=labels)\n",
        "cm_norm = confusion_matrix(y_test, test_pred, labels=labels, normalize='true')\n",
        "\n",
        "# Raw confusion matrix heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm_raw, cmap='Blues')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Confusion Matrix (Raw)\")\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        ax.text(j, i, cm_raw[i, j], ha='center', va='center', color='black')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Normalized confusion matrix heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm_norm, vmin=0, vmax=1, cmap='Blues')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Confusion Matrix (Normalized)\")\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha='center', va='center', color='black')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7CIOzVT8oJS2"
      },
      "id": "7CIOzVT8oJS2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "x29mmvw4m7",
      "source": [
        "## Computational Complexity: Understanding KNN's Performance Characteristics\n",
        "\n",
        "Unlike parametric models (e.g., logistic regression, decision trees) that learn a compact representation during training, KNN is a **lazy learner**—it stores all training examples and defers computation until prediction time. This has important implications for computational cost:\n",
        "\n",
        "- **Training time**: O(1) — essentially zero, just storing the data\n",
        "- **Prediction time**: O(n·d·k) where n = training samples, d = features, k = neighbors\n",
        "  - Must compute distance to all n training points\n",
        "  - Each distance calculation involves d features\n",
        "  - Must find k smallest distances (can use partial sort)\n",
        "\n",
        "Let's measure the actual time cost for training and prediction on our Wine dataset:"
      ],
      "metadata": {
        "id": "x29mmvw4m7"
      }
    },
    {
      "cell_type": "code",
      "id": "9w8fxe99jhj",
      "source": [
        "import time\n",
        "\n",
        "# Measure training time\n",
        "start = time.time()\n",
        "timing_knn = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric))\n",
        "])\n",
        "timing_knn.fit(X_train_all, y_train_all)\n",
        "train_time = time.time() - start\n",
        "\n",
        "# Measure prediction time on test set\n",
        "start = time.time()\n",
        "_ = timing_knn.predict(X_test)\n",
        "predict_time = time.time() - start\n",
        "\n",
        "# Measure single prediction time\n",
        "start = time.time()\n",
        "_ = timing_knn.predict(X_test[0:1])\n",
        "single_predict_time = time.time() - start\n",
        "\n",
        "print(f\"Training time: {train_time*1000:.2f} ms\")\n",
        "print(f\"Prediction time for {len(X_test)} samples: {predict_time*1000:.2f} ms\")\n",
        "print(f\"Average prediction time per sample: {predict_time/len(X_test)*1000:.3f} ms\")\n",
        "print(f\"Single prediction time: {single_predict_time*1000:.3f} ms\")\n",
        "print(f\"\\nDataset characteristics:\")\n",
        "print(f\"  Training samples: {len(X_train_all)}\")\n",
        "print(f\"  Features: {X_train_all.shape[1]}\")\n",
        "print(f\"  K: {chosen_k}\")\n",
        "print(f\"\\nNote: For datasets with >100K samples or low-latency requirements (<1ms),\")\n",
        "print(f\"consider approximate nearest neighbor methods (FAISS, Annoy, HNSW).\")"
      ],
      "metadata": {
        "id": "9w8fxe99jhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "wjamr5zx27r",
      "source": [
        "## Feature Importance: Which Chemical Properties Matter Most?\n",
        "\n",
        "Unlike tree-based models, KNN doesn't have built-in feature importance. However, we can use **permutation importance** to identify which features contribute most to classification accuracy. This technique randomly shuffles each feature and measures how much the model's performance degrades—important features cause larger drops in accuracy when permuted.\n",
        "\n",
        "Understanding feature importance helps:\n",
        "- **Interpret the model**: Which chemical properties distinguish wine cultivars?\n",
        "- **Feature selection**: Could we achieve similar accuracy with fewer features?\n",
        "- **Domain validation**: Do the important features align with wine chemistry knowledge?"
      ],
      "metadata": {
        "id": "wjamr5zx27r"
      }
    },
    {
      "cell_type": "code",
      "id": "0ft8qwpg8sln",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Compute permutation importance on test set\n",
        "perm_importance = permutation_importance(\n",
        "    final_pipe, X_test, y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Create DataFrame sorted by importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance_mean': perm_importance.importances_mean,\n",
        "    'importance_std': perm_importance.importances_std\n",
        "}).sort_values('importance_mean', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Permutation):\\n\")\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "indices = importance_df.index[:10]  # Top 10 features\n",
        "plt.barh(range(len(indices)), importance_df.loc[indices, 'importance_mean'],\n",
        "         xerr=importance_df.loc[indices, 'importance_std'], align='center')\n",
        "plt.yticks(range(len(indices)), importance_df.loc[indices, 'feature'])\n",
        "plt.xlabel('Decrease in Accuracy (Permutation Importance)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 10 Most Important Features for Wine Classification')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify top features\n",
        "top_features = importance_df.head(3)['feature'].tolist()\n",
        "print(f\"\\nTop 3 most important features: {', '.join(top_features)}\")\n",
        "print(f\"These chemical properties are most discriminative for identifying wine cultivars.\")"
      ],
      "metadata": {
        "id": "0ft8qwpg8sln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "tx7h9yopb3",
      "source": [
        "## Error Analysis: Understanding Misclassifications\n",
        "\n",
        "Not all predictions are equally confident. For some wines, the K nearest neighbors all agree on the class (high confidence), while for others, the neighbors are mixed between multiple classes (low confidence/high ambiguity). Analyzing where the model makes mistakes helps us:\n",
        "\n",
        "- **Identify boundary cases**: Wines that are chemically intermediate between cultivars\n",
        "- **Assess prediction confidence**: Use neighbor agreement as a proxy for uncertainty\n",
        "- **Guide data collection**: If high-ambiguity regions have many errors, collect more labeled samples there\n",
        "\n",
        "Let's analyze the test set predictions by examining neighbor homogeneity:"
      ],
      "metadata": {
        "id": "tx7h9yopb3"
      }
    },
    {
      "cell_type": "code",
      "id": "b371bumnzlt",
      "source": [
        "# Get the KNN model from pipeline and transform test data\n",
        "X_test_scaled = final_pipe.named_steps['scaler'].transform(X_test)\n",
        "knn_model = final_pipe.named_steps['knn']\n",
        "\n",
        "# Find nearest neighbors for each test point\n",
        "distances, neighbor_indices = knn_model.kneighbors(X_test_scaled)\n",
        "\n",
        "# For each test point, compute neighbor class agreement\n",
        "neighbor_homogeneity = []\n",
        "for i, neighbors in enumerate(neighbor_indices):\n",
        "    neighbor_classes = y_train_all[neighbors]\n",
        "    # Homogeneity: fraction of neighbors that match the majority class\n",
        "    majority_class = np.bincount(neighbor_classes).argmax()\n",
        "    agreement = np.sum(neighbor_classes == majority_class) / len(neighbor_classes)\n",
        "    neighbor_homogeneity.append(agreement)\n",
        "\n",
        "neighbor_homogeneity = np.array(neighbor_homogeneity)\n",
        "\n",
        "# Identify correct and incorrect predictions\n",
        "correct_mask = (test_pred == y_test)\n",
        "\n",
        "# Statistics\n",
        "print(\"Prediction Confidence Analysis:\\n\")\n",
        "print(f\"Correct predictions - Mean neighbor agreement: {neighbor_homogeneity[correct_mask].mean():.3f}\")\n",
        "print(f\"Incorrect predictions - Mean neighbor agreement: {neighbor_homogeneity[~correct_mask].mean():.3f}\")\n",
        "print(f\"\\nHigh confidence predictions (100% neighbor agreement): {np.sum(neighbor_homogeneity == 1.0)} / {len(y_test)}\")\n",
        "print(f\"Low confidence predictions (<80% neighbor agreement): {np.sum(neighbor_homogeneity < 0.8)} / {len(y_test)}\")\n",
        "\n",
        "# Visualize relationship between confidence and correctness\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot 1: Histogram of neighbor homogeneity by correctness\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(neighbor_homogeneity[correct_mask], bins=10, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
        "plt.hist(neighbor_homogeneity[~correct_mask], bins=10, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
        "plt.xlabel('Neighbor Homogeneity (Agreement)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Prediction Confidence vs Correctness')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Misclassifications by actual class\n",
        "plt.subplot(1, 2, 2)\n",
        "misclassified_classes = y_test[~correct_mask]\n",
        "if len(misclassified_classes) > 0:\n",
        "    class_counts = np.bincount(misclassified_classes, minlength=len(np.unique(y)))\n",
        "    plt.bar(range(len(class_counts)), class_counts, edgecolor='black', color='salmon')\n",
        "    plt.xlabel('True Class')\n",
        "    plt.ylabel('Number of Misclassifications')\n",
        "    plt.title('Errors by Wine Cultivar')\n",
        "    plt.xticks(range(len(class_counts)))\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No misclassifications!', ha='center', va='center', fontsize=14)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show details of ambiguous cases\n",
        "if np.sum(~correct_mask) > 0:\n",
        "    print(f\"\\nMisclassified samples: {np.sum(~correct_mask)}\")\n",
        "    print(\"Most ambiguous misclassifications (lowest neighbor agreement):\")\n",
        "    misclassified_indices = np.where(~correct_mask)[0]\n",
        "    ambiguous_errors = misclassified_indices[np.argsort(neighbor_homogeneity[~correct_mask])[:min(3, len(misclassified_indices))]]\n",
        "    for idx in ambiguous_errors:\n",
        "        neighbors = neighbor_indices[idx]\n",
        "        neighbor_classes = y_train_all[neighbors]\n",
        "        print(f\"  Test sample {idx}: True={y_test[idx]}, Predicted={test_pred[idx]}, \"\n",
        "              f\"Neighbor agreement={neighbor_homogeneity[idx]:.2f}, \"\n",
        "              f\"Neighbor classes={neighbor_classes}\")\n",
        "else:\n",
        "    print(\"\\nPerfect classification - no errors to analyze!\")"
      ],
      "metadata": {
        "id": "b371bumnzlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9feef4b9",
      "metadata": {
        "id": "9feef4b9"
      },
      "source": [
        "## Limitations (Current Scope) & What’s Next\n",
        "This notebook uses a **single hold‑out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k‑fold cross‑validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute‑force neighbor search (`algorithm='brute'`) and didn’t explore scalability techniques like KD‑trees, Ball Trees, or approximate nearest neighbor libraries (e.g. FAISS, HNSW). These become important when your archive grows to millions of rows or requires low‑latency predictions. Finally, we didn’t address class imbalance or cost‑sensitive KNN; these are natural extensions for more advanced courses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d61xxnu8",
      "source": [
        "## Common Pitfalls and Best Practices\n",
        "\n",
        "### Critical Mistakes to Avoid:\n",
        "\n",
        "1. **Forgetting to scale features** ❌  \n",
        "   KNN is distance-based; features on different scales will dominate distance calculations. Always use StandardScaler or similar normalization.\n",
        "\n",
        "2. **Using the test set for hyperparameter tuning** ❌  \n",
        "   This creates data leakage and inflates performance estimates. Use a separate validation set or cross-validation for all tuning decisions.\n",
        "\n",
        "3. **Choosing K=1 for production systems** ❌  \n",
        "   While K=1 may give perfect training accuracy, it's highly sensitive to noise and outliers. Always validate with K > 1 on held-out data.\n",
        "\n",
        "4. **Ignoring computational cost** ❌  \n",
        "   KNN requires computing distances to all training points at prediction time. For large datasets (>100K samples), consider approximate nearest neighbor methods or alternative algorithms.\n",
        "\n",
        "5. **Treating class imbalance casually** ❌  \n",
        "   If one class has 90% of samples, KNN will naturally favor that class. Consider using balanced class weights, stratified sampling, or appropriate evaluation metrics (balanced accuracy, F1).\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "✅ **Always scale features** before applying KNN to continuous data  \n",
        "✅ **Use stratified splits** to maintain class proportions across train/val/test sets  \n",
        "✅ **Validate hyperparameters** (K, distance metric) on separate validation data  \n",
        "✅ **Consider dimensionality**: KNN performance degrades in very high dimensions (curse of dimensionality); consider dimensionality reduction (PCA, feature selection) for >50 features  \n",
        "✅ **Monitor the train-validation gap** to detect overfitting early  \n",
        "✅ **Use domain knowledge**: For some applications (text, images), specialized distance metrics (cosine, Hamming) may work better than Euclidean"
      ],
      "metadata": {
        "id": "f4d61xxnu8"
      }
    },
    {
      "cell_type": "markdown",
      "id": "10c90ca2",
      "metadata": {
        "id": "10c90ca2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this case study, we've worked through a complete KNN classification workflow on the Wine dataset, covering the essential concepts and practical techniques:\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Feature scaling is non-negotiable** for distance-based algorithms. Without it, KNN effectively ignores smaller-scale features, leading to poor performance.\n",
        "\n",
        "2. **Hyperparameter tuning requires systematic validation**. We used a dedicated validation set to select K and the distance metric, ensuring our choices generalize beyond the training data.\n",
        "\n",
        "3. **The bias-variance tradeoff is visible in the train-validation gap**. Very small K (high flexibility) leads to overfitting; very large K (high rigidity) leads to underfitting.\n",
        "\n",
        "4. **KNN is computationally expensive at prediction time**. Unlike parametric models that compress knowledge into parameters, KNN must compare against all training examples for each prediction.\n",
        "\n",
        "5. **Error analysis reveals model confidence**. Neighbor homogeneity provides a natural measure of prediction uncertainty, helping identify boundary cases where the model is less certain.\n",
        "\n",
        "### When to Use KNN:\n",
        "\n",
        "✅ **Good fit:**\n",
        "- Small to medium datasets (<100K samples)\n",
        "- Problems where local similarity is meaningful\n",
        "- Situations requiring interpretable, example-based reasoning\n",
        "- Establishing baselines before trying complex models\n",
        "\n",
        "❌ **Poor fit:**\n",
        "- Large datasets requiring low-latency predictions\n",
        "- High-dimensional data (>50 features) without dimensionality reduction\n",
        "- Problems where global patterns matter more than local similarity\n",
        "- Datasets with severe class imbalance (without special handling)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Try cross-validation** instead of a single validation split for more robust hyperparameter selection\n",
        "- **Experiment with distance metrics** (Manhattan, Minkowski with different p values) tailored to your data\n",
        "- **Explore weighted KNN** where closer neighbors have more influence (use `weights='distance'`)\n",
        "- **Consider dimensionality reduction** (PCA, feature selection) if working with high-dimensional data\n",
        "- **Compare against other classifiers** (Logistic Regression, Random Forest, SVM) to see if KNN's simplicity is sufficient\n",
        "\n",
        "KNN remains one of the most intuitive machine learning algorithms—its \"similar inputs produce similar outputs\" principle mirrors how humans naturally reason about new situations by comparing to past experiences."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}