{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbhours%20Classification/KNN%20Classification%20with%20Wine%20Dataset%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b78c4",
   "metadata": {
    "id": "519b78c4"
   },
   "source": [
    "# Case Study: KNN Classification with Wine Dataset (UCI)\n",
    "\n",
    "K\u2011Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that classifies new examples based on similarity to known examples. In this case study, we\u2019ll step through a practical example using the **Wine recognition** dataset (from the UCI Machine Learning Repository) to illustrate key concepts and best practices of KNN classification. This dataset contains chemical analysis results for wines from three cultivars (classes), with 13 continuous features (e.g. alcohol content, acidity, magnesium, phenols, color intensity, etc.). We simulate the scenario of predicting a wine\u2019s cultivar from its chemical properties, akin to a chemist identifying origin by lab measurements.\n",
    "\n",
    "## What we\u2019ll cover\n",
    "- **Data exploration and preparation:** Understanding feature scales and splitting data into training, validation, and test sets.  \n",
    "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance.  \n",
    "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
    "- **Distance metric considerations:** How the choice of distance measure can affect KNN.  \n",
    "- **Model evaluation:** Evaluating the final model on a test set to ensure it generalizes well to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "di752ya0jq7",
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this case study, you will be able to:\n",
    "\n",
    "1. **Explain why feature scaling is critical for KNN** and demonstrate its impact on classification accuracy\n",
    "2. **Apply proper data splitting strategies** (train/validation/test) to avoid data leakage and obtain unbiased performance estimates\n",
    "3. **Tune hyperparameters systematically** by evaluating K values and distance metrics on validation data\n",
    "4. **Interpret the bias-variance tradeoff** in the context of KNN's K parameter and identify signs of overfitting\n",
    "5. **Evaluate classification models comprehensively** using accuracy, balanced accuracy, F1 scores, and confusion matrices\n",
    "6. **Understand when KNN is appropriate** for real-world classification problems and recognize its limitations\n",
    "\n",
    "These skills form the foundation for applying distance-based learning algorithms to practical classification tasks."
   ],
   "metadata": {
    "id": "di752ya0jq7"
   }
  },
  {
   "cell_type": "markdown",
   "id": "ab88f59c",
   "metadata": {
    "id": "ab88f59c"
   },
   "source": [
    "## Exploring the Dataset\n",
    "Before diving into modeling, let's load the dataset and examine its features. The dataset has 178 samples, each with 13 features. The target `class` is an integer (0, 1, or 2) representing the wine cultivar.\n",
    "\n",
    "**Typical feature ranges (intuition):**  \n",
    "- Alcohol ~ 11\u201315  \n",
    "- Malic acid ~ 0.7\u20136  \n",
    "- Alcalinity of ash ~ 10\u201330  \n",
    "- Magnesium ~ 70\u2013160  \n",
    "- Color intensity ~ 1\u201313  \n",
    "- Proline ~ 280\u20131700  \n",
    "\n",
    "Large differences in magnitude (e.g., *Proline* in hundreds vs *Malic acid* single digits) motivate **scaling** before using distance-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a85ef4",
   "metadata": {
    "id": "a2a85ef4"
   },
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['class'] = y\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2847407",
   "metadata": {
    "id": "f2847407"
   },
   "source": [
    "Let's examine the class distribution and feature statistics before splitting our data.\n",
    "\n",
    "**Data Splitting Strategy:**  \n",
    "We use a three-way split (60/20/20) to create distinct training, validation, and test sets:\n",
    "- **Training set (60%)**: Used to fit the model (learn patterns from the data)\n",
    "- **Validation set (20%)**: Used to tune hyperparameters (select best K, compare distance metrics, etc.)\n",
    "- **Test set (20%)**: Held out completely until final evaluation to assess real-world performance\n",
    "\n",
    "The split is **stratified** to ensure each subset maintains the same class proportions as the original dataset. This is critical for classification problems to avoid biased performance estimates.\n",
    "\n",
    "> **Question**: After tuning hyperparameters on the validation set, why do we evaluate the final model on a separate test set instead of reporting validation performance?\n",
    ">  \n",
    "> A) To validate that our chosen hyperparameters work well across different random seeds\n",
    ">\n",
    "> B) To get an unbiased estimate of how the model will perform on completely new data in production\n",
    ">\n",
    "> C) To ensure the model complexity matches the data complexity\n",
    ">\n",
    "> D) To verify that feature scaling was applied correctly across all splits\n",
    "\n",
    "The test set provides an honest assessment of generalization performance because it played no role in model selection or hyperparameter tuning.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**B is TRUE**: This is the fundamental purpose of a test set in machine learning.\n",
    "- During hyperparameter tuning, we evaluate many different K values on the validation set\n",
    "- We select the K that performs best on validation data (e.g., highest accuracy)\n",
    "- This selection process means our final model was **chosen specifically because it performed well on the validation set**\n",
    "- Validation performance is therefore **optimistically biased** - it represents the best case among all values we tried\n",
    "- The test set, which played no role in any decisions, provides an **unbiased estimate** of real-world performance\n",
    "\n",
    "**A is FALSE**: While cross-validation with different random seeds is good practice, that's not why we have a separate test set\n",
    "- The test set exists specifically to avoid the optimistic bias from hyperparameter selection\n",
    "- Cross-validation on different seeds would still be done on training/validation data, not the test set\n",
    "\n",
    "**C is FALSE**: The test set doesn't verify model complexity matching data complexity\n",
    "- Model complexity in KNN is controlled by K (and we already selected that using validation data)\n",
    "- The test set simply estimates generalization performance\n",
    "\n",
    "**D is FALSE**: Feature scaling verification happens during the validation phase, not testing\n",
    "- We apply the same scaling pipeline to all splits (train, validation, test)\n",
    "- The test set exists to measure generalization, not verify preprocessing\n",
    "\n",
    "**Key Insight**: The test set is your \"truth detector\" - it reveals whether your model selection process (which used validation data) produced a genuinely good model or just one that happened to work well on that particular validation split.\n",
    "\n",
    "**Real-World Analogy**: If you're studying for an exam by taking practice tests, your practice test scores will be optimistically biased because you're using them to guide your study strategy. The actual exam (test set) provides an unbiased measure of your true knowledge.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Examine class distribution\n",
    "print(\"Class distribution:\\n\", df['class'].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics of features\n",
    "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Stratified split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
   ],
   "metadata": {
    "id": "1SfshufVixc7"
   },
   "id": "1SfshufVixc7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "813c67d3",
   "metadata": {
    "id": "813c67d3"
   },
   "source": [
    "## Effect of Feature Scaling on KNN\n",
    "\n",
    "KNN uses distance to find nearest neighbors. If features are on vastly different scales, distance calculations will be dominated by the feature with the largest range. For example, in the Wine dataset, Proline ranges from 280\u20131700 while Malic acid ranges from 0.7\u20136. Without scaling, differences in Proline will completely overwhelm differences in Malic acid, causing KNN to effectively ignore the smaller-scale features.\n",
    "\n",
    "Let's demonstrate this by training a baseline KNN model with K=5 using both **unscaled** and **scaled** features, then comparing their validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936dd00",
   "metadata": {
    "id": "d936dd00"
   },
   "source": [
    "We fit the scaler using only the training set to avoid data leakage, then transform both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Baseline without scaling\n",
    "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "raw_val_acc = accuracy_score(y_val, knn_raw.predict(X_val))\n",
    "\n",
    "# Baseline with scaling (fit on train only)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_val_acc = accuracy_score(y_val, knn_scaled.predict(X_val_scaled))\n",
    "\n",
    "print(f\"Validation accuracy without scaling: {raw_val_acc:.3f}\")\n",
    "print(f\"Validation accuracy with scaling:  {scaled_val_acc:.3f}\")\n"
   ],
   "metadata": {
    "id": "ldQ8a-dFjJqk"
   },
   "id": "ldQ8a-dFjJqk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85ab069b",
   "metadata": {
    "id": "85ab069b"
   },
   "source": [
    "## Distance Metric Considerations\n",
    "\n",
    "The choice of distance metric is a hyperparameter that can significantly impact KNN performance, yet it's often overlooked. Common distance metrics for continuous features include:\n",
    "\n",
    "- **Euclidean (L2)**: Measures straight-line distance; computed as \u221a(\u03a3(x\u1d62 - y\u1d62)\u00b2)\n",
    "- **Manhattan (L1)**: Measures city-block distance; computed as \u03a3|x\u1d62 - y\u1d62|  \n",
    "- **Minkowski**: Generalizes both L1 and L2 with parameter p (p=1 gives Manhattan, p=2 gives Euclidean)\n",
    "- **Cosine**: Measures angle between vectors; commonly used for text data and high-dimensional sparse features\n",
    "\n",
    "Different metrics can produce different neighbor sets and different predictions, especially when features have varying scales or distributions. The Wine dataset contains chemical measurements that may occasionally include extreme values due to measurement errors, unusual growing conditions, or genuinely exceptional wines.\n",
    "\n",
    "> **Question**: For datasets with continuous features that may contain occasional extreme outliers, which statement about distance metrics in KNN is most accurate?\n",
    ">  \n",
    "> A) Euclidean distance is generally more robust to outliers because squaring differences normalizes their relative impact\n",
    ">\n",
    "> B) Manhattan distance is generally more robust to outliers because it uses absolute differences rather than squared differences\n",
    ">\n",
    "> C) Both metrics are equally affected by outliers once features are properly scaled to the same range\n",
    ">\n",
    "> D) Cosine distance is always the best choice for robustness because it normalizes for vector magnitude\n",
    "\n",
    "The choice of distance metric should be validated empirically on your specific dataset using held-out validation data.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**B is TRUE**: Manhattan distance is more robust to outliers than Euclidean distance.\n",
    "- **Euclidean distance** squares the differences: distance = \u221a(\u03a3(x\u1d62 - y\u1d62)\u00b2)\n",
    "  - Squaring amplifies the impact of outliers dramatically\n",
    "  - Example: If one feature has a difference of 10, squaring makes it 100\n",
    "  - A single outlier can dominate the entire distance calculation\n",
    "- **Manhattan distance** uses absolute values: distance = \u03a3|x\u1d62 - y\u1d62|\n",
    "  - Differences grow linearly, not quadratically\n",
    "  - Example: A difference of 10 contributes 10, not 100\n",
    "  - Outliers have proportionally less influence\n",
    "\n",
    "**Concrete Example:**\n",
    "```\n",
    "Point A: [1, 1, 1]\n",
    "Point B: [2, 2, 2]  \n",
    "Point C: [1, 1, 100]  # Outlier in 3rd feature\n",
    "\n",
    "Euclidean distance from A to B: \u221a((1)\u00b2 + (1)\u00b2 + (1)\u00b2) = \u221a3 \u2248 1.73\n",
    "Euclidean distance from A to C: \u221a((0)\u00b2 + (0)\u00b2 + (99)\u00b2) = 99.0\n",
    "\n",
    "Manhattan distance from A to B: |1| + |1| + |1| = 3\n",
    "Manhattan distance from A to C: |0| + |0| + |99| = 99\n",
    "\n",
    "The outlier dominates in both, but less extremely with Manhattan.\n",
    "More importantly, in squared space (Euclidean), the outlier gets 99\u00b2 = 9,801 weight!\n",
    "```\n",
    "\n",
    "**A is FALSE**: Squaring does NOT normalize - it **amplifies** outliers\n",
    "- Squaring makes large values exponentially larger\n",
    "- This is the opposite of robustness\n",
    "\n",
    "**C is FALSE**: Scaling to the same range doesn't eliminate the difference in outlier sensitivity\n",
    "- Even with scaled features (e.g., all in [0, 1]), Euclidean still squares differences\n",
    "- Manhattan's linear behavior remains more robust\n",
    "\n",
    "**D is FALSE**: Cosine distance is NOT always best for robustness\n",
    "- Cosine measures angular similarity, not distance\n",
    "- It's useful for high-dimensional sparse data (like text), not necessarily for outlier robustness\n",
    "- For continuous chemical measurements, Manhattan or Euclidean are more appropriate\n",
    "\n",
    "**Key Insight**: The mathematical operation matters. Squaring (L2) amplifies errors; absolute values (L1) treat them linearly. This is why robust regression often uses L1 loss instead of L2 (mean squared error).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157e321",
   "metadata": {
    "id": "4157e321"
   },
   "source": [
    "## Choosing K: Bias\u2013Variance Trade\u2011off\n",
    "\n",
    "The hyperparameter K fundamentally controls how KNN makes predictions and directly impacts model performance:\n",
    "\n",
    "- **Small K (e.g., K=1)**: Each prediction is determined by very few neighbors, creating highly flexible decision boundaries that adapt closely to individual training points\n",
    "- **Large K (e.g., K=50)**: Predictions average over many neighbors, producing smoother decision boundaries that change gradually across the feature space\n",
    "- **Optimal K**: Typically found somewhere in between, balancing the ability to capture genuine patterns while avoiding sensitivity to noise or outliers\n",
    "\n",
    "We'll systematically evaluate K values from 1 to 20, tracking both training and validation accuracy. The gap between these curves reveals how well each K value generalizes to unseen data. Large gaps suggest the model is memorizing training-specific patterns rather than learning generalizable relationships.\n",
    "\n",
    "> **Question**: After training KNN with K=1, you observe 100% training accuracy but only 88% validation accuracy (a 12 percentage point gap). Which approach is most likely to improve validation performance?\n",
    ">  \n",
    "> A) Increase K to create smoother decision boundaries and improve generalization to new data\n",
    ">\n",
    "> B) Keep K=1 but collect more training samples to reduce the performance gap\n",
    ">\n",
    "> C) Keep K=1 but apply more sophisticated feature engineering to capture better patterns  \n",
    ">\n",
    "> D) Switch to weighted KNN with K=1 where closer neighbors have more influence on predictions\n",
    "\n",
    "The train-validation gap is a key diagnostic for detecting when a model is too flexible for the available data.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: A**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**A is TRUE**: Increasing K is the direct solution to overfitting in KNN.\n",
    "- **The problem**: K=1 has 100% training accuracy but only 88% validation accuracy\n",
    "  - This 12-point gap indicates severe overfitting\n",
    "  - The model memorizes training data instead of learning general patterns\n",
    "  - Each training point becomes its own \"class island,\" perfect for training but terrible for generalization\n",
    "- **The solution**: Increase K (e.g., try K=5, K=10, K=15)\n",
    "  - Larger K averages over more neighbors, smoothing decision boundaries\n",
    "  - Reduces sensitivity to individual noisy training points\n",
    "  - Trades some training accuracy for better validation accuracy (which is what we want!)\n",
    "\n",
    "**Expected outcome**: As K increases, you'll see:\n",
    "- Training accuracy decrease (e.g., from 100% to 95%)\n",
    "- Validation accuracy increase (e.g., from 88% to 95%)\n",
    "- The gap shrinks, indicating better generalization\n",
    "\n",
    "**B is FALSE**: More training samples help, but don't fix the fundamental problem\n",
    "- K=1 will still memorize the training data, even with more samples\n",
    "- You'd just memorize more examples without generalizing better\n",
    "- This is treating the symptom, not the cause\n",
    "\n",
    "**C is FALSE**: Feature engineering doesn't address overfitting from K=1\n",
    "- The problem isn't lack of signal; it's the model's excessive flexibility\n",
    "- K=1 can already perfectly fit the training data - more features won't help validation performance\n",
    "- In fact, adding more features with K=1 could make overfitting worse (curse of dimensionality)\n",
    "\n",
    "**D is FALSE**: Weighted KNN with K=1 doesn't solve overfitting\n",
    "- Weighted KNN gives closer neighbors more influence\n",
    "- With K=1, there's only one neighbor, so weighting is meaningless\n",
    "- This doesn't address the core issue of decision boundaries that are too complex\n",
    "\n",
    "**Key Insight**: The train-validation gap is your overfitting detector. When you see a large gap with small K, the solution is to increase K to regularize the model (make it less flexible). This is the bias-variance tradeoff in action.\n",
    "\n",
    "**Analogy**: If you memorize answers to practice problems (K=1) instead of understanding concepts (larger K), you'll ace the practice test but fail the real exam.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a985951",
   "metadata": {
    "id": "3a985951"
   },
   "outputs": [],
   "source": [
    "train_acc, val_acc = [], []\n",
    "k_sweep = range(1, 21)\n",
    "\n",
    "for k in k_sweep:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_acc.append(accuracy_score(y_train, model.predict(X_train_scaled)))\n",
    "    val_acc.append(accuracy_score(y_val, model.predict(X_val_scaled)))\n",
    "\n",
    "# Best K by validation\n",
    "best_k_idx = int(np.argmax(val_acc))\n",
    "chosen_k = best_k_idx + 1\n",
    "best_val = max(val_acc)\n",
    "max_gap = np.max(np.array(train_acc) - np.array(val_acc))\n",
    "\n",
    "# Use Euclidean distance (default and most commonly used for continuous features)\n",
    "chosen_metric = 'euclidean'\n",
    "\n",
    "print(\"Selected hyperparameters:\")\n",
    "print(f\"  K = {chosen_k}\")\n",
    "print(f\"  Distance metric = {chosen_metric}\")\n",
    "print(f\"  Validation accuracy = {best_val:.3f}\")\n",
    "print(f\"Max (train - validation) gap across K: {max_gap:.3f}\")\n",
    "\n",
    "# Plot train vs validation accuracy vs K\n",
    "plt.figure()\n",
    "plt.scatter(list(k_sweep), train_acc, label='Train Accuracy')\n",
    "plt.scatter(list(k_sweep), val_acc, label='Validation Accuracy')\n",
    "plt.axvline(chosen_k, linestyle='--', label=f'Best K={chosen_k}')\n",
    "plt.axis([0, 20, 0.8, 1.05])\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e431ac",
   "metadata": {
    "id": "90e431ac"
   },
   "source": [
    "## Model Evaluation on Test Set\n",
    "\n",
    "After selecting our hyperparameters using the validation set, we're ready for the final evaluation. At this stage we:\n",
    "\n",
    "1. **Combine training and validation sets**: This provides the model with the maximum available data for learning, since we've already locked in our hyperparameter choices\n",
    "2. **Refit the complete pipeline**: The scaler learns standardization parameters from the combined dataset, and KNN memorizes all combined training examples\n",
    "3. **Evaluate once on the test set**: This held-out data provides our unbiased estimate of real-world performance\n",
    "\n",
    "**Critical principle**: The test set has played zero role in any modeling decisions\u2014no hyperparameter selection, no feature engineering choices, no model architecture decisions. It therefore provides an honest estimate of how the model will perform when deployed on genuinely new data from the same distribution.\n",
    "\n",
    "> **Question**: After final evaluation, your test accuracy (94.4%) is slightly lower than your best validation accuracy (97.2%). Before deployment, which interpretation and next step is most appropriate?\n",
    ">  \n",
    "> A) This small decrease is normal variation; verify that test performance meets your accuracy requirements and document the results\n",
    ">\n",
    "> B) Re-evaluate hyperparameters using the test set to identify values that achieve better performance on this data split\n",
    ">\n",
    "> C) This indicates potential data leakage between validation and test sets; recreate the splits and re-run the experiment  \n",
    ">\n",
    "> D) Average the validation and test accuracies to obtain a more stable estimate of expected production performance\n",
    "\n",
    "Remember: the test set is used exactly once for evaluation. Any optimization based on test results invalidates its role as an unbiased estimator.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: A**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**A is TRUE**: This small decrease is expected and normal; focus on whether it meets requirements.\n",
    "- **Why the decrease is normal**:\n",
    "  - Validation accuracy (97.2%) was from a specific 20% subset during hyperparameter tuning\n",
    "  - Test accuracy (94.4%) is from a different 20% subset\n",
    "  - Random variation in data splits naturally causes performance differences of ~1-3 percentage points\n",
    "  - The model was selected because it performed well on validation data specifically\n",
    "- **What to do next**:\n",
    "  - Check if 94.4% meets your business/application requirements\n",
    "  - Document both validation and test results\n",
    "  - Consider the test accuracy (94.4%) as your expected production performance\n",
    "  - Deploy if requirements are met\n",
    "\n",
    "**B is FALSE**: Re-evaluating hyperparameters using the test set defeats its purpose\n",
    "- **This invalidates the test set**: Once you optimize on test data, it's no longer \"held-out\"\n",
    "- **Creates data leakage**: You've now leaked test information into model selection\n",
    "- **Leads to optimistic bias**: Future performance will likely be worse than test results\n",
    "- **Breaks the ML workflow**: Test set must be used exactly once, for final evaluation only\n",
    "\n",
    "**C is FALSE**: This performance difference does NOT indicate data leakage\n",
    "- **Expected pattern**: Test performance slightly different from validation is normal\n",
    "- **Data leakage would show**: Suspiciously high performance on both validation and test (e.g., both >99%)\n",
    "- **2.8-point difference** is well within normal variation range\n",
    "- **Actual leakage signs**: Would be if test > validation, or both are unrealistically high\n",
    "\n",
    "**D is FALSE**: Averaging validation and test accuracies is statistically invalid\n",
    "- **Test set is the true estimate**: It's your unbiased measure of production performance\n",
    "- **Validation was used for selection**: It's optimistically biased\n",
    "- **Mixing them** combines biased and unbiased estimates incorrectly\n",
    "- **Report test accuracy**: 94.4% is your expected production performance\n",
    "\n",
    "**Key Insight**: The test set is your reality check. Small differences from validation are normal. Large unexpected gains or losses require investigation. Never re-optimize using test set results.\n",
    "\n",
    "**Real-World Practice**:\n",
    "```\n",
    "Acceptable patterns:\n",
    "- Validation: 97.2%, Test: 94.4% \u2713 (small expected decrease)\n",
    "- Validation: 95.0%, Test: 94.5% \u2713 (very close)\n",
    "\n",
    "Concerning patterns:  \n",
    "- Validation: 80%, Test: 95% \u26a0 (suggests data leakage or distribution shift)\n",
    "- Validation: 95%, Test: 70% \u26a0 (suggests severe overfitting to validation set)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0ff29",
   "metadata": {
    "id": "5ea0ff29"
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final training\n",
    "X_train_all = np.vstack([X_train, X_val])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "# Build pipeline (scaler + KNN) with chosen hyperparameters \u2014 no weights\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_pipe.predict(X_test)\n",
    "test_acc  = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(\"Test accuracy:\", round(test_acc, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb57ed",
   "metadata": {
    "id": "e9cb57ed"
   },
   "source": [
    "Beyond accuracy, we examine **balanced accuracy** (accounts for class imbalance) and **macro F1** (averages F1 across classes), print a **classification report**, show a **per-class table**, and plot both the raw and normalized confusion matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Print classification report and per-class metrics\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test, test_pred, digits=3, target_names=[str(c) for c in np.unique(y)]))\n",
    "\n",
    "labels = list(np.unique(y))\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(y_test, test_pred, labels=labels)\n",
    "\n",
    "per_class_df = pd.DataFrame({\n",
    "    'precision': prec,\n",
    "    'recall': rec,\n",
    "    'f1': f1,\n",
    "    'support': sup\n",
    "}, index=labels)\n",
    "display(per_class_df)\n",
    "\n",
    "# Balanced accuracy & macro F1\n",
    "print(\"Balanced accuracy (test):\", round(balanced_accuracy_score(y_test, test_pred), 3))\n",
    "print(\"Macro F1 (test):         \", round(f1_score(y_test, test_pred, average='macro'), 3))\n",
    "\n",
    "# Confusion matrices: raw and normalized\n",
    "cm_raw = confusion_matrix(y_test, test_pred, labels=labels)\n",
    "cm_norm = confusion_matrix(y_test, test_pred, labels=labels, normalize='true')\n",
    "\n",
    "# Raw confusion matrix heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_raw, cmap='Blues')\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion Matrix (Raw)\")\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        ax.text(j, i, cm_raw[i, j], ha='center', va='center', color='black')\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm, vmin=0, vmax=1, cmap='Blues')\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion Matrix (Normalized)\")\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "7CIOzVT8oJS2"
   },
   "id": "7CIOzVT8oJS2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x29mmvw4m7",
   "source": [
    "## Computational Complexity: Understanding KNN's Performance Characteristics\n",
    "\n",
    "Unlike parametric models (e.g., logistic regression, decision trees) that learn a compact representation during training, KNN is a **lazy learner**\u2014it stores all training examples and defers computation until prediction time. This has important implications for computational cost:\n",
    "\n",
    "- **Training time**: O(1) \u2014 essentially zero, just storing the data\n",
    "- **Prediction time**: O(n\u00b7d\u00b7k) where n = training samples, d = features, k = neighbors\n",
    "  - Must compute distance to all n training points\n",
    "  - Each distance calculation involves d features\n",
    "  - Must find k smallest distances (can use partial sort)\n",
    "\n",
    "Let's measure the actual time cost for training and prediction on our Wine dataset:"
   ],
   "metadata": {
    "id": "x29mmvw4m7"
   }
  },
  {
   "cell_type": "code",
   "id": "9w8fxe99jhj",
   "source": [
    "import time\n",
    "\n",
    "# Measure training time\n",
    "start = time.time()\n",
    "timing_knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "timing_knn.fit(X_train_all, y_train_all)\n",
    "train_time = time.time() - start\n",
    "\n",
    "# Measure prediction time on test set\n",
    "start = time.time()\n",
    "_ = timing_knn.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "# Measure single prediction time\n",
    "start = time.time()\n",
    "_ = timing_knn.predict(X_test[0:1])\n",
    "single_predict_time = time.time() - start\n",
    "\n",
    "print(f\"Training time: {train_time*1000:.2f} ms\")\n",
    "print(f\"Prediction time for {len(X_test)} samples: {predict_time*1000:.2f} ms\")\n",
    "print(f\"Average prediction time per sample: {predict_time/len(X_test)*1000:.3f} ms\")\n",
    "print(f\"Single prediction time: {single_predict_time*1000:.3f} ms\")\n",
    "print(f\"\\nDataset characteristics:\")\n",
    "print(f\"  Training samples: {len(X_train_all)}\")\n",
    "print(f\"  Features: {X_train_all.shape[1]}\")\n",
    "print(f\"  K: {chosen_k}\")\n",
    "print(f\"\\nNote: For datasets with >100K samples or low-latency requirements (<1ms),\")\n",
    "print(f\"consider approximate nearest neighbor methods (FAISS, Annoy, HNSW).\")"
   ],
   "metadata": {
    "id": "9w8fxe99jhj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wjamr5zx27r",
   "source": [
    "## Feature Importance: Which Chemical Properties Matter Most?\n",
    "\n",
    "Unlike tree-based models, KNN doesn't have built-in feature importance. However, we can use **permutation importance** to identify which features contribute most to classification accuracy. This technique randomly shuffles each feature and measures how much the model's performance degrades\u2014important features cause larger drops in accuracy when permuted.\n",
    "\n",
    "Understanding feature importance helps:\n",
    "- **Interpret the model**: Which chemical properties distinguish wine cultivars?\n",
    "- **Feature selection**: Could we achieve similar accuracy with fewer features?\n",
    "- **Domain validation**: Do the important features align with wine chemistry knowledge?"
   ],
   "metadata": {
    "id": "wjamr5zx27r"
   }
  },
  {
   "cell_type": "code",
   "id": "0ft8qwpg8sln",
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    final_pipe, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Create DataFrame sorted by importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Permutation):\\n\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "indices = importance_df.index[:10]  # Top 10 features\n",
    "plt.barh(range(len(indices)), importance_df.loc[indices, 'importance_mean'],\n",
    "         xerr=importance_df.loc[indices, 'importance_std'], align='center')\n",
    "plt.yticks(range(len(indices)), importance_df.loc[indices, 'feature'])\n",
    "plt.xlabel('Decrease in Accuracy (Permutation Importance)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Most Important Features for Wine Classification')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify top features\n",
    "top_features = importance_df.head(3)['feature'].tolist()\n",
    "print(f\"\\nTop 3 most important features: {', '.join(top_features)}\")\n",
    "print(f\"These chemical properties are most discriminative for identifying wine cultivars.\")"
   ],
   "metadata": {
    "id": "0ft8qwpg8sln"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tx7h9yopb3",
   "source": [
    "## Error Analysis: Understanding Misclassifications\n",
    "\n",
    "Not all predictions are equally confident. For some wines, the K nearest neighbors all agree on the class (high confidence), while for others, the neighbors are mixed between multiple classes (low confidence/high ambiguity). Analyzing where the model makes mistakes helps us:\n",
    "\n",
    "- **Identify boundary cases**: Wines that are chemically intermediate between cultivars\n",
    "- **Assess prediction confidence**: Use neighbor agreement as a proxy for uncertainty\n",
    "- **Guide data collection**: If high-ambiguity regions have many errors, collect more labeled samples there\n",
    "\n",
    "Let's analyze the test set predictions by examining neighbor homogeneity:"
   ],
   "metadata": {
    "id": "tx7h9yopb3"
   }
  },
  {
   "cell_type": "code",
   "id": "b371bumnzlt",
   "source": [
    "# Get the KNN model from pipeline and transform test data\n",
    "X_test_scaled = final_pipe.named_steps['scaler'].transform(X_test)\n",
    "knn_model = final_pipe.named_steps['knn']\n",
    "\n",
    "# Find nearest neighbors for each test point\n",
    "distances, neighbor_indices = knn_model.kneighbors(X_test_scaled)\n",
    "\n",
    "# For each test point, compute neighbor class agreement\n",
    "neighbor_homogeneity = []\n",
    "for i, neighbors in enumerate(neighbor_indices):\n",
    "    neighbor_classes = y_train_all[neighbors]\n",
    "    # Homogeneity: fraction of neighbors that match the majority class\n",
    "    majority_class = np.bincount(neighbor_classes).argmax()\n",
    "    agreement = np.sum(neighbor_classes == majority_class) / len(neighbor_classes)\n",
    "    neighbor_homogeneity.append(agreement)\n",
    "\n",
    "neighbor_homogeneity = np.array(neighbor_homogeneity)\n",
    "\n",
    "# Identify correct and incorrect predictions\n",
    "correct_mask = (test_pred == y_test)\n",
    "\n",
    "# Statistics\n",
    "print(\"Prediction Confidence Analysis:\\n\")\n",
    "print(f\"Correct predictions - Mean neighbor agreement: {neighbor_homogeneity[correct_mask].mean():.3f}\")\n",
    "print(f\"Incorrect predictions - Mean neighbor agreement: {neighbor_homogeneity[~correct_mask].mean():.3f}\")\n",
    "print(f\"\\nHigh confidence predictions (100% neighbor agreement): {np.sum(neighbor_homogeneity == 1.0)} / {len(y_test)}\")\n",
    "print(f\"Low confidence predictions (<80% neighbor agreement): {np.sum(neighbor_homogeneity < 0.8)} / {len(y_test)}\")\n",
    "\n",
    "# Visualize relationship between confidence and correctness\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot 1: Histogram of neighbor homogeneity by correctness\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(neighbor_homogeneity[correct_mask], bins=10, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "plt.hist(neighbor_homogeneity[~correct_mask], bins=10, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "plt.xlabel('Neighbor Homogeneity (Agreement)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Confidence vs Correctness')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Misclassifications by actual class\n",
    "plt.subplot(1, 2, 2)\n",
    "misclassified_classes = y_test[~correct_mask]\n",
    "if len(misclassified_classes) > 0:\n",
    "    class_counts = np.bincount(misclassified_classes, minlength=len(np.unique(y)))\n",
    "    plt.bar(range(len(class_counts)), class_counts, edgecolor='black', color='salmon')\n",
    "    plt.xlabel('True Class')\n",
    "    plt.ylabel('Number of Misclassifications')\n",
    "    plt.title('Errors by Wine Cultivar')\n",
    "    plt.xticks(range(len(class_counts)))\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No misclassifications!', ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show details of ambiguous cases\n",
    "if np.sum(~correct_mask) > 0:\n",
    "    print(f\"\\nMisclassified samples: {np.sum(~correct_mask)}\")\n",
    "    print(\"Most ambiguous misclassifications (lowest neighbor agreement):\")\n",
    "    misclassified_indices = np.where(~correct_mask)[0]\n",
    "    ambiguous_errors = misclassified_indices[np.argsort(neighbor_homogeneity[~correct_mask])[:min(3, len(misclassified_indices))]]\n",
    "    for idx in ambiguous_errors:\n",
    "        neighbors = neighbor_indices[idx]\n",
    "        neighbor_classes = y_train_all[neighbors]\n",
    "        print(f\"  Test sample {idx}: True={y_test[idx]}, Predicted={test_pred[idx]}, \"\n",
    "              f\"Neighbor agreement={neighbor_homogeneity[idx]:.2f}, \"\n",
    "              f\"Neighbor classes={neighbor_classes}\")\n",
    "else:\n",
    "    print(\"\\nPerfect classification - no errors to analyze!\")"
   ],
   "metadata": {
    "id": "b371bumnzlt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9feef4b9",
   "metadata": {
    "id": "9feef4b9"
   },
   "source": [
    "## Limitations (Current Scope) & What\u2019s Next\n",
    "This notebook uses a **single hold\u2011out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k\u2011fold cross\u2011validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute\u2011force neighbor search (`algorithm='brute'`) and didn\u2019t explore scalability techniques like KD\u2011trees, Ball Trees, or approximate nearest neighbor libraries (e.g. FAISS, HNSW). These become important when your archive grows to millions of rows or requires low\u2011latency predictions. Finally, we didn\u2019t address class imbalance or cost\u2011sensitive KNN; these are natural extensions for more advanced courses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d61xxnu8",
   "source": [
    "## Common Pitfalls and Best Practices\n",
    "\n",
    "### Critical Mistakes to Avoid:\n",
    "\n",
    "1. **Forgetting to scale features** \u274c  \n",
    "   KNN is distance-based; features on different scales will dominate distance calculations. Always use StandardScaler or similar normalization.\n",
    "\n",
    "2. **Using the test set for hyperparameter tuning** \u274c  \n",
    "   This creates data leakage and inflates performance estimates. Use a separate validation set or cross-validation for all tuning decisions.\n",
    "\n",
    "3. **Choosing K=1 for production systems** \u274c  \n",
    "   While K=1 may give perfect training accuracy, it's highly sensitive to noise and outliers. Always validate with K > 1 on held-out data.\n",
    "\n",
    "4. **Ignoring computational cost** \u274c  \n",
    "   KNN requires computing distances to all training points at prediction time. For large datasets (>100K samples), consider approximate nearest neighbor methods or alternative algorithms.\n",
    "\n",
    "5. **Treating class imbalance casually** \u274c  \n",
    "   If one class has 90% of samples, KNN will naturally favor that class. Consider using balanced class weights, stratified sampling, or appropriate evaluation metrics (balanced accuracy, F1).\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "\u2705 **Always scale features** before applying KNN to continuous data  \n",
    "\u2705 **Use stratified splits** to maintain class proportions across train/val/test sets  \n",
    "\u2705 **Validate hyperparameters** (K, distance metric) on separate validation data  \n",
    "\u2705 **Consider dimensionality**: KNN performance degrades in very high dimensions (curse of dimensionality); consider dimensionality reduction (PCA, feature selection) for >50 features  \n",
    "\u2705 **Monitor the train-validation gap** to detect overfitting early  \n",
    "\u2705 **Use domain knowledge**: For some applications (text, images), specialized distance metrics (cosine, Hamming) may work better than Euclidean"
   ],
   "metadata": {
    "id": "f4d61xxnu8"
   }
  },
  {
   "cell_type": "markdown",
   "id": "10c90ca2",
   "metadata": {
    "id": "10c90ca2"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this case study, we've worked through a complete KNN classification workflow on the Wine dataset, covering the essential concepts and practical techniques:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Feature scaling is non-negotiable** for distance-based algorithms. Without it, KNN effectively ignores smaller-scale features, leading to poor performance.\n",
    "\n",
    "2. **Hyperparameter tuning requires systematic validation**. We used a dedicated validation set to select K and the distance metric, ensuring our choices generalize beyond the training data.\n",
    "\n",
    "3. **The bias-variance tradeoff is visible in the train-validation gap**. Very small K (high flexibility) leads to overfitting; very large K (high rigidity) leads to underfitting.\n",
    "\n",
    "4. **KNN is computationally expensive at prediction time**. Unlike parametric models that compress knowledge into parameters, KNN must compare against all training examples for each prediction.\n",
    "\n",
    "5. **Error analysis reveals model confidence**. Neighbor homogeneity provides a natural measure of prediction uncertainty, helping identify boundary cases where the model is less certain.\n",
    "\n",
    "### When to Use KNN:\n",
    "\n",
    "\u2705 **Good fit:**\n",
    "- Small to medium datasets (<100K samples)\n",
    "- Problems where local similarity is meaningful\n",
    "- Situations requiring interpretable, example-based reasoning\n",
    "- Establishing baselines before trying complex models\n",
    "\n",
    "\u274c **Poor fit:**\n",
    "- Large datasets requiring low-latency predictions\n",
    "- High-dimensional data (>50 features) without dimensionality reduction\n",
    "- Problems where global patterns matter more than local similarity\n",
    "- Datasets with severe class imbalance (without special handling)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Try cross-validation** instead of a single validation split for more robust hyperparameter selection\n",
    "- **Experiment with distance metrics** (Manhattan, Minkowski with different p values) tailored to your data\n",
    "- **Explore weighted KNN** where closer neighbors have more influence (use `weights='distance'`)\n",
    "- **Consider dimensionality reduction** (PCA, feature selection) if working with high-dimensional data\n",
    "- **Compare against other classifiers** (Logistic Regression, Random Forest, SVM) to see if KNN's simplicity is sufficient\n",
    "\n",
    "KNN remains one of the most intuitive machine learning algorithms\u2014its \"similar inputs produce similar outputs\" principle mirrors how humans naturally reason about new situations by comparing to past experiences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}