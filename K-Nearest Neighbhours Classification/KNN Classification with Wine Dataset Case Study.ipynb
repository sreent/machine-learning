{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20(KNN)%20Classification/Case%20Study%3A%20K-Nearest%20Neighbours%20Classification%20with%20Wine%20Dataset%20(UCI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b78c4",
   "metadata": {
    "id": "519b78c4"
   },
   "source": [
    "# Case Study: KNN Classification with Wine Dataset (UCI)\n",
    "\n",
    "K‑Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that classifies new examples based on similarity to known examples. In this case study, we’ll step through a practical example using the **Wine recognition** dataset (from the UCI Machine Learning Repository) to illustrate key concepts and best practices of KNN classification. This dataset contains chemical analysis results for wines from three cultivars (classes), with 13 continuous features (e.g. alcohol content, acidity, magnesium, phenols, color intensity, etc.). We simulate the scenario of predicting a wine’s cultivar from its chemical properties, akin to a chemist identifying origin by lab measurements.\n",
    "\n",
    "## What we’ll cover\n",
    "- **Data exploration and preparation:** Understanding feature scales and splitting data into training, validation, and test sets.  \n",
    "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance.  \n",
    "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
    "- **Distance metric considerations:** How the choice of distance measure can affect KNN.  \n",
    "- **Model evaluation:** Evaluating the final model on a test set to ensure it generalizes well to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "di752ya0jq7",
   "source": "## Learning Objectives\n\nBy the end of this case study, you will be able to:\n\n1. **Explain why feature scaling is critical for KNN** and demonstrate its impact on classification accuracy\n2. **Apply proper data splitting strategies** (train/validation/test) to avoid data leakage and obtain unbiased performance estimates\n3. **Tune hyperparameters systematically** by evaluating K values and distance metrics on validation data\n4. **Interpret the bias-variance tradeoff** in the context of KNN's K parameter and identify signs of overfitting\n5. **Evaluate classification models comprehensively** using accuracy, balanced accuracy, F1 scores, and confusion matrices\n6. **Understand when KNN is appropriate** for real-world classification problems and recognize its limitations\n\nThese skills form the foundation for applying distance-based learning algorithms to practical classification tasks.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ab88f59c",
   "metadata": {
    "id": "ab88f59c"
   },
   "source": [
    "## Exploring the Dataset\n",
    "Before diving into modeling, let's load the dataset and examine its features. The dataset has 178 samples, each with 13 features. The target `class` is an integer (0, 1, or 2) representing the wine cultivar.\n",
    "\n",
    "**Typical feature ranges (intuition):**  \n",
    "- Alcohol ~ 11–15  \n",
    "- Malic acid ~ 0.7–6  \n",
    "- Alcalinity of ash ~ 10–30  \n",
    "- Magnesium ~ 70–160  \n",
    "- Color intensity ~ 1–13  \n",
    "- Proline ~ 280–1700  \n",
    "\n",
    "Large differences in magnitude (e.g., *Proline* in hundreds vs *Malic acid* single digits) motivate **scaling** before using distance-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a85ef4",
   "metadata": {
    "id": "a2a85ef4"
   },
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['class'] = y\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2847407",
   "metadata": {
    "id": "f2847407"
   },
   "source": "Let's examine the class distribution and feature statistics before splitting our data.\n\n**Data Splitting Strategy:**  \nWe use a three-way split (60/20/20) to create distinct training, validation, and test sets:\n- **Training set (60%)**: Used to fit the model (learn patterns from the data)\n- **Validation set (20%)**: Used to tune hyperparameters (select best K, compare distance metrics, etc.)\n- **Test set (20%)**: Held out completely until final evaluation to assess real-world performance\n\nThe split is **stratified** to ensure each subset maintains the same class proportions as the original dataset. This is critical for classification problems to avoid biased performance estimates.\n\n> **Question**: After tuning hyperparameters on the validation set, why do we evaluate the final model on a separate test set instead of reporting validation performance?\n>  \n> A) To validate that our chosen hyperparameters work well across different random seeds\n>\n> B) To get an unbiased estimate of how the model will perform on completely new data in production\n>\n> C) To ensure the model complexity matches the data complexity\n>\n> D) To verify that feature scaling was applied correctly across all splits\n\nThe test set provides an honest assessment of generalization performance because it played no role in model selection or hyperparameter tuning."
  },
  {
   "cell_type": "code",
   "source": [
    "# Examine class distribution\n",
    "print(\"Class distribution:\\n\", df['class'].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics of features\n",
    "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Stratified split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
   ],
   "metadata": {
    "id": "1SfshufVixc7"
   },
   "id": "1SfshufVixc7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "These figures match the scikit-learn load_wine description (178 samples, 13 features, 3 classes, with per-class counts)."
   ],
   "metadata": {
    "id": "ccokg4IsGYB8"
   },
   "id": "ccokg4IsGYB8"
  },
  {
   "cell_type": "markdown",
   "id": "813c67d3",
   "metadata": {
    "id": "813c67d3"
   },
   "source": [
    "## Effect of Feature Scaling on KNN\n",
    "\n",
    "KNN uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The tiny demo below illustrates how a difference in *Proline* (hundreds) can swamp a difference in *Malic acid* (tenths). Therefore, scaling features to comparable ranges is critical for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c14c88",
   "metadata": {
    "id": "73c14c88"
   },
   "outputs": [],
   "source": [
    "# Demonstrate distance dominance (hypothetical differences)\n",
    "from math import sqrt\n",
    "\n",
    "delta_proline_large = 100.0\n",
    "delta_malic_small = 0.5\n",
    "\n",
    "d1 = sqrt(delta_proline_large**2 + 0.0**2)\n",
    "d2 = sqrt(0.0**2 + delta_malic_small**2)\n",
    "\n",
    "print(\"Distance if only Proline differs by +100:\", round(d1, 3))\n",
    "print(\"Distance if only Malic differs by +0.5  :\", round(d2, 3))\n",
    "print(\"Ratio (Proline / Malic):\", round(d1 / d2, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936dd00",
   "metadata": {
    "id": "d936dd00"
   },
   "source": [
    "Next, we train a baseline KNN model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. Note that we scale using parameters learned from the training set only to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Baseline without scaling\n",
    "knn_raw = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "raw_val_acc = accuracy_score(y_val, knn_raw.predict(X_val))\n",
    "\n",
    "# Baseline with scaling (fit on train only)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_val_acc = accuracy_score(y_val, knn_scaled.predict(X_val_scaled))\n",
    "\n",
    "print(f\"Validation accuracy without scaling: {raw_val_acc:.3f}\")\n",
    "print(f\"Validation accuracy with scaling:  {scaled_val_acc:.3f}\")\n"
   ],
   "metadata": {
    "id": "ldQ8a-dFjJqk"
   },
   "id": "ldQ8a-dFjJqk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b9863c1c",
   "metadata": {
    "id": "b9863c1c"
   },
   "source": [
    "The scaled model often performs dramatically better because each feature contributes fairly to distance computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8322849",
   "metadata": {
    "id": "d8322849"
   },
   "source": [
    "**t‑SNE visualization of the wine dataset after feature scaling.**  \n",
    "t‑SNE is fit on the **full dataset** purely for visualization. It preserves local structure but should **not** be used for tuning or evaluation. This does **not** leak information into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_scaled_full = StandardScaler().fit_transform(X)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled_full)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.7)\n",
    "plt.xlabel(\"$X_1$ (t-SNE)\")\n",
    "plt.ylabel(\"$X_2$ (t-SNE)\")\n",
    "plt.axis([-20, 20, -20, 20])\n",
    "plt.title(\"Wine dataset — t-SNE (Scaled Features)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ZVTuE_9qjlJN"
   },
   "id": "ZVTuE_9qjlJN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85ab069b",
   "metadata": {
    "id": "85ab069b"
   },
   "source": "## Distance Metric Considerations\n\nThe choice of distance metric is a hyperparameter that can significantly impact KNN performance, yet it's often overlooked. Common distance metrics for continuous features include:\n\n- **Euclidean (L2)**: Measures straight-line distance; computed as √(Σ(xᵢ - yᵢ)²)\n- **Manhattan (L1)**: Measures city-block distance; computed as Σ|xᵢ - yᵢ|  \n- **Minkowski**: Generalizes both L1 and L2 with parameter p (p=1 gives Manhattan, p=2 gives Euclidean)\n- **Cosine**: Measures angle between vectors; commonly used for text data and high-dimensional sparse features\n\nDifferent metrics can produce different neighbor sets and different predictions, especially when features have varying scales or distributions. The Wine dataset contains chemical measurements that may occasionally include extreme values due to measurement errors, unusual growing conditions, or genuinely exceptional wines.\n\n> **Question**: For datasets with continuous features that may contain occasional extreme outliers, which statement about distance metrics in KNN is most accurate?\n>  \n> A) Euclidean distance is generally more robust to outliers because squaring differences normalizes their relative impact\n>\n> B) Manhattan distance is generally more robust to outliers because it uses absolute differences rather than squared differences\n>\n> C) Both metrics are equally affected by outliers once features are properly scaled to the same range\n>\n> D) Cosine distance is always the best choice for robustness because it normalizes for vector magnitude\n\nThe choice of distance metric should be validated empirically on your specific dataset using held-out validation data."
  },
  {
   "cell_type": "markdown",
   "id": "4157e321",
   "metadata": {
    "id": "4157e321"
   },
   "source": "## Choosing K: Bias–Variance Trade‑off\n\nThe hyperparameter K fundamentally controls how KNN makes predictions and directly impacts model performance:\n\n- **Small K (e.g., K=1)**: Each prediction is determined by very few neighbors, creating highly flexible decision boundaries that adapt closely to individual training points\n- **Large K (e.g., K=50)**: Predictions average over many neighbors, producing smoother decision boundaries that change gradually across the feature space\n- **Optimal K**: Typically found somewhere in between, balancing the ability to capture genuine patterns while avoiding sensitivity to noise or outliers\n\nWe'll systematically evaluate K values from 1 to 20, tracking both training and validation accuracy. The gap between these curves reveals how well each K value generalizes to unseen data. Large gaps suggest the model is memorizing training-specific patterns rather than learning generalizable relationships.\n\n> **Question**: After training KNN with K=1, you observe 100% training accuracy but only 88% validation accuracy (a 12 percentage point gap). Which approach is most likely to improve validation performance?\n>  \n> A) Increase K to create smoother decision boundaries and improve generalization to new data\n>\n> B) Keep K=1 but collect more training samples to reduce the performance gap\n>\n> C) Keep K=1 but apply more sophisticated feature engineering to capture better patterns  \n>\n> D) Switch to weighted KNN with K=1 where closer neighbors have more influence on predictions\n\nThe train-validation gap is a key diagnostic for detecting when a model is too flexible for the available data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a985951",
   "metadata": {
    "id": "3a985951"
   },
   "outputs": [],
   "source": "train_acc, val_acc = [], []\nk_sweep = range(1, 21)\n\nfor k in k_sweep:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train_scaled, y_train)\n    train_acc.append(accuracy_score(y_train, model.predict(X_train_scaled)))\n    val_acc.append(accuracy_score(y_val, model.predict(X_val_scaled)))\n\n# Best K by validation\nbest_k_idx = int(np.argmax(val_acc))\nchosen_k = best_k_idx + 1\nbest_val = max(val_acc)\nmax_gap = np.max(np.array(train_acc) - np.array(val_acc))\n\n# Use Euclidean distance (default and most commonly used for continuous features)\nchosen_metric = 'euclidean'\n\nprint(\"Selected hyperparameters:\")\nprint(f\"  K = {chosen_k}\")\nprint(f\"  Distance metric = {chosen_metric}\")\nprint(f\"  Validation accuracy = {best_val:.3f}\")\nprint(f\"Max (train - validation) gap across K: {max_gap:.3f}\")\n\n# Plot train vs validation accuracy vs K\nplt.figure()\nplt.scatter(list(k_sweep), train_acc, label='Train Accuracy')\nplt.scatter(list(k_sweep), val_acc, label='Validation Accuracy')\nplt.axvline(chosen_k, linestyle='--', label=f'Best K={chosen_k}')\nplt.axis([0, 20, 0.8, 1.05])\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nplt.grid()\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "90e431ac",
   "metadata": {
    "id": "90e431ac"
   },
   "source": "## Model Evaluation on Test Set\n\nAfter selecting our hyperparameters using the validation set, we're ready for the final evaluation. At this stage we:\n\n1. **Combine training and validation sets**: This provides the model with the maximum available data for learning, since we've already locked in our hyperparameter choices\n2. **Refit the complete pipeline**: The scaler learns standardization parameters from the combined dataset, and KNN memorizes all combined training examples\n3. **Evaluate once on the test set**: This held-out data provides our unbiased estimate of real-world performance\n\n**Critical principle**: The test set has played zero role in any modeling decisions—no hyperparameter selection, no feature engineering choices, no model architecture decisions. It therefore provides an honest estimate of how the model will perform when deployed on genuinely new data from the same distribution.\n\n> **Question**: After final evaluation, your test accuracy (94.4%) is slightly lower than your best validation accuracy (97.2%). Before deployment, which interpretation and next step is most appropriate?\n>  \n> A) This small decrease is normal variation; verify that test performance meets your accuracy requirements and document the results\n>\n> B) Re-evaluate hyperparameters using the test set to identify values that achieve better performance on this data split\n>\n> C) This indicates potential data leakage between validation and test sets; recreate the splits and re-run the experiment  \n>\n> D) Average the validation and test accuracies to obtain a more stable estimate of expected production performance\n\nRemember: the test set is used exactly once for evaluation. Any optimization based on test results invalidates its role as an unbiased estimator."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0ff29",
   "metadata": {
    "id": "5ea0ff29"
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final training\n",
    "X_train_all = np.vstack([X_train, X_val])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "# Build pipeline (scaler + KNN) with chosen hyperparameters — no weights\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_pipe.predict(X_test)\n",
    "test_acc  = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(\"Test accuracy:\", round(test_acc, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb57ed",
   "metadata": {
    "id": "e9cb57ed"
   },
   "source": [
    "Beyond accuracy, we examine **balanced accuracy** (accounts for class imbalance) and **macro F1** (averages F1 across classes), print a **classification report**, show a **per-class table**, and plot both the raw and normalized confusion matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Print classification report and per-class metrics\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test, test_pred, digits=3, target_names=[str(c) for c in np.unique(y)]))\n",
    "\n",
    "labels = list(np.unique(y))\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(y_test, test_pred, labels=labels)\n",
    "\n",
    "per_class_df = pd.DataFrame({\n",
    "    'precision': prec,\n",
    "    'recall': rec,\n",
    "    'f1': f1,\n",
    "    'support': sup\n",
    "}, index=labels)\n",
    "display(per_class_df)\n",
    "\n",
    "# Balanced accuracy & macro F1\n",
    "print(\"Balanced accuracy (test):\", round(balanced_accuracy_score(y_test, test_pred), 3))\n",
    "print(\"Macro F1 (test):         \", round(f1_score(y_test, test_pred, average='macro'), 3))\n",
    "\n",
    "# Confusion matrices: raw and normalized\n",
    "cm_raw = confusion_matrix(y_test, test_pred, labels=labels)\n",
    "cm_norm = confusion_matrix(y_test, test_pred, labels=labels, normalize='true')\n",
    "\n",
    "# Raw confusion matrix heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_raw, cmap='Blues')\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion Matrix (Raw)\")\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        ax.text(j, i, cm_raw[i, j], ha='center', va='center', color='black')\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm, vmin=0, vmax=1, cmap='Blues')\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion Matrix (Normalized)\")\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "7CIOzVT8oJS2"
   },
   "id": "7CIOzVT8oJS2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9feef4b9",
   "metadata": {
    "id": "9feef4b9"
   },
   "source": [
    "## Limitations (Current Scope) & What’s Next\n",
    "This notebook uses a **single hold‑out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k‑fold cross‑validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute‑force neighbor search (`algorithm='brute'`) and didn’t explore scalability techniques like KD‑trees, Ball Trees, or approximate nearest neighbor libraries (e.g. FAISS, HNSW). These become important when your archive grows to millions of rows or requires low‑latency predictions. Finally, we didn’t address class imbalance or cost‑sensitive KNN; these are natural extensions for more advanced courses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d61xxnu8",
   "source": "## Common Pitfalls and Best Practices\n\n### Critical Mistakes to Avoid:\n\n1. **Forgetting to scale features** ❌  \n   KNN is distance-based; features on different scales will dominate distance calculations. Always use StandardScaler or similar normalization.\n\n2. **Using the test set for hyperparameter tuning** ❌  \n   This creates data leakage and inflates performance estimates. Use a separate validation set or cross-validation for all tuning decisions.\n\n3. **Choosing K=1 for production systems** ❌  \n   While K=1 may give perfect training accuracy, it's highly sensitive to noise and outliers. Always validate with K > 1 on held-out data.\n\n4. **Ignoring computational cost** ❌  \n   KNN requires computing distances to all training points at prediction time. For large datasets (>100K samples), consider approximate nearest neighbor methods or alternative algorithms.\n\n5. **Treating class imbalance casually** ❌  \n   If one class has 90% of samples, KNN will naturally favor that class. Consider using balanced class weights, stratified sampling, or appropriate evaluation metrics (balanced accuracy, F1).\n\n### Best Practices:\n\n✅ **Always scale features** before applying KNN to continuous data  \n✅ **Use stratified splits** to maintain class proportions across train/val/test sets  \n✅ **Validate hyperparameters** (K, distance metric) on separate validation data  \n✅ **Consider dimensionality**: KNN performance degrades in very high dimensions (curse of dimensionality); consider dimensionality reduction (PCA, feature selection) for >50 features  \n✅ **Monitor the train-validation gap** to detect overfitting early  \n✅ **Use domain knowledge**: For some applications (text, images), specialized distance metrics (cosine, Hamming) may work better than Euclidean",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "10c90ca2",
   "metadata": {
    "id": "10c90ca2"
   },
   "source": [
    "## Conclusion\n",
    "- **Scaling** prevents large‑range features from dominating distance computations.  \n",
    "- **Tuning K** via validation balances bias and variance; a very small K overfits, a very large K underfits.  \n",
    "- **Distance metric and K** are hyperparameters; small grids reveal significant differences.  \n",
    "- KNN remains a powerful, intuitive baseline—use it to build intuition about distance and similarity before advancing to more sophisticated models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Appendix\n",
    "\n",
    "## Real‑World Applications: Where This Is Useful (Concrete Ops)\n",
    "- **Authenticity & origin (PDO/PGI):** Check that a lot labeled “Cultivar A / Region X” matches historical chemical profiles; flag likely mislabels or adulteration.  \n",
    "- **Supplier & intake QA:** Compare incoming lots against past lots of the same cultivar to catch off‑spec deliveries early, saving tank space and time.  \n",
    "- **Process monitoring & early warnings:** Periodic lab panels classified against expected states; abnormal neighbors trigger investigation of contamination or process drift.\n",
    "- **Counterfeit screening:** Rapidly triage shipments before expensive sensory panels or full mass‑spec profiling.  \n",
    "\n",
    "## Why KNN specifically\n",
    "- **Small/medium data with strong locality:** Wine labs typically have hundreds or thousands of historical lots—KNN thrives here without heavy parametric assumptions.  \n",
    "- **Example‑based explanation:** You can justify a prediction by saying, “8/10 nearest wines were Cultivar 2 with similar magnesium, phenolics, and color intensity.”  \n"
   ],
   "metadata": {
    "id": "AI48uriMIG71"
   },
   "id": "AI48uriMIG71"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PF4cv9GyDwsf"
   },
   "id": "PF4cv9GyDwsf",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}