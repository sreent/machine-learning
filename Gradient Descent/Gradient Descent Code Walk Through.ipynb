{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Gradient%20Descent/Gradient%20Descent%20Code%20Walk%20Through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Gradient Descent: Code Walk Through\n\nThis notebook walks through the **computational steps** of the Gradient Descent optimization algorithm from scratch.\n\n## What We'll Cover:\n1. **Visualize the data** - understand the dataset\n2. **Initialize weights** - start with random values\n3. **Iterative optimization** - gradually improve weights using gradients\n4. **Compute loss** - measure prediction error (SSE)\n5. **Compute gradients** - find direction to update weights\n6. **Update weights** - move in direction that reduces loss\n7. **Visualize convergence** - watch loss decrease over iterations\n\nWe'll show **both manual calculation** (to understand the logic) and **vectorized matrix operations** (for efficiency).\n\n### Key Concept:\n- Gradient descent finds the **best fit line** through **iterative optimization**\n- Unlike Linear Regression's closed-form solution, it uses **multiple iterations**\n- Updates weights in the direction that **reduces the loss function**\n- Essential for: neural networks, large datasets, complex models"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We need:\n",
    "- **NumPy** for numerical operations and matrix calculations\n",
    "- **Matplotlib** for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training Data\n",
    "\n",
    "We use the **same data as Linear Regression** to compare approaches:\n",
    "- **180 training points** generated from **x values ranging from -9.5 to 8.5** (step size 0.1)\n",
    "- **Continuous target values** that follow a linear relationship with added Gaussian noise\n",
    "- Our goal: find the line that best fits this data\n",
    "\n",
    "The data follows the relationship: **y = x + 1 + noise** where noise ~ N(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (matching Linear Regression Code Walk Through)\n",
    "# Generate X values from -9.5 to 8.5 with step size 0.1\n",
    "X_train = np.arange(-9.5, 8.5, 0.1)\n",
    "\n",
    "# Generate y values: y = x + 1 + noise\n",
    "# where noise follows a normal distribution with mean 0 and std 2\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data_points = X_train\n",
    "y_train = data_points + 1 + np.random.normal(0, 2, len(data_points))\n",
    "\n",
    "# Reshape X_train to be a column vector for matrix operations\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "\n",
    "# Test point\n",
    "X_test = np.array([5])\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)  # (180, 1) = 180 points, 1 feature\n",
    "print(\"Target values shape:\", y_train.shape)   # (180,) = 180 target values\n",
    "print(\"\\nFirst few training points:\")\n",
    "print(X_train[:3].ravel())\n",
    "print(\"\\nCorresponding target values:\")\n",
    "print(y_train[:3])\n",
    "print(f\"\\nTarget value range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "print(f\"\\nTest point: x = {X_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the Data\n",
    "\n",
    "Let's plot our training data to see the relationship between x and y.\n",
    "\n",
    "We can see the points roughly follow a **linear trend** - perfect for gradient descent optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of training data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='lightblue',\n",
    "           label='Training data')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Training Data: Looking for Linear Relationship', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"We have {len(X_train)} training points\")\n",
    "print(f\"Goal: Find the line y = w₁x + w₀ that best fits this data\")\n",
    "print(f\"      using iterative gradient descent optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Column of Ones (Bias Term)\n",
    "\n",
    "Just like in Linear Regression, we need to add a column of 1s to include an **intercept term**.\n",
    "\n",
    "**Why?**\n",
    "- Our model is: **y = w₁x + w₀**\n",
    "- We can rewrite this as: **y = w₀(1) + w₁x**\n",
    "- In matrix form: **y = [1, x] × [w₀, w₁]ᵀ**\n",
    "\n",
    "This matrix is called **Φ** (Phi) or the **design matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of ones using np.c_[]\n",
    "Phi = np.c_[np.ones(len(X_train)), X_train]\n",
    "\n",
    "print(\"Design matrix Φ shape:\", Phi.shape)  # (180, 2)\n",
    "print(\"\\nFirst few rows of Φ:\")\n",
    "print(Phi[:5])\n",
    "print(\"\\nEach row is now: [1, x]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Weights Randomly\n",
    "\n",
    "Unlike Linear Regression which computes optimal weights directly, gradient descent **starts with random weights** and improves them iteratively.\n",
    "\n",
    "**Key Difference:**\n",
    "- **Linear Regression (Normal Equation):** w = (ΦᵀΦ)⁻¹Φᵀy → Instant optimal solution\n",
    "- **Gradient Descent:** Start random → Iteratively improve → Converge to optimal\n",
    "\n",
    "We'll initialize weights with small random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(2) * 0.01  # Small random values\n",
    "\n",
    "print(\"Initial weights [w₀, w₁]:\")\n",
    "print(weights)\n",
    "print(f\"\\nInitial model: y = {weights[1]:.6f}x + {weights[0]:.6f}\")\n",
    "print(\"\\nThese are random! Gradient descent will improve them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Hyperparameters\n",
    "\n",
    "Gradient descent requires two key hyperparameters:\n",
    "\n",
    "1. **Learning Rate (α):** How big of a step to take in each iteration\n",
    "   - Too small → Slow convergence (many iterations needed)\n",
    "   - Too large → Overshooting, divergence (loss increases!)\n",
    "   - Typical values: 0.001 to 0.1\n",
    "\n",
    "2. **Number of Iterations (Epochs):** How many times to update weights\n",
    "   - Too few → Hasn't converged yet\n",
    "   - Too many → Wasted computation\n",
    "   - Monitor loss to know when to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01  # α (alpha)\n",
    "num_iterations = 100  # Number of gradient descent steps\n",
    "\n",
    "print(f\"Learning rate (α): {learning_rate}\")\n",
    "print(f\"Number of iterations: {num_iterations}\")\n",
    "\n",
    "# Store loss history for visualization\n",
    "loss_history = []\n",
    "weight_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: The Gradient Descent Algorithm\n\n### The Core Loop:\n\n```\nFor each iteration:\n    1. Compute predictions: ŷ = Φw\n    2. Compute loss (SSE): L = Σ(y - ŷ)²\n    3. Compute gradients: ∇w = -2Φᵀ(y - ŷ)\n    4. Update weights: w = w - α∇w\n```\n\n### Mathematical Derivation:\n\n**Loss Function (Sum of Squared Errors):**\n$$L(w) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 = \\|y - \\Phi w\\|^2$$\n\n**Gradient (derivative of loss with respect to weights):**\n$$\\nabla_w L = -2 \\Phi^T (y - \\Phi w)$$\n\n**Weight Update Rule:**\n$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\nabla_w L$$"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7a: One Iteration (Breaking It Down)\n",
    "\n",
    "Let's manually go through **one iteration** to understand each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ITERATION 1: Step-by-Step Breakdown\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Current weights (from initialization)\n",
    "print(\"\\n1. Current weights:\")\n",
    "print(f\"   w = {weights}\")\n",
    "print(f\"   Model: y = {weights[1]:.6f}x + {weights[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. Compute predictions: ŷ = Φw\")\n",
    "print(\"   (Matrix-vector multiplication)\")\n",
    "\n",
    "predictions = Phi @ weights\n",
    "\n",
    "print(f\"   Φ shape: {Phi.shape}\")\n",
    "print(f\"   w shape: {weights.shape}\")\n",
    "print(f\"   ŷ shape: {predictions.shape}\")\n",
    "print(f\"\\n   First few predictions: {predictions[:3]}\")\n",
    "print(f\"   First few actual:      {y_train[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. Compute errors (residuals): e = y - ŷ\")\n",
    "\n",
    "errors = y_train - predictions\n",
    "\n",
    "print(f\"   Errors shape: {errors.shape}\")\n",
    "print(f\"   First few errors: {errors[:3]}\")\n",
    "print(f\"   Mean error: {errors.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n4. Compute loss (SSE): L = Σ(y - ŷ)²\")\n\nN = len(y_train)\nloss = np.sum(errors ** 2)\n\n# Alternative calculation: using @ operator\nloss_alternative = errors @ errors\n\nprint(f\"   N = {N}\")\nprint(f\"   Loss (SSE) = {loss:.6f}\")\nprint(f\"   Alternative calculation: {loss_alternative:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n5. Compute gradients: ∇w = -2Φᵀ(y - ŷ)\")\nprint(\"   (Direction to move weights to reduce loss)\")\n\ngradients = -2 * (Phi.T @ errors)\n\nprint(f\"   Φᵀ shape: {Phi.T.shape}\")\nprint(f\"   errors shape: {errors.shape}\")\nprint(f\"   ∇w shape: {gradients.shape}\")\nprint(f\"\\n   Gradients: {gradients}\")\nprint(f\"   ∇w₀ (intercept gradient) = {gradients[0]:.6f}\")\nprint(f\"   ∇w₁ (slope gradient) = {gradients[1]:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n6. Update weights: w_new = w_old - α∇w\")\n",
    "print(f\"   Learning rate α = {learning_rate}\")\n",
    "\n",
    "weights_old = weights.copy()\n",
    "weights_new = weights_old - learning_rate * gradients\n",
    "\n",
    "print(f\"\\n   Old weights: {weights_old}\")\n",
    "print(f\"   Update (α∇w): {learning_rate * gradients}\")\n",
    "print(f\"   New weights: {weights_new}\")\n",
    "print(f\"\\n   Old model: y = {weights_old[1]:.6f}x + {weights_old[0]:.6f}\")\n",
    "print(f\"   New model: y = {weights_new[1]:.6f}x + {weights_new[0]:.6f}\")\n",
    "\n",
    "# Update weights\n",
    "weights = weights_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7b: Full Gradient Descent Loop\n",
    "\n",
    "Now let's run the complete algorithm for all iterations and track the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reset weights to initial random values\nnp.random.seed(42)\nweights = np.random.randn(2) * 0.01\n\nprint(\"Starting Gradient Descent...\")\nprint(f\"Initial weights: {weights}\")\nprint(f\"Initial model: y = {weights[1]:.6f}x + {weights[0]:.6f}\")\nprint(\"\\n\" + \"=\"*60)\n\n# Gradient Descent Loop\nloss_history = []\nweight_history = []\n\nfor iteration in range(num_iterations):\n    # 1. Compute predictions\n    predictions = Phi @ weights\n    \n    # 2. Compute errors\n    errors = y_train - predictions\n    \n    # 3. Compute loss (SSE)\n    loss = np.sum(errors ** 2)\n    \n    # 4. Compute gradients\n    gradients = -2 * (Phi.T @ errors)\n    \n    # 5. Update weights\n    weights = weights - learning_rate * gradients\n    \n    # Store history\n    loss_history.append(loss)\n    weight_history.append(weights.copy())\n    \n    # Print progress every 10 iterations\n    if (iteration + 1) % 10 == 0 or iteration == 0:\n        print(f\"Iteration {iteration+1:3d}: Loss = {loss:.6f}, w₀ = {weights[0]:7.4f}, w₁ = {weights[1]:7.4f}\")\n\nprint(\"=\"*60)\nprint(\"\\nGradient Descent Complete!\")\nprint(f\"\\nFinal weights: {weights}\")\nprint(f\"Final model: y = {weights[1]:.6f}x + {weights[0]:.6f}\")\nprint(f\"Final loss (SSE): {loss_history[-1]:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Convergence\n",
    "\n",
    "Let's plot how the loss decreases over iterations. This shows that gradient descent is **working** - the model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot loss over iterations\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_iterations + 1), loss_history, 'b-', linewidth=2)\nplt.xlabel('Iteration', fontsize=14)\nplt.ylabel('Loss (SSE)', fontsize=14)\nplt.title('Gradient Descent Convergence', fontsize=16)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Initial loss: {loss_history[0]:.6f}\")\nprint(f\"Final loss:   {loss_history[-1]:.6f}\")\nprint(f\"Loss reduction: {loss_history[0] - loss_history[-1]:.6f} ({(1 - loss_history[-1]/loss_history[0])*100:.2f}% improvement)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Weight Evolution\n",
    "\n",
    "Let's see how the weights changed over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weight histories\n",
    "weight_history = np.array(weight_history)\n",
    "w0_history = weight_history[:, 0]\n",
    "w1_history = weight_history[:, 1]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot w0 (intercept)\n",
    "axes[0].plot(range(1, num_iterations + 1), w0_history, 'r-', linewidth=2)\n",
    "axes[0].axhline(y=weights[0], color='k', linestyle='--', alpha=0.5, label=f'Final: {weights[0]:.4f}')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('w₀ (Intercept)', fontsize=12)\n",
    "axes[0].set_title('Intercept Evolution', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot w1 (slope)\n",
    "axes[1].plot(range(1, num_iterations + 1), w1_history, 'g-', linewidth=2)\n",
    "axes[1].axhline(y=weights[1], color='k', linestyle='--', alpha=0.5, label=f'Final: {weights[1]:.4f}')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('w₁ (Slope)', fontsize=12)\n",
    "axes[1].set_title('Slope Evolution', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"w₀ changed from {w0_history[0]:.6f} to {w0_history[-1]:.6f}\")\n",
    "print(f\"w₁ changed from {w1_history[0]:.6f} to {w1_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize the Final Fit\n",
    "\n",
    "Let's see how well our gradient descent model fits the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x values for plotting the line\n",
    "x_line = np.linspace(X_train.min(), X_train.max(), 100)\n",
    "\n",
    "# Compute y values using learned weights: y = w₁x + w₀\n",
    "y_line = weights[1] * x_line + weights[0]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='lightblue', alpha=0.6,\n",
    "           edgecolors='black', linewidths=0.5,\n",
    "           label='Training data', zorder=3)\n",
    "\n",
    "# Best fit line from gradient descent\n",
    "plt.plot(x_line, y_line,\n",
    "        'r-', linewidth=3,\n",
    "        label=f'Gradient Descent: y = {weights[1]:.3f}x + {weights[0]:.3f}')\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Gradient Descent: Final Best Fit Line', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model learned by Gradient Descent: y = {weights[1]:.3f}x + {weights[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Make Predictions on Test Data\n",
    "\n",
    "Now let's use our trained model to predict a new value.\n",
    "\n",
    "**Example:** What is the predicted y value when x = 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test point (defined earlier: x = 5)\n",
    "X_test_reshaped = X_test.reshape(-1, 1)\n",
    "\n",
    "# Add bias term\n",
    "X_test_with_bias = np.c_[np.ones(len(X_test_reshaped)), X_test_reshaped]\n",
    "\n",
    "# Compute prediction\n",
    "prediction = X_test_with_bias @ weights\n",
    "\n",
    "print(f\"For x = {X_test[0]:.1f}:\")\n",
    "print(f\"Predicted y = {prediction[0]:.3f}\")\n",
    "print(f\"Calculation: y = {weights[1]:.3f} × {X_test[0]:.1f} + {weights[0]:.3f} = {prediction[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='lightblue',\n",
    "           label='Training data', zorder=3)\n",
    "\n",
    "# Best fit line\n",
    "plt.plot(x_line, y_line,\n",
    "        'k-', linewidth=2, alpha=0.8,\n",
    "        label=f'Best fit line: y = {weights[1]:.3f}x + {weights[0]:.3f}')\n",
    "\n",
    "# Test point and prediction\n",
    "plt.scatter(X_test_reshaped, prediction,\n",
    "           c='red', s=200, marker='*',\n",
    "           edgecolors='black', linewidths=2,\n",
    "           label=f'Prediction: x={X_test_reshaped[0,0]:.1f}, ŷ={prediction[0]:.3f}',\n",
    "           zorder=4)\n",
    "\n",
    "# Draw dashed lines to show prediction\n",
    "plt.plot([X_train.min(), X_test_reshaped[0,0]], [prediction[0], prediction[0]], 'r--', alpha=0.5, linewidth=1)\n",
    "plt.plot([X_test_reshaped[0,0], X_test_reshaped[0,0]], [y_train.min(), prediction[0]], 'r--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Gradient Descent: Making a Prediction', fontsize=16)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFor test point x = {X_test_reshaped[0,0]:.1f}:\")\n",
    "print(f\"Predicted value ŷ = {prediction[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Compare with Closed-Form Solution (Normal Equation)\n",
    "\n",
    "Let's verify our gradient descent solution by comparing with Linear Regression's closed-form solution.\n",
    "\n",
    "They should give **very similar** (if not identical) results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute weights using normal equation: w = (Φ^T Φ)^{-1} Φ^T y\nweights_closed_form = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ y_train\n\nprint(\"=\"*70)\nprint(\"COMPARISON: Gradient Descent vs Closed-Form Solution\")\nprint(\"=\"*70)\n\nprint(f\"\\nGradient Descent weights: {weights}\")\nprint(f\"Closed-Form weights:      {weights_closed_form}\")\n\nprint(f\"\\nGradient Descent: y = {weights[1]:.6f}x + {weights[0]:.6f}\")\nprint(f\"Closed-Form:      y = {weights_closed_form[1]:.6f}x + {weights_closed_form[0]:.6f}\")\n\nprint(f\"\\nDifference in w₀ (intercept): {abs(weights[0] - weights_closed_form[0]):.6f}\")\nprint(f\"\\nDifference in w₁ (slope):     {abs(weights[1] - weights_closed_form[1]):.6f}\")\n\n# Compute loss for closed-form solution\npredictions_closed_form = Phi @ weights_closed_form\nloss_closed_form = np.sum((y_train - predictions_closed_form) ** 2)\n\nprint(f\"\\nGradient Descent loss: {loss_history[-1]:.6f}\")\nprint(f\"Closed-Form loss:      {loss_closed_form:.6f}\")\n\nprint(\"\\n\" + \"=\"*70)\nif abs(weights[0] - weights_closed_form[0]) < 0.01 and abs(weights[1] - weights_closed_form[1]) < 0.01:\n    print(\"✓ Results match! Gradient descent converged to the optimal solution.\")\nelse:\n    print(\"⚠ Results differ. Try more iterations or a different learning rate.\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Visualize Both Solutions Together\n",
    "\n",
    "Let's plot both lines to see if they overlap (they should!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute y values for both models\n",
    "y_line_gd = weights[1] * x_line + weights[0]\n",
    "y_line_cf = weights_closed_form[1] * x_line + weights_closed_form[0]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train, y_train,\n",
    "           c='lightblue', alpha=0.6,\n",
    "           edgecolors='black', linewidths=0.5,\n",
    "           label='Training data', zorder=3)\n",
    "\n",
    "# Gradient descent line\n",
    "plt.plot(x_line, y_line_gd,\n",
    "        'r-', linewidth=3, alpha=0.7,\n",
    "        label=f'Gradient Descent: y = {weights[1]:.3f}x + {weights[0]:.3f}')\n",
    "\n",
    "# Closed-form line (should overlap with GD)\n",
    "plt.plot(x_line, y_line_cf,\n",
    "        'g--', linewidth=3, alpha=0.7,\n",
    "        label=f'Closed-Form: y = {weights_closed_form[1]:.3f}x + {weights_closed_form[0]:.3f}')\n",
    "\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Gradient Descent vs Closed-Form Solution', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The two lines should overlap almost perfectly!\")\n",
    "print(\"This confirms gradient descent found the optimal solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've walked through all the computational steps of Gradient Descent:\n\n1. ✅ **Visualized data** - saw training points showing linear trend\n2. ✅ **Added bias term** - transformed data by adding column of 1s\n3. ✅ **Initialized weights randomly** - started with small random values\n4. ✅ **Iterative optimization** - repeated for multiple iterations:\n   - Computed predictions: ŷ = Φw\n   - Computed loss (SSE): L = Σ(y - ŷ)²\n   - Computed gradients: ∇w = -2Φᵀ(y - ŷ)\n   - Updated weights: w = w - α∇w\n5. ✅ **Visualized convergence** - watched loss decrease over time\n6. ✅ **Made predictions** - computed ŷ for new test points\n7. ✅ **Compared with closed-form** - verified we reached optimal solution\n\n### Key Gradient Descent Concepts:\n\n| Concept | Description |\n|---------|-------------|\n| **Learning Rate (α)** | Step size for weight updates (0.001 to 0.1 typical) |\n| **Iterations/Epochs** | Number of times to update weights |\n| **Loss Function** | SSE = Σ(y - ŷ)² measures prediction error |\n| **Gradient (∇w)** | Direction to move weights to reduce loss |\n| **Update Rule** | w_new = w_old - α∇w |\n| **Convergence** | Loss stops decreasing significantly |\n\n### Key NumPy Operations Used:\n\n- **`@`** - matrix/vector multiplication (Φw, Φᵀe)\n- **`.T`** - transpose matrix (Φ → Φᵀ)\n- **`np.sum()`** - sum elements for loss calculation\n- **Broadcasting** - operations on arrays of different shapes\n- **`.copy()`** - copy arrays to preserve history\n\n### Gradient Descent vs Normal Equation:\n\n| Aspect | Gradient Descent | Normal Equation |\n|--------|-----------------|----------------|\n| **Computation** | Iterative (multiple steps) | Direct (one calculation) |\n| **Speed** | Slower for small data | Fast for small data |\n| **Large Data** | Scales well (N > 100,000) | Slow (matrix inversion O(d³)) |\n| **Hyperparameters** | Needs α and iterations | None needed |\n| **Convergence** | Approximate (can stop early) | Exact optimal solution |\n| **Use Case** | Neural networks, large data | Small-medium regression |\n| **Variants** | Batch, Stochastic, Mini-batch | Only one version |\n\n### When to Use Gradient Descent:\n\n✅ **Use Gradient Descent when:**\n- Training neural networks (no closed-form solution exists)\n- Dataset is very large (N > 100,000 samples)\n- Online learning (updating model as new data arrives)\n- Need stochastic/mini-batch variants for efficiency\n- Model is non-linear (can extend gradient descent, not normal equation)\n\n❌ **Use Normal Equation when:**\n- Simple linear regression with small-medium data\n- Want exact solution without tuning hyperparameters\n- Number of features is small (d < 10,000)\n- Don't need iterative updates"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}