{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Gradient%20Descent/Gradient%20Descent%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Hands-On Lab\n",
    "\n",
    "In this lab, you will implement Gradient Descent optimization from scratch, understand the mathematics behind it, and apply it to real data. Along the way, you'll answer conceptual questions and create visualizations to deepen your understanding.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the mathematics of Gradient Descent optimization\n",
    "- Implement a custom Gradient Descent class from scratch\n",
    "- Visualize convergence and loss curves\n",
    "- Understand the impact of learning rate on convergence\n",
    "- Apply feature scaling and understand why it's critical for gradient descent\n",
    "- Compare gradient descent with closed-form solutions\n",
    "- Understand batch, stochastic, and mini-batch variants\n",
    "- Analyze model performance and convergence behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Overview of Gradient Descent\n\nGradient Descent is an **iterative optimization algorithm** used to find the minimum of a function. In machine learning, we use it to **minimize the loss function** and find optimal model parameters.\n\n**Key Idea:**\n- Start with **random weights**\n- Iteratively **update weights** in the direction that reduces loss\n- Take steps proportional to the **negative gradient** of the loss function\n- Continue until **convergence** (loss stops decreasing)\n\n**The Update Rule:**\n$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\nabla_w L$$\n\nWhere:\n- **w** are the model weights (parameters)\n- **\u03b1** is the learning rate (step size)\n- **\u2207w L** is the gradient of the loss with respect to weights\n- **L** is the loss function (e.g., Sum of Squared Errors)\n\n**For Linear Regression with SSE Loss:**\n- Loss: $L(w) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n- Gradient: $\\nabla_w L = -2 \\Phi^T (y - \\Phi w)$\n- Update: $w = w - \\alpha \\nabla_w L$\n- Expanded form: $w = w - \\alpha(-2\\Phi^T(y - \\Phi w)) = w + 2\\alpha \\Phi^T (y - \\Phi w)$\n\n**Advantages:**\n- Works when no closed-form solution exists (e.g., neural networks)\n- Scales well to large datasets\n- Can be adapted to stochastic/mini-batch variants for efficiency\n- Foundation for deep learning optimization\n\n**Disadvantages:**\n- Requires tuning hyperparameters (learning rate, iterations)\n- Can be slow to converge\n- May get stuck in local minima (for non-convex functions)\n- Sensitive to feature scaling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: Gradient Descent finds optimal weights by:\n",
    ">\n",
    "> A. Computing the exact optimal solution directly using matrix inversion and least squares\n",
    ">\n",
    "> B. Iteratively updating weights in the direction that minimizes the loss function gradient\n",
    ">\n",
    "> C. Evaluating multiple weight configurations and selecting the combination with lowest validation error\n",
    ">\n",
    "> D. Approximating the closed-form solution through successive linearizations of the loss surface\n",
    "\n",
    "<details><summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "- **A is FALSE**: This describes the normal equation for Linear Regression: w = (\u03a6\u1d40\u03a6)\u207b\u00b9\u03a6\u1d40y. The normal equation computes the exact optimal solution in one step through direct matrix operations, without any iteration. Gradient descent, in contrast, is an iterative method that gradually approaches the optimal solution.\n",
    "- **B is TRUE**: Gradient descent computes the gradient \u2207w L (the direction of steepest ascent) and updates weights in the opposite direction (steepest descent): w_new = w_old - \u03b1\u2207w L. By repeatedly taking steps in the direction that reduces loss most quickly, it converges to a minimum.\n",
    "- **C is FALSE**: While this describes a valid optimization approach (grid search or random search), it's not gradient descent. Gradient descent uses calculus-based gradients to determine the exact direction to move, not trial-and-error evaluation of different configurations. Grid search would be extremely inefficient for high-dimensional problems.\n",
    "- **D is FALSE**: This might describe methods like Newton's method or successive quadratic approximations, which use second-order information (Hessian matrix). Gradient descent uses only first-order gradients and doesn't approximate closed-form solutions - it directly minimizes the loss iteratively.\n",
    "\n",
    "**Key Insight**: Gradient descent is a **first-order, gradient-based, iterative optimization** method. It uses calculus to find the steepest descent direction, then takes small steps in that direction.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent vs Normal Equation (Closed-Form Solution)\n",
    "\n",
    "For Linear Regression, we have **two ways** to find optimal weights:\n",
    "\n",
    "### 1. Normal Equation (Closed-Form)\n",
    "$$w = (\\Phi^T \\Phi)^{-1} \\Phi^T y$$\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 Exact optimal solution in one calculation\n",
    "- \u2705 No hyperparameters to tune\n",
    "- \u2705 No iterations needed\n",
    "\n",
    "**Cons:**\n",
    "- \u274c Requires matrix inversion: O(d\u00b3) complexity (slow for many features)\n",
    "- \u274c Doesn't scale to very large datasets (memory intensive)\n",
    "- \u274c Only works for problems with closed-form solutions\n",
    "\n",
    "### 2. Gradient Descent (Iterative)\n",
    "$$w = w - \\alpha \\nabla_w L$$\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 Scales well to large datasets (especially mini-batch/stochastic variants)\n",
    "- \u2705 Works for any differentiable loss function\n",
    "- \u2705 Foundation for neural networks and deep learning\n",
    "- \u2705 Can stop early if convergence is good enough\n",
    "\n",
    "**Cons:**\n",
    "- \u274c Requires tuning learning rate and iterations\n",
    "- \u274c Slower convergence (multiple iterations)\n",
    "- \u274c Very sensitive to feature scaling\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "| Scenario | Best Choice |\n",
    "|----------|-------------|\n",
    "| Small dataset (N < 10,000), few features (d < 1,000) | Normal Equation |\n",
    "| Large dataset (N > 100,000) | Gradient Descent (Mini-batch) |\n",
    "| Many features (d > 10,000) | Gradient Descent |\n",
    "| Neural networks, non-linear models | Gradient Descent (only option) |\n",
    "| Need exact optimal solution | Normal Equation |\n",
    "| Online learning (streaming data) | Stochastic Gradient Descent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Learning Rate: Critical Hyperparameter\n",
    "\n",
    "The **learning rate (\u03b1)** controls how big of a step we take in each iteration.\n",
    "\n",
    "### Impact of Different Learning Rates:\n",
    "\n",
    "**\u03b1 too small (e.g., 0.0001):**\n",
    "- \u2705 Stable convergence (doesn't overshoot)\n",
    "- \u274c Very slow (needs many iterations)\n",
    "- \u274c May get stuck in plateaus\n",
    "\n",
    "**\u03b1 optimal (e.g., 0.01-0.1):**\n",
    "- \u2705 Fast convergence\n",
    "- \u2705 Reaches minimum efficiently\n",
    "- \u2705 Smooth loss curve\n",
    "\n",
    "**\u03b1 too large (e.g., 1.0+):**\n",
    "- \u274c Overshoots minimum\n",
    "- \u274c Loss oscillates or increases\n",
    "- \u274c May diverge (loss \u2192 \u221e)\n",
    "\n",
    "We'll visualize these effects later in the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: You're training a model with gradient descent and observe that the loss is increasing rather than decreasing over iterations. What is the MOST likely cause?\n",
    ">\n",
    "> A. The model architecture is too simple to capture the underlying data patterns effectively\n",
    ">\n",
    "> B. The learning rate is too large, causing the optimizer to overshoot the minimum\n",
    ">\n",
    "> C. The features need standardization because different scales are destabilizing gradient magnitudes\n",
    ">\n",
    "> D. The convergence tolerance is set too loose, allowing premature stopping at suboptimal solutions\n",
    "\n",
    "<details><summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "- **A is FALSE**: Model underfitting means the model can't fit the training data well, resulting in HIGH but STABLE loss that plateaus at a suboptimal value. The loss would remain consistently high across iterations, not increase over time. If loss is increasing, the optimization process itself is failing, not the model's representational capacity.\n",
    "- **B is TRUE**: When the learning rate \u03b1 is too large, the weight update w_new = w_old - \u03b1\u2207w overshoots the minimum. Instead of moving toward the optimal point, it jumps past it to a worse position with higher loss. In extreme cases, this causes divergence where loss \u2192 \u221e. The classic symptom of excessive learning rate is monotonically increasing loss or wild oscillations.\n",
    "- **C is FALSE**: While feature scaling IS very important for gradient descent stability, unscaled features typically cause SLOW and erratic convergence with oscillating loss, not monotonically increasing loss. Unscaled features create elongated loss surfaces that require careful learning rate tuning, but the loss would still trend downward overall, just very slowly and unstably.\n",
    "- **D is FALSE**: Convergence tolerance controls when training stops (when loss change falls below threshold). If tolerance is too loose, training might stop early, but this would result in HIGH loss at stopping time, not INCREASING loss. Increasing loss indicates the optimizer is actively making things worse, not stopping too early.\n",
    "\n",
    "**Key Insight**: Increasing loss during training almost always indicates \u03b1 is too large. Solution: reduce learning rate by 10\u00d7 (e.g., 0.1 \u2192 0.01).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling: Critical for Gradient Descent!\n",
    "\n",
    "While feature scaling is recommended for Linear Regression's normal equation, it's **ESSENTIAL** for gradient descent.\n",
    "\n",
    "**Why is scaling so important for gradient descent?**\n",
    "\n",
    "1. **Convergence Speed:** Unscaled features create elongated loss surfaces\n",
    "   - Gradient descent zigzags instead of going straight to minimum\n",
    "   - Can be 100\u00d7 slower or more!\n",
    "   \n",
    "2. **Learning Rate Sensitivity:** Different features need different learning rates\n",
    "   - Small-scale features (0-1) might need \u03b1 = 0.1\n",
    "   - Large-scale features (0-10000) might need \u03b1 = 0.00001\n",
    "   - With one global \u03b1, impossible to optimize all features well\n",
    "   \n",
    "3. **Numerical Stability:** Large feature values can cause gradient explosion\n",
    "   - Gradients become huge \u2192 weights explode \u2192 overflow errors\n",
    "\n",
    "**Solution: Z-Score Standardization**\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This transforms all features to:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "- Similar scales \u2192 uniform convergence\n",
    "\n",
    "**Critical Rule:** Fit scaler on training data ONLY!\n",
    "```python\n",
    "scaler.fit(X_train)  # Learn \u03bc and \u03c3 from training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)    # Use same \u03bc and \u03c3\n",
    "X_test_scaled = scaler.transform(X_test)  # Use same \u03bc and \u03c3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode for Gradient Descent\n",
    "\n",
    "### Formal Pseudocode\n",
    "\n",
    "```\n",
    "============================================\n",
    "Inputs\n",
    "============================================\n",
    "X       \u2190 training features (N \u00d7 d matrix)\n",
    "y       \u2190 training targets (N \u00d7 1 vector)\n",
    "\u03b1       \u2190 learning rate (e.g., 0.01)\n",
    "max_iter \u2190 maximum iterations (e.g., 1000)\n",
    "tol     \u2190 convergence tolerance (e.g., 1e-6)\n",
    "\n",
    "============================================\n",
    "----- fit -----\n",
    "============================================\n",
    "1. Add bias column: \u03a6 \u2190 [1, X]  # (N \u00d7 (d+1))\n",
    "2. Initialize weights randomly: w \u2190 random small values\n",
    "3. For iteration = 1 to max_iter:\n",
    "     a. Compute predictions: \u0177 \u2190 \u03a6w\n",
    "     b. Compute errors: e \u2190 y - \u0177\n",
    "     c. Compute loss: L \u2190 (1/N) \u03a3 e\u00b2\n",
    "     d. Compute gradients: \u2207w \u2190 -(2/N) \u03a6\u1d40e\n",
    "     e. Update weights: w \u2190 w - \u03b1\u2207w\n",
    "     f. If |L_new - L_old| < tol: STOP (converged)\n",
    "4. Store final weights w\n",
    "\n",
    "============================================\n",
    "----- predict -----\n",
    "============================================\n",
    "For each query point in X_query:\n",
    "1. Add bias: \u03a6_query \u2190 [1, X_query]\n",
    "2. Compute prediction: \u0177 \u2190 \u03a6_query \u00b7 w\n",
    "3. Return \u0177\n",
    "```\n",
    "\n",
    "### Key Observations\n",
    "- **Iterative process:** Weights improve gradually over multiple iterations\n",
    "- **Convergence check:** Stop when loss stops decreasing significantly\n",
    "- **Prediction:** Same as Linear Regression (just matrix multiplication)\n",
    "- **Memory efficient:** Only stores weights (not all training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Why Gradient Descent?\n\nGradient descent is one of several optimization methods available. Understanding WHY we use it (and when NOT to) is crucial for choosing the right algorithm.\n\n### Comparison with Alternative Optimizers\n\n| Method | Time Complexity | Memory | Best Use Case | Limitations |\n|--------|----------------|---------|---------------|-------------|\n| **Closed-Form (Normal Equation)** | O(d\u00b3) | O(d\u00b2) | Small d (<1000), convex problems | Doesn't scale; only works for linear models |\n| **Gradient Descent** | O(iterations \u00d7 N \u00d7 d) | O(d) | Large-scale, any differentiable loss | Requires tuning \u03b1; iterative |\n| **Newton's Method** | O(d\u00b3) per iteration | O(d\u00b2) | Small d, need fast convergence | Very expensive; requires Hessian |\n| **Stochastic GD** | O(iterations \u00d7 d) | O(d) | Very large N, online learning | Noisy; requires careful tuning |\n| **Coordinate Descent** | O(iterations \u00d7 d) | O(d) | Sparse problems, LASSO | Not for all loss functions |\n\n**Key Variables:**\n- **N** = number of training samples\n- **d** = number of features (dimensions)\n- **iterations** = typically 100-10,000 depending on convergence\n\n### When to Use Gradient Descent\n\n\u2705 **Choose Gradient Descent when:**\n- **Large-scale problems:** Millions of parameters (e.g., neural networks with d > 1,000,000)\n- **No closed-form solution exists:** Most non-linear models (neural nets, logistic regression)\n- **Memory-constrained:** Can't store d \u00d7 d matrices (common in big data)\n- **Online/streaming data:** Data arrives continuously, need to update model incrementally\n- **Non-convex optimization:** Need stochastic variants to escape local minima\n- **Deep learning:** The ONLY practical option for training neural networks\n\n\u274c **Don't Use Gradient Descent when:**\n- **Small problems with closed-form solutions:** Linear regression with N < 10,000 and d < 1,000\n  - Normal equation is faster and gives exact solution\n  - No hyperparameter tuning needed\n- **Need exact solution in one step:** Critical applications where iterative approximation isn't acceptable\n- **Second-order methods are feasible:** d < 100 and you can afford O(d\u00b3) computation\n  - Newton's method converges much faster (quadratic vs linear convergence)\n\n### Real-World Examples\n\n**Gradient Descent is ESSENTIAL for:**\n- Training neural networks (millions of parameters)\n- Logistic regression (no closed form)\n- Support Vector Machines with kernels\n- Deep learning models (CNNs, RNNs, Transformers)\n- Matrix factorization (e.g., recommender systems)\n\n**Normal Equation is BETTER for:**\n- Simple linear regression on tabular data (N < 10k, d < 1k)\n- Ridge regression (closed form: w = (X\u1d40X + \u03bbI)\u207b\u00b9X\u1d40y)\n- Prototyping and quick experimentation\n\n**The Bottom Line:**\nGradient descent is the **foundation of modern machine learning** because it:\n1. Scales to billions of parameters\n2. Works for ANY differentiable loss function\n3. Enables training of complex non-linear models\n4. Can be parallelized across GPUs/distributed systems\n\nWhile it requires careful tuning and is slower than closed-form solutions, its **flexibility and scalability** make it indispensable for real-world ML.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Custom Gradient Descent Class\n",
    "\n",
    "Below is a scaffold of the `MyGradientDescentRegressor` class. Fill in the TODO sections to complete the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class MyGradientDescentRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Custom Gradient Descent implementation for Linear Regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate (\u03b1) for gradient descent updates\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    tol : float, default=1e-6\n",
    "        Tolerance for convergence (stop if loss change < tol)\n",
    "    random_state : int, default=42\n",
    "        Random seed for weight initialization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    weights_ : array of shape (n_features + 1,)\n",
    "        Learned weights including bias term\n",
    "    loss_history_ : list\n",
    "        Loss value at each iteration\n",
    "    n_iter_ : int\n",
    "        Actual number of iterations performed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6, random_state=42):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # TODO: Create design matrix Phi by adding column of ones for bias term\n",
    "        Phi = None\n",
    "        \n",
    "        # TODO: Initialize weights randomly with small values (use self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        self.weights_ = None\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.loss_history_ = []\n",
    "        N = len(y)\n",
    "        \n",
    "        # Gradient Descent Loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            # TODO: Compute predictions using current weights\n",
    "            predictions = None\n",
    "            \n",
    "            # TODO: Compute errors (residuals)\n",
    "            errors = None\n",
    "            \n",
    "            # TODO: Compute loss (Mean Squared Error)\n",
    "            loss = None\n",
    "            \n",
    "            # Store loss\n",
    "            self.loss_history_.append(loss)\n",
    "            \n",
    "            # Check convergence\n",
    "            if iteration > 0 and abs(self.loss_history_[-2] - self.loss_history_[-1]) < self.tol:\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "            \n",
    "            # TODO: Compute gradients using the formula: -(2/N) * \u03a6\u1d40(y - \u0177)\n",
    "            gradients = None\n",
    "            \n",
    "            # TODO: Update weights using gradient descent update rule\n",
    "            pass\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the learned model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        # TODO: Create design matrix (same as in fit)\n",
    "        Phi = None\n",
    "        \n",
    "        # TODO: Compute predictions\n",
    "        y_pred = None\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Once you have filled in the implementation, let's test our custom gradient descent regressor on a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple test data\n",
    "np.random.seed(42)\n",
    "X_simple = np.array([[1], [2], [3], [4], [5]])\n",
    "y_simple = np.array([2, 4, 6, 8, 10])  # Perfect linear relationship: y = 2x\n",
    "\n",
    "# Fit model\n",
    "model = MyGradientDescentRegressor(learning_rate=0.01, max_iter=1000)\n",
    "model.fit(X_simple, y_simple)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_simple)\n",
    "\n",
    "print(\"Learned weights (w0=intercept, w1=slope):\", model.weights_)\n",
    "print(\"Expected: [0, 2] or very close to it\")\n",
    "print(\"\\nPredictions:\", predictions)\n",
    "print(\"Actual:     \", y_simple)\n",
    "print(\"\\nFinal MSE:\", model.loss_history_[-1])\n",
    "print(\"Expected: very close to 0\")\n",
    "print(f\"\\nConverged in {model.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Convergence\n",
    "\n",
    "Let's plot the loss over iterations to see how the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(model.loss_history_) + 1), model.loss_history_, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "plt.title('Gradient Descent Convergence', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {model.loss_history_[0]:.6f}\")\n",
    "print(f\"Final loss:   {model.loss_history_[-1]:.6f}\")\n",
    "print(f\"Improvement:  {(1 - model.loss_history_[-1]/model.loss_history_[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: In a well-tuned gradient descent setup, what should the loss curve look like?\n",
    ">\n",
    "> A. Monotonically decreasing at a constant rate until reaching exactly zero at convergence\n",
    ">\n",
    "> B. Decreasing rapidly at first, then gradually slowing and flattening near the minimum\n",
    ">\n",
    "> C. Fluctuating randomly around a central value with gradually decreasing variance over iterations\n",
    ">\n",
    "> D. Decreasing in distinct steps with plateaus between iterations where no progress occurs\n",
    "\n",
    "<details><summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "- **A is FALSE**: Loss rarely decreases at a constant rate or reaches exactly zero. The rate of decrease depends on the gradient magnitude, which changes as you approach the minimum (gradients get smaller \u2192 slower progress). For noisy data, loss plateaus at a positive value (residual error), not zero. A constant-rate decrease would indicate the learning rate isn't being adjusted for the changing gradient landscape.\n",
    "- **B is TRUE**: A healthy loss curve shows: (1) Rapid decrease initially when gradients are large and weights are far from optimal, (2) Gradual slowdown as gradients become smaller near the minimum, (3) Flattening/plateau at convergence when gradient \u2248 0. This \"fast then slow\" pattern is the signature of successful first-order optimization approaching a local minimum.\n",
    "- **C is FALSE**: Random fluctuation is characteristic of stochastic gradient descent (SGD) using single samples or small mini-batches, not well-tuned batch gradient descent. For batch GD using all training data, the gradient is deterministic and loss should decrease monotonically. While SGD's fluctuation can help escape shallow local minima, it's not the expected behavior for standard batch GD.\n",
    "- **D is FALSE**: Distinct steps with plateaus suggest the learning rate is poorly tuned or there are numerical precision issues. Smooth gradient descent should show continuous progress, not discrete jumps. Step-like behavior might indicate: batch updates (normal for mini-batch GD), learning rate schedules with sudden drops, or gradient clipping thresholds being hit.\n",
    "\n",
    "**Key Insight**: Loss should decrease monotonically (batch GD) or with downward trend (SGD/mini-batch) and flatten at convergence. The rate of decrease naturally slows as you approach the minimum.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset for Visualization\n",
    "\n",
    "Let's work with the same synthetic dataset from the Linear Regression lab to directly compare approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same data as in Linear Regression Code Walk Through\n",
    "np.random.seed(42)\n",
    "X_train = np.arange(-9.5, 8.5, 0.1).reshape(-1, 1)\n",
    "y_train = X_train.ravel() + 1 + np.random.normal(0, 2, len(X_train))\n",
    "\n",
    "print(f\"Training data: {len(X_train)} points\")\n",
    "print(f\"X range: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "print(f\"y range: [{y_train.min():.1f}, {y_train.max():.1f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Training Data: Linear Relationship with Noise', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Visualizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit your MyGradientDescentRegressor on the training data\n",
    "model = None\n",
    "\n",
    "print(f\"Learned weights: {model.weights_}\")\n",
    "print(f\"Model equation: y = {model.weights_[1]:.3f}x + {model.weights_[0]:.3f}\")\n",
    "print(f\"Converged in {model.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fit\n",
    "x_line = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
    "y_line = model.predict(x_line)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, edgecolors='black', linewidths=0.5, label='Training data')\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'GD fit: y={model.weights_[1]:.2f}x+{model.weights_[0]:.2f}')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Gradient Descent: Best Fit Line', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Convergence Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(model.loss_history_) + 1), model.loss_history_, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "plt.title('Training Loss Over Iterations', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {model.loss_history_[0]:.6f}\")\n",
    "print(f\"Final loss:   {model.loss_history_[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Learning Rate\n",
    "\n",
    "Let's experiment with different learning rates to see how they affect convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Diagnosing Convergence from Loss Curves\n\nLearning to read loss curves is a **critical debugging skill** for machine learning practitioners. Here's how to diagnose what's happening with your gradient descent optimization:\n\n### Pattern 1: Healthy Convergence \u2705\n\n**What it looks like:**\n- Loss decreases rapidly at first (steep decline)\n- Gradually slows down (curve flattens)\n- Stabilizes to a flat minimum\n- Smooth, monotonic decrease (no jumps or spikes)\n\n**Example behavior:**\n```\nIteration 1:    Loss = 100.00\nIteration 10:   Loss = 25.00\nIteration 50:   Loss = 5.20\nIteration 100:  Loss = 4.02\nIteration 200:  Loss = 4.00\nIteration 201:  Loss = 4.00  \u2190 Converged!\n```\n\n**What to do:**\n\u2705 **Training successful!** Model has converged to optimal solution.\n- Can safely stop training\n- Try increasing learning rate next time to converge faster\n- Current learning rate is well-tuned\n\n---\n\n### Pattern 2: Too Slow Convergence \u26a0\ufe0f\n\n**What it looks like:**\n- Loss decreasing, but very gradually\n- Linear decrease that doesn't flatten\n- Still decreasing at max_iter\n- Never reaches plateau\n\n**Example behavior:**\n```\nIteration 1:     Loss = 100.00\nIteration 100:   Loss = 95.00\nIteration 500:   Loss = 90.00\nIteration 1000:  Loss = 85.00  \u2190 Still decreasing!\n```\n\n**Diagnosis:**\n- Learning rate **too small** for this problem\n- Need many more iterations to converge\n- Wasting computation time\n\n**What to do:**\n1. **Increase \u03b1** by 2-10\u00d7: Try \u03b1 = 0.01 \u2192 0.05 or 0.1\n2. **Increase max_iter**: Allow 5000-10000 iterations\n3. **Check feature scaling**: Unscaled features can cause this!\n4. **Monitor:** Plot loss curve to verify improvement\n\n---\n\n### Pattern 3: Oscillating (Unstable) \u26a0\ufe0f\n\n**What it looks like:**\n- Loss bounces up and down\n- Zigzag pattern around some value\n- Never stabilizes\n- Average trend might be downward, but very noisy\n\n**Example behavior:**\n```\nIteration 1:   Loss = 100.00\nIteration 10:  Loss = 15.00\nIteration 11:  Loss = 25.00  \u2190 Jumped up!\nIteration 12:  Loss = 10.00  \u2190 Dropped again\nIteration 20:  Loss = 18.00\nIteration 30:  Loss = 12.00  \u2190 Bouncing around\n```\n\n**Diagnosis:**\n- Learning rate **too large**\n- Overshooting minimum on both sides\n- Steps are bigger than the valley width\n\n**What to do:**\n1. **Decrease \u03b1** by 2-10\u00d7: Try \u03b1 = 0.5 \u2192 0.1 or 0.05\n2. **Check feature scaling**: Unscaled features exacerbate this!\n3. **Use learning rate decay**: Start high, reduce over time\n4. **Try smaller steps**: \u03b1 = 0.01 is often safe starting point\n\n---\n\n### Pattern 4: Diverging (Exploding) \u274c\n\n**What it looks like:**\n- Loss **increases** over iterations\n- May reach infinity or NaN\n- Gets progressively worse\n- Model is getting further from optimal solution\n\n**Example behavior:**\n```\nIteration 1:   Loss = 100.00\nIteration 10:  Loss = 500.00\nIteration 20:  Loss = 2500.00\nIteration 30:  Loss = inf  \u2190 Exploded!\n```\n\n**Diagnosis:**\n- Learning rate **way too large**\n- Each step jumps far past the minimum\n- Gradient explosion (weights become huge)\n\n**What to do:**\n1. **Decrease \u03b1 dramatically**: Try \u03b1 = 0.5 \u2192 0.01 or even 0.001\n2. **Standardize features IMMEDIATELY**: Most common cause!\n3. **Check for NaN/Inf in data**: Data quality issues\n4. **Reinitialize weights**: Try different random seed\n\n---\n\n### Pattern 5: Stuck at High Loss (Plateau) \u26a0\ufe0f\n\n**What it looks like:**\n- Loss plateaus early at suboptimal value\n- Flattens out but loss is still high\n- No further progress after certain iteration\n- Converged, but to wrong solution\n\n**Example behavior:**\n```\nIteration 1:    Loss = 100.00\nIteration 10:   Loss = 50.00\nIteration 50:   Loss = 45.00\nIteration 100:  Loss = 45.00  \u2190 Stuck!\nIteration 500:  Loss = 45.00  \u2190 Still stuck!\n```\n\n**Possible causes:**\n- **Poor initialization:** Weights started in bad region\n- **Local minimum:** (Less common for linear regression, but happens in neural nets)\n- **Learning rate too small:** Can't escape saddle points\n- **Feature scaling issues:** Some features dominate\n\n**What to do:**\n1. **Re-initialize weights:** Try different `random_state`\n2. **Increase \u03b1 slightly:** Help escape plateaus\n3. **Check feature scaling:** Ensure all features are standardized\n4. **Try momentum:** Advanced technique to escape saddle points\n5. **Verify data quality:** Check for outliers or data issues\n\n---\n\n### Pattern 6: Step-Like Decrease (Unusual)\n\n**What it looks like:**\n- Loss decreases in distinct steps\n- Plateaus between jumps\n- Not smooth\n\n**Possible causes:**\n- Using mini-batch GD (this is normal for mini-batch!)\n- Learning rate schedule with discrete drops\n- Numerical precision issues\n\n**What to do:**\n- If using mini-batch: This is expected behavior \u2705\n- If using batch GD: Check learning rate schedule\n- Generally not a problem unless loss isn't decreasing overall\n\n---\n\n### Quick Diagnosis Flowchart\n\n```\nIs loss DECREASING overall?\n\u251c\u2500 YES: Is it SMOOTH and FLATTENING?\n\u2502   \u251c\u2500 YES \u2192 \u2705 Healthy convergence\n\u2502   \u2514\u2500 NO: Is it OSCILLATING/BOUNCING?\n\u2502       \u251c\u2500 YES \u2192 \u26a0\ufe0f Learning rate too large\n\u2502       \u2514\u2500 NO: Still decreasing at max_iter?\n\u2502           \u2514\u2500 YES \u2192 \u26a0\ufe0f Learning rate too small\n\u2514\u2500 NO: Is loss INCREASING?\n    \u251c\u2500 YES \u2192 \u274c Learning rate way too large\n    \u2514\u2500 NO: Is loss STUCK at high value?\n        \u2514\u2500 YES \u2192 \u26a0\ufe0f Poor initialization or local minimum\n```\n\n---\n\n### Practical Tips\n\n1. **Always plot loss curves** - Don't rely on final loss value alone\n2. **Use log scale** for y-axis when comparing multiple learning rates\n3. **Monitor first 10-20 iterations** - Catches divergence early\n4. **Expected shape:** Sharp drop \u2192 gradual slowdown \u2192 flat plateau\n5. **Save checkpoints:** Keep best weights seen so far (useful for oscillating loss)\n\n**Remember:** The loss curve tells you the entire story of your optimization. Learn to read it!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    model_lr = MyGradientDescentRegressor(learning_rate=lr, max_iter=100)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    \n",
    "    plt.plot(range(1, len(model_lr.loss_history_) + 1), model_lr.loss_history_,\n",
    "            linewidth=2, color=color, label=f'\u03b1 = {lr} ({model_lr.n_iter_} iter)')\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "plt.title('Impact of Learning Rate on Convergence', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see all curves\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- \u03b1 = 0.001: Very slow convergence (needs more iterations)\")\n",
    "print(\"- \u03b1 = 0.01:  Good convergence speed\")\n",
    "print(\"- \u03b1 = 0.1:   Fast convergence\")\n",
    "print(\"- \u03b1 = 0.5:   May oscillate or diverge (too large)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: You train gradient descent with \u03b1=0.001 for 100 iterations and the loss is still decreasing steadily. What should you do?\n",
    ">\n",
    "> A. Reduce the learning rate to \u03b1=0.0001 to ensure more stable and reliable convergence\n",
    ">\n",
    "> B. Increase max_iter to allow more iterations, or increase \u03b1 to converge faster\n",
    ">\n",
    "> C. Stop training now since 100 iterations provides sufficient convergence for most models\n",
    ">\n",
    "> D. Switch to the normal equation approach which guarantees finding the optimal solution faster\n",
    "\n",
    "<details><summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "- **A is FALSE**: The learning rate \u03b1=0.001 is already quite small. Decreasing it further to 0.0001 would make convergence even SLOWER, requiring even MORE iterations to reach the minimum. Since loss is steadily decreasing (not oscillating or diverging), there's no stability problem - the optimization is working correctly, just slowly.\n",
    "- **B is TRUE**: Steadily decreasing loss means gradient descent is working correctly but needs more time to converge. Two solutions: (1) Increase max_iter (e.g., to 1000 or 10000) to allow more iterations with the current learning rate, or (2) Increase \u03b1 (e.g., to 0.01 or 0.1) to take bigger steps and converge faster. The second option is usually more efficient for computational cost.\n",
    "- **C is FALSE**: There's no universal \"sufficient\" number of iterations. Required iterations depend on: learning rate, data size, feature scales, initialization, and convergence tolerance. If loss is still decreasing steadily after 100 iterations, the model hasn't converged yet. Stopping now would leave you with a suboptimal solution unnecessarily.\n",
    "- **D is FALSE**: \"Loss still decreasing steadily\" means gradient descent IS working properly - it just hasn't finished yet. This is not a failure case requiring a different algorithm. The normal equation would give the same final solution but doesn't provide insight into convergence behavior. For small problems, either approach works; switching algorithms mid-optimization is unnecessary.\n",
    "\n",
    "**Key Insight**: Steadily decreasing loss = working correctly but not converged yet. Solution: increase iterations or increase learning rate (carefully monitor for divergence). Oscillating/increasing loss = problem with learning rate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 1: Feature Scaling Impact\n\nFeature scaling is emphasized throughout the theory, but let's **actively see its impact** on gradient descent convergence!\n\n**Your task:**\nCompare gradient descent convergence with and without feature scaling on a dataset with different feature scales."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Feature Scaling Experiment\n# 1. Create dataset with different feature scales\n# 2. Train gradient descent WITHOUT scaling\n# 3. Train gradient descent WITH scaling (use StandardScaler)\n# 4. Compare convergence speed and final loss\n\nnp.random.seed(42)\n\n# Create data with different scales\n# Feature 1: small scale (0-1), Feature 2: large scale (0-1000)\nX_unscaled = np.random.rand(100, 2)\nX_unscaled[:, 1] *= 1000  # Scale second feature to 0-1000\ny_mixed = 3 * X_unscaled[:, 0] + 0.5 * X_unscaled[:, 1] + np.random.randn(100) * 0.1\n\nprint(\"Dataset with mixed scales:\")\nprint(f\"Feature 1 range: [{X_unscaled[:, 0].min():.2f}, {X_unscaled[:, 0].max():.2f}]\")\nprint(f\"Feature 2 range: [{X_unscaled[:, 1].min():.2f}, {X_unscaled[:, 1].max():.2f}]\")\n\n# TODO: Train model on UNSCALED data\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Training on UNSCALED features...\")\nprint(\"=\" * 70)\nmodel_unscaled = MyGradientDescentRegressor(learning_rate=0.01, max_iter=1000)\nmodel_unscaled.fit(X_unscaled, y_mixed)\n\nprint(f\"Converged in: {model_unscaled.n_iter_} iterations\")\nprint(f\"Final loss: {model_unscaled.loss_history_[-1]:.4f}\")\n\n# TODO: Scale the data and train on SCALED data\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Training on SCALED features...\")\nprint(\"=\" * 70)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_unscaled)\n\nmodel_scaled = MyGradientDescentRegressor(learning_rate=0.01, max_iter=1000)\nmodel_scaled.fit(X_scaled, y_mixed)\n\nprint(f\"Converged in: {model_scaled.n_iter_} iterations\")\nprint(f\"Final loss: {model_scaled.loss_history_[-1]:.4f}\")\n\n# TODO: Plot both loss curves for comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(model_unscaled.loss_history_, label='Unscaled features', linewidth=2, color='red')\nplt.plot(model_scaled.loss_history_, label='Scaled features', linewidth=2, color='green')\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Convergence: Unscaled vs Scaled', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\n# Zoom in on first 100 iterations\nplt.plot(model_unscaled.loss_history_[:100], label='Unscaled features', linewidth=2, color='red')\nplt.plot(model_scaled.loss_history_[:100], label='Scaled features', linewidth=2, color='green')\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('First 100 Iterations (Zoomed)', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSIS:\")\nprint(\"=\" * 70)\nprint(f\"Unscaled: {model_unscaled.n_iter_} iterations, final loss = {model_unscaled.loss_history_[-1]:.4f}\")\nprint(f\"Scaled:   {model_scaled.n_iter_} iterations, final loss = {model_scaled.loss_history_[-1]:.4f}\")\nprint(f\"\\nSpeedup from scaling: {model_unscaled.n_iter_ / model_scaled.n_iter_:.1f}x faster!\")\nprint(\"\\nObservation: Feature scaling dramatically improves convergence!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 2: Learning Rate Tuning\n\nYou've seen how different learning rates affect convergence. Now **actively experiment** to find the optimal learning rate!\n\n**Your task:**\nTest multiple learning rates and identify which one gives the best convergence."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Learning Rate Experiment\n# Test learning rates: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n# For each, train model and track:\n# - Number of iterations to convergence\n# - Final loss\n# - Convergence behavior (stable, oscillating, diverging)\n\nlearning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\nresults = []\n\nprint(\"Testing different learning rates on scaled data...\")\nprint(\"=\" * 70)\n\nfor lr in learning_rates:\n    model_lr = MyGradientDescentRegressor(learning_rate=lr, max_iter=500)\n    model_lr.fit(X_scaled, y_mixed)\n    \n    results.append({\n        'lr': lr,\n        'n_iter': model_lr.n_iter_,\n        'final_loss': model_lr.loss_history_[-1],\n        'history': model_lr.loss_history_\n    })\n    \n    status = \"\u2713 Good\" if model_lr.n_iter_ < 500 else \"\u26a0 Slow\"\n    if model_lr.loss_history_[-1] > model_lr.loss_history_[0]:\n        status = \"\u274c Diverged\"\n    \n    print(f\"\u03b1 = {lr:5.3f}: {model_lr.n_iter_:3d} iter, final loss = {model_lr.loss_history_[-1]:.4f} {status}\")\n\n# TODO: Plot all loss curves\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nfor res in results:\n    plt.plot(res['history'], label=f\"\u03b1 = {res['lr']}\", linewidth=2)\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Learning Rate Comparison', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\n# Bar chart of iterations to convergence\nlrs = [res['lr'] for res in results]\niters = [res['n_iter'] for res in results]\nplt.bar(range(len(lrs)), iters, color=['green' if i < 500 else 'red' for i in iters])\nplt.xticks(range(len(lrs)), [f\"{lr:.3f}\" for lr in lrs])\nplt.xlabel('Learning Rate (\u03b1)', fontsize=12)\nplt.ylabel('Iterations to Convergence', fontsize=12)\nplt.title('Convergence Speed vs Learning Rate', fontsize=14)\nplt.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Find optimal learning rate\nbest = min(results, key=lambda x: x['n_iter'])\nprint(f\"\\n\u2728 Optimal learning rate: \u03b1 = {best['lr']} ({best['n_iter']} iterations)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 3: Validate Against sklearn\n\nLet's verify your implementation by comparing it with sklearn's SGDRegressor!\n\n**Your task:**\nTrain both your custom implementation and sklearn's SGDRegressor, then compare results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Compare with sklearn's SGDRegressor\n# 1. Train your MyGradientDescentRegressor\n# 2. Train sklearn's SGDRegressor with same hyperparameters\n# 3. Compare:\n#    - Final weights (should be very similar)\n#    - Final loss\n#    - Number of iterations\n\nfrom sklearn.linear_model import SGDRegressor\n\nprint(\"=\" * 70)\nprint(\"Comparing Custom Implementation vs sklearn.linear_model.SGDRegressor\")\nprint(\"=\" * 70)\n\n# Train your custom implementation\nprint(\"\\n1. Training custom MyGradientDescentRegressor...\")\nprint(\"-\" * 70)\ncustom_model = MyGradientDescentRegressor(learning_rate=0.01, max_iter=1000, random_state=42)\ncustom_model.fit(X_scaled, y_mixed)\n\nprint(f\"Converged in: {custom_model.n_iter_} iterations\")\nprint(f\"Final loss: {custom_model.loss_history_[-1]:.6f}\")\nprint(f\"Weights: {custom_model.weights_}\")\n\n# Train sklearn's implementation\nprint(\"\\n2. Training sklearn.linear_model.SGDRegressor...\")\nprint(\"-\" * 70)\nsklearn_model = SGDRegressor(\n    learning_rate='constant',\n    eta0=0.01,  # Learning rate\n    max_iter=1000,\n    tol=1e-6,\n    random_state=42,\n    fit_intercept=True\n)\nsklearn_model.fit(X_scaled, y_mixed)\n\nprint(f\"Converged in: {sklearn_model.n_iter_} iterations\")\n# Compute final loss manually\ny_pred_sklearn = sklearn_model.predict(X_scaled)\nsklearn_loss = np.mean((y_mixed - y_pred_sklearn) ** 2)\nprint(f\"Final loss: {sklearn_loss:.6f}\")\nprint(f\"Weights: [intercept: {sklearn_model.intercept_[0]:.4f}, coefficients: {sklearn_model.coef_}]\")\n\n# Compare predictions\nprint(\"\\n3. Comparing predictions...\")\nprint(\"-\" * 70)\ny_pred_custom = custom_model.predict(X_scaled)\ny_pred_sklearn = sklearn_model.predict(X_scaled)\n\nmse_diff = np.mean((y_pred_custom - y_pred_sklearn) ** 2)\nprint(f\"MSE between predictions: {mse_diff:.8f}\")\nprint(f\"Predictions very similar? {mse_diff < 0.001}\")\n\n# Visualize comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_mixed, y_pred_custom, alpha=0.6, label='Custom', s=50)\nplt.scatter(y_mixed, y_pred_sklearn, alpha=0.6, label='sklearn', s=50, marker='x')\nplt.plot([y_mixed.min(), y_mixed.max()], [y_mixed.min(), y_mixed.max()], 'r--', linewidth=2)\nplt.xlabel('True Values', fontsize=12)\nplt.ylabel('Predicted Values', fontsize=12)\nplt.title('Predictions: Custom vs sklearn', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(custom_model.loss_history_, label='Custom GD', linewidth=2)\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Custom Implementation Convergence', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u2705 VALIDATION SUCCESSFUL!\")\nprint(\"=\" * 70)\nprint(\"Your implementation matches sklearn's behavior!\")\nprint(f\"Weight difference is minimal (MSE < {mse_diff:.6f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Practices for Gradient Descent\n\nThis section synthesizes professional-level guidance for applying gradient descent effectively in real-world machine learning projects.\n\n---\n\n### 1. Learning Rate Selection\n\nThe learning rate \u03b1 is the **most critical hyperparameter** in gradient descent.\n\n**Starting Point:**\n- For **standardized features**: Start with \u03b1 = 0.01\n- For **unscaled features**: Start with \u03b1 = 0.0001 (or better, standardize first!)\n- Typical working range: 0.001 to 0.1\n\n**Tuning Strategy:**\n```python\n# Try multiple learning rates\nlearning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\nfor lr in learning_rates:\n    model = MyGradientDescentRegressor(learning_rate=lr)\n    model.fit(X_train, y_train)\n    # Plot loss curves and compare\n```\n\n**Advanced Techniques:**\n- **Learning rate schedules:** Decay \u03b1 over time (e.g., \u03b1_t = \u03b1_0 / (1 + decay \u00d7 t))\n- **Adaptive methods:** Adam, RMSprop, AdaGrad (auto-adjust \u03b1 per parameter)\n- **Warm restarts:** Periodically reset \u03b1 to escape plateaus\n- **Line search:** Find optimal \u03b1 per iteration (expensive but effective)\n\n**Rule of Thumb:**\n- If loss oscillates \u2192 \u03b1 too large \u2192 decrease by 2-10\u00d7\n- If loss decreases slowly \u2192 \u03b1 too small \u2192 increase by 2-10\u00d7\n- Monitor loss curve for first 10-20 iterations to diagnose quickly\n\n---\n\n### 2. Feature Scaling\n\nFeature scaling is **MANDATORY** for gradient descent (optional for normal equation).\n\n**Why it's critical:**\n- Unscaled features create elongated loss surfaces \u2192 zigzag convergence\n- Different features need different learning rates \u2192 impossible with one global \u03b1\n- Large feature values cause gradient explosion \u2192 NaN/Inf errors\n\n**Best Practice: Z-Score Standardization**\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)                    # Learn \u03bc and \u03c3 from training data ONLY\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)  # Apply same \u03bc and \u03c3\nX_test_scaled = scaler.transform(X_test)\n```\n\n**CRITICAL:** \n- \u2705 **Fit scaler on training data ONLY** (avoid data leakage!)\n- \u2705 **Transform all sets** (train, val, test) using the same scaler\n- \u2705 **Scale before training** gradient descent\n- \u274c **Never fit scaler on val/test data**\n\n**Alternative Scaling Methods:**\n- **MinMaxScaler:** Scales to [0, 1] - good for bounded data\n- **RobustScaler:** Uses median/IQR - robust to outliers\n- **Normalizer:** Scales each sample to unit norm - for text/sparse data\n\n---\n\n### 3. Weight Initialization\n\nPoor initialization can lead to slow convergence or getting stuck in bad regions.\n\n**Best Practice: Small Random Values**\n```python\nnp.random.seed(42)\nweights = np.random.randn(n_features) * 0.01  # Small values near 0\n```\n\n**Initialization Strategies:**\n- **Random small values:** Standard for linear models\n- **Zeros:** \u274c Creates symmetry problem (all neurons learn same thing)\n- **Xavier/Glorot:** For deep networks: `w ~ N(0, sqrt(2/(n_in + n_out)))`\n- **He initialization:** For ReLU networks: `w ~ N(0, sqrt(2/n_in))`\n\n**Why small values?**\n- Large initial weights \u2192 large initial gradients \u2192 potential divergence\n- Too small \u2192 very slow initial learning\n- ~0.01 is good starting point\n\n---\n\n### 4. Convergence Criteria\n\nDecide when to stop training to avoid wasted computation.\n\n**Method 1: Maximum Iterations**\n```python\nmax_iter = 1000  # Conservative default\n# For large/complex problems, try 5000-10000\n```\n\n**Method 2: Tolerance on Loss Change**\n```python\nif abs(loss[t] - loss[t-1]) < tol:  # e.g., tol=1e-6\n    break  # Converged!\n```\n\n**Method 3: Gradient Norm**\n```python\nif np.linalg.norm(gradient) < tol:  # e.g., tol=1e-6\n    break  # At critical point\n```\n\n**Method 4: Early Stopping (for ML models with validation set)**\n```python\nif val_loss hasn't improved for 10 epochs:\n    break  # Prevent overfitting\n```\n\n**Recommendation:**\n- Use **both** max_iter AND tolerance for robustness\n- Set reasonable max_iter (don't run forever if not converging)\n- Monitor loss curve to verify convergence\n\n---\n\n### 5. Monitoring and Logging\n\nAlways track optimization progress for debugging and analysis.\n\n**Essential Metrics to Log:**\n```python\nhistory = {\n    'loss': [],           # Loss at each iteration\n    'gradients': [],      # Gradient norms (for debugging)\n    'learning_rate': [],  # If using decay/schedules\n    'iteration': []\n}\n```\n\n**Visualization:**\n```python\n# Loss curve (most important!)\nplt.plot(history['loss'])\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.yscale('log')  # Log scale helpful for comparing rates\n\n# Gradient norms (check for explosion/vanishing)\nplt.plot(history['gradients'])\n```\n\n**Red Flags to Watch:**\n- \u274c Loss increasing \u2192 \u03b1 too large\n- \u274c Loss = NaN/Inf \u2192 Gradient explosion (scale features!)\n- \u274c Loss plateaus early at high value \u2192 Poor initialization or stuck\n- \u2705 Loss decreasing and flattening \u2192 Healthy convergence\n\n---\n\n### 6. When Gradient Descent Fails\n\nUnderstand limitations and failure modes to troubleshoot effectively.\n\n**Problem 1: Non-Convex Landscapes**\n- Neural networks have many local minima\n- **Solution:** Use stochastic/mini-batch GD (noise helps escape), momentum, or adaptive methods\n\n**Problem 2: Saddle Points**\n- Points where gradient = 0 but not a minimum\n- Common in high-dimensional spaces\n- **Solution:** Momentum methods (accumulate velocity to push through)\n\n**Problem 3: Ill-Conditioned Problems**\n- Loss surface has very different curvatures in different directions\n- One learning rate can't work for all directions\n- **Solution:** Feature scaling, adaptive methods (Adam), or second-order methods\n\n**Problem 4: Vanishing/Exploding Gradients**\n- Gradients become too small (learning stops) or too large (divergence)\n- Common in deep networks\n- **Solution:** Proper initialization, gradient clipping, batch normalization\n\n**Problem 5: Poor Feature Scaling**\n- **Most common cause** of gradient descent failure!\n- **Solution:** ALWAYS standardize features first!\n\n---\n\n### 7. Advanced Techniques\n\nOnce you master basic gradient descent, explore these enhancements:\n\n#### Momentum\nAccumulates gradient history to smooth updates and accelerate convergence.\n```python\nvelocity = 0.9 * velocity + learning_rate * gradient\nweights = weights - velocity\n```\n**Benefits:** Faster convergence, dampens oscillations, escapes plateaus\n\n#### Adaptive Learning Rates\nMethods that adjust learning rate automatically per parameter.\n\n| Method | Key Idea | Use Case |\n|--------|----------|----------|\n| **AdaGrad** | Larger updates for infrequent features | Sparse data, NLP |\n| **RMSprop** | Exponential moving average of gradients | RNNs, non-stationary problems |\n| **Adam** | Momentum + RMSprop | Default for deep learning (most popular!) |\n\n**Adam is the industry standard** for deep learning - combines best of both worlds.\n\n#### Learning Rate Schedules\nReduce \u03b1 over time for better convergence.\n- **Step decay:** Reduce \u03b1 by 10\u00d7 every N epochs\n- **Exponential decay:** \u03b1_t = \u03b1_0 * e^(-kt)\n- **Cosine annealing:** \u03b1_t = \u03b1_min + 0.5(\u03b1_max - \u03b1_min)(1 + cos(\u03c0t/T))\n\n#### Line Search\nFind optimal step size per iteration (expensive but effective).\n```python\n# Instead of fixed \u03b1, search for best \u03b1 per iteration\n\u03b1_optimal = argmin_\u03b1 Loss(w - \u03b1 * gradient)\n```\n\n---\n\n### 8. Gradient Descent Checklist\n\nBefore training with gradient descent, verify:\n\n- \u2705 **Features are standardized** (StandardScaler fitted on training data)\n- \u2705 **Learning rate is reasonable** (start with 0.01)\n- \u2705 **Weights initialized** with small random values\n- \u2705 **Max iterations set** (1000-10000 depending on problem)\n- \u2705 **Convergence tolerance set** (e.g., 1e-6)\n- \u2705 **Loss history tracked** for visualization\n- \u2705 **Random seed set** for reproducibility\n\nDuring training, monitor:\n\n- \u2705 **Loss curve** - Should decrease and flatten\n- \u2705 **First 10 iterations** - Catch divergence early\n- \u2705 **Convergence message** - Verify it actually converged\n- \u2705 **Final loss value** - Compare with expected range\n\nAfter training, validate:\n\n- \u2705 **Compare with closed-form solution** (for linear regression)\n- \u2705 **Check learned weights** - Do they make sense?\n- \u2705 **Verify predictions** - Test on held-out data\n- \u2705 **Inspect residuals** - Look for patterns (suggests model issues)\n\n---\n\n### 9. Comparing Gradient Descent Variants\n\n| Variant | Samples/Iteration | Memory | Speed/Iter | Convergence | Best For |\n|---------|------------------|---------|------------|-------------|----------|\n| **Batch GD** | All N | O(N\u00d7d) | Slow | Smooth, stable | Small data (N<10k) |\n| **Stochastic GD** | 1 | O(d) | Very fast | Noisy, erratic | Online learning |\n| **Mini-Batch GD** | B (32-256) | O(B\u00d7d) | Fast | Smooth-ish | Large data, deep learning |\n\n**Industry Practice:**\n- **Small datasets (N < 10,000):** Use batch GD or normal equation\n- **Large datasets (N > 100,000):** Use mini-batch GD with B=32-256\n- **Online learning (streaming data):** Use stochastic GD with learning rate decay\n- **Deep learning:** Mini-batch GD with Adam optimizer (almost universal)\n\n---\n\n### 10. Summary: Gradient Descent in Practice\n\n**Key Takeaways:**\n\n1. **Gradient Descent = Foundation of Modern ML**\n   - Only practical option for neural networks\n   - Scales to billions of parameters\n   - Works for any differentiable loss function\n\n2. **Feature Scaling is Non-Negotiable**\n   - **#1 cause** of gradient descent failure\n   - Always use StandardScaler (fit on training data only!)\n   - Enables faster, more stable convergence\n\n3. **Learning Rate is Critical**\n   - Start with \u03b1 = 0.01 for scaled features\n   - Monitor loss curve to diagnose issues\n   - Too large \u2192 divergence, too small \u2192 slow convergence\n\n4. **Monitor Convergence**\n   - Always plot loss curves\n   - Check first 10-20 iterations for early warning signs\n   - Use both max_iter and tolerance for robustness\n\n5. **Advanced Methods Help**\n   - Adam optimizer for deep learning\n   - Momentum for faster convergence\n   - Learning rate schedules for fine-tuning\n\n**When to Use Gradient Descent:**\n- \u2705 Neural networks (only option)\n- \u2705 Large datasets (N > 100,000)\n- \u2705 No closed-form solution exists\n- \u2705 Online/streaming data\n\n**When NOT to Use:**\n- \u274c Small linear regression (use normal equation)\n- \u274c Need exact solution in one step\n- \u274c Can afford second-order methods\n\n**The Bottom Line:**\nGradient descent is the workhorse of modern machine learning. Master its fundamentals (learning rate, feature scaling, convergence monitoring) and you'll be equipped to train everything from simple linear models to state-of-the-art deep neural networks."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: You have a dataset with 10 million training examples and want to train a linear regression model. Which optimization approach is MOST practical?\n",
    ">\n",
    "> A. Normal equation using closed-form solution for guaranteed optimal weights in one computation\n",
    ">\n",
    "> B. Batch gradient descent processing all 10 million samples in each iteration for exact gradients\n",
    ">\n",
    "> C. Mini-batch gradient descent with batches of 128-256 samples for efficiency and stability\n",
    ">\n",
    "> D. Stochastic gradient descent using exactly one random sample per iteration for maximum speed\n",
    "\n",
    "<details><summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: C**\n",
    "\n",
    "**Explanation:**\n",
    "- **A is FALSE**: The normal equation requires computing (\u03a6\u1d40\u03a6)\u207b\u00b9\u03a6\u1d40y, where \u03a6 is 10M \u00d7 d. This involves: (1) Matrix multiplication O(Nd\u00b2) \u2248 billions of operations, (2) Matrix inversion O(d\u00b3), (3) Storing a 10M \u00d7 d matrix in memory (potentially gigabytes). For N=10M, this is extremely slow and memory-intensive. The normal equation doesn't scale to large datasets.\n",
    "- **B is FALSE**: Batch GD must process all 10 million samples in EACH iteration to compute one gradient. Even if each iteration takes 10 seconds and you need 100 iterations, that's 1000 seconds (16 minutes) of computation. Additionally, loading all 10M samples into memory simultaneously is impractical. Full batch GD doesn't scale to large datasets.\n",
    "- **C is TRUE**: Mini-batch GD with batch size B=128 means: (1) Only 128 samples in memory at once (feasible), (2) Fast iterations (~milliseconds each), (3) Can stream data from disk in chunks, (4) Parallelizes well on GPUs, (5) Good gradient estimates with much less computation. With B=128 and N=10M, each epoch is ~78,000 mini-batches, but each is very fast. This is standard for large-scale ML.\n",
    "- **D is FALSE**: While SGD (B=1) has very fast iterations, it's TOO noisy for stable convergence on 10M samples. The gradient from a single sample is a poor estimate of the true gradient direction, requiring many epochs and careful learning rate decay. Mini-batch (B=128-256) provides better balance: more stable than pure SGD, much faster than batch GD.\n",
    "\n",
    "**Key Insight**: For large datasets (N > 100,000), mini-batch gradient descent is the practical choice. It balances computational efficiency, memory usage, and convergence stability. This is why deep learning uses mini-batch GD almost exclusively.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Gradient Descent is an iterative optimization algorithm**\n",
    "   - Updates weights in direction that reduces loss: w = w - \u03b1\u2207w L\n",
    "   - Converges to optimal solution through many small steps\n",
    "   - Foundation for training neural networks and deep learning\n",
    "\n",
    "2. **Learning rate (\u03b1) is critical**\n",
    "   - Too small \u2192 slow convergence (many iterations needed)\n",
    "   - Too large \u2192 divergence (loss increases or oscillates)\n",
    "   - Typical range: 0.001 to 0.1\n",
    "   - Monitor loss curve to diagnose issues\n",
    "\n",
    "3. **Feature scaling is ESSENTIAL for gradient descent**\n",
    "   - Unscaled features cause slow/unstable convergence\n",
    "   - Different features need different step sizes \u2192 impossible with one \u03b1\n",
    "   - Always use StandardScaler or MinMaxScaler\n",
    "   - Fit scaler on training data ONLY!\n",
    "\n",
    "4. **Convergence monitoring**\n",
    "   - Plot loss over iterations\n",
    "   - Healthy curve: decreasing and flattening\n",
    "   - Stop when loss change < tolerance\n",
    "   - Early stopping prevents wasted computation\n",
    "\n",
    "5. **Gradient descent vs Normal equation**\n",
    "   - Normal equation: Fast for small data, exact solution, no tuning\n",
    "   - Gradient descent: Scales to large data, works for any model, needs tuning\n",
    "   - Both converge to same solution for linear regression\n",
    "\n",
    "### When to Use Gradient Descent\n",
    "\n",
    "\u2705 **Use Gradient Descent when:**\n",
    "- Training neural networks (only option available)\n",
    "- Dataset is very large (N > 100,000)\n",
    "- Online learning (data arrives in streams)\n",
    "- Need mini-batch or stochastic variants\n",
    "- Working with distributed systems (can parallelize mini-batches)\n",
    "\n",
    "\u274c **Use Normal Equation when:**\n",
    "- Small-medium dataset (N < 10,000)\n",
    "- Few features (d < 1,000)\n",
    "- Want exact solution without tuning\n",
    "- Simple linear regression\n",
    "\n",
    "### Gradient Descent Variants Summary\n",
    "\n",
    "| Variant | Samples/Iter | Best For |\n",
    "|---------|--------------|----------|\n",
    "| **Batch GD** | All N | Small datasets, smooth convergence |\n",
    "| **Stochastic GD** | 1 | Online learning, escaping local minima |\n",
    "| **Mini-Batch GD** | 32-256 | Large datasets, deep learning (MOST COMMON) |\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "- \u2705 Always standardize features using StandardScaler\n",
    "- \u2705 Fit scaler on training data only (avoid data leakage)\n",
    "- \u2705 Start with learning rate \u03b1 = 0.01, adjust based on loss curve\n",
    "- \u2705 Monitor loss over iterations to diagnose convergence\n",
    "- \u2705 Use early stopping to prevent wasted computation\n",
    "- \u2705 Initialize weights with small random values\n",
    "- \u2705 For large data, use mini-batch variant (B=32-256)\n",
    "- \u2705 Compare with closed-form solution when possible (validation)\n",
    "- \u2705 Use learning rate decay for better convergence (advanced)\n",
    "- \u2705 Visualize loss curves to understand convergence behavior\n",
    "\n",
    "### Debugging Gradient Descent\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Loss increasing | \u03b1 too large | Decrease learning rate by 10\u00d7 |\n",
    "| Loss oscillating | \u03b1 too large | Decrease learning rate |\n",
    "| Very slow convergence | \u03b1 too small OR unscaled features | Increase \u03b1 or standardize features |\n",
    "| Loss stuck at high value | Poor initialization OR bad \u03b1 | Try different random seed or \u03b1 |\n",
    "| NaN/Inf values | Gradient explosion | Standardize features, decrease \u03b1 |\n",
    "| Doesn't converge after 10k iter | Unscaled features | Standardize features! |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Final Question**: You're training a neural network and observe that the training loss decreases smoothly for 50 epochs, then suddenly starts increasing. What is the MOST likely explanation?\n",
    ">\n",
    "> A. The model has successfully converged to the optimal region and is now oscillating around it\n",
    ">\n",
    "> B. The learning rate should be increased to accelerate convergence and escape local plateaus\n",
    ">\n",
    "> C. The learning rate is too high for later epochs and needs decay or reduction\n",
    ">\n",
    "> D. Feature standardization was skipped, causing numerical instability in later training stages only\n",
    "\n",
    "<details><summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: C**\n",
    "\n",
    "**Explanation:**\n",
    "- **A is FALSE**: If the model had converged optimally, the loss would plateau (stay relatively constant with small fluctuations), not suddenly increase. Increasing loss means the model is moving AWAY from the optimal solution, which is the opposite of convergence. Oscillation around a minimum would show small fluctuations, not systematic increase.\n",
    "- **B is FALSE**: If loss is already increasing after epoch 50, making the learning rate LARGER will make the problem dramatically worse! The model is already overshooting the minimum, so bigger steps would cause even more overshooting and potentially divergence to infinity. This would accelerate the problem, not solve it.\n",
    "- **C is TRUE**: This is a classic pattern: initially, when weights are far from optimal, a larger learning rate (e.g., \u03b1=0.1) works well for rapid progress. But as the model approaches the minimum (after ~50 epochs), those same large steps start overshooting. The loss surface becomes very narrow near the minimum, so the learning rate that worked early now causes instability. Solution: learning rate decay/scheduling (reduce \u03b1 over time).\n",
    "- **D is FALSE**: If features weren't standardized, you'd see problems from the VERY FIRST epoch - extremely slow convergence, erratic oscillations, or immediate divergence. The fact that loss decreased smoothly for 50 epochs proves features were properly scaled. This issue starting at epoch 50 indicates a learning rate problem specific to later training stages, not a data preprocessing issue.\n",
    "\n",
    "**Key Insight**: Loss decreasing then increasing suggests the learning rate is too large for the current optimization stage. Use learning rate schedules/decay: start high for fast initial progress, reduce over time for stable convergence near the minimum. This is standard practice in deep learning (e.g., cosine annealing, step decay).\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}