{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Gradient%20Descent/Gradient%20Descent%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Hands-On Lab\n",
    "\n",
    "In this lab, you will implement Gradient Descent optimization from scratch, understand the mathematics behind it, and apply it to real data. Along the way, you'll answer conceptual questions and create visualizations to deepen your understanding.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the mathematics of Gradient Descent optimization\n",
    "- Implement a custom Gradient Descent class from scratch\n",
    "- Visualize convergence and loss curves\n",
    "- Understand the impact of learning rate on convergence\n",
    "- Apply feature scaling and understand why it's critical for gradient descent\n",
    "- Compare gradient descent with closed-form solutions\n",
    "- Understand batch, stochastic, and mini-batch variants\n",
    "- Analyze model performance and convergence behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Overview of Gradient Descent\n\nGradient Descent is an **iterative optimization algorithm** used to find the minimum of a function. In machine learning, we use it to **minimize the loss function** and find optimal model parameters.\n\n**Key Idea:**\n- Start with **random weights**\n- Iteratively **update weights** in the direction that reduces loss\n- Take steps proportional to the **negative gradient** of the loss function\n- Continue until **convergence** (loss stops decreasing)\n\n**The Update Rule:**\n$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\nabla_w L$$\n\nWhere:\n- **w** are the model weights (parameters)\n- **α** is the learning rate (step size)\n- **∇w L** is the gradient of the loss with respect to weights\n- **L** is the loss function (e.g., Sum of Squared Errors)\n\n**For Linear Regression with SSE Loss:**\n- Loss: $L(w) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n- Gradient: $\\nabla_w L = -2 \\Phi^T (y - \\Phi w)$\n- Update: $w = w - \\alpha \\nabla_w L$\n- Expanded form: $w = w - \\alpha(-2\\Phi^T(y - \\Phi w)) = w + 2\\alpha \\Phi^T (y - \\Phi w)$\n\n**Advantages:**\n- Works when no closed-form solution exists (e.g., neural networks)\n- Scales well to large datasets\n- Can be adapted to stochastic/mini-batch variants for efficiency\n- Foundation for deep learning optimization\n\n**Disadvantages:**\n- Requires tuning hyperparameters (learning rate, iterations)\n- Can be slow to converge\n- May get stuck in local minima (for non-convex functions)\n- Sensitive to feature scaling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: Gradient Descent finds optimal weights by:\n>\n> A. Computing the exact optimal solution directly in one step using matrix inversion operations and ordinary least squares without any iterative optimization\n>\n> B. Iteratively updating weights in the direction opposite to the gradient, which is the direction that most quickly reduces the loss function\n>\n> C. Evaluating multiple random weight configurations through grid search and selecting the specific combination that achieves the lowest validation error score\n>\n> D. Approximating the closed-form solution through successive linearizations of the loss surface using second-order Taylor series expansion and Hessian matrices\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: B**\n\n**Key Insight:** Gradient descent is a first-order iterative optimization algorithm that uses calculus to find the direction of steepest descent (negative gradient) and repeatedly updates weights by taking small steps in that direction: w_new = w_old - α∇w L.\n\n**Explanation:**\n- **A is FALSE**: This describes the normal equation for Linear Regression: w = (ΦᵀΦ)⁻¹Φᵀy. The normal equation computes the exact optimal solution in one step through direct matrix operations, without any iteration. Gradient descent, in contrast, is an iterative method that gradually approaches the optimal solution through many small updates.\n- **B is TRUE**: Gradient descent computes the gradient ∇w L (the direction of steepest ascent) and updates weights in the opposite direction (steepest descent): w_new = w_old - α∇w L. By repeatedly taking steps in the direction that reduces loss most quickly, it converges to a minimum. The learning rate α controls step size.\n- **C is FALSE**: While this describes a valid optimization approach (grid search or random search), it's not gradient descent. Gradient descent uses calculus-based gradients to determine the exact direction to move, not trial-and-error evaluation of different configurations. Grid search would be extremely inefficient for high-dimensional problems (e.g., millions of parameters in neural networks).\n- **D is FALSE**: This might describe methods like Newton's method or successive quadratic approximations, which use second-order information (Hessian matrix containing second derivatives). Gradient descent uses only first-order gradients and doesn't approximate closed-form solutions - it directly minimizes the loss iteratively without needing the Hessian.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent vs Normal Equation (Closed-Form Solution)\n",
    "\n",
    "For Linear Regression, we have **two ways** to find optimal weights:\n",
    "\n",
    "### 1. Normal Equation (Closed-Form)\n",
    "$$w = (\\Phi^T \\Phi)^{-1} \\Phi^T y$$\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Exact optimal solution in one calculation\n",
    "- ✅ No hyperparameters to tune\n",
    "- ✅ No iterations needed\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Requires matrix inversion: O(d³) complexity (slow for many features)\n",
    "- ❌ Doesn't scale to very large datasets (memory intensive)\n",
    "- ❌ Only works for problems with closed-form solutions\n",
    "\n",
    "### 2. Gradient Descent (Iterative)\n",
    "$$w = w - \\alpha \\nabla_w L$$\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Scales well to large datasets (especially mini-batch/stochastic variants)\n",
    "- ✅ Works for any differentiable loss function\n",
    "- ✅ Foundation for neural networks and deep learning\n",
    "- ✅ Can stop early if convergence is good enough\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Requires tuning learning rate and iterations\n",
    "- ❌ Slower convergence (multiple iterations)\n",
    "- ❌ Very sensitive to feature scaling\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "| Scenario | Best Choice |\n",
    "|----------|-------------|\n",
    "| Small dataset (N < 10,000), few features (d < 1,000) | Normal Equation |\n",
    "| Large dataset (N > 100,000) | Gradient Descent (Mini-batch) |\n",
    "| Many features (d > 10,000) | Gradient Descent |\n",
    "| Neural networks, non-linear models | Gradient Descent (only option) |\n",
    "| Need exact optimal solution | Normal Equation |\n",
    "| Online learning (streaming data) | Stochastic Gradient Descent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Learning Rate: Critical Hyperparameter\n",
    "\n",
    "The **learning rate (α)** controls how big of a step we take in each iteration.\n",
    "\n",
    "### Impact of Different Learning Rates:\n",
    "\n",
    "**α too small (e.g., 0.0001):**\n",
    "- ✅ Stable convergence (doesn't overshoot)\n",
    "- ❌ Very slow (needs many iterations)\n",
    "- ❌ May get stuck in plateaus\n",
    "\n",
    "**α optimal (e.g., 0.01-0.1):**\n",
    "- ✅ Fast convergence\n",
    "- ✅ Reaches minimum efficiently\n",
    "- ✅ Smooth loss curve\n",
    "\n",
    "**α too large (e.g., 1.0+):**\n",
    "- ❌ Overshoots minimum\n",
    "- ❌ Loss oscillates or increases\n",
    "- ❌ May diverge (loss → ∞)\n",
    "\n",
    "We'll visualize these effects later in the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: You're training a model with gradient descent and observe that the loss is increasing rather than decreasing over iterations. What is the MOST likely cause?\n>\n> A. The model architecture is too simple and lacks sufficient capacity to capture the underlying complex patterns in the training data effectively\n>\n> B. The learning rate is excessively large, causing the optimizer to overshoot the minimum and jump to positions with higher loss\n>\n> C. The features require standardization because different feature scales are causing destabilizing gradient magnitudes that prevent convergence throughout the entire training process\n>\n> D. The convergence tolerance threshold is set too loose, allowing the algorithm to stop prematurely at suboptimal solutions before reaching the minimum\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: B**\n\n**Key Insight:** Increasing loss during training is the hallmark of a learning rate that's too large. The gradient descent steps overshoot the minimum, landing at positions with higher loss. Solution: reduce α by 10× (e.g., 0.1 → 0.01).\n\n**Explanation:**\n- **A is FALSE**: Model underfitting means the model can't fit the training data well, resulting in HIGH but STABLE loss that plateaus at a suboptimal value. The loss would remain consistently high across iterations, not increase over time. If loss is increasing, the optimization process itself is failing, not the model's representational capacity. Underfitting would show as: iteration 1 loss=100, iteration 50 loss=98, iteration 100 loss=98 (stuck at high value).\n- **B is TRUE**: When the learning rate α is too large, the weight update w_new = w_old - α∇w overshoots the minimum. Instead of moving toward the optimal point, it jumps past it to a worse position with higher loss. In extreme cases, this causes divergence where loss → ∞. The classic symptom of excessive learning rate is monotonically increasing loss or wild oscillations. Example: iteration 1 loss=10, iteration 2 loss=25, iteration 3 loss=100, iteration 4 loss=∞.\n- **C is FALSE**: While feature scaling IS very important for gradient descent stability, unscaled features typically cause SLOW and erratic convergence with oscillating loss, not monotonically increasing loss. Unscaled features create elongated loss surfaces that require careful learning rate tuning, but the loss would still trend downward overall, just very slowly and unstably. The symptom would be zigzagging loss curves, not consistent increases.\n- **D is FALSE**: Convergence tolerance controls when training stops (when loss change falls below threshold). If tolerance is too loose (e.g., 1e-2 instead of 1e-6), training might stop early, but this would result in HIGH loss at stopping time, not INCREASING loss. Increasing loss indicates the optimizer is actively making things worse, not stopping too early. Early stopping from loose tolerance would show: decreasing loss that stops at iteration 10 instead of iteration 100.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling: Critical for Gradient Descent!\n",
    "\n",
    "While feature scaling is recommended for Linear Regression's normal equation, it's **ESSENTIAL** for gradient descent.\n",
    "\n",
    "**Why is scaling so important for gradient descent?**\n",
    "\n",
    "1. **Convergence Speed:** Unscaled features create elongated loss surfaces\n",
    "   - Gradient descent zigzags instead of going straight to minimum\n",
    "   - Can be 100× slower or more!\n",
    "   \n",
    "2. **Learning Rate Sensitivity:** Different features need different learning rates\n",
    "   - Small-scale features (0-1) might need α = 0.1\n",
    "   - Large-scale features (0-10000) might need α = 0.00001\n",
    "   - With one global α, impossible to optimize all features well\n",
    "   \n",
    "3. **Numerical Stability:** Large feature values can cause gradient explosion\n",
    "   - Gradients become huge → weights explode → overflow errors\n",
    "\n",
    "**Solution: Z-Score Standardization**\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This transforms all features to:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "- Similar scales → uniform convergence\n",
    "\n",
    "**Critical Rule:** Fit scaler on training data ONLY!\n",
    "```python\n",
    "scaler.fit(X_train)  # Learn μ and σ from training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)    # Use same μ and σ\n",
    "X_test_scaled = scaler.transform(X_test)  # Use same μ and σ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode for Gradient Descent\n",
    "\n",
    "### Formal Pseudocode\n",
    "\n",
    "```\n",
    "============================================\n",
    "Inputs\n",
    "============================================\n",
    "X       ← training features (N × d matrix)\n",
    "y       ← training targets (N × 1 vector)\n",
    "α       ← learning rate (e.g., 0.01)\n",
    "max_iter ← maximum iterations (e.g., 1000)\n",
    "tol     ← convergence tolerance (e.g., 1e-6)\n",
    "\n",
    "============================================\n",
    "----- fit -----\n",
    "============================================\n",
    "1. Add bias column: Φ ← [1, X]  # (N × (d+1))\n",
    "2. Initialize weights randomly: w ← random small values\n",
    "3. For iteration = 1 to max_iter:\n",
    "     a. Compute predictions: ŷ ← Φw\n",
    "     b. Compute errors: e ← y - ŷ\n",
    "     c. Compute loss: L ← (1/N) Σ e²\n",
    "     d. Compute gradients: ∇w ← -(2/N) Φᵀe\n",
    "     e. Update weights: w ← w - α∇w\n",
    "     f. If |L_new - L_old| < tol: STOP (converged)\n",
    "4. Store final weights w\n",
    "\n",
    "============================================\n",
    "----- predict -----\n",
    "============================================\n",
    "For each query point in X_query:\n",
    "1. Add bias: Φ_query ← [1, X_query]\n",
    "2. Compute prediction: ŷ ← Φ_query · w\n",
    "3. Return ŷ\n",
    "```\n",
    "\n",
    "### Key Observations\n",
    "- **Iterative process:** Weights improve gradually over multiple iterations\n",
    "- **Convergence check:** Stop when loss stops decreasing significantly\n",
    "- **Prediction:** Same as Linear Regression (just matrix multiplication)\n",
    "- **Memory efficient:** Only stores weights (not all training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Why Gradient Descent?\n\nGradient descent is one of several optimization methods available. Understanding WHY we use it (and when NOT to) is crucial for choosing the right algorithm.\n\n### Comparison with Alternative Optimizers\n\n| Method | Time Complexity | Memory | Best Use Case | Limitations |\n|--------|----------------|---------|---------------|-------------|\n| **Closed-Form (Normal Equation)** | O(d³) | O(d²) | Small d (<1000), convex problems | Doesn't scale; only works for linear models |\n| **Gradient Descent** | O(iterations × N × d) | O(d) | Large-scale, any differentiable loss | Requires tuning α; iterative |\n| **Newton's Method** | O(d³) per iteration | O(d²) | Small d, need fast convergence | Very expensive; requires Hessian |\n| **Stochastic GD** | O(iterations × d) | O(d) | Very large N, online learning | Noisy; requires careful tuning |\n| **Coordinate Descent** | O(iterations × d) | O(d) | Sparse problems, LASSO | Not for all loss functions |\n\n**Key Variables:**\n- **N** = number of training samples\n- **d** = number of features (dimensions)\n- **iterations** = typically 100-10,000 depending on convergence\n\n### When to Use Gradient Descent\n\n✅ **Choose Gradient Descent when:**\n- **Large-scale problems:** Millions of parameters (e.g., neural networks with d > 1,000,000)\n- **No closed-form solution exists:** Most non-linear models (neural nets, logistic regression)\n- **Memory-constrained:** Can't store d × d matrices (common in big data)\n- **Online/streaming data:** Data arrives continuously, need to update model incrementally\n- **Non-convex optimization:** Need stochastic variants to escape local minima\n- **Deep learning:** The ONLY practical option for training neural networks\n\n❌ **Don't Use Gradient Descent when:**\n- **Small problems with closed-form solutions:** Linear regression with N < 10,000 and d < 1,000\n  - Normal equation is faster and gives exact solution\n  - No hyperparameter tuning needed\n- **Need exact solution in one step:** Critical applications where iterative approximation isn't acceptable\n- **Second-order methods are feasible:** d < 100 and you can afford O(d³) computation\n  - Newton's method converges much faster (quadratic vs linear convergence)\n\n### Real-World Examples\n\n**Gradient Descent is ESSENTIAL for:**\n- Training neural networks (millions of parameters)\n- Logistic regression (no closed form)\n- Support Vector Machines with kernels\n- Deep learning models (CNNs, RNNs, Transformers)\n- Matrix factorization (e.g., recommender systems)\n\n**Normal Equation is BETTER for:**\n- Simple linear regression on tabular data (N < 10k, d < 1k)\n- Ridge regression (closed form: w = (XᵀX + λI)⁻¹Xᵀy)\n- Prototyping and quick experimentation\n\n**The Bottom Line:**\nGradient descent is the **foundation of modern machine learning** because it:\n1. Scales to billions of parameters\n2. Works for ANY differentiable loss function\n3. Enables training of complex non-linear models\n4. Can be parallelized across GPUs/distributed systems\n\nWhile it requires careful tuning and is slower than closed-form solutions, its **flexibility and scalability** make it indispensable for real-world ML.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Custom Gradient Descent Class\n",
    "\n",
    "Below is a scaffold of the `MyGradientDescentRegressor` class. Fill in the TODO sections to complete the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class MyGradientDescentRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Custom Gradient Descent implementation for Linear Regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate (α) for gradient descent updates\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    tol : float, default=1e-6\n",
    "        Tolerance for convergence (stop if loss change < tol)\n",
    "    random_state : int, default=42\n",
    "        Random seed for weight initialization\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    weights_ : array of shape (n_features + 1,)\n",
    "        Learned weights including bias term\n",
    "    loss_history_ : list\n",
    "        Loss value at each iteration\n",
    "    n_iter_ : int\n",
    "        Actual number of iterations performed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6, random_state=42):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # TODO: Create design matrix Phi by adding column of ones for bias term\n",
    "        Phi = None\n",
    "        \n",
    "        # TODO: Initialize weights randomly with small values (use self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        self.weights_ = None\n",
    "        \n",
    "        # Initialize loss history\n",
    "        self.loss_history_ = []\n",
    "        N = len(y)\n",
    "        \n",
    "        # Gradient Descent Loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            # TODO: Compute predictions using current weights\n",
    "            predictions = None\n",
    "            \n",
    "            # TODO: Compute errors (residuals)\n",
    "            errors = None\n",
    "            \n",
    "            # TODO: Compute loss (Mean Squared Error)\n",
    "            loss = None\n",
    "            \n",
    "            # Store loss\n",
    "            self.loss_history_.append(loss)\n",
    "            \n",
    "            # Check convergence\n",
    "            if iteration > 0 and abs(self.loss_history_[-2] - self.loss_history_[-1]) < self.tol:\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "            \n",
    "            # TODO: Compute gradients using the formula: -(2/N) * Φᵀ(y - ŷ)\n",
    "            gradients = None\n",
    "            \n",
    "            # TODO: Update weights using gradient descent update rule\n",
    "            pass\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iter\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the learned model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        # TODO: Create design matrix (same as in fit)\n",
    "        Phi = None\n",
    "        \n",
    "        # TODO: Compute predictions\n",
    "        y_pred = None\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Once you have filled in the implementation, let's test our custom gradient descent regressor on a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple test data\n",
    "np.random.seed(42)\n",
    "X_simple = np.array([[1], [2], [3], [4], [5]])\n",
    "y_simple = np.array([2, 4, 6, 8, 10])  # Perfect linear relationship: y = 2x\n",
    "\n",
    "# Fit model\n",
    "model = MyGradientDescentRegressor(learning_rate=0.01, max_iter=1000)\n",
    "model.fit(X_simple, y_simple)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_simple)\n",
    "\n",
    "print(\"Learned weights (w0=intercept, w1=slope):\", model.weights_)\n",
    "print(\"Expected: [0, 2] or very close to it\")\n",
    "print(\"\\nPredictions:\", predictions)\n",
    "print(\"Actual:     \", y_simple)\n",
    "print(\"\\nFinal MSE:\", model.loss_history_[-1])\n",
    "print(\"Expected: very close to 0\")\n",
    "print(f\"\\nConverged in {model.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Convergence\n",
    "\n",
    "Let's plot the loss over iterations to see how the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(model.loss_history_) + 1), model.loss_history_, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "plt.title('Gradient Descent Convergence', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {model.loss_history_[0]:.6f}\")\n",
    "print(f\"Final loss:   {model.loss_history_[-1]:.6f}\")\n",
    "print(f\"Improvement:  {(1 - model.loss_history_[-1]/model.loss_history_[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: In a well-tuned gradient descent setup, what should the loss curve look like?\n>\n> A. Monotonically decreasing at a constant linear rate throughout training until reaching exactly zero loss at the point of convergence completion\n>\n> B. Decreasing rapidly at first when gradients are large, then gradually slowing and flattening as it approaches the minimum near convergence\n>\n> C. Fluctuating randomly around a central value with gradually decreasing variance over iterations as the model stabilizes toward the optimal solution\n>\n> D. Decreasing in distinct discrete steps with plateaus between iterations where no measurable progress occurs before sudden drops to lower loss\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: B**\n\n**Key Insight:** A healthy loss curve shows fast initial decrease (large gradients far from optimum), gradual slowdown (gradients shrink near minimum), and flattening at convergence (gradient ≈ 0). This \"fast then slow\" pattern is the signature of successful first-order optimization.\n\n**Explanation:**\n- **A is FALSE**: Loss rarely decreases at a constant rate or reaches exactly zero. The rate of decrease depends on the gradient magnitude, which changes as you approach the minimum (gradients get smaller → slower progress). For noisy data, loss plateaus at a positive value (residual error), not zero. A constant-rate decrease would indicate the learning rate isn't being adjusted for the changing gradient landscape. Example: iteration 1 loss=100, iteration 10 loss=90, iteration 20 loss=80 (constant -1 per iteration) - this is unrealistic for real gradient descent.\n- **B is TRUE**: A healthy loss curve shows: (1) Rapid decrease initially when gradients are large and weights are far from optimal (e.g., iteration 1-10: loss drops from 100 to 20), (2) Gradual slowdown as gradients become smaller near the minimum (iteration 10-50: loss drops from 20 to 5), (3) Flattening/plateau at convergence when gradient ≈ 0 (iteration 50+: loss stays near 4.8). This \"fast then slow\" pattern is the signature of successful first-order optimization approaching a local minimum.\n- **C is FALSE**: Random fluctuation is characteristic of stochastic gradient descent (SGD) using single samples or small mini-batches, not well-tuned batch gradient descent. For batch GD using all training data, the gradient is deterministic and loss should decrease monotonically. While SGD's fluctuation can help escape shallow local minima, it's not the expected behavior for standard batch GD. SGD would show: iteration 10 loss=50, iteration 11 loss=55, iteration 12 loss=48 (bouncing around).\n- **D is FALSE**: Distinct steps with plateaus suggest the learning rate is poorly tuned or there are numerical precision issues. Smooth gradient descent should show continuous progress, not discrete jumps. Step-like behavior might indicate: batch updates (normal for mini-batch GD), learning rate schedules with sudden drops, or gradient clipping thresholds being hit. For well-tuned batch GD, the curve should be smooth, not step-wise.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset for Visualization\n",
    "\n",
    "Let's work with the same synthetic dataset from the Linear Regression lab to directly compare approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same data as in Linear Regression Code Walk Through\n",
    "np.random.seed(42)\n",
    "X_train = np.arange(-9.5, 8.5, 0.1).reshape(-1, 1)\n",
    "y_train = X_train.ravel() + 1 + np.random.normal(0, 2, len(X_train))\n",
    "\n",
    "print(f\"Training data: {len(X_train)} points\")\n",
    "print(f\"X range: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "print(f\"y range: [{y_train.min():.1f}, {y_train.max():.1f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Training Data: Linear Relationship with Noise', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Visualizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit your MyGradientDescentRegressor on the training data\n",
    "model = None\n",
    "\n",
    "print(f\"Learned weights: {model.weights_}\")\n",
    "print(f\"Model equation: y = {model.weights_[1]:.3f}x + {model.weights_[0]:.3f}\")\n",
    "print(f\"Converged in {model.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fit\n",
    "x_line = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
    "y_line = model.predict(x_line)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, c='lightblue', alpha=0.6, edgecolors='black', linewidths=0.5, label='Training data')\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'GD fit: y={model.weights_[1]:.2f}x+{model.weights_[0]:.2f}')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.title('Gradient Descent: Best Fit Line', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Convergence Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(model.loss_history_) + 1), model.loss_history_, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "plt.title('Training Loss Over Iterations', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {model.loss_history_[0]:.6f}\")\n",
    "print(f\"Final loss:   {model.loss_history_[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Learning Rate\n",
    "\n",
    "Let's experiment with different learning rates to see how they affect convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Diagnosing Convergence from Loss Curves\n\nLearning to read loss curves is a **critical debugging skill** for machine learning practitioners. Here's how to diagnose what's happening with your gradient descent optimization:\n\n### Pattern 1: Healthy Convergence ✅\n\n**What it looks like:**\n- Loss decreases rapidly at first (steep decline)\n- Gradually slows down (curve flattens)\n- Stabilizes to a flat minimum\n- Smooth, monotonic decrease (no jumps or spikes)\n\n**Example behavior:**\n```\nIteration 1:    Loss = 100.00\nIteration 10:   Loss = 25.00\nIteration 50:   Loss = 5.20\nIteration 100:  Loss = 4.02\nIteration 200:  Loss = 4.00\nIteration 201:  Loss = 4.00  ← Converged!\n```\n\n**What to do:**\n✅ **Training successful!** Model has converged to optimal solution.\n- Can safely stop training\n- Try increasing learning rate next time to converge faster\n- Current learning rate is well-tuned\n\n---\n\n### Pattern 2: Too Slow Convergence ⚠️\n\n**What it looks like:**\n- Loss decreasing, but very gradually\n- Linear decrease that doesn't flatten\n- Still decreasing at max_iter\n- Never reaches plateau\n\n**Example behavior:**\n```\nIteration 1:     Loss = 100.00\nIteration 100:   Loss = 95.00\nIteration 500:   Loss = 90.00\nIteration 1000:  Loss = 85.00  ← Still decreasing!\n```\n\n**Diagnosis:**\n- Learning rate **too small** for this problem\n- Need many more iterations to converge\n- Wasting computation time\n\n**What to do:**\n1. **Increase α** by 2-10×: Try α = 0.01 → 0.05 or 0.1\n2. **Increase max_iter**: Allow 5000-10000 iterations\n3. **Check feature scaling**: Unscaled features can cause this!\n4. **Monitor:** Plot loss curve to verify improvement\n\n---\n\n### Pattern 3: Oscillating (Unstable) ⚠️\n\n**What it looks like:**\n- Loss bounces up and down\n- Zigzag pattern around some value\n- Never stabilizes\n- Average trend might be downward, but very noisy\n\n**Example behavior:**\n```\nIteration 1:   Loss = 100.00\nIteration 10:  Loss = 15.00\nIteration 11:  Loss = 25.00  ← Jumped up!\nIteration 12:  Loss = 10.00  ← Dropped again\nIteration 20:  Loss = 18.00\nIteration 30:  Loss = 12.00  ← Bouncing around\n```\n\n**Diagnosis:**\n- Learning rate **too large**\n- Overshooting minimum on both sides\n- Steps are bigger than the valley width\n\n**What to do:**\n1. **Decrease α** by 2-10×: Try α = 0.5 → 0.1 or 0.05\n2. **Check feature scaling**: Unscaled features exacerbate this!\n3. **Use learning rate decay**: Start high, reduce over time\n4. **Try smaller steps**: α = 0.01 is often safe starting point\n\n---\n\n### Pattern 4: Diverging (Exploding) ❌\n\n**What it looks like:**\n- Loss **increases** over iterations\n- May reach infinity or NaN\n- Gets progressively worse\n- Model is getting further from optimal solution\n\n**Example behavior:**\n```\nIteration 1:   Loss = 100.00\nIteration 10:  Loss = 500.00\nIteration 20:  Loss = 2500.00\nIteration 30:  Loss = inf  ← Exploded!\n```\n\n**Diagnosis:**\n- Learning rate **way too large**\n- Each step jumps far past the minimum\n- Gradient explosion (weights become huge)\n\n**What to do:**\n1. **Decrease α dramatically**: Try α = 0.5 → 0.01 or even 0.001\n2. **Standardize features IMMEDIATELY**: Most common cause!\n3. **Check for NaN/Inf in data**: Data quality issues\n4. **Reinitialize weights**: Try different random seed\n\n---\n\n### Pattern 5: Stuck at High Loss (Plateau) ⚠️\n\n**What it looks like:**\n- Loss plateaus early at suboptimal value\n- Flattens out but loss is still high\n- No further progress after certain iteration\n- Converged, but to wrong solution\n\n**Example behavior:**\n```\nIteration 1:    Loss = 100.00\nIteration 10:   Loss = 50.00\nIteration 50:   Loss = 45.00\nIteration 100:  Loss = 45.00  ← Stuck!\nIteration 500:  Loss = 45.00  ← Still stuck!\n```\n\n**Possible causes:**\n- **Poor initialization:** Weights started in bad region\n- **Local minimum:** (Less common for linear regression, but happens in neural nets)\n- **Learning rate too small:** Can't escape saddle points\n- **Feature scaling issues:** Some features dominate\n\n**What to do:**\n1. **Re-initialize weights:** Try different `random_state`\n2. **Increase α slightly:** Help escape plateaus\n3. **Check feature scaling:** Ensure all features are standardized\n4. **Try momentum:** Advanced technique to escape saddle points\n5. **Verify data quality:** Check for outliers or data issues\n\n---\n\n### Pattern 6: Step-Like Decrease (Unusual)\n\n**What it looks like:**\n- Loss decreases in distinct steps\n- Plateaus between jumps\n- Not smooth\n\n**Possible causes:**\n- Using mini-batch GD (this is normal for mini-batch!)\n- Learning rate schedule with discrete drops\n- Numerical precision issues\n\n**What to do:**\n- If using mini-batch: This is expected behavior ✅\n- If using batch GD: Check learning rate schedule\n- Generally not a problem unless loss isn't decreasing overall\n\n---\n\n### Quick Diagnosis Flowchart\n\n```\nIs loss DECREASING overall?\n├─ YES: Is it SMOOTH and FLATTENING?\n│   ├─ YES → ✅ Healthy convergence\n│   └─ NO: Is it OSCILLATING/BOUNCING?\n│       ├─ YES → ⚠️ Learning rate too large\n│       └─ NO: Still decreasing at max_iter?\n│           └─ YES → ⚠️ Learning rate too small\n└─ NO: Is loss INCREASING?\n    ├─ YES → ❌ Learning rate way too large\n    └─ NO: Is loss STUCK at high value?\n        └─ YES → ⚠️ Poor initialization or local minimum\n```\n\n---\n\n### Practical Tips\n\n1. **Always plot loss curves** - Don't rely on final loss value alone\n2. **Use log scale** for y-axis when comparing multiple learning rates\n3. **Monitor first 10-20 iterations** - Catches divergence early\n4. **Expected shape:** Sharp drop → gradual slowdown → flat plateau\n5. **Save checkpoints:** Keep best weights seen so far (useful for oscillating loss)\n\n**Remember:** The loss curve tells you the entire story of your optimization. Learn to read it!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    model_lr = MyGradientDescentRegressor(learning_rate=lr, max_iter=100)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    \n",
    "    plt.plot(range(1, len(model_lr.loss_history_) + 1), model_lr.loss_history_,\n",
    "            linewidth=2, color=color, label=f'α = {lr} ({model_lr.n_iter_} iter)')\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=14)\n",
    "plt.ylabel('Loss (MSE)', fontsize=14)\n",
    "plt.title('Impact of Learning Rate on Convergence', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see all curves\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- α = 0.001: Very slow convergence (needs more iterations)\")\n",
    "print(\"- α = 0.01:  Good convergence speed\")\n",
    "print(\"- α = 0.1:   Fast convergence\")\n",
    "print(\"- α = 0.5:   May oscillate or diverge (too large)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: You train gradient descent with α=0.001 for 100 iterations and the loss is still decreasing steadily. What should you do?\n>\n> A. Reduce the learning rate to α=0.0001 or even smaller to ensure more stable and reliable convergence without oscillations or divergence\n>\n> B. Increase max_iter to allow more iterations for convergence, or increase α to larger values to converge faster with fewer iterations\n>\n> C. Stop training immediately since 100 iterations provides sufficient convergence for most practical models and additional iterations provide diminishing returns for accuracy\n>\n> D. Switch to the normal equation approach which guarantees finding the globally optimal solution faster without requiring iterative optimization or hyperparameter tuning\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: B**\n\n**Key Insight:** Steadily decreasing loss means gradient descent is working correctly but hasn't converged yet. Solutions: (1) increase max_iter to allow more time, or (2) increase α to take bigger steps and converge faster. Never decrease α when loss is decreasing smoothly - that would make convergence even slower.\n\n**Explanation:**\n- **A is FALSE**: The learning rate α=0.001 is already quite small. Decreasing it further to 0.0001 would make convergence even SLOWER, requiring even MORE iterations to reach the minimum. Since loss is steadily decreasing (not oscillating or diverging), there's no stability problem - the optimization is working correctly, just slowly. Reducing α would turn 100 iterations into potentially 1000+ iterations needed. This is moving in the wrong direction!\n- **B is TRUE**: Steadily decreasing loss means gradient descent is working correctly but needs more time to converge. Two solutions: (1) Increase max_iter (e.g., to 1000 or 10000) to allow more iterations with the current learning rate, or (2) Increase α (e.g., to 0.01 or 0.1) to take bigger steps and converge faster. The second option is usually more efficient for computational cost. Example: with α=0.01 instead of 0.001, you might converge in 50 iterations instead of 500.\n- **C is FALSE**: There's no universal \"sufficient\" number of iterations. Required iterations depend on: learning rate, data size, feature scales, initialization, and convergence tolerance. If loss is still decreasing steadily after 100 iterations, the model hasn't converged yet. Stopping now would leave you with a suboptimal solution unnecessarily. Example: stopping at iteration 100 with loss=10 when you could reach loss=4 with more iterations wastes the training effort so far.\n- **D is FALSE**: \"Loss still decreasing steadily\" means gradient descent IS working properly - it just hasn't finished yet. This is not a failure case requiring a different algorithm. The normal equation would give the same final solution but doesn't provide insight into convergence behavior. For small problems, either approach works; switching algorithms mid-optimization is unnecessary and would waste the 100 iterations already completed. Plus, gradient descent is the ONLY option for non-linear models where no closed form exists.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Practices for Gradient Descent\n\nThis section synthesizes professional-level guidance for applying gradient descent effectively in real-world machine learning projects.\n\n---\n\n### 1. Learning Rate Selection\n\nThe learning rate α is the **most critical hyperparameter** in gradient descent.\n\n**Starting Point:**\n- For **standardized features**: Start with α = 0.01\n- For **unscaled features**: Start with α = 0.0001 (or better, standardize first!)\n- Typical working range: 0.001 to 0.1\n\n**Tuning Strategy:**\n```python\n# Try multiple learning rates\nlearning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\nfor lr in learning_rates:\n    model = MyGradientDescentRegressor(learning_rate=lr)\n    model.fit(X_train, y_train)\n    # Plot loss curves and compare\n```\n\n**Advanced Techniques:**\n- **Learning rate schedules:** Decay α over time (e.g., α_t = α_0 / (1 + decay × t))\n- **Adaptive methods:** Adam, RMSprop, AdaGrad (auto-adjust α per parameter)\n- **Warm restarts:** Periodically reset α to escape plateaus\n- **Line search:** Find optimal α per iteration (expensive but effective)\n\n**Rule of Thumb:**\n- If loss oscillates → α too large → decrease by 2-10×\n- If loss decreases slowly → α too small → increase by 2-10×\n- Monitor loss curve for first 10-20 iterations to diagnose quickly\n\n---\n\n### 2. Feature Scaling\n\nFeature scaling is **MANDATORY** for gradient descent (optional for normal equation).\n\n**Why it's critical:**\n- Unscaled features create elongated loss surfaces → zigzag convergence\n- Different features need different learning rates → impossible with one global α\n- Large feature values cause gradient explosion → NaN/Inf errors\n\n**Best Practice: Z-Score Standardization**\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)                    # Learn μ and σ from training data ONLY\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)  # Apply same μ and σ\nX_test_scaled = scaler.transform(X_test)\n```\n\n**CRITICAL:** \n- ✅ **Fit scaler on training data ONLY** (avoid data leakage!)\n- ✅ **Transform all sets** (train, val, test) using the same scaler\n- ✅ **Scale before training** gradient descent\n- ❌ **Never fit scaler on val/test data**\n\n**Alternative Scaling Methods:**\n- **MinMaxScaler:** Scales to [0, 1] - good for bounded data\n- **RobustScaler:** Uses median/IQR - robust to outliers\n- **Normalizer:** Scales each sample to unit norm - for text/sparse data\n\n---\n\n### 3. Weight Initialization\n\nPoor initialization can lead to slow convergence or getting stuck in bad regions.\n\n**Best Practice: Small Random Values**\n```python\nnp.random.seed(42)\nweights = np.random.randn(n_features) * 0.01  # Small values near 0\n```\n\n**Initialization Strategies:**\n- **Random small values:** Standard for linear models\n- **Zeros:** ❌ Creates symmetry problem (all neurons learn same thing)\n- **Xavier/Glorot:** For deep networks: `w ~ N(0, sqrt(2/(n_in + n_out)))`\n- **He initialization:** For ReLU networks: `w ~ N(0, sqrt(2/n_in))`\n\n**Why small values?**\n- Large initial weights → large initial gradients → potential divergence\n- Too small → very slow initial learning\n- ~0.01 is good starting point\n\n---\n\n### 4. Convergence Criteria\n\nDecide when to stop training to avoid wasted computation.\n\n**Method 1: Maximum Iterations**\n```python\nmax_iter = 1000  # Conservative default\n# For large/complex problems, try 5000-10000\n```\n\n**Method 2: Tolerance on Loss Change**\n```python\nif abs(loss[t] - loss[t-1]) < tol:  # e.g., tol=1e-6\n    break  # Converged!\n```\n\n**Method 3: Gradient Norm**\n```python\nif np.linalg.norm(gradient) < tol:  # e.g., tol=1e-6\n    break  # At critical point\n```\n\n**Method 4: Early Stopping (for ML models with validation set)**\n```python\nif val_loss hasn't improved for 10 epochs:\n    break  # Prevent overfitting\n```\n\n**Recommendation:**\n- Use **both** max_iter AND tolerance for robustness\n- Set reasonable max_iter (don't run forever if not converging)\n- Monitor loss curve to verify convergence\n\n---\n\n### 5. Monitoring and Logging\n\nAlways track optimization progress for debugging and analysis.\n\n**Essential Metrics to Log:**\n```python\nhistory = {\n    'loss': [],           # Loss at each iteration\n    'gradients': [],      # Gradient norms (for debugging)\n    'learning_rate': [],  # If using decay/schedules\n    'iteration': []\n}\n```\n\n**Visualization:**\n```python\n# Loss curve (most important!)\nplt.plot(history['loss'])\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.yscale('log')  # Log scale helpful for comparing rates\n\n# Gradient norms (check for explosion/vanishing)\nplt.plot(history['gradients'])\n```\n\n**Red Flags to Watch:**\n- ❌ Loss increasing → α too large\n- ❌ Loss = NaN/Inf → Gradient explosion (scale features!)\n- ❌ Loss plateaus early at high value → Poor initialization or stuck\n- ✅ Loss decreasing and flattening → Healthy convergence\n\n---\n\n### 6. When Gradient Descent Fails\n\nUnderstand limitations and failure modes to troubleshoot effectively.\n\n**Problem 1: Non-Convex Landscapes**\n- Neural networks have many local minima\n- **Solution:** Use stochastic/mini-batch GD (noise helps escape), momentum, or adaptive methods\n\n**Problem 2: Saddle Points**\n- Points where gradient = 0 but not a minimum\n- Common in high-dimensional spaces\n- **Solution:** Momentum methods (accumulate velocity to push through)\n\n**Problem 3: Ill-Conditioned Problems**\n- Loss surface has very different curvatures in different directions\n- One learning rate can't work for all directions\n- **Solution:** Feature scaling, adaptive methods (Adam), or second-order methods\n\n**Problem 4: Vanishing/Exploding Gradients**\n- Gradients become too small (learning stops) or too large (divergence)\n- Common in deep networks\n- **Solution:** Proper initialization, gradient clipping, batch normalization\n\n**Problem 5: Poor Feature Scaling**\n- **Most common cause** of gradient descent failure!\n- **Solution:** ALWAYS standardize features first!\n\n---\n\n### 7. Advanced Techniques\n\nOnce you master basic gradient descent, explore these enhancements:\n\n#### Momentum\nAccumulates gradient history to smooth updates and accelerate convergence.\n```python\nvelocity = 0.9 * velocity + learning_rate * gradient\nweights = weights - velocity\n```\n**Benefits:** Faster convergence, dampens oscillations, escapes plateaus\n\n#### Adaptive Learning Rates\nMethods that adjust learning rate automatically per parameter.\n\n| Method | Key Idea | Use Case |\n|--------|----------|----------|\n| **AdaGrad** | Larger updates for infrequent features | Sparse data, NLP |\n| **RMSprop** | Exponential moving average of gradients | RNNs, non-stationary problems |\n| **Adam** | Momentum + RMSprop | Default for deep learning (most popular!) |\n\n**Adam is the industry standard** for deep learning - combines best of both worlds.\n\n#### Learning Rate Schedules\nReduce α over time for better convergence.\n- **Step decay:** Reduce α by 10× every N epochs\n- **Exponential decay:** α_t = α_0 * e^(-kt)\n- **Cosine annealing:** α_t = α_min + 0.5(α_max - α_min)(1 + cos(πt/T))\n\n#### Line Search\nFind optimal step size per iteration (expensive but effective).\n```python\n# Instead of fixed α, search for best α per iteration\nα_optimal = argmin_α Loss(w - α * gradient)\n```\n\n---\n\n### 8. Gradient Descent Checklist\n\nBefore training with gradient descent, verify:\n\n- ✅ **Features are standardized** (StandardScaler fitted on training data)\n- ✅ **Learning rate is reasonable** (start with 0.01)\n- ✅ **Weights initialized** with small random values\n- ✅ **Max iterations set** (1000-10000 depending on problem)\n- ✅ **Convergence tolerance set** (e.g., 1e-6)\n- ✅ **Loss history tracked** for visualization\n- ✅ **Random seed set** for reproducibility\n\nDuring training, monitor:\n\n- ✅ **Loss curve** - Should decrease and flatten\n- ✅ **First 10 iterations** - Catch divergence early\n- ✅ **Convergence message** - Verify it actually converged\n- ✅ **Final loss value** - Compare with expected range\n\nAfter training, validate:\n\n- ✅ **Compare with closed-form solution** (for linear regression)\n- ✅ **Check learned weights** - Do they make sense?\n- ✅ **Verify predictions** - Test on held-out data\n- ✅ **Inspect residuals** - Look for patterns (suggests model issues)\n\n---\n\n### 9. Comparing Gradient Descent Variants\n\n| Variant | Samples/Iteration | Memory | Speed/Iter | Convergence | Best For |\n|---------|------------------|---------|------------|-------------|----------|\n| **Batch GD** | All N | O(N×d) | Slow | Smooth, stable | Small data (N<10k) |\n| **Stochastic GD** | 1 | O(d) | Very fast | Noisy, erratic | Online learning |\n| **Mini-Batch GD** | B (32-256) | O(B×d) | Fast | Smooth-ish | Large data, deep learning |\n\n**Industry Practice:**\n- **Small datasets (N < 10,000):** Use batch GD or normal equation\n- **Large datasets (N > 100,000):** Use mini-batch GD with B=32-256\n- **Online learning (streaming data):** Use stochastic GD with learning rate decay\n- **Deep learning:** Mini-batch GD with Adam optimizer (almost universal)\n\n---\n\n### 10. Summary: Gradient Descent in Practice\n\n**Key Takeaways:**\n\n1. **Gradient Descent = Foundation of Modern ML**\n   - Only practical option for neural networks\n   - Scales to billions of parameters\n   - Works for any differentiable loss function\n\n2. **Feature Scaling is Non-Negotiable**\n   - **#1 cause** of gradient descent failure\n   - Always use StandardScaler (fit on training data only!)\n   - Enables faster, more stable convergence\n\n3. **Learning Rate is Critical**\n   - Start with α = 0.01 for scaled features\n   - Monitor loss curve to diagnose issues\n   - Too large → divergence, too small → slow convergence\n\n4. **Monitor Convergence**\n   - Always plot loss curves\n   - Check first 10-20 iterations for early warning signs\n   - Use both max_iter and tolerance for robustness\n\n5. **Advanced Methods Help**\n   - Adam optimizer for deep learning\n   - Momentum for faster convergence\n   - Learning rate schedules for fine-tuning\n\n**When to Use Gradient Descent:**\n- ✅ Neural networks (only option)\n- ✅ Large datasets (N > 100,000)\n- ✅ No closed-form solution exists\n- ✅ Online/streaming data\n\n**When NOT to Use:**\n- ❌ Small linear regression (use normal equation)\n- ❌ Need exact solution in one step\n- ❌ Can afford second-order methods\n\n**The Bottom Line:**\nGradient descent is the workhorse of modern machine learning. Master its fundamentals (learning rate, feature scaling, convergence monitoring) and you'll be equipped to train everything from simple linear models to state-of-the-art deep neural networks."
  },
  {
   "cell_type": "code",
   "source": "# TODO: Experiment 3 - sklearn Validation\n# Compare your implementation with sklearn's SGDRegressor\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Use the scaled data from Experiment 1\n# If you haven't run Experiment 1, uncomment and run this:\n# from sklearn.preprocessing import StandardScaler\n# np.random.seed(42)\n# X_temp = np.c_[np.random.uniform(0, 10, 200), np.random.uniform(0, 10000, 200)]\n# y_temp = 2*X_temp[:, 0] + 0.001*X_temp[:, 1] + np.random.normal(0, 5, 200)\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X_temp)\n# y_mixed = y_temp\n\n# TODO: Train YOUR custom implementation\n# Use the best learning rate from Experiment 2 (or 0.01 if you skipped Experiment 2)\n# Set max_iter=1000, tol=1e-6\nmy_model = None\n\nprint(\"=\"*80)\nprint(\"EXPERIMENT 3 RESULTS: Your Implementation vs sklearn\")\nprint(\"=\"*80)\n\nprint(\"\\n**Your Implementation:**\")\nprint(f\"  Learned weights: {my_model.weights_}\")\nprint(f\"  Iterations: {my_model.n_iter_}\")\nprint(f\"  Final loss: {my_model.loss_history_[-1]:.6f}\")\n\n# TODO: Train sklearn's SGDRegressor\n# Parameters to match your implementation:\n#   - learning_rate='constant'  # Use constant learning rate like yours\n#   - eta0=0.01                 # Initial learning rate (same as yours)\n#   - max_iter=1000            # Maximum iterations\n#   - tol=1e-6                 # Convergence tolerance\n#   - penalty=None             # No regularization (to match your implementation)\n#   - random_state=42          # For reproducibility\n# Hint: sklearn_model = SGDRegressor(params...)\nsklearn_model = None\n\n# sklearn stores intercept and coef separately, combine them like your implementation\nsklearn_weights = np.concatenate([[sklearn_model.intercept_[0]], sklearn_model.coef_])\n\nprint(\"\\n**sklearn's SGDRegressor:**\")\nprint(f\"  Learned weights: {sklearn_weights}\")\nprint(f\"  Iterations: {sklearn_model.n_iter_}\")\n\n# TODO: Compute sklearn's loss on training data\n# Use mean_squared_error\nsklearn_predictions = None\nsklearn_loss = None\n\nprint(f\"  Final loss: {sklearn_loss:.6f}\")\n\n# Compare weights\nweight_diff = np.abs(my_model.weights_ - sklearn_weights)\nprint(\"\\n**Comparison:**\")\nprint(f\"  Weight difference (absolute): {weight_diff}\")\nprint(f\"  Max weight difference: {weight_diff.max():.6f}\")\nprint(f\"  Loss difference: {abs(my_model.loss_history_[-1] - sklearn_loss):.6f}\")\n\n# Determine if implementation is correct\nif weight_diff.max() < 0.1:\n    print(\"\\n✅ **Validation PASSED!** Your implementation matches sklearn closely!\")\n    print(\"   Your gradient descent implementation is correct! 🎉\")\nelif weight_diff.max() < 1.0:\n    print(\"\\n⚠️ **Validation ACCEPTABLE.** Small differences due to implementation details.\")\n    print(\"   This is normal - sklearn uses additional optimizations.\")\nelse:\n    print(\"\\n❌ **Validation FAILED.** Significant difference detected.\")\n    print(\"   Check your gradient computation and weight update logic.\")\n\n# Visualize loss curves comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(my_model.loss_history_, linewidth=2, label='Your Implementation', color='blue')\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Your Implementation - Loss Curve', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.subplot(1, 2, 2)\n# Plot final predictions comparison\ny_pred_my = my_model.predict(X_scaled)\ny_pred_sklearn = sklearn_model.predict(X_scaled)\n\nplt.scatter(y_mixed, y_pred_my, alpha=0.5, label='Your Model', color='blue')\nplt.scatter(y_mixed, y_pred_sklearn, alpha=0.5, label='sklearn', color='red', marker='x')\nplt.plot([y_mixed.min(), y_mixed.max()], [y_mixed.min(), y_mixed.max()], \n         'k--', linewidth=2, label='Perfect Prediction')\nplt.xlabel('True Values', fontsize=12)\nplt.ylabel('Predicted Values', fontsize=12)\nplt.title('Predictions Comparison', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 **Key Insight:** Your custom gradient descent implementation produces results\")\nprint(\"   very similar to sklearn's professional implementation, validating your understanding!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Experiment 3: Validation with sklearn\n\n**Goal:** Compare your custom implementation with sklearn's professional SGDRegressor to verify correctness.\n\n**What you'll observe:**\n- Similar final weights (proves your implementation is correct!)\n- Similar loss values\n- Possibly different iterations (sklearn uses advanced optimizations)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TODO: Experiment 2 - Learning Rate Tuning\n# Test multiple learning rates to find the optimal value\n\n# Use the scaled data from Experiment 1\n# If you haven't run Experiment 1, uncomment and run this:\n# from sklearn.preprocessing import StandardScaler\n# np.random.seed(42)\n# X_temp = np.c_[np.random.uniform(0, 10, 200), np.random.uniform(0, 10000, 200)]\n# y_temp = 2*X_temp[:, 0] + 0.001*X_temp[:, 1] + np.random.normal(0, 5, 200)\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X_temp)\n# y_mixed = y_temp\n\n# TODO: Define a list of learning rates to test\n# Try: [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0]\nlearning_rates_to_test = None\n\n# Dictionary to store results\nresults = {}\n\n# TODO: Train a model for each learning rate\n# For each α in learning_rates_to_test:\n#   1. Create a MyGradientDescentRegressor with that learning rate\n#   2. Fit it on X_scaled and y_mixed\n#   3. Store the model, iterations, and final loss in results dict\n# Hint: results[lr] = {'model': model, 'n_iter': model.n_iter_, 'loss': model.loss_history_[-1]}\n\nfor lr in None:  # Replace None with your learning_rates list\n    pass  # TODO: Implement the training loop here\n\n# Display results in a table\nprint(\"=\"*80)\nprint(\"EXPERIMENT 2 RESULTS: Learning Rate Tuning\")\nprint(\"=\"*80)\nprint(f\"\\n{'Learning Rate':<15} {'Iterations':<12} {'Final Loss':<15} {'Status':<20}\")\nprint(\"-\" * 80)\n\nfor lr in learning_rates_to_test:\n    n_iter = results[lr]['n_iter']\n    final_loss = results[lr]['loss']\n    \n    # Determine status\n    if np.isnan(final_loss) or np.isinf(final_loss):\n        status = \"❌ Diverged (NaN/Inf)\"\n    elif n_iter >= 1000:\n        status = \"⚠️ Slow (max iter)\"\n    elif final_loss > 100:\n        status = \"❌ Too large (high loss)\"\n    else:\n        status = \"✅ Converged\"\n    \n    print(f\"{lr:<15.4f} {n_iter:<12} {final_loss:<15.6f} {status:<20}\")\n\n# TODO: Find the best learning rate (lowest final loss among converged models)\n# Hint: Filter out NaN/Inf, then find min loss\nbest_lr = None\nprint(f\"\\n🎯 **Best Learning Rate:** α = {best_lr}\")\nprint(f\"   Converged in {results[best_lr]['n_iter']} iterations\")\nprint(f\"   Final loss: {results[best_lr]['loss']:.6f}\")\n\n# Visualize all loss curves\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nfor lr in learning_rates_to_test:\n    if not (np.isnan(results[lr]['loss']) or np.isinf(results[lr]['loss'])):\n        plt.plot(results[lr]['model'].loss_history_, \n                linewidth=2, label=f'α={lr}', alpha=0.8)\n\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Learning Rate Comparison - All Iterations', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\n# Plot first 50 iterations for detail\nfor lr in learning_rates_to_test:\n    if not (np.isnan(results[lr]['loss']) or np.isinf(results[lr]['loss'])):\n        history = results[lr]['model'].loss_history_\n        plt.plot(range(min(50, len(history))), history[:50], \n                linewidth=2, label=f'α={lr}', alpha=0.8)\n\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('First 50 Iterations (Detail)', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 **Key Insights:**\")\nprint(\"   - Too small α (0.0001): Very slow, needs many iterations\")\nprint(\"   - Optimal α (0.01-0.1): Fast, smooth convergence\")\nprint(\"   - Too large α (>0.5): Oscillates or diverges\")\nprint(\"   - Always tune α based on loss curve behavior!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Experiment 2: Learning Rate Tuning\n\n**Goal:** Systematically test different learning rates to find the optimal α and understand the learning rate-convergence relationship.\n\n**What you'll observe:**\n- Too small α → very slow convergence\n- Optimal α → fast, smooth convergence\n- Too large α → oscillation or divergence",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TODO: Experiment 1 - Feature Scaling Impact\n# Create a dataset with features at different scales to demonstrate why scaling matters\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Create synthetic data with two features at vastly different scales\nnp.random.seed(42)\nn_samples = 200\n\n# Feature 1: small scale (0-10)\n# Feature 2: large scale (0-10000)\n# TODO: Create X_unscaled with shape (n_samples, 2)\n#   - First column: random values between 0 and 10\n#   - Second column: random values between 0 and 10000\n# Hint: Use np.random.uniform(low, high, size)\nX_unscaled = None\n\n# TODO: Create target variable y_mixed that depends on both features\n# Formula: y = 2*feature1 + 0.001*feature2 + noise\n# Hint: noise = np.random.normal(0, 5, n_samples)\ny_mixed = None\n\nprint(\"Dataset created:\")\nprint(f\"Feature 1 range: [{X_unscaled[:, 0].min():.2f}, {X_unscaled[:, 0].max():.2f}]\")\nprint(f\"Feature 2 range: [{X_unscaled[:, 1].min():.2f}, {X_unscaled[:, 1].max():.2f}]\")\nprint(f\"Feature scale ratio: {X_unscaled[:, 1].max() / X_unscaled[:, 0].max():.0f}:1\")\n\n# TODO: Train model on UNSCALED data\n# Use learning_rate=0.01, max_iter=1000\nmodel_unscaled = None\n\n# TODO: Standardize the features using StandardScaler\n# Remember: fit on the data, then transform\nscaler = StandardScaler()\n# Fit and transform\nX_scaled = None\n\nprint(f\"\\nAfter scaling:\")\nprint(f\"Feature 1 mean: {X_scaled[:, 0].mean():.4f}, std: {X_scaled[:, 0].std():.4f}\")\nprint(f\"Feature 2 mean: {X_scaled[:, 1].mean():.4f}, std: {X_scaled[:, 1].std():.4f}\")\n\n# TODO: Train model on SCALED data\n# Use the same learning_rate=0.01, max_iter=1000\nmodel_scaled = None\n\n# Compare results\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXPERIMENT 1 RESULTS: Feature Scaling Impact\")\nprint(\"=\"*70)\nprint(f\"\\n**Unscaled Features:**\")\nprint(f\"  Iterations to converge: {model_unscaled.n_iter_}\")\nprint(f\"  Final loss: {model_unscaled.loss_history_[-1]:.6f}\")\nprint(f\"  Learned weights: {model_unscaled.weights_}\")\n\nprint(f\"\\n**Scaled Features:**\")\nprint(f\"  Iterations to converge: {model_scaled.n_iter_}\")\nprint(f\"  Final loss: {model_scaled.loss_history_[-1]:.6f}\")\nprint(f\"  Learned weights: {model_scaled.weights_}\")\n\nprint(f\"\\n**Speedup:** {model_unscaled.n_iter_ / model_scaled.n_iter_:.1f}× faster with scaling!\")\n\n# Visualize convergence comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(model_unscaled.loss_history_, linewidth=2, label='Unscaled', color='red')\nplt.plot(model_scaled.loss_history_, linewidth=2, label='Scaled', color='green')\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Convergence Comparison', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\n# Plot first 100 iterations for detail\nmax_iter_plot = min(100, len(model_unscaled.loss_history_))\nplt.plot(range(max_iter_plot), model_unscaled.loss_history_[:max_iter_plot], \n         linewidth=2, label='Unscaled', color='red')\nplt.plot(range(max_iter_plot), model_scaled.loss_history_[:max_iter_plot], \n         linewidth=2, label='Scaled', color='green')\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('First 100 Iterations (Detail)', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 **Key Insight:** Feature scaling is ESSENTIAL for gradient descent!\")\nprint(\"   Without scaling, convergence is dramatically slower (or may not converge at all).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Hands-On Experiments\n\nNow that you understand the theory, let's run three experiments to see these concepts in action! These experiments will help you develop intuition for how gradient descent behaves in different scenarios.\n\n### Experiment 1: Feature Scaling Impact\n\n**Goal:** Compare convergence with and without feature scaling to see why standardization is ESSENTIAL for gradient descent.\n\n**What you'll observe:**\n- Unscaled features → very slow, zigzag convergence\n- Scaled features → fast, direct convergence\n- Dramatic difference in iterations needed",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Question**: You have a dataset with 10 million training examples and want to train a linear regression model. Which optimization approach is MOST practical?\n>\n> A. Normal equation using closed-form solution computing the exact optimal weights in one step through matrix inversion for guaranteed global optimum\n>\n> B. Batch gradient descent processing all 10 million samples in each iteration to compute exact gradients for stable and reliable convergence\n>\n> C. Mini-batch gradient descent with batches of 128-256 samples for computational efficiency, memory feasibility, and good gradient estimation with stability\n>\n> D. Stochastic gradient descent using exactly one randomly selected sample per iteration for maximum computational speed and fastest possible weight updates\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: C**\n\n**Key Insight:** For large datasets (N > 100,000), mini-batch gradient descent is the practical choice. It balances computational efficiency (fast iterations), memory usage (only B samples in memory), and convergence stability (better gradient estimates than pure SGD). This is why deep learning uses mini-batch GD almost exclusively.\n\n**Explanation:**\n- **A is FALSE**: The normal equation requires computing (ΦᵀΦ)⁻¹Φᵀy, where Φ is 10M × d. This involves: (1) Matrix multiplication O(Nd²) ≈ billions of operations, (2) Matrix inversion O(d³), (3) Storing a 10M × d matrix in memory (potentially gigabytes). For N=10M, this is extremely slow and memory-intensive. Even on modern hardware, storing and manipulating a matrix with 10 million rows is impractical. The normal equation doesn't scale to large datasets.\n- **B is FALSE**: Batch GD must process all 10 million samples in EACH iteration to compute one gradient. Even if each iteration takes 10 seconds and you need 100 iterations, that's 1000 seconds (16 minutes) of computation. Additionally, loading all 10M samples into memory simultaneously is impractical (would require ~GB of RAM). Full batch GD doesn't scale to large datasets - it's too slow per iteration and too memory-intensive.\n- **C is TRUE**: Mini-batch GD with batch size B=128 means: (1) Only 128 samples in memory at once (feasible even on laptops), (2) Fast iterations (~milliseconds each on GPU), (3) Can stream data from disk in chunks, (4) Good gradient estimates with much less computation than full batch. With B=128 and N=10M, each epoch is ~78,000 mini-batches, but each is very fast. This is standard for large-scale ML and deep learning.\n- **D is FALSE**: While SGD (B=1) has very fast iterations, it's TOO noisy for stable convergence on 10M samples. The gradient from a single sample is a poor estimate of the true gradient direction, requiring many epochs (potentially hundreds) and careful learning rate decay to converge. Mini-batch (B=128-256) provides better balance: more stable than pure SGD (less noise), much faster than batch GD. Example: SGD might need 500 epochs vs 50 epochs for mini-batch to reach same loss.\n\n**Numerical Comparison:**\n```\nBatch GD (N=10M):\n  - Time per iteration: 10 seconds\n  - Iterations needed: 100\n  - Total time: 1000 seconds\n  - Memory: All 10M samples (~10GB)\n\nMini-batch GD (B=256):\n  - Time per iteration: 0.01 seconds\n  - Iterations per epoch: 39,000\n  - Epochs needed: 50\n  - Total iterations: 1,950,000\n  - Total time: 195 seconds (9× faster!)\n  - Memory: 256 samples at a time (~2MB)\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Gradient Descent is an iterative optimization algorithm**\n",
    "   - Updates weights in direction that reduces loss: w = w - α∇w L\n",
    "   - Converges to optimal solution through many small steps\n",
    "   - Foundation for training neural networks and deep learning\n",
    "\n",
    "2. **Learning rate (α) is critical**\n",
    "   - Too small → slow convergence (many iterations needed)\n",
    "   - Too large → divergence (loss increases or oscillates)\n",
    "   - Typical range: 0.001 to 0.1\n",
    "   - Monitor loss curve to diagnose issues\n",
    "\n",
    "3. **Feature scaling is ESSENTIAL for gradient descent**\n",
    "   - Unscaled features cause slow/unstable convergence\n",
    "   - Different features need different step sizes → impossible with one α\n",
    "   - Always use StandardScaler or MinMaxScaler\n",
    "   - Fit scaler on training data ONLY!\n",
    "\n",
    "4. **Convergence monitoring**\n",
    "   - Plot loss over iterations\n",
    "   - Healthy curve: decreasing and flattening\n",
    "   - Stop when loss change < tolerance\n",
    "   - Early stopping prevents wasted computation\n",
    "\n",
    "5. **Gradient descent vs Normal equation**\n",
    "   - Normal equation: Fast for small data, exact solution, no tuning\n",
    "   - Gradient descent: Scales to large data, works for any model, needs tuning\n",
    "   - Both converge to same solution for linear regression\n",
    "\n",
    "### When to Use Gradient Descent\n",
    "\n",
    "✅ **Use Gradient Descent when:**\n",
    "- Training neural networks (only option available)\n",
    "- Dataset is very large (N > 100,000)\n",
    "- Online learning (data arrives in streams)\n",
    "- Need mini-batch or stochastic variants\n",
    "- Working with distributed systems (can parallelize mini-batches)\n",
    "\n",
    "❌ **Use Normal Equation when:**\n",
    "- Small-medium dataset (N < 10,000)\n",
    "- Few features (d < 1,000)\n",
    "- Want exact solution without tuning\n",
    "- Simple linear regression\n",
    "\n",
    "### Gradient Descent Variants Summary\n",
    "\n",
    "| Variant | Samples/Iter | Best For |\n",
    "|---------|--------------|----------|\n",
    "| **Batch GD** | All N | Small datasets, smooth convergence |\n",
    "| **Stochastic GD** | 1 | Online learning, escaping local minima |\n",
    "| **Mini-Batch GD** | 32-256 | Large datasets, deep learning (MOST COMMON) |\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "- ✅ Always standardize features using StandardScaler\n",
    "- ✅ Fit scaler on training data only (avoid data leakage)\n",
    "- ✅ Start with learning rate α = 0.01, adjust based on loss curve\n",
    "- ✅ Monitor loss over iterations to diagnose convergence\n",
    "- ✅ Use early stopping to prevent wasted computation\n",
    "- ✅ Initialize weights with small random values\n",
    "- ✅ For large data, use mini-batch variant (B=32-256)\n",
    "- ✅ Compare with closed-form solution when possible (validation)\n",
    "- ✅ Use learning rate decay for better convergence (advanced)\n",
    "- ✅ Visualize loss curves to understand convergence behavior\n",
    "\n",
    "### Debugging Gradient Descent\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Loss increasing | α too large | Decrease learning rate by 10× |\n",
    "| Loss oscillating | α too large | Decrease learning rate |\n",
    "| Very slow convergence | α too small OR unscaled features | Increase α or standardize features |\n",
    "| Loss stuck at high value | Poor initialization OR bad α | Try different random seed or α |\n",
    "| NaN/Inf values | Gradient explosion | Standardize features, decrease α |\n",
    "| Doesn't converge after 10k iter | Unscaled features | Standardize features! |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Final Question**: You're training a neural network and observe that the training loss decreases smoothly for 50 epochs, then suddenly starts increasing. What is the MOST likely explanation?\n>\n> A. The model has successfully converged to the optimal minimum region and is now oscillating around it with small fluctuations\n>\n> B. The learning rate should be increased to larger values to accelerate convergence and escape local plateaus or saddle points\n>\n> C. The learning rate is too high for later epochs when approaching minimum and needs decay or reduction to prevent overshooting\n>\n> D. Feature standardization was skipped during preprocessing, causing numerical instability that manifests only in later training stages after many weight updates\n\n<details><summary>Click to reveal answer</summary>\n\n**Correct Answer: C**\n\n**Key Insight:** Loss decreasing then increasing indicates the learning rate is too large for the current optimization stage. Early in training (far from minimum), large steps work well. Near the minimum (after ~50 epochs), the same large steps overshoot. Solution: use learning rate decay/scheduling to reduce α over time.\n\n**Explanation:**\n- **A is FALSE**: If the model had converged optimally, the loss would plateau (stay relatively constant with small fluctuations around the minimum), not suddenly increase. Increasing loss means the model is moving AWAY from the optimal solution, which is the opposite of convergence. Oscillation around a minimum would show small fluctuations (±0.01), not systematic increase (e.g., loss going from 0.5 to 0.8 to 1.2). A converged model would show: epoch 50 loss=0.50, epoch 51 loss=0.51, epoch 52 loss=0.49 (tiny variations).\n- **B is FALSE**: If loss is already increasing after epoch 50, making the learning rate LARGER will make the problem dramatically worse! The model is already overshooting the minimum (taking steps too large for the narrow loss valley), so bigger steps would cause even more overshooting and potentially divergence to infinity. This would accelerate the problem, not solve it. Example: if α=0.1 causes overshooting, increasing to α=0.5 would cause massive divergence.\n- **C is TRUE**: This is a classic pattern in neural network training: initially, when weights are far from optimal (random initialization), a larger learning rate (e.g., α=0.1) works well for rapid progress. But as the model approaches the minimum (after ~50 epochs), those same large steps start overshooting because the loss surface becomes very narrow near the minimum. The learning rate that worked early now causes instability. Solution: learning rate decay/scheduling (reduce α over time, e.g., α_epoch50 = α_initial × 0.1).\n- **D is FALSE**: If features weren't standardized, you'd see problems from the VERY FIRST epoch - extremely slow convergence, erratic oscillations, or immediate divergence (loss increasing from epoch 1). The fact that loss decreased smoothly for 50 epochs proves features were properly scaled and the optimization was working correctly. This issue starting specifically at epoch 50 indicates a learning rate problem related to later training stages (approaching minimum), not a data preprocessing issue that would appear immediately.\n\n**Real-world solution: Learning Rate Schedules**\n```python\n# Common schedules to prevent this problem:\n\n# 1. Step Decay: Reduce α every N epochs\nif epoch == 50:\n    learning_rate *= 0.1  # Reduce by 10×\n\n# 2. Exponential Decay: Gradual reduction\nlearning_rate = initial_lr * exp(-decay_rate * epoch)\n\n# 3. Cosine Annealing: Smooth curve\nlearning_rate = min_lr + 0.5*(max_lr - min_lr)*(1 + cos(π*epoch/total_epochs))\n```\n\nThis is standard practice in deep learning - start high for fast progress, reduce over time for stable convergence.\n\n</details>"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}