{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20with%20California%20Housing%20Dataset%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX4UdkmNEu5v"
   },
   "source": [
    "# Case Study: KNN Regression with California Housing Dataset\n",
    "\n",
    "K\u2011Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that can be applied to both classification and regression tasks. In **regression**, instead of voting for a class label, KNN predicts a continuous value by **averaging** the target values of the K nearest neighbors. In this case study, we'll step through a practical example using the **California Housing** dataset to illustrate key concepts and best practices of KNN regression. This dataset contains information about housing blocks in California from the 1990 census, with 8 features (e.g., median income, house age, average rooms, location) and a target variable representing the median house value.\n",
    "\n",
    "## What we'll cover\n",
    "- **Data exploration and preparation:** Understanding feature distributions and splitting data into training, validation, and test sets.  \n",
    "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance in regression tasks.  \n",
    "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
    "- **Distance metric considerations:** How the choice of distance measure can affect KNN predictions.  \n",
    "- **Model evaluation:** Evaluating the final model using regression metrics (RMSE, MAE, R\u00b2) on a test set to ensure it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Learning Objectives\n\nBy the end of this case study, you will be able to:\n\n1. **Understand the critical role of feature scaling** in distance-based algorithms and demonstrate its impact on model performance\n2. **Apply systematic hyperparameter tuning** using train/validation/test splits to avoid overfitting\n3. **Recognize and explain the bias-variance tradeoff** when selecting the number of neighbors (K)\n4. **Evaluate regression models** using multiple metrics (RMSE, MAE, R\u00b2, MAPE) and interpret their meaning\n5. **Analyze prediction errors** using percentage error plots to identify model strengths and limitations\n6. **Make informed decisions** about when KNN regression is appropriate for a given problem\n7. **Understand computational implications** of KNN for real-world deployment scenarios",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGQ_Ft3-Eu5w"
   },
   "source": [
    "## Exploring the Dataset\n",
    "Before diving into modeling, let's load the dataset and examine its features. The California Housing dataset has 20,640 samples, each with 8 features. The target is `MedHouseVal` (median house value in $100,000s).\n",
    "\n",
    "**Features:**  \n",
    "- MedInc: median income in block group  \n",
    "- HouseAge: median house age in block group  \n",
    "- AveRooms: average number of rooms per household  \n",
    "- AveBedrms: average number of bedrooms per household  \n",
    "- Population: block group population  \n",
    "- AveOccup: average number of household members  \n",
    "- Latitude: block group latitude  \n",
    "- Longitude: block group longitude  \n",
    "\n",
    "Large differences in magnitude (e.g., *Population* in thousands vs *AveBedrms* around 1) motivate **scaling** before using distance-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4U_4SkjEu5w"
   },
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['MedHouseVal'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s73rtt_6Eu5x"
   },
   "source": [
    "Let's examine the target variable distribution and basic feature statistics before proceeding. \n",
    "\n",
    "**Data Splitting Strategy:**  \n",
    "We split the data into three sets with a 60/20/20 ratio:\n",
    "- **Training set (60%)**: Used to fit the model (learn from the data)\n",
    "- **Validation set (20%)**: Used to tune hyperparameters (select best K, distance metric, etc.)\n",
    "- **Test set (20%)**: Held out completely until final evaluation\n",
    "\n",
    "This three-way split is crucial because hyperparameter tuning on the validation set can lead to overfitting those specific choices to that particular data subset. The test set provides an unbiased performance estimate on truly unseen data.\n",
    "\n",
    "> **Question**: You've used the validation set to tune K from 1 to 30, ultimately selecting K=7 with validation RMSE of $0.52. Why is it critical to evaluate on a separate test set before deploying the model?\n",
    ">  \n",
    "> A) The validation set was used for hyperparameter selection, which can lead to optimistic performance estimates.\n",
    ">\n",
    "> B) The test set provides additional opportunities to fine-tune hyperparameters for better accuracy.\n",
    ">\n",
    "> C) Validation RMSE is systematically biased upward and always overestimates real-world error.\n",
    ">\n",
    "> D) The test set helps identify which features should be added or removed from the model.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: A**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**A is TRUE**: The validation set was used for hyperparameter selection, creating optimistic bias.\n",
    "- **What happened during tuning**:\n",
    "  - You tried 30 different K values (K=1 through K=30) on the validation set\n",
    "  - You selected K=7 specifically because it achieved the best validation RMSE ($0.52)\n",
    "  - This means K=7 was **chosen to perform well on this particular validation subset**\n",
    "- **Why this creates bias**:\n",
    "  - The validation RMSE of $0.52 represents the **best case** among 30 trials\n",
    "  - Some of this performance may be due to chance alignment between K=7 and the specific validation examples\n",
    "  - This is called \"peeking\" or \"data snooping\" - you've indirectly fit to the validation set through selection\n",
    "- **Why the test set is critical**:\n",
    "  - The test set played **zero role** in choosing K=7\n",
    "  - It provides an **unbiased estimate** of how K=7 performs on new data\n",
    "  - Test RMSE might be higher (e.g., $0.58) due to removing selection bias\n",
    "\n",
    "**B is FALSE**: The test set must NEVER be used for hyperparameter tuning\n",
    "- Using the test set to fine-tune hyperparameters destroys its value as an unbiased estimator\n",
    "- This is a fundamental violation of the train/validation/test methodology\n",
    "- Once you optimize on test data, you have no remaining unbiased estimate\n",
    "\n",
    "**C is FALSE**: Validation RMSE is not systematically biased upward\n",
    "- In fact, it's typically biased **downward** (optimistic) due to hyperparameter selection\n",
    "- We choose hyperparameters that minimize validation error, so validation performance tends to be better than test\n",
    "- The direction of bias is toward overestimation of performance, not underestimation\n",
    "\n",
    "**D is FALSE**: The test set doesn't guide feature engineering\n",
    "- Feature engineering decisions should be made using training/validation data\n",
    "- The test set is only for final performance evaluation\n",
    "- Using test set to decide feature changes would introduce data leakage\n",
    "\n",
    "**Key Insight**: Think of hyperparameter tuning as \"training\" your modeling choices. The validation set is the data you use for this meta-training. Therefore, validation performance is optimistically biased, just like training performance would be. The test set is the only truly untouched dataset that can give you an honest answer.\n",
    "\n",
    "**Expected Pattern**:\n",
    "```\n",
    "Validation RMSE: $0.52 (best of 30 trials)\n",
    "Test RMSE: $0.55-0.60 (unbiased estimate, likely slightly worse)\n",
    "\n",
    "If test RMSE is much worse (e.g., $0.80), it suggests overfitting to validation set.\n",
    "If test RMSE is better (e.g., $0.48), it suggests lucky test split or possible data leakage.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWETnC40Eu5x"
   },
   "outputs": [],
   "source": [
    "# Examine target distribution\n",
    "print(\"Target variable (MedHouseVal) statistics:\")\n",
    "print(df['MedHouseVal'].describe(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics of features\n",
    "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Plot target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Median House Value ($100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y)\n",
    "plt.ylabel('Median House Value ($100k)')\n",
    "plt.title('Box Plot of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPJrOdksEu5y"
   },
   "source": [
    "## Effect of Feature Scaling on KNN Regression\n",
    "\n",
    "Just like in classification, KNN regression uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The example below illustrates how a difference in *Population* (thousands) can swamp a difference in *AveBedrms* (around 1). Therefore, scaling features to comparable ranges is critical for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG7bsMzZEu5y"
   },
   "outputs": [],
   "source": [
    "# Demonstrate distance dominance (hypothetical differences)\n",
    "from math import sqrt\n",
    "\n",
    "delta_population_large = 1000.0\n",
    "delta_bedrooms_small = 0.5\n",
    "\n",
    "d1 = sqrt(delta_population_large**2 + 0.0**2)\n",
    "d2 = sqrt(0.0**2 + delta_bedrooms_small**2)\n",
    "\n",
    "print(\"Distance if only Population differs by +1000:\", round(d1, 3))\n",
    "print(\"Distance if only AveBedrms differs by +0.5  :\", round(d2, 3))\n",
    "print(\"Ratio (Population / Bedrooms):\", round(d1 / d2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zowu7MFEu5y"
   },
   "source": [
    "Next, we train a baseline KNN regression model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. We'll use RMSE (Root Mean Squared Error) as our primary metric. Note that we scale using parameters learned from the training set only to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDufnDnNEu5y"
   },
   "outputs": [],
   "source": [
    "# Baseline without scaling\n",
    "knn_raw = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "raw_val_pred = knn_raw.predict(X_val)\n",
    "raw_val_rmse = np.sqrt(mean_squared_error(y_val, raw_val_pred))\n",
    "\n",
    "# Baseline with scaling (fit on train only)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_val_pred = knn_scaled.predict(X_val_scaled)\n",
    "scaled_val_rmse = np.sqrt(mean_squared_error(y_val, scaled_val_pred))\n",
    "\n",
    "print(f\"Validation RMSE without scaling: ${raw_val_rmse:.3f} (\u00d7100k)\")\n",
    "print(f\"Validation RMSE with scaling:    ${scaled_val_rmse:.3f} (\u00d7100k)\")\n",
    "print(f\"Improvement: {((raw_val_rmse - scaled_val_rmse) / raw_val_rmse * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEG3ejXFEu5y"
   },
   "source": [
    "The scaled model typically performs significantly better because each feature contributes fairly to distance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Key Takeaway:** Feature scaling is essential for KNN regression. Without scaling, features with larger ranges (like Population) dominate distance calculations, while important features with smaller ranges (like AveBedrms) are effectively ignored. **Always scale features before using KNN.** From this point forward, all models will use scaled features.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJqvWHYAEu5z"
   },
   "source": [
    "## Distance Metric Considerations\n",
    "\n",
    "Beyond scaling, the choice of **distance metric** itself is a hyperparameter that affects KNN predictions. Common options include:\n",
    "- **Euclidean (L2)**: Straight-line distance; squares differences before summing (default choice)\n",
    "- **Manhattan (L1)**: Sum of absolute differences; can be more robust to outliers in some cases\n",
    "\n",
    "The distance metric determines how we measure similarity between points. When features are unscaled, the metric choice matters less than the scaling issue\u2014features with larger numerical ranges will dominate the distance calculation regardless of whether you use Euclidean or Manhattan distance.\n",
    "\n",
    "> **Question**: Your KNN model uses 3 features: 'MedInc' (range 0-15), 'Population' (range 0-35,000), and 'Latitude' (range 32-42). Without scaling, which feature will dominate the distance calculations, and why?\n",
    ">\n",
    "> A) MedInc\u2014it has the strongest correlation with house prices\n",
    ">\n",
    "> B) Population\u2014it has the largest numerical range\n",
    ">\n",
    "> C) Latitude\u2014geographic features always have higher weight in distance metrics\n",
    ">\n",
    "> D) All features contribute equally because KNN normalizes distances automatically\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**B is TRUE**: Population will dominate because it has the largest numerical range.\n",
    "- **How KNN calculates distance** (using Euclidean as example):\n",
    "  ```\n",
    "  distance = \u221a[(MedInc\u2081 - MedInc\u2082)\u00b2 + (Pop\u2081 - Pop\u2082)\u00b2 + (Lat\u2081 - Lat\u2082)\u00b2]\n",
    "  ```\n",
    "- **Numerical ranges**:\n",
    "  - MedInc: 0-15 \u2192 maximum difference = 15\n",
    "  - Population: 0-35,000 \u2192 maximum difference = 35,000\n",
    "  - Latitude: 32-42 \u2192 maximum difference = 10\n",
    "- **Impact in distance calculation**:\n",
    "  - MedInc contribution: (15)\u00b2 = 225\n",
    "  - Population contribution: (35,000)\u00b2 = 1,225,000,000\n",
    "  - Latitude contribution: (10)\u00b2 = 100\n",
    "- **Result**: Population dominates by a factor of ~5,000,000x\n",
    "  - The model essentially becomes a 1-feature model based on Population alone\n",
    "  - MedInc and Latitude are effectively ignored in neighbor selection\n",
    "\n",
    "**Concrete Example**:\n",
    "```\n",
    "House A: MedInc=5.0, Population=1000, Lat=37.5\n",
    "House B: MedInc=5.1, Population=1001, Lat=37.6  # Similar in all features\n",
    "House C: MedInc=10.0, Population=20000, Lat=39.0  # Different in all features\n",
    "\n",
    "Distance A to B: \u221a[(0.1)\u00b2 + (1)\u00b2 + (0.1)\u00b2] \u2248 1.01  (mostly from Population)\n",
    "Distance A to C: \u221a[(5)\u00b2 + (19000)\u00b2 + (1.5)\u00b2] \u2248 19,000  (dominated by Population)\n",
    "\n",
    "KNN will consider B as House A's nearest neighbor, even though they're similar \n",
    "in all features, solely because Population difference is minimal.\n",
    "```\n",
    "\n",
    "**A is FALSE**: Correlation with the target doesn't affect distance calculations\n",
    "- Distance metrics (Euclidean, Manhattan) only use feature values, not correlations\n",
    "- Even if MedInc has r=0.9 correlation with price, it won't dominate the distance if its numerical range is small\n",
    "- Feature importance and distance dominance are completely different concepts\n",
    "\n",
    "**C is FALSE**: Geographic features don't inherently have higher weight\n",
    "- Latitude is treated identically to any other numeric feature\n",
    "- Its weight in distance calculations depends solely on its numerical range and scale\n",
    "- Geographic coordinates only matter if they have large ranges relative to other features\n",
    "\n",
    "**D is FALSE**: KNN does NOT automatically normalize distances\n",
    "- KNN uses raw feature values directly in distance calculations\n",
    "- No automatic scaling, standardization, or normalization happens\n",
    "- Features with larger ranges mathematically dominate the distance calculation\n",
    "- **You must manually scale features** (e.g., using StandardScaler) to ensure fair contribution\n",
    "\n",
    "**Key Insight**: Distance-based algorithms like KNN are **scale-sensitive**. Without preprocessing, features with larger numerical ranges (like counts, populations, income in dollars) will dominate over features with smaller ranges (like rates, percentages, normalized scores). This is why feature scaling is not optional - it's essential for KNN.\n",
    "\n",
    "**Solution**: Apply StandardScaler before KNN:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Mean=0, Std=1 for all features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now all features contribute equally to distance calculations\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6r8SSVHEu50"
   },
   "source": [
    "## Choosing K: Bias\u2013Variance Trade\u2011off\n",
    "\n",
    "The number of neighbors (K) controls the model's complexity and represents a fundamental bias-variance tradeoff:\n",
    "\n",
    "**Small K (e.g., K=1)**:\n",
    "- Highly flexible, fits training data very closely\n",
    "- **High variance**: Sensitive to noise in individual training points\n",
    "- Risk of overfitting: Excellent training performance but poor generalization\n",
    "\n",
    "**Large K (e.g., K=100+)**:\n",
    "- Averages over many neighbors, produces smoother predictions\n",
    "- **High bias**: May miss local patterns and underfit the data\n",
    "- More stable but potentially too simple\n",
    "\n",
    "We'll sweep K from 1 to 30 and compare training vs. validation RMSE to find the sweet spot. A large gap between training and validation error signals overfitting.\n",
    "\n",
    "> **Question**: After training KNN models with different K values, you observe that K=1 achieves training RMSE of \\$0.05 but validation RMSE of \\$0.75, while K=5 achieves training RMSE of \\$0.45 and validation RMSE of \\$0.52. What does this pattern suggest about the K=1 model?\n",
    ">  \n",
    "> A) The model has high variance and is overfitting to training noise.\n",
    ">\n",
    "> B) The model has high bias and requires more complex features.\n",
    ">\n",
    "> C) The training dataset is too small and more samples would fix the issue.\n",
    ">\n",
    "> D) Add more features to improve generalization.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: A**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**A is TRUE**: The K=1 model has high variance and is overfitting to training noise.\n",
    "- **The evidence**:\n",
    "  - Training RMSE: $0.05 (nearly perfect)\n",
    "  - Validation RMSE: $0.75 (very poor)\n",
    "  - **Massive gap**: $0.70 difference indicates severe overfitting\n",
    "- **Why K=1 causes overfitting**:\n",
    "  - Each prediction uses only the single nearest training point\n",
    "  - The model memorizes individual training examples instead of learning patterns\n",
    "  - **High variance**: Predictions are extremely sensitive to which single point happens to be nearest\n",
    "  - Random noise in training data gets fully incorporated into predictions\n",
    "- **What's happening**:\n",
    "  - For training data, K=1 achieves near-perfect predictions (each point predicts itself or a very similar neighbor)\n",
    "  - For validation data, K=1 finds the single nearest training point, which may be an outlier or noisy example\n",
    "  - No averaging or smoothing occurs, so predictions are unstable\n",
    "\n",
    "**Comparison with K=5**:\n",
    "- K=5 training RMSE: $0.45 (good but not perfect)\n",
    "- K=5 validation RMSE: $0.52 (much better generalization)\n",
    "- **Small gap**: Only $0.07 difference indicates much better bias-variance tradeoff\n",
    "- **Averaging over 5 neighbors** smooths out noise and captures genuine patterns\n",
    "\n",
    "**B is FALSE**: This is not a bias problem; K=1 has very LOW bias\n",
    "- **Bias** = systematic error from model assumptions that are too simple\n",
    "- K=1 can fit extremely complex patterns (in fact, infinitely complex decision boundaries)\n",
    "- The issue is **variance** (sensitivity to training data), not bias (oversimplification)\n",
    "- High bias would show as poor performance on BOTH training and validation sets\n",
    "\n",
    "**C is FALSE**: More training data won't fix fundamental overfitting from K=1\n",
    "- K=1 will continue to memorize new training data rather than generalizing\n",
    "- You'd just memorize more examples without improving validation performance\n",
    "- The solution is to increase K, not increase data\n",
    "- More data helps when you have the right model complexity; K=1 is too flexible\n",
    "\n",
    "**D is FALSE**: More features would make overfitting worse, not better\n",
    "- With K=1 already overfitting, adding features increases model complexity\n",
    "- More dimensions \u2192 distances become less meaningful (curse of dimensionality)\n",
    "- K=1 with more features would memorize training data even more precisely\n",
    "- The solution is to reduce model flexibility (increase K), not add complexity\n",
    "\n",
    "**Key Insight**: The bias-variance tradeoff in KNN is controlled entirely by K:\n",
    "- **Small K** (like K=1): Low bias (flexible), High variance (unstable) \u2192 Overfitting\n",
    "- **Large K** (like K=100): High bias (inflexible), Low variance (stable) \u2192 Underfitting  \n",
    "- **Optimal K** (like K=5-15): Balanced bias and variance \u2192 Best generalization\n",
    "\n",
    "**The fix**: Increase K to smooth predictions:\n",
    "```\n",
    "Try K=5:  Training might drop to $0.45, but validation improves to $0.52\n",
    "Try K=10: Training might drop to $0.50, but validation improves to $0.48  \n",
    "Try K=20: Training might drop to $0.55, but validation might drop to $0.50\n",
    "\n",
    "Stop when validation RMSE stops improving.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "source": "train_rmse, val_rmse = [], []\nk_sweep = range(1, 31)\n\nfor k in k_sweep:\n    model = KNeighborsRegressor(n_neighbors=k)\n    model.fit(X_train_scaled, y_train)\n    train_pred = model.predict(X_train_scaled)\n    val_pred = model.predict(X_val_scaled)\n    train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))\n    val_rmse.append(np.sqrt(mean_squared_error(y_val, val_pred)))\n\n# Best K by validation RMSE\nbest_k_idx = int(np.argmin(val_rmse))\nchosen_k = best_k_idx + 1\nbest_val = min(val_rmse)\n\n# Use Euclidean distance (default and most commonly used)\nchosen_metric = 'euclidean'\n\nprint(\"Selected hyperparameters:\")\nprint(f\"  K = {chosen_k}\")\nprint(f\"  Distance metric = {chosen_metric}\")\nprint(f\"  Validation RMSE = ${best_val:.3f} (\u00d7100k)\")\n\n# Plot train vs validation RMSE vs K\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_sweep), train_rmse, marker='o', label='Train RMSE', linewidth=2)\nplt.plot(list(k_sweep), val_rmse, marker='s', label='Validation RMSE', linewidth=2)\nplt.axvline(chosen_k, linestyle='--', color='red', label=f'Best K={chosen_k}')\nplt.xlabel('K (Number of Neighbors)', fontsize=12)\nplt.ylabel('RMSE ($100k)', fontsize=12)\nplt.title('Bias-Variance Trade-off: RMSE vs K', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90-W_X4wEu50"
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final training\n",
    "X_train_all = np.vstack([X_train, X_val])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "# Build pipeline (scaler + KNN) with chosen hyperparameters\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_pipe.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE:  ${test_rmse:.3f} (\u00d7100k) = ${test_rmse*100:.0f}k\")\n",
    "print(f\"MAE:   ${test_mae:.3f} (\u00d7100k) = ${test_mae*100:.0f}k\")\n",
    "print(f\"R\u00b2:    {test_r2:.3f}\")\n",
    "print(f\"MAPE:  {test_mape*100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmzXwto8Eu50"
   },
   "source": [
    "## Model Evaluation on Test Set\n",
    "\n",
    "After selecting optimal hyperparameters using the validation set, we perform a final evaluation on the **held-out test set**. This is critical for obtaining an unbiased estimate of real-world performance.\n",
    "\n",
    "**Final Training Process:**\n",
    "1. Combine training + validation sets (now that hyperparameter tuning is complete)\n",
    "2. Refit the model on this combined dataset\n",
    "3. Evaluate once on the test set\n",
    "4. Compare test performance to validation performance\n",
    "\n",
    "**Expected Behavior:**  \n",
    "Test performance typically matches validation performance closely. A small difference (e.g., validation RMSE $0.52 vs. test RMSE $0.56) is normal due to random variation in data splits. A large gap would suggest overfitting to the validation set during hyperparameter tuning.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **RMSE (Root Mean Squared Error)**: Penalizes large errors more heavily; same units as target ($100k)\n",
    "- **MAE (Mean Absolute Error)**: Average absolute prediction error; more interpretable\n",
    "- **R\u00b2 Score**: Proportion of variance explained (1.0 = perfect, 0.0 = baseline)\n",
    "- **MAPE (Mean Absolute Percentage Error)**: Percentage error; useful for relative comparison\n",
    "\n",
    "> **Question**: Your final KNN model achieves validation RMSE of $0.52 and test RMSE of \\$0.56. Before deployment, what's the most appropriate interpretation?\n",
    ">  \n",
    "> A) The difference is normal variation; verify test performance meets business requirements.\n",
    ">\n",
    "> B) The test set has data leakage and should be regenerated with better random seeds.\n",
    ">\n",
    "> C) Re-tune hyperparameters using the test set to minimize the performance gap.\n",
    ">\n",
    "> D) The model is underfitting and needs a smaller K value for better flexibility.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Correct Answer: A**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**A is TRUE**: The small difference is normal variation; verify that 0.56 meets your requirements.\n",
    "- **Why the difference is normal**:\n",
    "  - Validation RMSE ($0.52) came from a specific 20% subset used during hyperparameter tuning\n",
    "  - Test RMSE ($0.56) comes from a different 20% subset held out completely\n",
    "  - Random variation in data splits naturally causes differences of ~$0.02-0.08\n",
    "  - A $0.04 increase (7.7% relative increase) is well within expected range\n",
    "- **What this tells you**:\n",
    "  - The model generalizes reasonably well to new data\n",
    "  - No signs of severe overfitting to the validation set\n",
    "  - Test RMSE ($0.56) is your best estimate of real-world performance\n",
    "- **Next steps**:\n",
    "  - **Check business requirements**: Can you tolerate $56,000 average prediction error?\n",
    "  - **Compare to baseline**: How does this compare to simpler models (e.g., predicting the mean)?\n",
    "  - **Document results**: Report test RMSE as expected production performance\n",
    "  - **Deploy if acceptable**: If $0.56 meets requirements, proceed with deployment\n",
    "\n",
    "**B is FALSE**: This difference does NOT indicate data leakage\n",
    "- **Data leakage symptoms**:\n",
    "  - Unrealistically good performance on both validation and test (e.g., both RMSE < $0.10)\n",
    "  - Test performance significantly better than validation (unexpected improvement)\n",
    "  - Perfect or near-perfect predictions that don't make sense\n",
    "- **This case**: Test is slightly worse than validation, which is the expected pattern\n",
    "- **No reason to suspect leakage**: The results show normal train/validation/test behavior\n",
    "\n",
    "**C is FALSE**: Re-tuning on the test set violates fundamental ML principles\n",
    "- **Why this is wrong**:\n",
    "  - The test set must remain completely untouched for hyperparameter decisions\n",
    "  - Using test set for tuning destroys its value as an unbiased estimator  \n",
    "  - Creates data snooping bias - you've now optimized for this specific test set\n",
    "  - Future performance will likely be worse than the re-tuned test results\n",
    "- **Correct approach**:\n",
    "  - Accept test RMSE ($0.56) as your honest performance estimate\n",
    "  - If you want to improve, collect new data and redo the entire train/val/test split\n",
    "  - NEVER peek at test set during development\n",
    "\n",
    "**D is FALSE**: The model is not underfitting based on this evidence\n",
    "- **Underfitting signs**:\n",
    "  - Large RMSE on both training AND validation sets\n",
    "  - Model too simple to capture data patterns (e.g., K=50+ might underfit)\n",
    "  - Little difference between training and validation performance (both poor)\n",
    "- **This case**: Validation RMSE of $0.52 suggests reasonable fit\n",
    "  - Without seeing training RMSE, we can't diagnose underfitting\n",
    "  - Small increase from validation ($0.52) to test ($0.56) indicates good generalization\n",
    "- **Smaller K warning**: Decreasing K would increase variance and risk overfitting\n",
    "  - You already selected optimal K using the validation set\n",
    "  - Changing K based on test results is inappropriate\n",
    "\n",
    "**Key Insight**: The test set provides your reality check. Small differences from validation (typically 5-15%) are expected and normal. Accept this as your true performance estimate and make deployment decisions based on business requirements, not by trying to optimize the test set.\n",
    "\n",
    "**Performance Interpretation**:\n",
    "```python\n",
    "Test RMSE: $0.56 (or $56,000 average error per prediction)\n",
    "\n",
    "Context needed:\n",
    "- If houses range from $100k-$500k, 11-56% error \u2192 might be acceptable\n",
    "- If predicting for luxury homes ($1M+), $56k error is excellent  \n",
    "- If predicting low-cost housing ($100k-150k), $56k error is poor\n",
    "\n",
    "Compare to baselines:\n",
    "- Predicting mean price: RMSE might be $0.80\n",
    "- Predicting median by region: RMSE might be $0.65\n",
    "- Your KNN model: RMSE $0.56 \u2192 better than simple baselines \u2713\n",
    "```\n",
    "\n",
    "**Deployment Decision**:\n",
    "- \u2713 Test RMSE ($0.56) < business requirement \u2192 Deploy\n",
    "- \u2717 Test RMSE ($0.56) > business requirement \u2192 Don't deploy, try different approach\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Where Do Errors Come From?\n",
    "\n",
    "Understanding where KNN regression makes larger errors helps us interpret model performance and identify areas for improvement. As shown in the lecture slides, prediction errors in KNN regression come from two types of regions:\n",
    "\n",
    "### High Certainty Regions\n",
    "- **Characteristics**: Neighbors have similar target values with low variation\n",
    "- **Prediction Quality**: Low residual errors (high confidence predictions)\n",
    "- **Why**: When K nearest neighbors have similar values, their average is a reliable estimate\n",
    "- **Example**: In dense neighborhoods where houses have similar prices\n",
    "\n",
    "### High Ambiguity Regions  \n",
    "- **Characteristics**: Neighbors have high variation in target values\n",
    "- **Prediction Quality**: High residual errors (uncertain predictions)\n",
    "- **Why**: When K nearest neighbors have very different values, averaging produces less reliable estimates\n",
    "- **Example**: Boundary regions between expensive and affordable neighborhoods, or sparse data regions\n",
    "\n",
    "### Additional Error Sources\n",
    "- **Sparse Regions**: Areas with few training points lead to unreliable neighbor selection\n",
    "- **Boundary Regions**: Transition zones where the K neighbors remain constant but represent different underlying patterns\n",
    "\n",
    "> **Key Insight**: KNN performs best in regions where neighbors have consistent target values. The model struggles in regions with high local variability or sparse data, as the averaging assumption breaks down."
   ],
   "metadata": {
    "id": "BvPFVKtYEu50"
   }
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate high certainty vs high ambiguity regions\nfrom sklearn.metrics import pairwise_distances\n\n# Get predictions for all training points\ntrain_pred_all = final_pipe.predict(X_train_all)\ntrain_errors = np.abs(y_train_all - train_pred_all)\n\n# For each training point, calculate the standard deviation of its K nearest neighbors' target values\nX_train_all_scaled = final_pipe.named_steps['scaler'].transform(X_train_all)\nneighbor_std = []\n\nfor i in range(len(X_train_all_scaled)):\n    # Calculate distances from this point to all other training points\n    distances = pairwise_distances(X_train_all_scaled[i:i+1], X_train_all_scaled, metric=chosen_metric).ravel()\n    # Find K+1 nearest (including itself)\n    k_nearest_idx = np.argsort(distances)[1:chosen_k+1]  # Skip index 0 (itself)\n    # Calculate std of neighbors' target values\n    neighbor_targets = y_train_all[k_nearest_idx]\n    neighbor_std.append(np.std(neighbor_targets))\n\nneighbor_std = np.array(neighbor_std)\n\n# Identify high certainty and high ambiguity regions\nlow_std_threshold = np.percentile(neighbor_std, 25)\nhigh_std_threshold = np.percentile(neighbor_std, 75)\n\nhigh_certainty_mask = neighbor_std < low_std_threshold\nhigh_ambiguity_mask = neighbor_std > high_std_threshold\n\nprint(\"=\"*60)\nprint(\"ERROR ANALYSIS BY REGION TYPE\")\nprint(\"=\"*60)\nprint(f\"\\nHigh Certainty Regions (neighbor std < {low_std_threshold:.3f}):\")\nprint(f\"  - Number of points: {high_certainty_mask.sum()}\")\nprint(f\"  - Avg neighbor std: ${np.mean(neighbor_std[high_certainty_mask]):.3f} (\u00d7100k)\")\nprint(f\"  - Avg prediction error: ${np.mean(train_errors[high_certainty_mask]):.3f} (\u00d7100k)\")\n\nprint(f\"\\nHigh Ambiguity Regions (neighbor std > {high_std_threshold:.3f}):\")\nprint(f\"  - Number of points: {high_ambiguity_mask.sum()}\")\nprint(f\"  - Avg neighbor std: ${np.mean(neighbor_std[high_ambiguity_mask]):.3f} (\u00d7100k)\")\nprint(f\"  - Avg prediction error: ${np.mean(train_errors[high_ambiguity_mask]):.3f} (\u00d7100k)\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Error is {np.mean(train_errors[high_ambiguity_mask]) / np.mean(train_errors[high_certainty_mask]):.1f}x higher in high ambiguity regions!\")\nprint(\"=\"*60)\n\n# Visualize the relationship\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(neighbor_std, train_errors, alpha=0.3, s=10)\nplt.xlabel('Neighbor Target Std Dev ($100k)')\nplt.ylabel('Prediction Error ($100k)')\nplt.title('Error vs Neighbor Variation')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.hist([neighbor_std[high_certainty_mask], neighbor_std[high_ambiguity_mask]], \n         bins=30, label=['High Certainty', 'High Ambiguity'], alpha=0.7)\nplt.xlabel('Neighbor Target Std Dev ($100k)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Region Types')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "19aZ827hEu51"
   },
   "source": "# Plot actual vs predicted for test set\nplt.figure(figsize=(12, 5))\n\n# Scatter plot\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, test_pred, alpha=0.5, edgecolor='k', s=20)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\nplt.xlabel('Actual House Value ($100k)')\nplt.ylabel('Predicted House Value ($100k)')\nplt.title(f'Test Set: Actual vs Predicted\\n(R\u00b2 = {test_r2:.3f}, RMSE = ${test_rmse:.3f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Residual plot - using percentage error\nplt.subplot(1, 2, 2)\npercentage_error = (y_test - test_pred) / y_test * 100\nplt.scatter(test_pred, percentage_error, alpha=0.5, edgecolor='k', s=20)\nplt.axhline(0, color='r', linestyle='--', lw=2)\nplt.xlabel('Predicted House Value ($100k)')\nplt.ylabel('Percentage Error (%)')\nplt.title('Residual Plot (Percentage Error)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Distribution of residuals - using percentage error\nplt.figure(figsize=(10, 4))\nplt.hist(percentage_error, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Percentage Error (%)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Prediction Errors (Percentage)')\nplt.axvline(0, color='r', linestyle='--', lw=2, label='Zero error')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**Understanding the Visualizations:**\n\n1. **Actual vs Predicted Plot (left)**: Points closer to the diagonal line indicate better predictions. Deviations show where the model struggles.\n\n2. **Percentage Error Plot (right)**: Shows prediction errors as a percentage of actual values. Using percentage error instead of absolute error provides better insights:\n   - **Removes scale dependency**: A $50k error on a $500k house (10%) is very different from a $50k error on a $100k house (50%)\n   - **Prevents fan-out pattern**: Absolute residuals often increase with predicted values; percentage errors should be more evenly distributed\n   - **Easier interpretation**: We can quickly identify if errors are acceptable (e.g., within \u00b120%)\n\n**What to look for:** Ideally, percentage errors should be randomly scattered around zero with no clear patterns. Systematic patterns (e.g., consistent over/under-prediction for certain price ranges) suggest model limitations.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Importance Analysis\n\nUnlike tree-based models, KNN doesn't have built-in feature importance scores. However, we can use **permutation importance** to understand which features contribute most to predictions. This technique randomly shuffles each feature one at a time and measures how much the model's performance drops\u2014larger drops indicate more important features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N-iWR6yEu51"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    final_pipe, X_test, y_test,\n",
    "    n_repeats=10, random_state=42,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], xerr=importance_df['std'])\n",
    "plt.xlabel('Decrease in R\u00b2 (Importance)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Common Pitfalls and Best Practices\n\nWatch out for these common mistakes when using KNN regression:\n\n### Critical Mistakes to Avoid:\n1. **Forgetting to scale features** \u274c  \n   - This is the #1 mistake with KNN. Features with larger ranges will completely dominate distance calculations\n   - **Always** use StandardScaler or MinMaxScaler before applying KNN\n\n2. **Using test set for hyperparameter tuning** \u274c  \n   - Never tune K or distance metrics using the test set\n   - Use a separate validation set or cross-validation for hyperparameter selection\n\n3. **Choosing K=1 for production** \u274c  \n   - K=1 is extremely sensitive to noise and outliers\n   - While it may show perfect training performance, it rarely generalizes well\n   - Start with K=5 as a reasonable default and tune from there\n\n4. **Ignoring computational cost** \u274c  \n   - KNN stores all training data and computes distances at prediction time\n   - For large datasets (millions of rows), KNN can be prohibitively slow\n   - Consider approximate nearest neighbor methods for large-scale applications\n\n### Best Practices:\n- \u2705 **Always visualize** your predictions vs actuals to spot patterns in errors\n- \u2705 **Use cross-validation** for more robust hyperparameter tuning (see Limitations section)\n- \u2705 **Consider distance-weighted KNN** (`weights='distance'`) to give closer neighbors more influence\n- \u2705 **Remove outliers** or use robust scaling if your target variable has extreme values\n- \u2705 **Try feature selection** to reduce dimensionality and improve performance (curse of dimensionality)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Computational Complexity\n\nUnderstanding KNN's computational characteristics is crucial for real-world deployment:\n\n### Training Complexity: O(1)\n- **KNN is a \"lazy learner\"**: It doesn't actually learn a model during training\n- Training simply stores the feature vectors and target values in memory\n- This makes training instantaneous, regardless of dataset size\n\n### Prediction Complexity: O(n \u00d7 d)\n- **For each prediction**, KNN must:\n  - Calculate distance to all n training points\n  - Each distance calculation involves d features\n  - Sort or partially sort distances to find K nearest neighbors\n- This becomes expensive for large datasets or real-time applications\n\n### Example: Prediction Time\nLet's measure the prediction time for our model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\n\n# Measure training time\nstart = time.time()\nquick_knn = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric))\n])\nquick_knn.fit(X_train_all, y_train_all)\ntrain_time = time.time() - start\n\n# Measure prediction time for single sample\nstart = time.time()\n_ = quick_knn.predict(X_test[:1])\nsingle_pred_time = time.time() - start\n\n# Measure prediction time for all test samples\nstart = time.time()\n_ = quick_knn.predict(X_test)\nbatch_pred_time = time.time() - start\n\nprint(\"=\"*60)\nprint(\"COMPUTATIONAL PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"\\nTraining Set Size: {len(X_train_all)} samples, {X_train_all.shape[1]} features\")\nprint(f\"Training Time: {train_time*1000:.2f} ms (essentially instant)\")\nprint(f\"\\nSingle Prediction Time: {single_pred_time*1000:.2f} ms\")\nprint(f\"Batch Prediction ({len(X_test)} samples): {batch_pred_time*1000:.2f} ms\")\nprint(f\"Average Prediction Time: {batch_pred_time/len(X_test)*1000:.4f} ms per sample\")\nprint(f\"\\nThroughput: {len(X_test)/batch_pred_time:.0f} predictions/second\")\nprint(\"=\"*60)\n\nprint(\"\\n\ud83d\udca1 Key Takeaway:\")\nprint(\"   Training is instant, but prediction scales linearly with training set size.\")\nprint(\"   For 1M+ training samples, consider approximate nearest neighbor methods.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbrAX4fuEu51"
   },
   "source": "## Limitations and Advanced Topics\n\n### Current Scope Limitations\n\n**1. Single Hold-Out Validation**  \nThis notebook uses a **single train/validation/test split**, which is simple but has drawbacks:\n- Performance estimates depend on the specific random split\n- Small datasets may have high variance in estimates\n- We might get \"lucky\" or \"unlucky\" with our particular validation set\n\n**Better Approach: K-Fold Cross-Validation**  \nInstead of one validation set, k-fold CV:\n- Splits training data into k folds (typically k=5 or k=10)\n- Trains k models, each using k-1 folds for training and 1 fold for validation\n- Averages performance across all k folds for more robust estimates\n- Reduces dependence on any single data split\n\nExample code structure:\n```python\nfrom sklearn.model_selection import cross_val_score\n\npipe = Pipeline([('scaler', StandardScaler()), \n                 ('knn', KNeighborsRegressor(n_neighbors=7))])\nscores = cross_val_score(pipe, X_train, y_train, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f\"CV RMSE: {-scores.mean():.3f} (+/- {scores.std():.3f})\")\n```\n\n**2. Brute-Force Neighbor Search**  \nWe used sklearn's default brute-force algorithm, which works well for small/medium datasets but doesn't scale. For large datasets, consider:\n- **KD-Trees**: Efficient for low-dimensional data (d < 20)\n- **Ball Trees**: Better for higher dimensions than KD-trees\n- **Approximate methods**: FAISS, Annoy, HNSW for millions of points\n\n**3. Additional Considerations**\n- **Distance-weighted KNN**: Use `weights='distance'` to give closer neighbors more influence\n- **Outlier handling**: KNN averages can be affected by extreme target values\n- **Feature engineering**: Interaction or polynomial features might improve performance\n- **Ensemble methods**: Combine KNN with other models for better robustness\n- **Curse of dimensionality**: KNN performance degrades in very high dimensions (d > 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yG1-qXBEu51"
   },
   "source": "## Conclusion and Key Takeaways\n\n### What We Learned\n\n1. **Feature scaling is non-negotiable** for KNN. Without it, features with larger ranges dominate distance calculations and render other features useless.\n\n2. **Hyperparameter tuning requires careful validation.** Using train/validation/test splits (or cross-validation) prevents overfitting to a specific data subset.\n\n3. **K controls the bias-variance tradeoff.** Small K leads to high variance (overfitting), large K leads to high bias (underfitting). Tune K systematically using validation data.\n\n4. **Multiple evaluation metrics** (RMSE, MAE, R\u00b2, MAPE, percentage errors) provide different perspectives on model performance and help identify specific weaknesses.\n\n5. **Error patterns reveal model limitations.** High ambiguity regions (neighbors with diverse targets) produce larger errors\u2014understanding where KNN struggles is as important as measuring overall performance.\n\n### When to Use KNN Regression\n\n**\u2705 KNN is a Good Choice When:**\n\n- **Small to medium datasets** (< 100K samples) where prediction latency isn't critical\n- **Non-linear relationships** exist between features and target (KNN makes no linearity assumptions)\n- **You need an interpretable baseline** before trying complex models\n- **Feature interactions are complex** and difficult to engineer explicitly\n- **You want quick prototyping** without extensive hyperparameter tuning\n- **Local patterns matter** more than global trends\n\n**\u274c Avoid KNN When:**\n\n- **Large datasets** (millions of rows) where prediction time becomes prohibitive\n- **Real-time predictions** are required with strict latency requirements (< 10ms)\n- **High-dimensional data** (d > 50) due to the curse of dimensionality\n- **Features have no natural distance metric** (e.g., categorical data, text)\n- **Interpretability of individual predictions** is critical (KNN doesn't explain why it made a specific prediction beyond \"these are the nearest neighbors\")\n- **Memory constraints exist** (KNN stores entire training set)\n\n### Comparison with Other Regression Models\n\n| Model | Training Speed | Prediction Speed | Interpretability | Handles Non-linearity | Scales to Large Data |\n|-------|---------------|------------------|------------------|----------------------|----------------------|\n| **KNN** | Instant | Slow | Medium | Yes | No |\n| **Linear Regression** | Fast | Fast | High | No | Yes |\n| **Random Forest** | Slow | Fast | Medium | Yes | Yes |\n| **Gradient Boosting** | Very Slow | Fast | Low | Yes | Yes |\n| **Neural Networks** | Very Slow | Fast | Very Low | Yes | Yes |\n\n### Next Steps\n\nKNN regression is an excellent **baseline model** that provides intuition about distance-based prediction. After establishing this baseline:\n\n1. Try **tree-based ensembles** (Random Forest, XGBoost) for better performance on most tabular data\n2. Experiment with **feature engineering** to capture domain knowledge\n3. Use **cross-validation** for more robust hyperparameter selection\n4. Consider **model stacking** that combines KNN with other regressors\n5. For production systems with large data, explore **approximate nearest neighbor** methods\n\n**Remember:** Simple models like KNN often provide surprising performance and valuable insights\u2014don't immediately jump to complex deep learning without establishing a solid baseline first."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}