{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20with%20California%20Housing%20Dataset%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX4UdkmNEu5v"
   },
   "source": [
    "# Case Study: KNN Regression with California Housing Dataset\n",
    "\n",
    "K‑Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that can be applied to both classification and regression tasks. In **regression**, instead of voting for a class label, KNN predicts a continuous value by **averaging** the target values of the K nearest neighbors. In this case study, we'll step through a practical example using the **California Housing** dataset to illustrate key concepts and best practices of KNN regression. This dataset contains information about housing blocks in California from the 1990 census, with 8 features (e.g., median income, house age, average rooms, location) and a target variable representing the median house value.\n",
    "\n",
    "## What we'll cover\n",
    "- **Data exploration and preparation:** Understanding feature distributions and splitting data into training, validation, and test sets.  \n",
    "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance in regression tasks.  \n",
    "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
    "- **Distance metric considerations:** How the choice of distance measure can affect KNN predictions.  \n",
    "- **Model evaluation:** Evaluating the final model using regression metrics (RMSE, MAE, R²) on a test set to ensure it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGQ_Ft3-Eu5w"
   },
   "source": [
    "## Exploring the Dataset\n",
    "Before diving into modeling, let's load the dataset and examine its features. The California Housing dataset has 20,640 samples, each with 8 features. The target is `MedHouseVal` (median house value in $100,000s).\n",
    "\n",
    "**Features:**  \n",
    "- MedInc: median income in block group  \n",
    "- HouseAge: median house age in block group  \n",
    "- AveRooms: average number of rooms per household  \n",
    "- AveBedrms: average number of bedrooms per household  \n",
    "- Population: block group population  \n",
    "- AveOccup: average number of household members  \n",
    "- Latitude: block group latitude  \n",
    "- Longitude: block group longitude  \n",
    "\n",
    "Large differences in magnitude (e.g., *Population* in thousands vs *AveBedrms* around 1) motivate **scaling** before using distance-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4U_4SkjEu5w"
   },
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['MedHouseVal'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s73rtt_6Eu5x"
   },
   "source": "Let's examine the target variable distribution and basic feature statistics before proceeding. \n\n**Data Splitting Strategy:**  \nWe split the data into three sets with a 60/20/20 ratio:\n- **Training set (60%)**: Used to fit the model (learn from the data)\n- **Validation set (20%)**: Used to tune hyperparameters (select best K, distance metric, etc.)\n- **Test set (20%)**: Held out completely until final evaluation\n\nThis three-way split is crucial because hyperparameter tuning on the validation set can lead to overfitting those specific choices to that particular data subset. The test set provides an unbiased performance estimate on truly unseen data.\n\n> **Question**: You've used the validation set to tune K from 1 to 30, ultimately selecting K=7 with validation RMSE of $0.52. Why is it critical to evaluate on a separate test set before deploying the model?\n>  \n> A) The validation set was used for hyperparameter selection, which can lead to optimistic performance estimates.\n>\n> B) The test set provides additional opportunities to fine-tune hyperparameters for better accuracy.\n>\n> C) Validation RMSE is systematically biased upward and always overestimates real-world error.\n>\n> D) The test set helps identify which features should be added or removed from the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWETnC40Eu5x"
   },
   "outputs": [],
   "source": [
    "# Examine target distribution\n",
    "print(\"Target variable (MedHouseVal) statistics:\")\n",
    "print(df['MedHouseVal'].describe(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics of features\n",
    "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Plot target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Median House Value ($100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y)\n",
    "plt.ylabel('Median House Value ($100k)')\n",
    "plt.title('Box Plot of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPJrOdksEu5y"
   },
   "source": [
    "## Effect of Feature Scaling on KNN Regression\n",
    "\n",
    "Just like in classification, KNN regression uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The example below illustrates how a difference in *Population* (thousands) can swamp a difference in *AveBedrms* (around 1). Therefore, scaling features to comparable ranges is critical for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG7bsMzZEu5y"
   },
   "outputs": [],
   "source": [
    "# Demonstrate distance dominance (hypothetical differences)\n",
    "from math import sqrt\n",
    "\n",
    "delta_population_large = 1000.0\n",
    "delta_bedrooms_small = 0.5\n",
    "\n",
    "d1 = sqrt(delta_population_large**2 + 0.0**2)\n",
    "d2 = sqrt(0.0**2 + delta_bedrooms_small**2)\n",
    "\n",
    "print(\"Distance if only Population differs by +1000:\", round(d1, 3))\n",
    "print(\"Distance if only AveBedrms differs by +0.5  :\", round(d2, 3))\n",
    "print(\"Ratio (Population / Bedrooms):\", round(d1 / d2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zowu7MFEu5y"
   },
   "source": [
    "Next, we train a baseline KNN regression model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. We'll use RMSE (Root Mean Squared Error) as our primary metric. Note that we scale using parameters learned from the training set only to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDufnDnNEu5y"
   },
   "outputs": [],
   "source": [
    "# Baseline without scaling\n",
    "knn_raw = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "raw_val_pred = knn_raw.predict(X_val)\n",
    "raw_val_rmse = np.sqrt(mean_squared_error(y_val, raw_val_pred))\n",
    "\n",
    "# Baseline with scaling (fit on train only)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_val_pred = knn_scaled.predict(X_val_scaled)\n",
    "scaled_val_rmse = np.sqrt(mean_squared_error(y_val, scaled_val_pred))\n",
    "\n",
    "print(f\"Validation RMSE without scaling: ${raw_val_rmse:.3f} (×100k)\")\n",
    "print(f\"Validation RMSE with scaling:    ${scaled_val_rmse:.3f} (×100k)\")\n",
    "print(f\"Improvement: {((raw_val_rmse - scaled_val_rmse) / raw_val_rmse * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEG3ejXFEu5y"
   },
   "source": [
    "The scaled model typically performs significantly better because each feature contributes fairly to distance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Key Takeaway:** Feature scaling is essential for KNN regression. Without scaling, features with larger ranges (like Population) dominate distance calculations, while important features with smaller ranges (like AveBedrms) are effectively ignored. **Always scale features before using KNN.** From this point forward, all models will use scaled features.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step-by-Step: How KNN Makes a Prediction\n",
    "\n",
    "To understand exactly how KNN regression works, let's walk through the prediction process step-by-step for a single test point, using the same approach shown in the lecture slides.\n",
    "\n",
    "**Steps:**\n",
    "1. Calculate pairwise distances from the test point to all training points\n",
    "2. Sort distances and select the K nearest neighbors (smallest distances)\n",
    "3. Get the target values of these K neighbors\n",
    "4. **Average** these values to make the prediction\n",
    "\n",
    "> **Key Difference from Classification**: In classification, KNN uses **voting** (most common class). In regression, KNN uses **averaging** (mean of continuous values)."
   ],
   "metadata": {
    "id": "gK4ltxaiEu5z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJqvWHYAEu5z"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_val, scaled_val_pred, alpha=0.5, edgecolor='k', s=20)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual House Value ($100k)')\n",
    "plt.ylabel('Predicted House Value ($100k)')\n",
    "plt.title('Actual vs Predicted (K=5, Scaled Features)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUS6i-LcEu5z"
   },
   "source": [
    "## Distance Metric Considerations\n",
    "Choosing a distance metric is itself a hyperparameter. For continuous features, **Euclidean (L2)** is the default and measures straight-line distance; **Manhattan (L1)** sums absolute differences and can be more robust to outliers. In practice, treat the metric as something to tune by validation.\n",
    "\n",
    "> **Question**: Your KNN model uses 3 features: 'MedInc' (range 0-15), 'Population' (range 0-35,000), and 'Latitude' (range 32-42). Without scaling, which feature will dominate the distance calculations, and why?\n",
    ">\n",
    "> A) MedInc—it has the strongest correlation with house prices\n",
    ">\n",
    "> B) Population—it has the largest numerical range\n",
    ">\n",
    "> C) Latitude—geographic features always have higher weight in distance metrics\n",
    ">\n",
    "> D) All features contribute equally because KNN normalizes distances automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6a-KVbaEu5z"
   },
   "source": [
    "To see the effect of different metrics and K values, we can do a small grid of {Euclidean, Manhattan} × {3, 5, 7, 9} on the validation set. This isn't exhaustive but shows that **distance metric** is a tunable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQM-obNLEu5z"
   },
   "outputs": [],
   "source": "Let's conduct a simple grid search over distance metrics {Euclidean, Manhattan} and several K values to see which combination performs best on the validation set. This demonstrates that distance metric selection matters and should be tuned alongside K.\n\n**Note:** We'll explore the bias-variance tradeoff of K in more detail in the next section. For now, this grid search shows that both hyperparameters affect performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdQFlvRKEu5z"
   },
   "outputs": [],
   "source": [
    "# Pick best (metric, k) by validation RMSE (lower is better)\n",
    "grid_long = (\n",
    "    grid_df.stack()                 # -> Series with MultiIndex (metric, k)\n",
    "           .rename('val_rmse')\n",
    "           .reset_index()           # -> columns: ['metric', 'k', 'val_rmse']\n",
    ")\n",
    "\n",
    "# Tie-breaker: prefer smaller k, then 'euclidean' over 'manhattan'\n",
    "grid_long['tie_metric_rank'] = grid_long['metric'].map({'euclidean': 0, 'manhattan': 1})\n",
    "\n",
    "best_row = (\n",
    "    grid_long.sort_values(\n",
    "        ['val_rmse', 'k', 'tie_metric_rank'],\n",
    "        ascending=[True, True, True]  # Lower RMSE is better\n",
    "    )\n",
    "    .iloc[0]\n",
    ")\n",
    "\n",
    "chosen_metric = best_row['metric']\n",
    "chosen_k      = int(best_row['k'])\n",
    "best_val_rmse = float(best_row['val_rmse'])\n",
    "\n",
    "print(\"Decision log — chosen params (metric & k):\")\n",
    "print({\"metric\": chosen_metric, \"n_neighbors\": chosen_k, \"val_rmse\": round(best_val_rmse, 3)})\n",
    "\n",
    "# Sanity check with chosen params on validation data\n",
    "_knn = KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric).fit(X_train_scaled, y_train)\n",
    "val_rmse_check = np.sqrt(mean_squared_error(y_val, _knn.predict(X_val_scaled)))\n",
    "print(f\"Validation RMSE (chosen metric & k): {val_rmse_check:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm1uBP4AEu50"
   },
   "source": [
    "## Choosing K: Bias–Variance Trade‑off\n",
    "A small K (e.g., K=1) is highly flexible and fits training data very closely—**high variance** and potential overfitting. A very large K (approaching the size of the training set) averages over many neighbors—**high bias** and potential underfitting. We sweep K from 1 to 30 and plot training vs. validation RMSE to pick the best K by validation performance.\n",
    "\n",
    "> **Question**: After training KNN models with different K values, you observe that K=1 achieves training RMSE of \\$0.05 but validation RMSE of \\$0.75, while K=5 achieves training RMSE of \\$0.45 and validation RMSE of \\$0.52. What does this pattern suggest about the K=1 model?\n",
    ">  \n",
    "> A) The model has high variance and is overfitting to training noise.\n",
    ">\n",
    "> B) The model has high bias and requires more complex features.\n",
    ">\n",
    "> C) The training dataset is too small and more samples would fix the issue.\n",
    ">\n",
    "> D) Add more features to improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7eLvtO_Eu50"
   },
   "outputs": [],
   "source": [
    "train_rmse, val_rmse = [], []\n",
    "k_sweep = range(1, 31)\n",
    "\n",
    "for k in k_sweep:\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))\n",
    "    val_rmse.append(np.sqrt(mean_squared_error(y_val, val_pred)))\n",
    "\n",
    "# Best K by validation RMSE\n",
    "best_k_idx = int(np.argmin(val_rmse))\n",
    "best_k = best_k_idx + 1\n",
    "best_val = min(val_rmse)\n",
    "\n",
    "print(\"Best K (by validation RMSE):\", best_k, \"Validation RMSE:\", round(best_val, 3))\n",
    "\n",
    "# Plot train vs validation RMSE vs K\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(k_sweep), train_rmse, marker='o', label='Train RMSE', linewidth=2)\n",
    "plt.plot(list(k_sweep), val_rmse, marker='s', label='Validation RMSE', linewidth=2)\n",
    "plt.axvline(best_k, linestyle='--', color='red', label=f'Best K={best_k}')\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('RMSE ($100k)', fontsize=12)\n",
    "plt.title('Bias-Variance Trade-off: RMSE vs K', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6r8SSVHEu50"
   },
   "source": "## Choosing K: Bias–Variance Trade‑off\n\nThe number of neighbors (K) controls the model's complexity and represents a fundamental bias-variance tradeoff:\n\n**Small K (e.g., K=1)**:\n- Highly flexible, fits training data very closely\n- **High variance**: Sensitive to noise in individual training points\n- Risk of overfitting: Excellent training performance but poor generalization\n\n**Large K (e.g., K=100+)**:\n- Averages over many neighbors, produces smoother predictions\n- **High bias**: May miss local patterns and underfit the data\n- More stable but potentially too simple\n\nWe'll sweep K from 1 to 30 and compare training vs. validation RMSE to find the sweet spot. A large gap between training and validation error signals overfitting.\n\n> **Question**: After training KNN models with different K values, you observe that K=1 achieves training RMSE of \\$0.05 but validation RMSE of \\$0.75, while K=5 achieves training RMSE of \\$0.45 and validation RMSE of \\$0.52. What does this pattern suggest about the K=1 model?\n>  \n> A) The model has high variance and is overfitting to training noise.\n>\n> B) The model has high bias and requires more complex features.\n>\n> C) The training dataset is too small and more samples would fix the issue.\n>\n> D) Add more features to improve generalization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90-W_X4wEu50"
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final training\n",
    "X_train_all = np.vstack([X_train, X_val])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "# Build pipeline (scaler + KNN) with chosen hyperparameters\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_pipe.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE:  ${test_rmse:.3f} (×100k) = ${test_rmse*100:.0f}k\")\n",
    "print(f\"MAE:   ${test_mae:.3f} (×100k) = ${test_mae*100:.0f}k\")\n",
    "print(f\"R²:    {test_r2:.3f}\")\n",
    "print(f\"MAPE:  {test_mape*100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmzXwto8Eu50"
   },
   "source": "## Model Evaluation on Test Set\n\nAfter selecting optimal hyperparameters using the validation set, we perform a final evaluation on the **held-out test set**. This is critical for obtaining an unbiased estimate of real-world performance.\n\n**Final Training Process:**\n1. Combine training + validation sets (now that hyperparameter tuning is complete)\n2. Refit the model on this combined dataset\n3. Evaluate once on the test set\n4. Compare test performance to validation performance\n\n**Expected Behavior:**  \nTest performance typically matches validation performance closely. A small difference (e.g., validation RMSE $0.52 vs. test RMSE $0.56) is normal due to random variation in data splits. A large gap would suggest overfitting to the validation set during hyperparameter tuning.\n\n**Evaluation Metrics:**\n- **RMSE (Root Mean Squared Error)**: Penalizes large errors more heavily; same units as target ($100k)\n- **MAE (Mean Absolute Error)**: Average absolute prediction error; more interpretable\n- **R² Score**: Proportion of variance explained (1.0 = perfect, 0.0 = baseline)\n- **MAPE (Mean Absolute Percentage Error)**: Percentage error; useful for relative comparison\n\n> **Question**: Your final KNN model achieves validation RMSE of $0.52 and test RMSE of \\$0.56. Before deployment, what's the most appropriate interpretation?\n>  \n> A) The difference is normal variation; verify test performance meets business requirements.\n>\n> B) The test set has data leakage and should be regenerated with better random seeds.\n>\n> C) Re-tune hyperparameters using the test set to minimize the performance gap.\n>\n> D) The model is underfitting and needs a smaller K value for better flexibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy_705gmEu50"
   },
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for test set\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, test_pred, alpha=0.5, edgecolor='k', s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual House Value ($100k)')\n",
    "plt.ylabel('Predicted House Value ($100k)')\n",
    "plt.title(f'Test Set: Actual vs Predicted\\n(R² = {test_r2:.3f}, RMSE = ${test_rmse:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - test_pred\n",
    "plt.scatter(test_pred, residuals, alpha=0.5, edgecolor='k', s=20)\n",
    "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted House Value ($100k)')\n",
    "plt.ylabel('Residuals ($100k)')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of residuals\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Residuals ($100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(0, color='r', linestyle='--', lw=2, label='Zero error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Where Do Errors Come From?\n",
    "\n",
    "Understanding where KNN regression makes larger errors helps us interpret model performance and identify areas for improvement. As shown in the lecture slides, prediction errors in KNN regression come from two types of regions:\n",
    "\n",
    "### High Certainty Regions\n",
    "- **Characteristics**: Neighbors have similar target values with low variation\n",
    "- **Prediction Quality**: Low residual errors (high confidence predictions)\n",
    "- **Why**: When K nearest neighbors have similar values, their average is a reliable estimate\n",
    "- **Example**: In dense neighborhoods where houses have similar prices\n",
    "\n",
    "### High Ambiguity Regions  \n",
    "- **Characteristics**: Neighbors have high variation in target values\n",
    "- **Prediction Quality**: High residual errors (uncertain predictions)\n",
    "- **Why**: When K nearest neighbors have very different values, averaging produces less reliable estimates\n",
    "- **Example**: Boundary regions between expensive and affordable neighborhoods, or sparse data regions\n",
    "\n",
    "### Additional Error Sources\n",
    "- **Sparse Regions**: Areas with few training points lead to unreliable neighbor selection\n",
    "- **Boundary Regions**: Transition zones where the K neighbors remain constant but represent different underlying patterns\n",
    "\n",
    "> **Key Insight**: KNN performs best in regions where neighbors have consistent target values. The model struggles in regions with high local variability or sparse data, as the averaging assumption breaks down."
   ],
   "metadata": {
    "id": "BvPFVKtYEu50"
   }
  },
  {
   "cell_type": "markdown",
   "source": "**Understanding the Visualizations:**\n\n1. **Actual vs Predicted Plot (left)**: Points closer to the diagonal line indicate better predictions. Deviations show where the model struggles.\n\n2. **Percentage Error Plot (right)**: Shows prediction errors as a percentage of actual values. Using percentage error instead of absolute error provides better insights:\n   - **Removes scale dependency**: A $50k error on a $500k house (10%) is very different from a $50k error on a $100k house (50%)\n   - **Prevents fan-out pattern**: Absolute residuals often increase with predicted values; percentage errors should be more evenly distributed\n   - **Easier interpretation**: We can quickly identify if errors are acceptable (e.g., within ±20%)\n\n**What to look for:** Ideally, percentage errors should be randomly scattered around zero with no clear patterns. Systematic patterns (e.g., consistent over/under-prediction for certain price ranges) suggest model limitations.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19aZ827hEu51"
   },
   "source": "# Plot actual vs predicted for test set\nplt.figure(figsize=(12, 5))\n\n# Scatter plot\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, test_pred, alpha=0.5, edgecolor='k', s=20)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\nplt.xlabel('Actual House Value ($100k)')\nplt.ylabel('Predicted House Value ($100k)')\nplt.title(f'Test Set: Actual vs Predicted\\n(R² = {test_r2:.3f}, RMSE = ${test_rmse:.3f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Residual plot - using percentage error\nplt.subplot(1, 2, 2)\npercentage_error = (y_test - test_pred) / y_test * 100\nplt.scatter(test_pred, percentage_error, alpha=0.5, edgecolor='k', s=20)\nplt.axhline(0, color='r', linestyle='--', lw=2)\nplt.xlabel('Predicted House Value ($100k)')\nplt.ylabel('Percentage Error (%)')\nplt.title('Residual Plot (Percentage Error)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Distribution of residuals - using percentage error\nplt.figure(figsize=(10, 4))\nplt.hist(percentage_error, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Percentage Error (%)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Prediction Errors (Percentage)')\nplt.axvline(0, color='r', linestyle='--', lw=2, label='Zero error')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N-iWR6yEu51"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    final_pipe, X_test, y_test,\n",
    "    n_repeats=10, random_state=42,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], xerr=importance_df['std'])\n",
    "plt.xlabel('Decrease in R² (Importance)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbrAX4fuEu51"
   },
   "source": [
    "## Limitations (Current Scope) & What's Next\n",
    "This notebook uses a **single hold‑out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k‑fold cross‑validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute‑force neighbor search and didn't explore scalability techniques like KD‑trees, Ball Trees, or approximate nearest neighbor libraries (e.g., FAISS, HNSW). These become important when your dataset grows to millions of rows or requires low‑latency predictions.\n",
    "\n",
    "**Additional considerations for regression:**\n",
    "- **Weighting neighbors by distance**: Closer neighbors can have more influence (weights='distance' in sklearn)\n",
    "- **Handling outliers in target variable**: KNN averages can be affected by extreme values\n",
    "- **Feature engineering**: Creating interaction features or polynomial features might improve performance\n",
    "- **Ensemble methods**: Combining KNN with other regressors can improve robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yG1-qXBEu51"
   },
   "source": [
    "## Conclusion\n",
    "- **Scaling** prevents large‑range features from dominating distance computations in regression, just as in classification.  \n",
    "- **Tuning K** via validation balances bias and variance; a very small K overfits, a very large K underfits.  \n",
    "- **Distance metric and K** are hyperparameters; small grids can reveal significant differences in RMSE.  \n",
    "- **Regression metrics** (RMSE, MAE, R²) provide different perspectives on model performance.  \n",
    "- KNN regression remains a powerful, intuitive baseline—use it to build understanding about distance-based prediction before advancing to more sophisticated models like Random Forest or Gradient Boosting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}