{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20with%20California%20Housing%20Dataset%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX4UdkmNEu5v"
   },
   "source": [
    "# Case Study: KNN Regression with California Housing Dataset\n",
    "\n",
    "K‚ÄëNearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that can be applied to both classification and regression tasks. In **regression**, instead of voting for a class label, KNN predicts a continuous value by **averaging** the target values of the K nearest neighbors. In this case study, we'll step through a practical example using the **California Housing** dataset to illustrate key concepts and best practices of KNN regression. This dataset contains information about housing blocks in California from the 1990 census, with 8 features (e.g., median income, house age, average rooms, location) and a target variable representing the median house value.\n",
    "\n",
    "## What we'll cover\n",
    "- **Data exploration and preparation:** Understanding feature distributions and splitting data into training, validation, and test sets.  \n",
    "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance in regression tasks.  \n",
    "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
    "- **Distance metric considerations:** How the choice of distance measure can affect KNN predictions.  \n",
    "- **Model evaluation:** Evaluating the final model using regression metrics (RMSE, MAE, R¬≤) on a test set to ensure it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Learning Objectives\n\nBy the end of this case study, you will be able to:\n\n1. **Understand the critical role of feature scaling** in distance-based algorithms and demonstrate its impact on model performance\n2. **Apply systematic hyperparameter tuning** using train/validation/test splits to avoid overfitting\n3. **Recognize and explain the bias-variance tradeoff** when selecting the number of neighbors (K)\n4. **Evaluate regression models** using multiple metrics (RMSE, MAE, R¬≤, MAPE) and interpret their meaning\n5. **Analyze prediction errors** using percentage error plots to identify model strengths and limitations\n6. **Make informed decisions** about when KNN regression is appropriate for a given problem\n7. **Understand computational implications** of KNN for real-world deployment scenarios",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGQ_Ft3-Eu5w"
   },
   "source": [
    "## Exploring the Dataset\n",
    "Before diving into modeling, let's load the dataset and examine its features. The California Housing dataset has 20,640 samples, each with 8 features. The target is `MedHouseVal` (median house value in $100,000s).\n",
    "\n",
    "**Features:**  \n",
    "- MedInc: median income in block group  \n",
    "- HouseAge: median house age in block group  \n",
    "- AveRooms: average number of rooms per household  \n",
    "- AveBedrms: average number of bedrooms per household  \n",
    "- Population: block group population  \n",
    "- AveOccup: average number of household members  \n",
    "- Latitude: block group latitude  \n",
    "- Longitude: block group longitude  \n",
    "\n",
    "Large differences in magnitude (e.g., *Population* in thousands vs *AveBedrms* around 1) motivate **scaling** before using distance-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4U_4SkjEu5w"
   },
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['MedHouseVal'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s73rtt_6Eu5x"
   },
   "source": "Let's examine the target variable distribution and basic feature statistics before proceeding. \n\n**Data Splitting Strategy:**  \nWe split the data into three sets with a 60/20/20 ratio:\n- **Training set (60%)**: Used to fit the model (learn from the data)\n- **Validation set (20%)**: Used to tune hyperparameters (select best K, distance metric, etc.)\n- **Test set (20%)**: Held out completely until final evaluation\n\nThis three-way split is crucial because hyperparameter tuning on the validation set can lead to overfitting those specific choices to that particular data subset. The test set provides an unbiased performance estimate on truly unseen data.\n\n> **Question**: You've used the validation set to tune K from 1 to 30, ultimately selecting K=7 with validation RMSE of $0.52. Why is it critical to evaluate on a separate test set before deploying the model?\n>  \n> A) The validation set was used for hyperparameter selection, which can lead to optimistic performance estimates.\n>\n> B) The test set provides additional opportunities to fine-tune hyperparameters for better accuracy.\n>\n> C) Validation RMSE is systematically biased upward and always overestimates real-world error.\n>\n> D) The test set helps identify which features should be added or removed from the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWETnC40Eu5x"
   },
   "outputs": [],
   "source": [
    "# Examine target distribution\n",
    "print(\"Target variable (MedHouseVal) statistics:\")\n",
    "print(df['MedHouseVal'].describe(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics of features\n",
    "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Plot target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Median House Value ($100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y)\n",
    "plt.ylabel('Median House Value ($100k)')\n",
    "plt.title('Box Plot of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPJrOdksEu5y"
   },
   "source": [
    "## Effect of Feature Scaling on KNN Regression\n",
    "\n",
    "Just like in classification, KNN regression uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The example below illustrates how a difference in *Population* (thousands) can swamp a difference in *AveBedrms* (around 1). Therefore, scaling features to comparable ranges is critical for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG7bsMzZEu5y"
   },
   "outputs": [],
   "source": [
    "# Demonstrate distance dominance (hypothetical differences)\n",
    "from math import sqrt\n",
    "\n",
    "delta_population_large = 1000.0\n",
    "delta_bedrooms_small = 0.5\n",
    "\n",
    "d1 = sqrt(delta_population_large**2 + 0.0**2)\n",
    "d2 = sqrt(0.0**2 + delta_bedrooms_small**2)\n",
    "\n",
    "print(\"Distance if only Population differs by +1000:\", round(d1, 3))\n",
    "print(\"Distance if only AveBedrms differs by +0.5  :\", round(d2, 3))\n",
    "print(\"Ratio (Population / Bedrooms):\", round(d1 / d2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zowu7MFEu5y"
   },
   "source": [
    "Next, we train a baseline KNN regression model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. We'll use RMSE (Root Mean Squared Error) as our primary metric. Note that we scale using parameters learned from the training set only to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDufnDnNEu5y"
   },
   "outputs": [],
   "source": [
    "# Baseline without scaling\n",
    "knn_raw = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "raw_val_pred = knn_raw.predict(X_val)\n",
    "raw_val_rmse = np.sqrt(mean_squared_error(y_val, raw_val_pred))\n",
    "\n",
    "# Baseline with scaling (fit on train only)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_val_pred = knn_scaled.predict(X_val_scaled)\n",
    "scaled_val_rmse = np.sqrt(mean_squared_error(y_val, scaled_val_pred))\n",
    "\n",
    "print(f\"Validation RMSE without scaling: ${raw_val_rmse:.3f} (√ó100k)\")\n",
    "print(f\"Validation RMSE with scaling:    ${scaled_val_rmse:.3f} (√ó100k)\")\n",
    "print(f\"Improvement: {((raw_val_rmse - scaled_val_rmse) / raw_val_rmse * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEG3ejXFEu5y"
   },
   "source": [
    "The scaled model typically performs significantly better because each feature contributes fairly to distance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Key Takeaway:** Feature scaling is essential for KNN regression. Without scaling, features with larger ranges (like Population) dominate distance calculations, while important features with smaller ranges (like AveBedrms) are effectively ignored. **Always scale features before using KNN.** From this point forward, all models will use scaled features.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJqvWHYAEu5z"
   },
   "source": "## Distance Metric Considerations\n\nBeyond scaling, the choice of **distance metric** itself is a hyperparameter that affects KNN predictions. Common options include:\n- **Euclidean (L2)**: Straight-line distance; squares differences before summing (default choice)\n- **Manhattan (L1)**: Sum of absolute differences; can be more robust to outliers in some cases\n\nThe distance metric determines how we measure similarity between points. When features are unscaled, the metric choice matters less than the scaling issue‚Äîfeatures with larger numerical ranges will dominate the distance calculation regardless of whether you use Euclidean or Manhattan distance.\n\n> **Question**: Your KNN model uses 3 features: 'MedInc' (range 0-15), 'Population' (range 0-35,000), and 'Latitude' (range 32-42). Without scaling, which feature will dominate the distance calculations, and why?\n>\n> A) MedInc‚Äîit has the strongest correlation with house prices\n>\n> B) Population‚Äîit has the largest numerical range\n>\n> C) Latitude‚Äîgeographic features always have higher weight in distance metrics\n>\n> D) All features contribute equally because KNN normalizes distances automatically"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6r8SSVHEu50"
   },
   "source": "## Choosing K: Bias‚ÄìVariance Trade‚Äëoff\n\nThe number of neighbors (K) controls the model's complexity and represents a fundamental bias-variance tradeoff:\n\n**Small K (e.g., K=1)**:\n- Highly flexible, fits training data very closely\n- **High variance**: Sensitive to noise in individual training points\n- Risk of overfitting: Excellent training performance but poor generalization\n\n**Large K (e.g., K=100+)**:\n- Averages over many neighbors, produces smoother predictions\n- **High bias**: May miss local patterns and underfit the data\n- More stable but potentially too simple\n\nWe'll sweep K from 1 to 30 and compare training vs. validation RMSE to find the sweet spot. A large gap between training and validation error signals overfitting.\n\n> **Question**: After training KNN models with different K values, you observe that K=1 achieves training RMSE of \\$0.05 but validation RMSE of \\$0.75, while K=5 achieves training RMSE of \\$0.45 and validation RMSE of \\$0.52. What does this pattern suggest about the K=1 model?\n>  \n> A) The model has high variance and is overfitting to training noise.\n>\n> B) The model has high bias and requires more complex features.\n>\n> C) The training dataset is too small and more samples would fix the issue.\n>\n> D) Add more features to improve generalization."
  },
  {
   "cell_type": "code",
   "source": "train_rmse, val_rmse = [], []\nk_sweep = range(1, 31)\n\nfor k in k_sweep:\n    model = KNeighborsRegressor(n_neighbors=k)\n    model.fit(X_train_scaled, y_train)\n    train_pred = model.predict(X_train_scaled)\n    val_pred = model.predict(X_val_scaled)\n    train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))\n    val_rmse.append(np.sqrt(mean_squared_error(y_val, val_pred)))\n\n# Best K by validation RMSE\nbest_k_idx = int(np.argmin(val_rmse))\nchosen_k = best_k_idx + 1\nbest_val = min(val_rmse)\n\n# Use Euclidean distance (default and most commonly used)\nchosen_metric = 'euclidean'\n\nprint(\"Selected hyperparameters:\")\nprint(f\"  K = {chosen_k}\")\nprint(f\"  Distance metric = {chosen_metric}\")\nprint(f\"  Validation RMSE = ${best_val:.3f} (√ó100k)\")\n\n# Plot train vs validation RMSE vs K\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_sweep), train_rmse, marker='o', label='Train RMSE', linewidth=2)\nplt.plot(list(k_sweep), val_rmse, marker='s', label='Validation RMSE', linewidth=2)\nplt.axvline(chosen_k, linestyle='--', color='red', label=f'Best K={chosen_k}')\nplt.xlabel('K (Number of Neighbors)', fontsize=12)\nplt.ylabel('RMSE ($100k)', fontsize=12)\nplt.title('Bias-Variance Trade-off: RMSE vs K', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90-W_X4wEu50"
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final training\n",
    "X_train_all = np.vstack([X_train, X_val])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "# Build pipeline (scaler + KNN) with chosen hyperparameters\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_pipe.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE:  ${test_rmse:.3f} (√ó100k) = ${test_rmse*100:.0f}k\")\n",
    "print(f\"MAE:   ${test_mae:.3f} (√ó100k) = ${test_mae*100:.0f}k\")\n",
    "print(f\"R¬≤:    {test_r2:.3f}\")\n",
    "print(f\"MAPE:  {test_mape*100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmzXwto8Eu50"
   },
   "source": "## Model Evaluation on Test Set\n\nAfter selecting optimal hyperparameters using the validation set, we perform a final evaluation on the **held-out test set**. This is critical for obtaining an unbiased estimate of real-world performance.\n\n**Final Training Process:**\n1. Combine training + validation sets (now that hyperparameter tuning is complete)\n2. Refit the model on this combined dataset\n3. Evaluate once on the test set\n4. Compare test performance to validation performance\n\n**Expected Behavior:**  \nTest performance typically matches validation performance closely. A small difference (e.g., validation RMSE $0.52 vs. test RMSE $0.56) is normal due to random variation in data splits. A large gap would suggest overfitting to the validation set during hyperparameter tuning.\n\n**Evaluation Metrics:**\n- **RMSE (Root Mean Squared Error)**: Penalizes large errors more heavily; same units as target ($100k)\n- **MAE (Mean Absolute Error)**: Average absolute prediction error; more interpretable\n- **R¬≤ Score**: Proportion of variance explained (1.0 = perfect, 0.0 = baseline)\n- **MAPE (Mean Absolute Percentage Error)**: Percentage error; useful for relative comparison\n\n> **Question**: Your final KNN model achieves validation RMSE of $0.52 and test RMSE of \\$0.56. Before deployment, what's the most appropriate interpretation?\n>  \n> A) The difference is normal variation; verify test performance meets business requirements.\n>\n> B) The test set has data leakage and should be regenerated with better random seeds.\n>\n> C) Re-tune hyperparameters using the test set to minimize the performance gap.\n>\n> D) The model is underfitting and needs a smaller K value for better flexibility."
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Where Do Errors Come From?\n",
    "\n",
    "Understanding where KNN regression makes larger errors helps us interpret model performance and identify areas for improvement. As shown in the lecture slides, prediction errors in KNN regression come from two types of regions:\n",
    "\n",
    "### High Certainty Regions\n",
    "- **Characteristics**: Neighbors have similar target values with low variation\n",
    "- **Prediction Quality**: Low residual errors (high confidence predictions)\n",
    "- **Why**: When K nearest neighbors have similar values, their average is a reliable estimate\n",
    "- **Example**: In dense neighborhoods where houses have similar prices\n",
    "\n",
    "### High Ambiguity Regions  \n",
    "- **Characteristics**: Neighbors have high variation in target values\n",
    "- **Prediction Quality**: High residual errors (uncertain predictions)\n",
    "- **Why**: When K nearest neighbors have very different values, averaging produces less reliable estimates\n",
    "- **Example**: Boundary regions between expensive and affordable neighborhoods, or sparse data regions\n",
    "\n",
    "### Additional Error Sources\n",
    "- **Sparse Regions**: Areas with few training points lead to unreliable neighbor selection\n",
    "- **Boundary Regions**: Transition zones where the K neighbors remain constant but represent different underlying patterns\n",
    "\n",
    "> **Key Insight**: KNN performs best in regions where neighbors have consistent target values. The model struggles in regions with high local variability or sparse data, as the averaging assumption breaks down."
   ],
   "metadata": {
    "id": "BvPFVKtYEu50"
   }
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate high certainty vs high ambiguity regions\nfrom sklearn.metrics import pairwise_distances\n\n# Get predictions for all training points\ntrain_pred_all = final_pipe.predict(X_train_all)\ntrain_errors = np.abs(y_train_all - train_pred_all)\n\n# For each training point, calculate the standard deviation of its K nearest neighbors' target values\nX_train_all_scaled = final_pipe.named_steps['scaler'].transform(X_train_all)\nneighbor_std = []\n\nfor i in range(len(X_train_all_scaled)):\n    # Calculate distances from this point to all other training points\n    distances = pairwise_distances(X_train_all_scaled[i:i+1], X_train_all_scaled, metric=chosen_metric).ravel()\n    # Find K+1 nearest (including itself)\n    k_nearest_idx = np.argsort(distances)[1:chosen_k+1]  # Skip index 0 (itself)\n    # Calculate std of neighbors' target values\n    neighbor_targets = y_train_all[k_nearest_idx]\n    neighbor_std.append(np.std(neighbor_targets))\n\nneighbor_std = np.array(neighbor_std)\n\n# Identify high certainty and high ambiguity regions\nlow_std_threshold = np.percentile(neighbor_std, 25)\nhigh_std_threshold = np.percentile(neighbor_std, 75)\n\nhigh_certainty_mask = neighbor_std < low_std_threshold\nhigh_ambiguity_mask = neighbor_std > high_std_threshold\n\nprint(\"=\"*60)\nprint(\"ERROR ANALYSIS BY REGION TYPE\")\nprint(\"=\"*60)\nprint(f\"\\nHigh Certainty Regions (neighbor std < {low_std_threshold:.3f}):\")\nprint(f\"  - Number of points: {high_certainty_mask.sum()}\")\nprint(f\"  - Avg neighbor std: ${np.mean(neighbor_std[high_certainty_mask]):.3f} (√ó100k)\")\nprint(f\"  - Avg prediction error: ${np.mean(train_errors[high_certainty_mask]):.3f} (√ó100k)\")\n\nprint(f\"\\nHigh Ambiguity Regions (neighbor std > {high_std_threshold:.3f}):\")\nprint(f\"  - Number of points: {high_ambiguity_mask.sum()}\")\nprint(f\"  - Avg neighbor std: ${np.mean(neighbor_std[high_ambiguity_mask]):.3f} (√ó100k)\")\nprint(f\"  - Avg prediction error: ${np.mean(train_errors[high_ambiguity_mask]):.3f} (√ó100k)\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Error is {np.mean(train_errors[high_ambiguity_mask]) / np.mean(train_errors[high_certainty_mask]):.1f}x higher in high ambiguity regions!\")\nprint(\"=\"*60)\n\n# Visualize the relationship\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(neighbor_std, train_errors, alpha=0.3, s=10)\nplt.xlabel('Neighbor Target Std Dev ($100k)')\nplt.ylabel('Prediction Error ($100k)')\nplt.title('Error vs Neighbor Variation')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.hist([neighbor_std[high_certainty_mask], neighbor_std[high_ambiguity_mask]], \n         bins=30, label=['High Certainty', 'High Ambiguity'], alpha=0.7)\nplt.xlabel('Neighbor Target Std Dev ($100k)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Region Types')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "19aZ827hEu51"
   },
   "source": "# Plot actual vs predicted for test set\nplt.figure(figsize=(12, 5))\n\n# Scatter plot\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, test_pred, alpha=0.5, edgecolor='k', s=20)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\nplt.xlabel('Actual House Value ($100k)')\nplt.ylabel('Predicted House Value ($100k)')\nplt.title(f'Test Set: Actual vs Predicted\\n(R¬≤ = {test_r2:.3f}, RMSE = ${test_rmse:.3f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Residual plot - using percentage error\nplt.subplot(1, 2, 2)\npercentage_error = (y_test - test_pred) / y_test * 100\nplt.scatter(test_pred, percentage_error, alpha=0.5, edgecolor='k', s=20)\nplt.axhline(0, color='r', linestyle='--', lw=2)\nplt.xlabel('Predicted House Value ($100k)')\nplt.ylabel('Percentage Error (%)')\nplt.title('Residual Plot (Percentage Error)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Distribution of residuals - using percentage error\nplt.figure(figsize=(10, 4))\nplt.hist(percentage_error, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Percentage Error (%)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Prediction Errors (Percentage)')\nplt.axvline(0, color='r', linestyle='--', lw=2, label='Zero error')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**Understanding the Visualizations:**\n\n1. **Actual vs Predicted Plot (left)**: Points closer to the diagonal line indicate better predictions. Deviations show where the model struggles.\n\n2. **Percentage Error Plot (right)**: Shows prediction errors as a percentage of actual values. Using percentage error instead of absolute error provides better insights:\n   - **Removes scale dependency**: A $50k error on a $500k house (10%) is very different from a $50k error on a $100k house (50%)\n   - **Prevents fan-out pattern**: Absolute residuals often increase with predicted values; percentage errors should be more evenly distributed\n   - **Easier interpretation**: We can quickly identify if errors are acceptable (e.g., within ¬±20%)\n\n**What to look for:** Ideally, percentage errors should be randomly scattered around zero with no clear patterns. Systematic patterns (e.g., consistent over/under-prediction for certain price ranges) suggest model limitations.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Importance Analysis\n\nUnlike tree-based models, KNN doesn't have built-in feature importance scores. However, we can use **permutation importance** to understand which features contribute most to predictions. This technique randomly shuffles each feature one at a time and measures how much the model's performance drops‚Äîlarger drops indicate more important features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N-iWR6yEu51"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    final_pipe, X_test, y_test,\n",
    "    n_repeats=10, random_state=42,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], xerr=importance_df['std'])\n",
    "plt.xlabel('Decrease in R¬≤ (Importance)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Common Pitfalls and Best Practices\n\nWatch out for these common mistakes when using KNN regression:\n\n### Critical Mistakes to Avoid:\n1. **Forgetting to scale features** ‚ùå  \n   - This is the #1 mistake with KNN. Features with larger ranges will completely dominate distance calculations\n   - **Always** use StandardScaler or MinMaxScaler before applying KNN\n\n2. **Using test set for hyperparameter tuning** ‚ùå  \n   - Never tune K or distance metrics using the test set\n   - Use a separate validation set or cross-validation for hyperparameter selection\n\n3. **Choosing K=1 for production** ‚ùå  \n   - K=1 is extremely sensitive to noise and outliers\n   - While it may show perfect training performance, it rarely generalizes well\n   - Start with K=5 as a reasonable default and tune from there\n\n4. **Ignoring computational cost** ‚ùå  \n   - KNN stores all training data and computes distances at prediction time\n   - For large datasets (millions of rows), KNN can be prohibitively slow\n   - Consider approximate nearest neighbor methods for large-scale applications\n\n### Best Practices:\n- ‚úÖ **Always visualize** your predictions vs actuals to spot patterns in errors\n- ‚úÖ **Use cross-validation** for more robust hyperparameter tuning (see Limitations section)\n- ‚úÖ **Consider distance-weighted KNN** (`weights='distance'`) to give closer neighbors more influence\n- ‚úÖ **Remove outliers** or use robust scaling if your target variable has extreme values\n- ‚úÖ **Try feature selection** to reduce dimensionality and improve performance (curse of dimensionality)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Computational Complexity\n\nUnderstanding KNN's computational characteristics is crucial for real-world deployment:\n\n### Training Complexity: O(1)\n- **KNN is a \"lazy learner\"**: It doesn't actually learn a model during training\n- Training simply stores the feature vectors and target values in memory\n- This makes training instantaneous, regardless of dataset size\n\n### Prediction Complexity: O(n √ó d)\n- **For each prediction**, KNN must:\n  - Calculate distance to all n training points\n  - Each distance calculation involves d features\n  - Sort or partially sort distances to find K nearest neighbors\n- This becomes expensive for large datasets or real-time applications\n\n### Example: Prediction Time\nLet's measure the prediction time for our model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\n\n# Measure training time\nstart = time.time()\nquick_knn = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric))\n])\nquick_knn.fit(X_train_all, y_train_all)\ntrain_time = time.time() - start\n\n# Measure prediction time for single sample\nstart = time.time()\n_ = quick_knn.predict(X_test[:1])\nsingle_pred_time = time.time() - start\n\n# Measure prediction time for all test samples\nstart = time.time()\n_ = quick_knn.predict(X_test)\nbatch_pred_time = time.time() - start\n\nprint(\"=\"*60)\nprint(\"COMPUTATIONAL PERFORMANCE\")\nprint(\"=\"*60)\nprint(f\"\\nTraining Set Size: {len(X_train_all)} samples, {X_train_all.shape[1]} features\")\nprint(f\"Training Time: {train_time*1000:.2f} ms (essentially instant)\")\nprint(f\"\\nSingle Prediction Time: {single_pred_time*1000:.2f} ms\")\nprint(f\"Batch Prediction ({len(X_test)} samples): {batch_pred_time*1000:.2f} ms\")\nprint(f\"Average Prediction Time: {batch_pred_time/len(X_test)*1000:.4f} ms per sample\")\nprint(f\"\\nThroughput: {len(X_test)/batch_pred_time:.0f} predictions/second\")\nprint(\"=\"*60)\n\nprint(\"\\nüí° Key Takeaway:\")\nprint(\"   Training is instant, but prediction scales linearly with training set size.\")\nprint(\"   For 1M+ training samples, consider approximate nearest neighbor methods.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbrAX4fuEu51"
   },
   "source": "## Limitations and Advanced Topics\n\n### Current Scope Limitations\n\n**1. Single Hold-Out Validation**  \nThis notebook uses a **single train/validation/test split**, which is simple but has drawbacks:\n- Performance estimates depend on the specific random split\n- Small datasets may have high variance in estimates\n- We might get \"lucky\" or \"unlucky\" with our particular validation set\n\n**Better Approach: K-Fold Cross-Validation**  \nInstead of one validation set, k-fold CV:\n- Splits training data into k folds (typically k=5 or k=10)\n- Trains k models, each using k-1 folds for training and 1 fold for validation\n- Averages performance across all k folds for more robust estimates\n- Reduces dependence on any single data split\n\nExample code structure:\n```python\nfrom sklearn.model_selection import cross_val_score\n\npipe = Pipeline([('scaler', StandardScaler()), \n                 ('knn', KNeighborsRegressor(n_neighbors=7))])\nscores = cross_val_score(pipe, X_train, y_train, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f\"CV RMSE: {-scores.mean():.3f} (+/- {scores.std():.3f})\")\n```\n\n**2. Brute-Force Neighbor Search**  \nWe used sklearn's default brute-force algorithm, which works well for small/medium datasets but doesn't scale. For large datasets, consider:\n- **KD-Trees**: Efficient for low-dimensional data (d < 20)\n- **Ball Trees**: Better for higher dimensions than KD-trees\n- **Approximate methods**: FAISS, Annoy, HNSW for millions of points\n\n**3. Additional Considerations**\n- **Distance-weighted KNN**: Use `weights='distance'` to give closer neighbors more influence\n- **Outlier handling**: KNN averages can be affected by extreme target values\n- **Feature engineering**: Interaction or polynomial features might improve performance\n- **Ensemble methods**: Combine KNN with other models for better robustness\n- **Curse of dimensionality**: KNN performance degrades in very high dimensions (d > 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yG1-qXBEu51"
   },
   "source": "## Conclusion and Key Takeaways\n\n### What We Learned\n\n1. **Feature scaling is non-negotiable** for KNN. Without it, features with larger ranges dominate distance calculations and render other features useless.\n\n2. **Hyperparameter tuning requires careful validation.** Using train/validation/test splits (or cross-validation) prevents overfitting to a specific data subset.\n\n3. **K controls the bias-variance tradeoff.** Small K leads to high variance (overfitting), large K leads to high bias (underfitting). Tune K systematically using validation data.\n\n4. **Multiple evaluation metrics** (RMSE, MAE, R¬≤, MAPE, percentage errors) provide different perspectives on model performance and help identify specific weaknesses.\n\n5. **Error patterns reveal model limitations.** High ambiguity regions (neighbors with diverse targets) produce larger errors‚Äîunderstanding where KNN struggles is as important as measuring overall performance.\n\n### When to Use KNN Regression\n\n**‚úÖ KNN is a Good Choice When:**\n\n- **Small to medium datasets** (< 100K samples) where prediction latency isn't critical\n- **Non-linear relationships** exist between features and target (KNN makes no linearity assumptions)\n- **You need an interpretable baseline** before trying complex models\n- **Feature interactions are complex** and difficult to engineer explicitly\n- **You want quick prototyping** without extensive hyperparameter tuning\n- **Local patterns matter** more than global trends\n\n**‚ùå Avoid KNN When:**\n\n- **Large datasets** (millions of rows) where prediction time becomes prohibitive\n- **Real-time predictions** are required with strict latency requirements (< 10ms)\n- **High-dimensional data** (d > 50) due to the curse of dimensionality\n- **Features have no natural distance metric** (e.g., categorical data, text)\n- **Interpretability of individual predictions** is critical (KNN doesn't explain why it made a specific prediction beyond \"these are the nearest neighbors\")\n- **Memory constraints exist** (KNN stores entire training set)\n\n### Comparison with Other Regression Models\n\n| Model | Training Speed | Prediction Speed | Interpretability | Handles Non-linearity | Scales to Large Data |\n|-------|---------------|------------------|------------------|----------------------|----------------------|\n| **KNN** | Instant | Slow | Medium | Yes | No |\n| **Linear Regression** | Fast | Fast | High | No | Yes |\n| **Random Forest** | Slow | Fast | Medium | Yes | Yes |\n| **Gradient Boosting** | Very Slow | Fast | Low | Yes | Yes |\n| **Neural Networks** | Very Slow | Fast | Very Low | Yes | Yes |\n\n### Next Steps\n\nKNN regression is an excellent **baseline model** that provides intuition about distance-based prediction. After establishing this baseline:\n\n1. Try **tree-based ensembles** (Random Forest, XGBoost) for better performance on most tabular data\n2. Experiment with **feature engineering** to capture domain knowledge\n3. Use **cross-validation** for more robust hyperparameter selection\n4. Consider **model stacking** that combines KNN with other regressors\n5. For production systems with large data, explore **approximate nearest neighbor** methods\n\n**Remember:** Simple models like KNN often provide surprising performance and valuable insights‚Äîdon't immediately jump to complex deep learning without establishing a solid baseline first."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}