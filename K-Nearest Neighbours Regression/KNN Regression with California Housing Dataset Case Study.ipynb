{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20with%20California%20Housing%20Dataset%20Case%20Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: KNN Regression with California Housing Dataset\n",
    "\n",
    "K‑Nearest Neighbors (KNN) is a simple yet powerful machine learning algorithm that can be applied to both classification and regression tasks. In **regression**, instead of voting for a class label, KNN predicts a continuous value by **averaging** the target values of the K nearest neighbors. In this case study, we'll step through a practical example using the **California Housing** dataset to illustrate key concepts and best practices of KNN regression. This dataset contains information about housing blocks in California from the 1990 census, with 8 features (e.g., median income, house age, average rooms, location) and a target variable representing the median house value.\n",
    "\n",
    "## What we'll cover\n",
    "- **Data exploration and preparation:** Understanding feature distributions and splitting data into training, validation, and test sets.  \n",
    "- **Impact of feature scaling:** Demonstrating how scaling features affects KNN performance in regression tasks.  \n",
    "- **Choosing the number of neighbors (K):** Tuning K to balance model complexity (bias vs. variance).  \n",
    "- **Distance metric considerations:** How the choice of distance measure can affect KNN predictions.  \n",
    "- **Model evaluation:** Evaluating the final model using regression metrics (RMSE, MAE, R²) on a test set to ensure it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "Before diving into modeling, let's load the dataset and examine its features. The California Housing dataset has 20,640 samples, each with 8 features. The target is `MedHouseVal` (median house value in $100,000s).\n",
    "\n",
    "**Features:**  \n",
    "- MedInc: median income in block group  \n",
    "- HouseAge: median house age in block group  \n",
    "- AveRooms: average number of rooms per household  \n",
    "- AveBedrms: average number of bedrooms per household  \n",
    "- Population: block group population  \n",
    "- AveOccup: average number of household members  \n",
    "- Latitude: block group latitude  \n",
    "- Longitude: block group longitude  \n",
    "\n",
    "Large differences in magnitude (e.g., *Population* in thousands vs *AveBedrms* around 1) motivate **scaling** before using distance-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['MedHouseVal'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's examine the target variable distribution and basic feature statistics, then split the data into training, validation, and test sets using a 60/20/20 split.\n\n> **Question**: You've used the validation set to tune K from 1 to 30, ultimately selecting K=7 with validation RMSE of $0.52. Why is it critical to evaluate on a separate test set before deploying the model?\n>  \n> A) The validation set was used for hyperparameter selection, which can lead to optimistic performance estimates.\n>\n> B) The test set provides additional opportunities to fine-tune hyperparameters for better accuracy.\n>\n> C) Validation RMSE is systematically biased upward and always overestimates real-world error.\n>\n> D) The test set helps identify which features should be added or removed from the model.\n\nHolding out a test set is standard to avoid overfitting and obtain an unbiased estimate of performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target distribution\n",
    "print(\"Target variable (MedHouseVal) statistics:\")\n",
    "print(df['MedHouseVal'].describe(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics of features\n",
    "display(df[feature_names].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Plot target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Median House Value ($100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y)\n",
    "plt.ylabel('Median House Value ($100k)')\n",
    "plt.title('Box Plot of Target Variable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \"Validation size:\", X_val.shape[0], \"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Feature Scaling on KNN Regression\n",
    "\n",
    "Just like in classification, KNN regression uses distance to find nearest neighbors; if features are on very different scales, distance calculations will be dominated by the feature with the largest range. The example below illustrates how a difference in *Population* (thousands) can swamp a difference in *AveBedrms* (around 1). Therefore, scaling features to comparable ranges is critical for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate distance dominance (hypothetical differences)\n",
    "from math import sqrt\n",
    "\n",
    "delta_population_large = 1000.0\n",
    "delta_bedrooms_small = 0.5\n",
    "\n",
    "d1 = sqrt(delta_population_large**2 + 0.0**2)\n",
    "d2 = sqrt(0.0**2 + delta_bedrooms_small**2)\n",
    "\n",
    "print(\"Distance if only Population differs by +1000:\", round(d1, 3))\n",
    "print(\"Distance if only AveBedrms differs by +0.5  :\", round(d2, 3))\n",
    "print(\"Ratio (Population / Bedrooms):\", round(d1 / d2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a baseline KNN regression model with `K=5` using **unscaled** features and **scaled** features to compare validation performance. We'll use RMSE (Root Mean Squared Error) as our primary metric. Note that we scale using parameters learned from the training set only to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline without scaling\n",
    "knn_raw = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "raw_val_pred = knn_raw.predict(X_val)\n",
    "raw_val_rmse = np.sqrt(mean_squared_error(y_val, raw_val_pred))\n",
    "\n",
    "# Baseline with scaling (fit on train only)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_val_pred = knn_scaled.predict(X_val_scaled)\n",
    "scaled_val_rmse = np.sqrt(mean_squared_error(y_val, scaled_val_pred))\n",
    "\n",
    "print(f\"Validation RMSE without scaling: ${raw_val_rmse:.3f} (×100k)\")\n",
    "print(f\"Validation RMSE with scaling:    ${scaled_val_rmse:.3f} (×100k)\")\n",
    "print(f\"Improvement: {((raw_val_rmse - scaled_val_rmse) / raw_val_rmse * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled model typically performs significantly better because each feature contributes fairly to distance computation."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Step-by-step prediction for a single test point\nfrom sklearn.metrics import pairwise_distances\n\n# Pick the first validation example for demonstration\nx_test_example = X_val_scaled[0:1]  # Shape: (1, n_features)\ny_test_actual = y_val[0]\n\nprint(\"=\"*60)\nprint(\"STEP-BY-STEP PREDICTION WALKTHROUGH\")\nprint(\"=\"*60)\nprint(f\"\\nActual target value: ${y_test_actual:.2f} (×100k) = ${y_test_actual*100:.0f}k\\n\")\n\n# Step 1: Calculate distances from test point to all training points\ndistances = pairwise_distances(x_test_example, X_train_scaled, metric='euclidean').ravel()\nprint(f\"Step 1: Calculated {len(distances)} distances from test point to training points\")\nprint(f\"        Distance range: [{distances.min():.3f}, {distances.max():.3f}]\")\n\n# Step 2: Find indices of K nearest neighbors\nK = 5\nk_nearest_indices = np.argsort(distances)[:K]\nk_nearest_distances = distances[k_nearest_indices]\nprint(f\"\\nStep 2: Found K={K} nearest neighbors\")\nprint(f\"        Indices: {k_nearest_indices}\")\nprint(f\"        Distances: {[f'{d:.3f}' for d in k_nearest_distances]}\")\n\n# Step 3: Get target values of K nearest neighbors\nk_nearest_targets = y_train[k_nearest_indices]\nprint(f\"\\nStep 3: Retrieved target values of K nearest neighbors\")\nprint(f\"        NN's Labels: {[f'{t:.2f}' for t in k_nearest_targets]}\")\n\n# Step 4: Average the targets (THIS IS THE PREDICTION!)\nprediction = np.mean(k_nearest_targets)\nprint(f\"\\nStep 4: AVERAGE the neighbor targets\")\nprint(f\"        Prediction = mean({[f'{t:.2f}' for t in k_nearest_targets]})\")\nprint(f\"        Prediction = {prediction:.2f} (×100k) = ${prediction*100:.0f}k\")\n\nprint(f\"\\n\" + \"=\"*60)\nprint(f\"RESULT:\")\nprint(f\"  Actual:     ${y_test_actual:.2f} (×100k)\")\nprint(f\"  Predicted:  ${prediction:.2f} (×100k)\")\nprint(f\"  Error:      ${abs(y_test_actual - prediction):.2f} (×100k)\")\nprint(\"=\"*60)\n\n# Verify this matches sklearn's prediction\nsklearn_prediction = knn_scaled.predict(x_test_example)[0]\nprint(f\"\\nVerification: sklearn prediction = ${sklearn_prediction:.2f} ✓\")\nassert np.isclose(prediction, sklearn_prediction), \"Predictions should match!\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step-by-Step: How KNN Makes a Prediction\n\nTo understand exactly how KNN regression works, let's walk through the prediction process step-by-step for a single test point, using the same approach shown in the lecture slides.\n\n**Steps:**\n1. Calculate pairwise distances from the test point to all training points\n2. Sort distances and select the K nearest neighbors (smallest distances)\n3. Get the target values of these K neighbors\n4. **Average** these values to make the prediction\n\n> **Key Difference from Classification**: In classification, KNN uses **voting** (most common class). In regression, KNN uses **averaging** (mean of continuous values).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization: Actual vs Predicted values**  \n",
    "Let's visualize how well our scaled KNN model predicts on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_val, scaled_val_pred, alpha=0.5, edgecolor='k', s=20)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual House Value ($100k)')\n",
    "plt.ylabel('Predicted House Value ($100k)')\n",
    "plt.title('Actual vs Predicted (K=5, Scaled Features)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Distance Metric Considerations\nChoosing a distance metric is itself a hyperparameter. For continuous features, **Euclidean (L2)** is the default and measures straight-line distance; **Manhattan (L1)** sums absolute differences and can be more robust to outliers. In practice, treat the metric as something to tune by validation.\n\n> **Question**: Your KNN model uses 3 features: 'MedInc' (range 0-15), 'Population' (range 0-35,000), and 'Latitude' (range 32-42). Without scaling, which feature will dominate the distance calculations, and why?\n>\n> A) MedInc—it has the strongest correlation with house prices\n>\n> B) Population—it has the largest numerical range\n>\n> C) Latitude—geographic features always have higher weight in distance metrics\n>\n> D) All features contribute equally because KNN normalizes distances automatically"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of different metrics and K values, we can do a small grid of {Euclidean, Manhattan} × {3, 5, 7, 9} on the validation set. This isn't exhaustive but shows that **distance metric** is a tunable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "metrics = ['euclidean', 'manhattan']\n",
    "k_values = [3, 5, 7, 9]\n",
    "rows = []\n",
    "\n",
    "for metric, k in product(metrics, k_values):\n",
    "    mdl = KNeighborsRegressor(n_neighbors=k, metric=metric).fit(X_train_scaled, y_train)\n",
    "    pred = mdl.predict(X_val_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    rows.append((metric, k, rmse))\n",
    "\n",
    "grid_df = pd.DataFrame(rows, columns=['metric', 'k', 'val_rmse']) \\\n",
    "          .pivot(index='metric', columns='k', values='val_rmse')\n",
    "print(\"Validation RMSE Grid (lower is better):\")\n",
    "display(grid_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best (metric, k) by validation RMSE (lower is better)\n",
    "grid_long = (\n",
    "    grid_df.stack()                 # -> Series with MultiIndex (metric, k)\n",
    "           .rename('val_rmse')\n",
    "           .reset_index()           # -> columns: ['metric', 'k', 'val_rmse']\n",
    ")\n",
    "\n",
    "# Tie-breaker: prefer smaller k, then 'euclidean' over 'manhattan'\n",
    "grid_long['tie_metric_rank'] = grid_long['metric'].map({'euclidean': 0, 'manhattan': 1})\n",
    "\n",
    "best_row = (\n",
    "    grid_long.sort_values(\n",
    "        ['val_rmse', 'k', 'tie_metric_rank'],\n",
    "        ascending=[True, True, True]  # Lower RMSE is better\n",
    "    )\n",
    "    .iloc[0]\n",
    ")\n",
    "\n",
    "chosen_metric = best_row['metric']\n",
    "chosen_k      = int(best_row['k'])\n",
    "best_val_rmse = float(best_row['val_rmse'])\n",
    "\n",
    "print(\"Decision log — chosen params (metric & k):\")\n",
    "print({\"metric\": chosen_metric, \"n_neighbors\": chosen_k, \"val_rmse\": round(best_val_rmse, 3)})\n",
    "\n",
    "# Sanity check with chosen params on validation data\n",
    "_knn = KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric).fit(X_train_scaled, y_train)\n",
    "val_rmse_check = np.sqrt(mean_squared_error(y_val, _knn.predict(X_val_scaled)))\n",
    "print(f\"Validation RMSE (chosen metric & k): {val_rmse_check:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Choosing K: Bias–Variance Trade‑off\nA small K (e.g., K=1) is highly flexible and fits training data very closely—**high variance** and potential overfitting. A very large K (approaching the size of the training set) averages over many neighbors—**high bias** and potential underfitting. We sweep K from 1 to 30 and plot training vs. validation RMSE to pick the best K by validation performance.\n\n> **Question**: After training KNN models with different K values, you observe that K=1 achieves training RMSE of $0.05 but validation RMSE of $0.75, while K=5 achieves training RMSE of $0.45 and validation RMSE of $0.52. What does this pattern suggest about the K=1 model?\n>  \n> A) The model has high variance and is overfitting to training noise.\n>\n> B) The model has high bias and requires more complex features.\n>\n> C) The training dataset is too small and more samples would fix the issue.\n>\n> D) Add more features to improve generalization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, val_rmse = [], []\n",
    "k_sweep = range(1, 31)\n",
    "\n",
    "for k in k_sweep:\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))\n",
    "    val_rmse.append(np.sqrt(mean_squared_error(y_val, val_pred)))\n",
    "\n",
    "# Best K by validation RMSE\n",
    "best_k_idx = int(np.argmin(val_rmse))\n",
    "best_k = best_k_idx + 1\n",
    "best_val = min(val_rmse)\n",
    "\n",
    "print(\"Best K (by validation RMSE):\", best_k, \"Validation RMSE:\", round(best_val, 3))\n",
    "\n",
    "# Plot train vs validation RMSE vs K\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(k_sweep), train_rmse, marker='o', label='Train RMSE', linewidth=2)\n",
    "plt.plot(list(k_sweep), val_rmse, marker='s', label='Validation RMSE', linewidth=2)\n",
    "plt.axvline(best_k, linestyle='--', color='red', label=f'Best K={best_k}')\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('RMSE ($100k)', fontsize=12)\n",
    "plt.title('Bias-Variance Trade-off: RMSE vs K', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Evaluation on Test Set\nWith the chosen hyperparameters, we refit KNN (within a pipeline to avoid leakage) on the combined training + validation data and evaluate performance on the **held-out test** set. This provides an unbiased estimate of real-world performance. We'll use multiple regression metrics:\n\n- **RMSE (Root Mean Squared Error)**: Penalizes large errors more heavily\n- **MAE (Mean Absolute Error)**: Average absolute prediction error\n- **R² Score**: Proportion of variance explained (1.0 is perfect, 0.0 is baseline)\n- **MAPE (Mean Absolute Percentage Error)**: Percentage error\n\n> **Question**: Your final KNN model achieves validation RMSE of $0.52 and test RMSE of $0.56. Before deployment, what's the most appropriate interpretation?\n>  \n> A) The difference is normal variation; verify test performance meets business requirements.\n>\n> B) The test set has data leakage and should be regenerated with better random seeds.\n>\n> C) Re-tune hyperparameters using the test set to minimize the performance gap.\n>\n> D) The model is underfitting and needs a smaller K value for better flexibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final training\n",
    "X_train_all = np.vstack([X_train, X_val])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "# Build pipeline (scaler + KNN) with chosen hyperparameters\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=chosen_k, metric=chosen_metric))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_pipe.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE:  ${test_rmse:.3f} (×100k) = ${test_rmse*100:.0f}k\")\n",
    "print(f\"MAE:   ${test_mae:.3f} (×100k) = ${test_mae*100:.0f}k\")\n",
    "print(f\"R²:    {test_r2:.3f}\")\n",
    "print(f\"MAPE:  {test_mape*100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the test set predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for test set\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, test_pred, alpha=0.5, edgecolor='k', s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual House Value ($100k)')\n",
    "plt.ylabel('Predicted House Value ($100k)')\n",
    "plt.title(f'Test Set: Actual vs Predicted\\n(R² = {test_r2:.3f}, RMSE = ${test_rmse:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - test_pred\n",
    "plt.scatter(test_pred, residuals, alpha=0.5, edgecolor='k', s=20)\n",
    "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted House Value ($100k)')\n",
    "plt.ylabel('Residuals ($100k)')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of residuals\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Residuals ($100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(0, color='r', linestyle='--', lw=2, label='Zero error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Where Do Errors Come From?\n\nUnderstanding where KNN regression makes larger errors helps us interpret model performance and identify areas for improvement. As shown in the lecture slides, prediction errors in KNN regression come from two types of regions:\n\n### High Certainty Regions\n- **Characteristics**: Neighbors have similar target values with low variation\n- **Prediction Quality**: Low residual errors (high confidence predictions)\n- **Why**: When K nearest neighbors have similar values, their average is a reliable estimate\n- **Example**: In dense neighborhoods where houses have similar prices\n\n### High Ambiguity Regions  \n- **Characteristics**: Neighbors have high variation in target values\n- **Prediction Quality**: High residual errors (uncertain predictions)\n- **Why**: When K nearest neighbors have very different values, averaging produces less reliable estimates\n- **Example**: Boundary regions between expensive and affordable neighborhoods, or sparse data regions\n\n### Additional Error Sources\n- **Sparse Regions**: Areas with few training points lead to unreliable neighbor selection\n- **Boundary Regions**: Transition zones where the K neighbors remain constant but represent different underlying patterns\n\n> **Key Insight**: KNN performs best in regions where neighbors have consistent target values. The model struggles in regions with high local variability or sparse data, as the averaging assumption breaks down.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance via Permutation\n",
    "Unlike tree-based models, KNN doesn't have built-in feature importance. However, we can use **permutation importance** to understand which features matter most. This technique randomly shuffles each feature and measures the drop in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    final_pipe, X_test, y_test, \n",
    "    n_repeats=10, random_state=42, \n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], xerr=importance_df['std'])\n",
    "plt.xlabel('Decrease in R² (Importance)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations (Current Scope) & What's Next\n",
    "This notebook uses a **single hold‑out validation** set, which is simple but sensitive to data splits. In practice, data scientists often use **k‑fold cross‑validation** or nested validation to obtain more reliable estimates and avoid overfitting hyperparameters to a single split. We also used brute‑force neighbor search and didn't explore scalability techniques like KD‑trees, Ball Trees, or approximate nearest neighbor libraries (e.g., FAISS, HNSW). These become important when your dataset grows to millions of rows or requires low‑latency predictions.\n",
    "\n",
    "**Additional considerations for regression:**\n",
    "- **Weighting neighbors by distance**: Closer neighbors can have more influence (weights='distance' in sklearn)\n",
    "- **Handling outliers in target variable**: KNN averages can be affected by extreme values\n",
    "- **Feature engineering**: Creating interaction features or polynomial features might improve performance\n",
    "- **Ensemble methods**: Combining KNN with other regressors can improve robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- **Scaling** prevents large‑range features from dominating distance computations in regression, just as in classification.  \n",
    "- **Tuning K** via validation balances bias and variance; a very small K overfits, a very large K underfits.  \n",
    "- **Distance metric and K** are hyperparameters; small grids can reveal significant differences in RMSE.  \n",
    "- **Regression metrics** (RMSE, MAE, R²) provide different perspectives on model performance.  \n",
    "- KNN regression remains a powerful, intuitive baseline—use it to build understanding about distance-based prediction before advancing to more sophisticated models like Random Forest or Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Real‑World Applications: Where KNN Regression Is Useful\n",
    "- **Real estate valuation**: Predict property prices based on location, size, and features by finding similar properties that recently sold.  \n",
    "- **Recommender systems**: Predict user ratings for products by averaging ratings from users with similar preferences.  \n",
    "- **Stock price prediction**: Estimate future prices based on historical patterns of similar market conditions.  \n",
    "- **Energy consumption forecasting**: Predict building energy usage based on similar historical weather and occupancy patterns.  \n",
    "- **Medical diagnosis**: Estimate patient outcomes or test results based on similar cases in medical databases.  \n",
    "- **Sensor calibration**: Predict true measurements from sensors by comparing to known calibration points.\n",
    "\n",
    "## Why KNN Regression Specifically\n",
    "- **No assumptions about functional form**: Unlike linear regression, KNN doesn't assume a specific relationship between features and target.  \n",
    "- **Handles non-linear relationships naturally**: Can capture complex patterns without manual feature engineering.  \n",
    "- **Local adaptation**: Predictions adapt to local patterns in the feature space.  \n",
    "- **Interpretability**: You can explain predictions by showing the K nearest similar examples from the training set.  \n",
    "- **Quick prototyping**: Simple to implement and understand, making it great for initial exploration.\n",
    "\n",
    "## When to Consider Alternatives\n",
    "- **Large datasets**: KNN becomes slow as data grows; consider tree-based methods or neural networks.\n",
    "- **High dimensionality**: The curse of dimensionality makes distances less meaningful; consider dimensionality reduction first.\n",
    "- **Need for parametric model**: If you need to understand feature effects explicitly, use linear regression or GAMs.\n",
    "- **Real-time predictions at scale**: KNN requires scanning the entire training set; use models with faster inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}