{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LiyLga1Qb7f"
      },
      "source": [
        "# K-Nearest Neighbors (KNN) Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6unq0EGGQb7h"
      },
      "source": [
        "In this lab, you will implement a K-Nearest Neighbors regressor from scratch, tune its hyperparameters, and apply it to synthetic and real datasets. Along the way, you'll explore the bias-variance tradeoff, understand the importance of feature scaling, and visualize how different K values affect model predictions. By the end, you will have a clear grasp of how KNN regression works, how to evaluate it using metrics like MSE and R\u00b2, and why practices like proper data splitting and standardization are critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goLE2YSIHsY9"
      },
      "source": [
        "## Overview of KNN Regression\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "**Algorithm:**\n",
        "1. Given a new data point $\\mathbf{x}_{\\text{new}}$, compute the distance from $\\mathbf{x}_{\\text{new}}$ to every point in the training set.\n",
        "2. Select the K training points closest to $\\mathbf{x}_{\\text{new}}$ (the K nearest neighbors).\n",
        "3. **Average** the target values of these K neighbors to produce the prediction:\n",
        "\n",
        "$$\\hat{y}_{\\text{new}} = \\frac{1}{K} \\sum_{i=1}^{K} y_i$$\n",
        "\n",
        "where $y_i$ are the target values of the K nearest neighbors.\n",
        "\n",
        "**Key Difference from Classification:**\n",
        "- **Classification**: Uses majority voting among K neighbors' class labels\n",
        "- **Regression**: Averages the K neighbors' continuous target values\n",
        "\n",
        "**Example:**\n",
        "If K=3 and the 3 nearest neighbors have target values [2.5, 3.0, 2.8], the prediction would be:\n",
        "$$\\hat{y} = \\frac{2.5 + 3.0 + 2.8}{3} = 2.77$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eN_d8fRQb7k"
      },
      "source": [
        "## Visual K Progression\n",
        "\n",
        "Before diving into implementation, let's visualize how the K parameter affects predictions step by step.### Understanding K Through VisualizationWhen K=1, the algorithm considers only the single nearest neighbor, making predictions highly sensitive to individual data points. As K increases, predictions become smoother by averaging more neighbors, reducing sensitivity to outliers but potentially oversimplifying patterns in the data.**Note on Distance in 1D**: In the visualization below, we use a single feature (1D data). Here, the Euclidean distance between two points $x$ and $x'$ simplifies to the absolute difference: $d(x, x') = |x - x'|$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tU-KI0DQb7k"
      },
      "outputs": [],
      "source": [
        "# Visualize how K affects predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create simple dataset\n",
        "np.random.seed(42)\n",
        "X_simple = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y_simple = np.array([2.1, 2.8, 3.9, 4.1, 5.2, 5.8, 6.1, 7.2, 8.9, 9.1])\n",
        "\n",
        "# Test point\n",
        "x_test = 5.5\n",
        "\n",
        "# Visualize K progression\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "k_values = [1, 2, 3, 4, 5, 10]\n",
        "\n",
        "for idx, k in enumerate(k_values):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "\n",
        "    # Calculate distances\n",
        "    distances = np.abs(X_simple.flatten() - x_test)\n",
        "    nearest_indices = np.argsort(distances)[:k]\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = np.mean(y_simple[nearest_indices])\n",
        "\n",
        "    # Plot\n",
        "    ax.scatter(X_simple, y_simple, c='blue', s=100, alpha=0.6, label='Training data')\n",
        "    ax.scatter(X_simple[nearest_indices], y_simple[nearest_indices],\n",
        "               c='red', s=200, marker='*', label=f'K={k} neighbors', zorder=3)\n",
        "    ax.scatter(x_test, prediction, c='green', s=300, marker='X',\n",
        "               label=f'Prediction={prediction:.2f}', zorder=4)\n",
        "\n",
        "    # Draw circle showing radius\n",
        "    circle = plt.Circle((x_test, prediction), distances[nearest_indices[-1]],\n",
        "                        color='red', fill=False, linestyle='--', alpha=0.3)\n",
        "    ax.add_patch(circle)\n",
        "\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'K = {k}: Prediction = {prediction:.2f}')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Test point X = {x_test}\")\n",
        "for k in k_values:\n",
        "    distances = np.abs(X_simple.flatten() - x_test)\n",
        "    nearest_indices = np.argsort(distances)[:k]\n",
        "    prediction = np.mean(y_simple[nearest_indices])\n",
        "    print(f\"K={k:2d}: Prediction = {prediction:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKgu6j9HsY9"
      },
      "source": [
        "> **Question**: In KNN regression, the parameter K refers to:\n",
        "\n",
        "> A. The number of features in the dataset\n",
        ">\n",
        "> B. The number of nearest neighbors whose target values are averaged for prediction\n",
        ">\n",
        "> C. The number of clusters in the data\n",
        ">\n",
        "> D. The number of iterations for training\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation:**\n",
        "- **A is FALSE**: The number of features is denoted by other variables (like n or p), not K\n",
        "- **B is TRUE**: K specifically refers to the number of nearest neighbors used to make a prediction by averaging their target values\n",
        "- **C is FALSE**: KNN is not a clustering algorithm; K-means clustering uses K for clusters, but this is KNN regression\n",
        "- **D is FALSE**: KNN is a lazy learner with no training iterations; it simply stores the training data\n",
        "\n",
        "**Key Insight**: K is the most important hyperparameter in KNN regression that controls the bias-variance tradeoff. Small K values create flexible models (high variance, low bias) that can overfit, while large K values create simpler models (high bias, low variance) that may underfit. Finding the optimal K through validation is crucial for good generalization.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAZ9wveaHsY9"
      },
      "source": [
        "## Z-Score Standardization\n",
        "\n",
        "Before we explore feature scaling in practice, let's understand the mathematical foundation.\n",
        "\n",
        "### The Z-Score Formula\n",
        "\n",
        "Standardization (Z-score normalization) transforms features to have mean $\\mu = 0$ and standard deviation $\\sigma = 1$:\n",
        "\n",
        "$$Z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the original value\n",
        "- $\\mu$ is the mean of the feature\n",
        "- $\\sigma$ is the standard deviation of the feature\n",
        "\n",
        "### Why Z-Score Standardization Works\n",
        "\n",
        "1. **Centers the data**: Subtracting the mean shifts the distribution to be centered at zero\n",
        "2. **Scales the spread**: Dividing by standard deviation makes the spread consistent across features\n",
        "3. **Preserves distribution shape**: Unlike min-max scaling, z-score maintains the original distribution's shape\n",
        "4. **Handles outliers better**: Less sensitive to extreme values compared to min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmn36k3LQb7l"
      },
      "outputs": [],
      "source": [
        "# Visualize Z-score standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "# Create two features with different scales\n",
        "feature1 = np.random.normal(loc=100, scale=15, size=200)  # Mean=100, SD=15\n",
        "feature2 = np.random.normal(loc=5, scale=2, size=200)     # Mean=5, SD=2\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "feature1_scaled = scaler.fit_transform(feature1.reshape(-1, 1)).flatten()\n",
        "feature2_scaled = scaler.fit_transform(feature2.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Before standardization - Feature 1\n",
        "axes[0, 0].hist(feature1, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(feature1.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature1.mean():.1f}')\n",
        "axes[0, 0].set_xlabel('Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title(f'Feature 1 (Before)\\n\u03bc={feature1.mean():.1f}, \u03c3={feature1.std():.1f}')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Before standardization - Feature 2\n",
        "axes[0, 1].hist(feature2, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(feature2.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature2.mean():.1f}')\n",
        "axes[0, 1].set_xlabel('Value')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title(f'Feature 2 (Before)\\n\u03bc={feature2.mean():.1f}, \u03c3={feature2.std():.1f}')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 1\n",
        "axes[1, 0].hist(feature1_scaled, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].axvline(feature1_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature1_scaled.mean():.1f}')\n",
        "axes[1, 0].set_xlabel('Value (Z-score)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'Feature 1 (After)\\n\u03bc={feature1_scaled.mean():.2f}, \u03c3={feature1_scaled.std():.2f}')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 2\n",
        "axes[1, 1].hist(feature2_scaled, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].axvline(feature2_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature2_scaled.mean():.1f}')\n",
        "axes[1, 1].set_xlabel('Value (Z-score)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title(f'Feature 2 (After)\\n\u03bc={feature2_scaled.mean():.2f}, \u03c3={feature2_scaled.std():.2f}')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Before Standardization:\")\n",
        "print(f\"  Feature 1: \u03bc={feature1.mean():.2f}, \u03c3={feature1.std():.2f}, range=[{feature1.min():.2f}, {feature1.max():.2f}]\")\n",
        "print(f\"  Feature 2: \u03bc={feature2.mean():.2f}, \u03c3={feature2.std():.2f}, range=[{feature2.min():.2f}, {feature2.max():.2f}]\")\n",
        "print(\"\\nAfter Standardization:\")\n",
        "print(f\"  Feature 1: \u03bc={feature1_scaled.mean():.2f}, \u03c3={feature1_scaled.std():.2f}, range=[{feature1_scaled.min():.2f}, {feature1_scaled.max():.2f}]\")\n",
        "print(f\"  Feature 2: \u03bc={feature2_scaled.mean():.2f}, \u03c3={feature2_scaled.std():.2f}, range=[{feature2_scaled.min():.2f}, {feature2_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJYpmTLnHsY9"
      },
      "source": [
        "Here\u2019s a cleaned-up and corrected version:\n",
        "\n",
        "> **Question**: You're building a KNN regression model with three features: Age (range 18\u201365), Income (range $20,000\u2013$150,000), and Credit Score (range 300\u2013850). If you train KNN without scaling these features, what will happen?\n",
        ">\n",
        "> A. The model will automatically normalize features internally, so scaling is unnecessary for KNN algorithms\n",
        ">\n",
        "> B. The Income feature will dominate distance calculations, making Age and Credit Score almost irrelevant to predictions\n",
        ">\n",
        "> C. All features will contribute equally because KNN uses relative distances rather than absolute values\n",
        ">\n",
        "> D. The model will fail to train because KNN requires all features to have the same scale before fitting\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "This question tests the **practical effect** of not scaling for distance-based methods like KNN.\n",
        "\n",
        "* **B is TRUE** \u2014 Income dominates the distance:\n",
        "\n",
        "  * **Age range:** 18\u201365 \u21d2 max diff = 47\n",
        "  * **Income range:** $20k\u2013$150k \u21d2 max diff = $130,000\n",
        "  * **Credit range:** 300\u2013850 \u21d2 max diff = 550\n",
        "  * **Scale imbalance:** Income differences are \u2248 **2,766\u00d7** larger than Age and \u2248 **236\u00d7** larger than Credit.\n",
        "    In Euclidean distance, squared terms magnify this further: (130{,}000^2) vs (47^2) makes Income contribute \u2248 **7.6 million\u00d7** more than Age to the **squared** distance.\n",
        "\n",
        "* **A is FALSE** \u2014 KNN does **not** normalize for you:\n",
        "  KNN computes distances (e.g., Euclidean/Manhattan) on the raw inputs. No automatic scaling occurs; you must scale explicitly (e.g., `StandardScaler`/`MinMaxScaler`, ideally via a pipeline).\n",
        "\n",
        "* **C is FALSE** \u2014 KNN uses **absolute** differences, not \u201crelative\u201d distances that equalize scales.\n",
        "  Euclidean: (d=\\sqrt{(\u0394\\text{age})^2 + (\u0394\\text{income})^2 + (\u0394\\text{credit})^2}).\n",
        "  With unscaled features, the Income term dominates this sum.\n",
        "\n",
        "* **D is FALSE** \u2014 The model still \u201ctrains\u201d (lazy learning):\n",
        "  `KNeighborsRegressor` stores the training data and will predict, but performance will be biased toward the largest-scale feature (Income).\n",
        "\n",
        "**Concrete example**\n",
        "\n",
        "* Point A: [Age=25, Income=$50k, Credit=700]\n",
        "* Point B: [Age=26, Income=$51k, Credit=750]\n",
        "* Point C: [Age=50, Income=$50k, Credit=700]\n",
        "\n",
        "Distances (unscaled, Euclidean):\n",
        "\n",
        "* d(A,B)=$\\sqrt{(1)^2 + (1000)^2 + (50)^2}\\approx \\sqrt{1{,}002{,}501}\\approx 1001.25$\n",
        "* d(A,C)=$\\sqrt{(25)^2 + 0^2 + 0^2}=25$\n",
        "\n",
        "Even though **B** is very similar overall, the $1,000 income difference makes **B appear much farther** from A than **C**, which shares the same income and credit. This is the scaling problem in action.\n",
        "\n",
        "**Key Insight:** Not scaling doesn\u2019t **break** KNN\u2014it **silences** small-scale features. Always scale features for distance-based models.\n",
        "\n",
        "</details>\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudocode for KNN Regressor\n",
        "\n",
        "Before implementing the algorithm, let's understand the pseudocode structure (from lecture slides):\n",
        "\n",
        "```\n",
        "# Inputs\n",
        "#   data      \u2190 training set of N examples (x, y)\n",
        "#   k         \u2190 number of neighbours\n",
        "#   metric    \u2190 distance function (e.g., Euclidean, Manhattan)\n",
        "#   X_query   \u2190 set of examples to predict\n",
        "\n",
        "# ----- \"fit\" (lazy) -----\n",
        "X_train \u2190 data.x\n",
        "y_train \u2190 data.y\n",
        "\n",
        "# ----- predict -----\n",
        "\u0177 \u2190 list of length |X_query|  # outputs align 1:1 with X_query\n",
        "\n",
        "FOR i = 1 TO |X_query| DO\n",
        "    x* \u2190 X_query[i]\n",
        "    d \u2190 distances from x* to all X_train using metric\n",
        "    J \u2190 indices of the k smallest values in d\n",
        "    # regression prediction = average of the targets of the k nearest neighbours\n",
        "    \u0177[i] \u2190 mean(y_train[J])\n",
        "END FOR\n",
        "\n",
        "RETURN \u0177\n",
        "```\n",
        "\n",
        "**Key Points:**\n",
        "- KNN is a \"lazy learner\" - the `fit` method just stores the training data\n",
        "- The `predict` method does all the work: compute distances, find K nearest, average their targets\n",
        "- **Regression uses averaging** (not voting like classification)"
      ],
      "metadata": {
        "id": "vZYOgx4NHsY-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWwk1-fnHsY-"
      },
      "source": [
        "## Implementing a Custom KNN Regressor\n",
        "\n",
        "Below is a scaffold of the `MyKNNRegressor` class. Fill in the TODO sections to complete the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PkxAmvXHsY-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "class MyKNNRegressor:\n",
        "    \"\"\"\n",
        "    A simple K-Nearest Neighbors Regressor implementation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_neighbors : int, default=5\n",
        "        Number of neighbors to use for prediction\n",
        "    metric : str, default='euclidean'\n",
        "        Distance metric to use ('euclidean', 'manhattan', etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.metric = metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the KNN regressor by storing the training data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        # TODO: Store the training data\n",
        "        # Hint: KNN is a \"lazy learner\" - it just stores the training data\n",
        "        self.X_train = ___\n",
        "        self.y_train = ___\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for test data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array of shape (n_samples,)\n",
        "            Predicted target values\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for x_test in X:\n",
        "            # TODO: For each test point:\n",
        "            # 1. Compute distances from x_test to all training points\n",
        "            # 2. Find indices of K nearest neighbors\n",
        "            # 3. Get target values of those K neighbors\n",
        "            # 4. Average them to get the prediction\n",
        "\n",
        "            # Step 1: Compute distances\n",
        "            distances = pairwise_distances(\n",
        "                x_test.reshape(1, -1),\n",
        "                self._____,\n",
        "                metric=self.metric\n",
        "            ).ravel()\n",
        "\n",
        "            # Step 2: Find K nearest neighbor indices\n",
        "            k_nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
        "\n",
        "            # Step 3: Get target values of K nearest neighbors\n",
        "            k_nearest_targets = self.y_train[______]\n",
        "\n",
        "            # Step 4: Average the target values\n",
        "            prediction = np.mean(_______)\n",
        "\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "print(\"MyKNNRegressor class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNGRBbBHsY-"
      },
      "source": [
        "Once you have filled in the implementation, let's test our custom regressor on a simple dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODx0DF2HsY_"
      },
      "source": [
        "## A Dataset for Visualization\n",
        "\n",
        "We'll create a synthetic 1D regression dataset to visualize how KNN regression works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdmajwTrHsY_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic 1D data\n",
        "def f(x):\n",
        "    \"\"\"True underlying function: sinusoidal pattern\"\"\"\n",
        "    return np.sin(x) + 0.1 * x\n",
        "\n",
        "# Training data\n",
        "X_train_1d = np.sort(np.random.uniform(0, 10, 100))\n",
        "y_train_1d = f(X_train_1d) + np.random.normal(0, 0.2, 100)  # Add noise\n",
        "\n",
        "# Reshape for sklearn compatibility\n",
        "X_train_1d = X_train_1d.reshape(-1, 1)\n",
        "\n",
        "# Test data (for smooth prediction curve)\n",
        "X_test_1d = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "y_test_1d_true = f(X_test_1d.ravel())\n",
        "\n",
        "print(f\"Training data shape: X={X_train_1d.shape}, y={y_train_1d.shape}\")\n",
        "print(f\"Test data shape: X={X_test_1d.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10nBkhcNHsY_"
      },
      "source": [
        "Let's visualize the training data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual step-by-step prediction (matching lecture slides 11-13)\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Choose a test point (let's pick x=5.0 from our test range)\n",
        "x_test_single = np.array([[5.0]])  # Must be 2D for sklearn\n",
        "print(\"=\"*70)\n",
        "print(\"STEP-BY-STEP: How KNN Predicts for x_test = 5.0\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Calculate distances from test point to ALL training points\n",
        "print(\"\\nSTEP 1: Pairwise Distance Calculation\")\n",
        "print(\"-\" * 70)\n",
        "distances = pairwise_distances(x_test_single, X_train_1d, metric='euclidean').ravel()\n",
        "print(f\"Computed {len(distances)} distances from x_test=5.0 to all training points\")\n",
        "print(f\"Distance array shape: {distances.shape}\")\n",
        "print(f\"First 10 distances: {distances[:10].round(3)}\")\n",
        "\n",
        "# Step 2: Find K nearest neighbors\n",
        "print(\"\\nSTEP 2: Finding K Nearest Neighbors\")\n",
        "print(\"-\" * 70)\n",
        "K = 5\n",
        "k_indices = np.argsort(distances)[:K]  # Indices of K smallest distances\n",
        "k_distances = distances[k_indices]\n",
        "print(f\"K = {K}\")\n",
        "print(f\"Indices of K nearest neighbors: {k_indices}\")\n",
        "print(f\"Their distances: {k_distances.round(3)}\")\n",
        "print(f\"Their x-coordinates: {X_train_1d[k_indices].ravel().round(3)}\")\n",
        "\n",
        "# Step 3: Get target values of K nearest neighbors\n",
        "print(\"\\nSTEP 3: Get Neighbor Targets\")\n",
        "print(\"-\" * 70)\n",
        "k_targets = y_train_1d[k_indices]\n",
        "print(f\"NN's Labels (target values): {k_targets.round(3)}\")\n",
        "\n",
        "# Step 4: Average to get prediction\n",
        "print(\"\\nSTEP 4: Average (THE PREDICTION!)\")\n",
        "print(\"-\" * 70)\n",
        "manual_prediction = np.mean(k_targets)\n",
        "print(f\"Prediction = mean({k_targets.round(3)})\")\n",
        "print(f\"Prediction = {manual_prediction:.3f}\")\n",
        "\n",
        "# Verify with our custom KNN\n",
        "knn_verify = MyKNNRegressor(n_neighbors=K)\n",
        "knn_verify.fit(X_train_1d, y_train_1d)\n",
        "verify_prediction = knn_verify.predict(x_test_single)[0]\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFICATION:\")\n",
        "print(f\"  Manual calculation:  {manual_prediction:.3f}\")\n",
        "print(f\"  MyKNNRegressor:      {verify_prediction:.3f}\")\n",
        "print(f\"  Match: {np.isclose(manual_prediction, verify_prediction)} \u2713\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Visualize this specific prediction\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(X_train_1d, y_train_1d, c='lightblue', alpha=0.6, edgecolor='k', s=50, label='All training data')\n",
        "plt.scatter(X_train_1d[k_indices], k_targets, c='red', s=100, edgecolor='k',\n",
        "           label=f'K={K} nearest neighbors', zorder=5)\n",
        "plt.scatter(x_test_single, manual_prediction, c='green', s=200, marker='*',\n",
        "           edgecolor='k', label='Prediction (average)', zorder=10)\n",
        "plt.axhline(manual_prediction, color='green', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title(f'Step-by-Step KNN Prediction: x_test={x_test_single[0,0]}, K={K}, Prediction={manual_prediction:.3f}')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uVUFYcjhHsY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Prediction Process: Step-by-Step\n",
        "\n",
        "Before we test different K values, let's manually walk through exactly what happens when KNN makes a single prediction. This matches the step-by-step process from the lecture slides (slides 11-13).\n",
        "\n",
        "**The 4 Steps:**\n",
        "1. **Pairwise Distance Calculation**: Compute distance from test point to all training points\n",
        "2. **Finding K Nearest Neighbors**: Sort distances and select K smallest\n",
        "3. **Get Neighbor Targets**: Retrieve the y-values of those K neighbors  \n",
        "4. **Average**: Compute mean of the K target values \u2192 this is the prediction!\n",
        "\n",
        "Let's demonstrate this process:"
      ],
      "metadata": {
        "id": "8vHx0M4NHsY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W58gNJ3hHsY_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training data\n",
        "plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, edgecolor='k', s=50, label='Training data')\n",
        "plt.plot(X_test_1d, y_test_1d_true, 'g--', lw=2, label='True function')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Training Data with True Underlying Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here\u2019s a cleaned-up, properly formatted version:\n",
        "\n",
        "> **Question**: You\u2019ve trained a KNN regressor on 10,000 training samples. When comparing prediction time for different K values, which statement is most accurate?\n",
        ">\n",
        "> A. Increasing K from 5 to 50 will significantly slow down predictions because the algorithm must average more neighbors\u2019 values\n",
        ">\n",
        "> B. Prediction time is nearly identical for K=5 and K=50 because the dominant cost is computing distances to all 10,000 training points, not averaging K values\n",
        ">\n",
        "> C. Larger K values (like K=50) are faster because the algorithm can stop searching after finding the first 50 neighbors instead of examining all 10,000 points\n",
        ">\n",
        "> D. K=5 and K=50 have identical prediction times in theory, but K=50 uses more memory during prediction which can slow down performance on large datasets\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation**\n",
        "This question tests understanding of **where KNN\u2019s prediction cost comes from**.\n",
        "\n",
        "* **B is TRUE** \u2014 Distance computation dominates:\n",
        "\n",
        "  * **Step 1 (major cost):** compute distances to all (N) training points \u21d2 (O(N \\times d)). With (N=10{,}000), you still evaluate **all** distances regardless of (K).\n",
        "  * **Step 2:** maintain top-(K) neighbors (e.g., via a small heap) \u21d2 (O(N \\log K)) or (O(N)) with selection.\n",
        "  * **Step 3:** average the (K) targets \u21d2 (O(K)), which is tiny compared to Step 1.\n",
        "\n",
        "* **A is FALSE** \u2014 Averaging more neighbors is negligible:\n",
        "\n",
        "  * Averaging 5 vs 50 numbers is micro-work relative to computing 10,000 distances.\n",
        "\n",
        "* **C is FALSE** \u2014 Brute-force KNN does **not** stop early:\n",
        "\n",
        "  * To know the nearest (K), you must compare against (nearly) **all** points.\n",
        "  * *Exception:* Tree/ANN indexes (KD/ball trees, HNSW, etc.) can prune/approximate, but that\u2019s not the default brute-force behavior and helps mainly in low dimensions.\n",
        "\n",
        "* **D is FALSE** \u2014 Memory impact of (K) at prediction is minimal:\n",
        "\n",
        "  * Storing (N) distances dominates; keeping an extra list of (K) neighbors is (O(K)) and negligible for (K=50).\n",
        "\n",
        "**Why times look similar in practice (illustrative breakdown)**\n",
        "\n",
        "```text\n",
        "Prediction time per query point (brute force):\n",
        "\n",
        "K = 5\n",
        "- Distance computations: ~95%\n",
        "- Find K neighbors:      ~4%\n",
        "- Average K targets:     ~1%\n",
        "\n",
        "K = 50\n",
        "- Distance computations: ~94%\n",
        "- Find K neighbors:      ~5%\n",
        "- Average K targets:     ~1%\n",
        "\n",
        "=> Total time \u2248 the same; distance work dominates.\n",
        "```\n",
        "\n",
        "**Key Insight**\n",
        "\n",
        "* Prediction time scales mainly with **training set size (N)** and dimensionality (d): (O(N \\times d)).\n",
        "* Changing (K) (within a reasonable range) has **minimal** effect on wall-clock time for brute-force KNN.\n",
        "\n",
        "**If you need speed**\n",
        "\n",
        "* Use approximate NN (e.g., HNSW, LSH), dimensionality reduction (e.g., PCA), or tree indexes (KD/ball trees in low (d)).\n",
        "* Or switch to a model with sublinear prediction (e.g., random forests, gradient boosting).\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "p4d5ux-idgNw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyEKj8S3Qb7n"
      },
      "source": [
        "## Model Complexity and Bias-Variance Tradeoff\n",
        "\n",
        "Let's explore how different K values affect model complexity by testing a wider range: K=1, 5, 10, 15, 30, 50.\n",
        "\n",
        "### Understanding the Tradeoff\n",
        "\n",
        "- **Small K (e.g., K=1)**: High complexity, low bias, high variance \u2192 **Overfitting**\n",
        "- **Moderate K (e.g., K=5-10)**: Balanced complexity \u2192 **Optimal**\n",
        "- **Large K (e.g., K=30-50)**: Low complexity, high bias, low variance \u2192 **Underfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy4br3GKQb7n"
      },
      "outputs": [],
      "source": [
        "# Test extended range of K values with 2x3 subplot comparison\n",
        "k_values_extended = [1, 5, 10, 15, 30, 50]\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "for idx, k in enumerate(k_values_extended):\n",
        "    # Train model\n",
        "    knn_model = MyKNNRegressor(n_neighbors=k)\n",
        "    knn_model.fit(X_train_1d, y_train_1d)\n",
        "\n",
        "    # Predictions\n",
        "    X_plot = np.linspace(X_train_1d.min(), X_train_1d.max(), 300).reshape(-1, 1)\n",
        "    y_plot = knn_model.predict(X_plot)\n",
        "\n",
        "    # Create subplot\n",
        "    plt.subplot(2, 3, idx + 1)\n",
        "    plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, s=50, label='Training data')\n",
        "    plt.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'K={k} prediction')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "    # Label overfitting/optimal/underfitting\n",
        "    if k <= 3:\n",
        "        region_label = 'OVERFITTING'\n",
        "        color = 'red'\n",
        "    elif k <= 15:\n",
        "        region_label = 'OPTIMAL'\n",
        "        color = 'green'\n",
        "    else:\n",
        "        region_label = 'UNDERFITTING'\n",
        "        color = 'orange'\n",
        "\n",
        "    plt.title(f'K = {k}\\n[{region_label}]', color=color, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Model Complexity Analysis:\")\n",
        "print(\"  K=1:  Overfitting - follows training data too closely\")\n",
        "print(\"  K=5:  Optimal - good balance\")\n",
        "print(\"  K=10: Optimal - smooth and generalizes well\")\n",
        "print(\"  K=15: Still good - slightly smoother\")\n",
        "print(\"  K=30: Underfitting - too smooth, misses patterns\")\n",
        "print(\"  K=50: Severe underfitting - barely captures the trend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZHRGKrHsY_"
      },
      "source": [
        "> **Question**: Looking at the plots above, which K value shows signs of overfitting?\n",
        "\n",
        "> A. K=30 (too smooth, doesn't follow the data closely)\n",
        ">\n",
        "> B. K=1 (too wiggly, follows every noise point)\n",
        ">\n",
        "> C. K=10 (balanced smoothness)\n",
        ">\n",
        "> D. All of them equally\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation:**\n",
        "- **A is FALSE**: K=30 shows underfitting (high bias) - too smooth and misses important patterns\n",
        "- **B is TRUE**: K=1 shows overfitting (high variance) - the prediction line is too wiggly and captures noise instead of the underlying pattern\n",
        "- **C is FALSE**: K=10 shows a good balance between bias and variance, capturing the trend without fitting noise\n",
        "- **D is FALSE**: Different K values demonstrate different tradeoffs; they are not equally prone to overfitting\n",
        "\n",
        "**Key Insight**: Small K values lead to overfitting (memorizing noise), while large K values lead to underfitting (oversimplifying patterns).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3MNGMcQb7n"
      },
      "source": [
        "## Regression Line and MSE Formulas\n",
        "\n",
        "### Understanding Prediction Errors\n",
        "\n",
        "In regression, we measure how well our predictions match the actual values. The **error** (also called **residual**) for a single prediction is:\n",
        "\n",
        "$$\\text{Error (Residual)} = y - \\hat{y}$$\n",
        "\n",
        "Where:- $y$ is the actual value- $\\hat{y}$ is the predicted value### Mean Squared Error (MSE)To evaluate overall model performance, we use Mean Squared Error:\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
        "\n",
        "Where:- $N$ is the number of samples- Squaring emphasizes larger errors- Lower MSE indicates better predictions### R\u00b2 Score (Coefficient of Determination)R\u00b2 measures how well the model explains the variance in the data, ranging from 0 to 1:\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^{N} (y^{(i)} - \\bar{y})^2}$$\n",
        "\n",
        "Where:- $\\bar{y}$ is the mean of actual values- **R\u00b2 = 1**: Perfect predictions (model explains all variance)- **R\u00b2 = 0**: Model performs no better than predicting the mean- **R\u00b2 < 0**: Model performs worse than predicting the mean**Interpretation**: R\u00b2 tells us the proportion of variance in $y$ that is explained by the model. For example, R\u00b2 = 0.85 means the model explains 85% of the variance in the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_O7BtMZQb7o"
      },
      "outputs": [],
      "source": [
        "# Visualize over-predicted and under-predicted points\n",
        "np.random.seed(42)\n",
        "X_error = np.linspace(0, 10, 20).reshape(-1, 1)\n",
        "y_true = 2 * X_error.flatten() + np.random.randn(20) * 2\n",
        "\n",
        "# Simple predictions (not perfect)\n",
        "y_pred = 2 * X_error.flatten() + np.random.randn(20) * 1.5\n",
        "\n",
        "# Calculate errors\n",
        "errors = y_true - y_pred\n",
        "mse = np.mean(errors**2)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot 1: Predictions vs Actual\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_error, y_true, c='blue', s=100, alpha=0.6, label='Actual values')\n",
        "plt.scatter(X_error, y_pred, c='red', s=100, marker='s', alpha=0.6, label='Predictions')\n",
        "\n",
        "# Draw error lines\n",
        "for i in range(len(X_error)):\n",
        "    color = 'green' if y_pred[i] > y_true[i] else 'orange'\n",
        "    plt.plot([X_error[i], X_error[i]], [y_true[i], y_pred[i]],\n",
        "             color=color, linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Predictions vs Actual Values')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Error distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "colors = ['green' if e < 0 else 'orange' for e in errors]\n",
        "plt.bar(range(len(errors)), errors, color=colors, alpha=0.6)\n",
        "plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Error (Residual)')\n",
        "plt.title(f'Prediction Errors\\nMSE = {mse:.2f}\\nGreen=Over-predicted, Orange=Under-predicted')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {np.sqrt(mse):.2f}\")\n",
        "print(f\"Mean Absolute Error: {np.mean(np.abs(errors)):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkGHoWtlHsY_"
      },
      "source": [
        "## Working with a Real Dataset\n",
        "\n",
        "Now let's work with a real 2D regression dataset and compare our implementation with scikit-learn's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX9c--OnHsY_"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate a 2D regression dataset with 300 samples and 2 features\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_regression(n_samples=_________, n_features=_________,\n",
        "                       n_informative=2, noise=15.0, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yePX5lxHsY_"
      },
      "source": [
        "Visualize the 2D dataset with target values as colors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMkQN3_HHsZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis',\n",
        "                     edgecolor='k', s=50, alpha=0.7)\n",
        "plt.colorbar(scatter, label='Target Value')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('2D Regression Dataset (color represents target value)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR-wkI2AHsZA"
      },
      "source": [
        "## Splitting into Train, Validation, and Test Sets\n",
        "\n",
        "We split the data into:\n",
        "- **Training set (60%)**: To fit the model\n",
        "- **Validation set (20%)**: To tune hyperparameters (K)\n",
        "- **Test set (20%)**: For final unbiased performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYs7A6NhHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Split data into train (60%), validation (20%), and test (20%) sets\n",
        "# Split into train, validation, and test sets (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=_________)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=_________, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzm6uTyVHsZA"
      },
      "source": [
        "After this split, you should see roughly: Train size 180, Validation size 60, Test size 60."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZqv4VLpHsZA"
      },
      "source": [
        "> **Question**: Why do we use a separate validation set instead of tuning the hyperparameters directly on the test set?\n",
        "\n",
        "> A. The test set is too small\n",
        ">\n",
        "> B. To prevent overfitting to the test set and get an unbiased final performance estimate\n",
        ">\n",
        "> C. The validation set trains faster\n",
        ">\n",
        "> D. It's just a convention with no real benefit\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: B**\n",
        "\n",
        "**Explanation:**\n",
        "- **A is FALSE**: Test set size is independent of the reason for using a validation set\n",
        "- **B is TRUE**: If we tune hyperparameters on the test set, we \"leak\" information about the test set into our model selection process, leading to overly optimistic performance estimates. The validation set allows us to tune hyperparameters while keeping the test set completely unseen until final evaluation.\n",
        "- **C is FALSE**: Training speed is not affected by which dataset we use for hyperparameter tuning\n",
        "- **D is FALSE**: This is a critical practice, not just convention - it ensures unbiased performance estimation**Proper Workflow**: Train set (fit model) \u2192 Validation set (tune K) \u2192 Test set (final evaluation, only once)\n",
        "\n",
        "**Key Insight**: The test set acts as a proxy for truly unseen real-world data. If we tune hyperparameters on the test set, we leak information about it into our model selection, making our performance estimates overly optimistic. The three-way split (train/validation/test) ensures unbiased final evaluation.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHWY7kokQb7o"
      },
      "source": [
        "## Z-Score Standardization\n",
        "\n",
        "Before we explore feature scaling in practice, let's understand the mathematical foundation.\n",
        "\n",
        "### The Z-Score Formula\n",
        "\n",
        "Standardization (Z-score normalization) transforms features to have mean $\\mu = 0$ and standard deviation $\\sigma = 1$:\n",
        "\n",
        "$$Z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the original value\n",
        "- $\\mu$ is the mean of the feature\n",
        "- $\\sigma$ is the standard deviation of the feature\n",
        "\n",
        "### Why Z-Score Standardization Works\n",
        "\n",
        "1. **Centers the data**: Subtracting the mean shifts the distribution to be centered at zero\n",
        "2. **Scales the spread**: Dividing by standard deviation makes the spread consistent across features\n",
        "3. **Preserves distribution shape**: Unlike min-max scaling, z-score maintains the original distribution's shape\n",
        "4. **Handles outliers better**: Less sensitive to extreme values compared to min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qy_k9gIQb7o"
      },
      "outputs": [],
      "source": [
        "# Visualize Z-score standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "# Create two features with different scales\n",
        "feature1 = np.random.normal(loc=100, scale=15, size=200)  # Mean=100, SD=15\n",
        "feature2 = np.random.normal(loc=5, scale=2, size=200)     # Mean=5, SD=2\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "feature1_scaled = scaler.fit_transform(feature1.reshape(-1, 1)).flatten()\n",
        "feature2_scaled = scaler.fit_transform(feature2.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Before standardization - Feature 1\n",
        "axes[0, 0].hist(feature1, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(feature1.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature1.mean():.1f}')\n",
        "axes[0, 0].set_xlabel('Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title(f'Feature 1 (Before)\\n\u03bc={feature1.mean():.1f}, \u03c3={feature1.std():.1f}')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Before standardization - Feature 2\n",
        "axes[0, 1].hist(feature2, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(feature2.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature2.mean():.1f}')\n",
        "axes[0, 1].set_xlabel('Value')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title(f'Feature 2 (Before)\\n\u03bc={feature2.mean():.1f}, \u03c3={feature2.std():.1f}')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 1\n",
        "axes[1, 0].hist(feature1_scaled, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].axvline(feature1_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature1_scaled.mean():.1f}')\n",
        "axes[1, 0].set_xlabel('Value (Z-score)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'Feature 1 (After)\\n\u03bc={feature1_scaled.mean():.2f}, \u03c3={feature1_scaled.std():.2f}')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 2\n",
        "axes[1, 1].hist(feature2_scaled, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].axvline(feature2_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'\u03bc={feature2_scaled.mean():.1f}')\n",
        "axes[1, 1].set_xlabel('Value (Z-score)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title(f'Feature 2 (After)\\n\u03bc={feature2_scaled.mean():.2f}, \u03c3={feature2_scaled.std():.2f}')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Before Standardization:\")\n",
        "print(f\"  Feature 1: \u03bc={feature1.mean():.2f}, \u03c3={feature1.std():.2f}, range=[{feature1.min():.2f}, {feature1.max():.2f}]\")\n",
        "print(f\"  Feature 2: \u03bc={feature2.mean():.2f}, \u03c3={feature2.std():.2f}, range=[{feature2.min():.2f}, {feature2.max():.2f}]\")\n",
        "print(\"\\nAfter Standardization:\")\n",
        "print(f\"  Feature 1: \u03bc={feature1_scaled.mean():.2f}, \u03c3={feature1_scaled.std():.2f}, range=[{feature1_scaled.min():.2f}, {feature1_scaled.max():.2f}]\")\n",
        "print(f\"  Feature 2: \u03bc={feature2_scaled.mean():.2f}, \u03c3={feature2_scaled.std():.2f}, range=[{feature2_scaled.min():.2f}, {feature2_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-CTEjFjHsZA"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Let's demonstrate the importance of feature scaling by comparing scaled vs. unscaled features.\n",
        "\n",
        "### 1. Unscaled Features\n",
        "\n",
        "When features are on different scales (e.g., one feature ranges from 0-100, another from 0-1), KNN will be dominated by the feature with the larger scale, since distance calculations are sensitive to magnitude.\n",
        "\n",
        "### 2. Scaled Features\n",
        "\n",
        "After standardization, all features contribute equally to the distance calculation, typically leading to much better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOqf3NHVHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare model performance with and without feature scaling\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Without scaling\n",
        "knn_raw = MyKNNRegressor(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "y_val_pred_raw = knn_raw.predict(X_val)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(_________)\n",
        "X_val_scaled = scaler.transform(_________)\n",
        "\n",
        "knn_scaled = MyKNNRegressor(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_val_pred_scaled = knn_scaled.predict(X_val_scaled)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Without Scaling - MSE: {mean_squared_error(y_val, y_val_pred_raw):.2f}, \"\n",
        "      f\"R\u00b2: {r2_score(y_val, y_val_pred_raw):.4f}\")\n",
        "print(f\"With Scaling    - MSE: {mean_squared_error(y_val, y_val_pred_scaled):.2f}, \"\n",
        "      f\"R\u00b2: {r2_score(y_val, y_val_pred_scaled):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here\u2019s a polished, well-formatted version:\n",
        "\n",
        "> **Question**: When implementing feature scaling in a KNN regression pipeline with train/validation/test splits, which approach correctly prevents data leakage?\n",
        ">\n",
        "> A. Fit `StandardScaler` on the **training set only**, then use the same fitted scaler to transform validation and test sets\n",
        ">\n",
        "> B. Fit `StandardScaler` **separately on each** dataset (train, validation, test) to ensure each is properly normalized\n",
        ">\n",
        "> C. Fit `StandardScaler` on the **combined training+validation** sets (since both are used during development), then transform the test set\n",
        ">\n",
        "> D. Fit `StandardScaler` on **all data before splitting** to ensure consistent scaling parameters across all sets\n",
        "\n",
        "<details><summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: A**\n",
        "\n",
        "**Explanation**\n",
        "**Data leakage** happens when information from validation/test influences training or tuning.\n",
        "\n",
        "* **A is TRUE** \u2014 Fit on **train only**, apply to val/test:\n",
        "\n",
        "  * `StandardScaler` learns **\u03bc** and **\u03c3** from the training set, then you **reuse** those parameters for validation and test.\n",
        "  * This mirrors production: you won\u2019t know future data\u2019s distribution.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)   # learn \u03bc, \u03c3 on TRAIN\n",
        "X_val_scaled   = scaler.transform(X_val)         # apply train \u03bc, \u03c3\n",
        "X_test_scaled  = scaler.transform(X_test)        # apply train \u03bc, \u03c3\n",
        "```\n",
        "\n",
        "* **B is FALSE** \u2014 Separate scalers per split create **incompatible feature spaces**:\n",
        "\n",
        "  * Train points and val/test points end up on **different scales**, making KNN distances meaningless.\n",
        "  * Also gives an **unrealistic evaluation** (you centered the test set using test \u03bc, \u03c3, which you wouldn\u2019t have in production).\n",
        "\n",
        "* **C is FALSE** \u2014 Fitting on **train+val** leaks validation statistics into preprocessing used during tuning.\n",
        "\n",
        "  * Correct practice for tuning: scaler must be fit on **train only**.\n",
        "  * *(Nuance: after final hyperparameters are chosen, you may refit the **entire pipeline** on train+val and then evaluate once on test.)*\n",
        "\n",
        "* **D is FALSE** \u2014 Fitting before the split leaks **test** information and invalidates the final evaluation.\n",
        "\n",
        "**Best practice with scikit-learn**\n",
        "Wrap scaling **inside** the model pipeline so each split/fold is handled correctly:\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsRegressor())\n",
        "])\n",
        "# During CV, the scaler is fit on the training fold only.\n",
        "```\n",
        "\n",
        "**Key Insight:** Any preprocessing that learns parameters (scaling, imputation, encoding) must be **fit on training data only** and then **applied** to validation/test.\n",
        "\n",
        "</details>\n",
        ""
      ],
      "metadata": {
        "id": "98tbBOYteQ_N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRCEI0OjQb7p"
      },
      "source": [
        "## Where Do Errors Come From?\n",
        "\n",
        "Not all regions of the feature space are equally predictable. Let's visualize areas of **high certainty** vs **high ambiguity**.\n",
        "\n",
        "### Understanding Prediction Certainty\n",
        "\n",
        "- **High Certainty Regions**: Areas with many nearby training points, where neighbors agree\n",
        "- **High Ambiguity Regions**: Sparse areas or regions where neighbors have varying target values\n",
        "\n",
        "KNN errors are typically higher in:\n",
        "1. Sparse regions (few training samples nearby)\n",
        "2. Boundary regions (where the underlying function changes rapidly)\n",
        "3. Noisy regions (where similar inputs have very different outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yBOfTMHQb7p"
      },
      "outputs": [],
      "source": [
        "# Visualize high certainty vs high ambiguity regions\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create dataset with varying density\n",
        "X_dense = np.random.uniform(0, 5, (100, 2))  # Dense region\n",
        "X_sparse = np.random.uniform(7, 10, (20, 2))  # Sparse region\n",
        "X_combined = np.vstack([X_dense, X_sparse])\n",
        "\n",
        "# Create target with noise\n",
        "y_combined = (2 * X_combined[:, 0] + X_combined[:, 1] +\n",
        "              np.random.randn(len(X_combined)) * 0.5)\n",
        "\n",
        "# Train a KNN model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_combined, y_combined)\n",
        "\n",
        "# Create prediction grid\n",
        "x1_range = np.linspace(-1, 11, 100)\n",
        "x2_range = np.linspace(-1, 11, 100)\n",
        "xx1, xx2 = np.meshgrid(x1_range, x2_range)\n",
        "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "\n",
        "# Get predictions\n",
        "predictions = knn.predict(grid_points)\n",
        "\n",
        "# Calculate local density (number of neighbors within radius)\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "nn_model = NearestNeighbors(radius=1.5)\n",
        "nn_model.fit(X_combined)\n",
        "densities = [len(nn_model.radius_neighbors([point], return_distance=False)[0])\n",
        "             for point in grid_points]\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Predictions with training data\n",
        "scatter1 = axes[0].scatter(X_combined[:, 0], X_combined[:, 1],\n",
        "                           c=y_combined, s=100, cmap='viridis',\n",
        "                           edgecolors='black', linewidth=1.5, zorder=3)\n",
        "contour1 = axes[0].contourf(xx1, xx2, predictions.reshape(xx1.shape),\n",
        "                            levels=20, cmap='viridis', alpha=0.3)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('KNN Predictions (K=5)')\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Target Value')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add region labels\n",
        "axes[0].text(2.5, 8, 'High Certainty\\n(Dense Region)',\n",
        "            bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
        "            fontsize=10, ha='center')\n",
        "axes[0].text(8.5, 8, 'High Ambiguity\\n(Sparse Region)',\n",
        "            bbox=dict(boxstyle='round', facecolor='red', alpha=0.3),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "# Plot 2: Prediction certainty (based on density)\n",
        "contour2 = axes[1].contourf(xx1, xx2, np.array(densities).reshape(xx1.shape),\n",
        "                            levels=20, cmap='RdYlGn', alpha=0.8)\n",
        "axes[1].scatter(X_combined[:, 0], X_combined[:, 1],\n",
        "               c='black', s=50, marker='x', zorder=3, label='Training points')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title('Prediction Certainty\\n(Green=High, Red=Low)')\n",
        "plt.colorbar(contour2, ax=axes[1], label='Number of neighbors')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Prediction Certainty Analysis:\")\n",
        "print(f\"  Dense region (0-5): {len(X_dense)} training points\")\n",
        "print(f\"  Sparse region (7-10): {len(X_sparse)} training points\")\n",
        "print(f\"  Maximum neighbors in radius: {max(densities)}\")\n",
        "print(f\"  Minimum neighbors in radius: {min(densities)}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Green regions: High certainty (many nearby training points)\")\n",
        "print(\"  - Red regions: High ambiguity (few nearby training points)\")\n",
        "print(\"  - Errors are typically higher in red (sparse/ambiguous) regions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP--IGDiHsZA"
      },
      "source": [
        "## Tuning the Hyperparameter K\n",
        "\n",
        "Let's find the optimal K by evaluating different values on the validation set.\n",
        "\n",
        "**Choosing K values to test:**\n",
        "- **Rule of thumb**: Start with $K \\approx \\sqrt{n}$ where $n$ is the number of training samples\n",
        "- For our 180 training samples: $\\sqrt{180} \\approx 13$\n",
        "- We'll test a range around this value: K from 1 to 50\n",
        "- Use validation set performance to select the best K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v0wDkNSHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Find optimal K by evaluating on validation set\n",
        "# TODO: Track both training and validation RMSE\n",
        "\n",
        "# Lists to store metrics\n",
        "train_rmse = []\n",
        "val_rmse = []\n",
        "k_range = range(1, 51)\n",
        "\n",
        "# Loop through different K values\n",
        "for k in k_range:\n",
        "    # Train model with current K\n",
        "    knn = MyKNNRegressor(n_neighbors=_________)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = knn.predict(X_train_scaled)\n",
        "    y_val_pred = knn.predict(_________)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "    val_rmse.append(np.sqrt(mean_squared_error(y_val, _________)))\n",
        "\n",
        "# Find best K\n",
        "best_k = list(k_range)[np.argmin(val_rmse)]\n",
        "print(f\"\\nBest K: {best_k}\")\n",
        "print(f\"Best Validation RMSE: {min(val_rmse):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPHayoNaHsZA"
      },
      "source": [
        "Now, let's plot the RMSE vs. K to visualize the bias-variance tradeoff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nZ3yJGUHsZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(k_range), train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=6)\n",
        "plt.plot(list(k_range), val_rmse, 's-', label='Validation RMSE', linewidth=2, markersize=6)\n",
        "plt.axvline(best_k, linestyle='--', color='red', linewidth=2, label=f'Best K={best_k}')\n",
        "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
        "plt.ylabel('RMSE', fontsize=12)\n",
        "plt.title('Bias-Variance Tradeoff: RMSE vs K', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Small K: Low training RMSE (fits training data closely) but higher validation RMSE (overfitting)\")\n",
        "print(\"- Large K: Training and validation RMSE converge (underfitting - too much smoothing)\")\n",
        "print(f\"- Optimal K={best_k}: Best balance between bias and variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U9eWmHXQb7u"
      },
      "source": [
        "### 3D Regression Surface Visualization\n",
        "\n",
        "Let's visualize how the regression surface changes with different K values in 3D space.\n",
        "This helps us understand:\n",
        "- **K=5**: Optimal balance, smooth surface that captures the trend\n",
        "- **K=10**: Slightly smoother, may miss some local variations\n",
        "- **K=30**: Over-smoothed, underfitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocQc5N06Qb7u"
      },
      "outputs": [],
      "source": [
        "# 3D Regression Surface for different K values\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Use only first 2 features for visualization\n",
        "X_2d = X_train_scaled[:, :2]\n",
        "y_2d = y_train\n",
        "\n",
        "# Create grid for surface\n",
        "x1_range = np.linspace(X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5, 30)\n",
        "x2_range = np.linspace(X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5, 30)\n",
        "xx1, xx2 = np.meshgrid(x1_range, x2_range)\n",
        "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "k_values_3d = [5, 10, 30]\n",
        "titles = ['K=5: Optimal', 'K=10: Balanced', 'K=30: Underfitting']\n",
        "\n",
        "for idx, (k, title) in enumerate(zip(k_values_3d, titles)):\n",
        "    # Train model\n",
        "    knn_3d = MyKNNRegressor(n_neighbors=k)\n",
        "    knn_3d.fit(X_2d, y_2d)\n",
        "\n",
        "    # Predict on grid\n",
        "    predictions = knn_3d.predict(grid_points)\n",
        "    zz = predictions.reshape(xx1.shape)\n",
        "\n",
        "    # Create 3D subplot\n",
        "    ax = fig.add_subplot(1, 3, idx + 1, projection='3d')\n",
        "\n",
        "    # Plot surface\n",
        "    surf = ax.plot_surface(xx1, xx2, zz, cmap='viridis', alpha=0.6, edgecolor='none')\n",
        "\n",
        "    # Plot training points\n",
        "    ax.scatter(X_2d[:, 0], X_2d[:, 1], y_2d, c='red', s=20, alpha=0.6, label='Training data')\n",
        "\n",
        "    ax.set_xlabel('Feature 1 (scaled)')\n",
        "    ax.set_ylabel('Feature 2 (scaled)')\n",
        "    ax.set_zlabel('Target Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(elev=20, azim=45)\n",
        "\n",
        "    # Add colorbar\n",
        "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"3D Surface Interpretation:\")\n",
        "print(\"  K=5:  Captures data trends well, some local variations\")\n",
        "print(\"  K=10: Smoother surface, good generalization\")\n",
        "print(\"  K=30: Very smooth, may be too simple (underfitting)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBEjbpWHsZA"
      },
      "source": [
        "## Comparing with Scikit-Learn's Implementation\n",
        "\n",
        "Let's verify our implementation matches scikit-learn's KNeighborsRegressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAmMXv0PHsZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare your implementation with scikit-learn's KNeighborsRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Our implementation\n",
        "our_knn = MyKNNRegressor(n_neighbors=best_k)\n",
        "our_knn.fit(_________, y_train)\n",
        "y_val_pred_ours = our_knn.predict(X_val_scaled)\n",
        "\n",
        "# Scikit-learn's implementation\n",
        "sklearn_knn = KNeighborsRegressor(n_neighbors=_________)\n",
        "sklearn_knn.fit(X_train_scaled, y_train)\n",
        "y_val_pred_sklearn = sklearn_knn.predict(_________)\n",
        "\n",
        "# Compare\n",
        "print(f\"Our Implementation    - RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred_ours)):.2f}\")\n",
        "print(f\"Scikit-learn          - RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn)):.2f}\")\n",
        "print(f\"Difference: {abs(np.sqrt(mean_squared_error(y_val, y_val_pred_ours)) - np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn))):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_-ncFMQHsZB"
      },
      "source": [
        "## Final Evaluation on Test Set\n",
        "\n",
        "Now that we've chosen the best K using the validation set, let's evaluate on the held-out test set to get an unbiased performance estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgkgrOeHHsZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate final model on test set\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Scale test set\n",
        "X_test_scaled = scaler.transform(_________)\n",
        "\n",
        "# Train final model on combined train+val data\n",
        "X_train_val = np.vstack([X_train_scaled, X_val_scaled])\n",
        "y_train_val = np.concatenate([y_train, y_val])\n",
        "\n",
        "final_knn = MyKNNRegressor(n_neighbors=_________)\n",
        "final_knn.fit(X_train_val, y_train_val)\n",
        "y_test_pred = final_knn.predict(_________)\n",
        "\n",
        "# Evaluation metrics\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Test Set Performance:\")\n",
        "print(f\"  RMSE: {test_rmse:.2f}\")\n",
        "print(f\"  MAE:  {test_mae:.2f}\")\n",
        "print(f\"  R\u00b2:   {test_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUrVeb6DHsZB"
      },
      "source": [
        "Visualize predictions vs. actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnFAyEEVHsZB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k', s=50)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "         'r--', lw=2, label='Perfect prediction')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'Test Set: Actual vs Predicted\\n(R\u00b2 = {test_r2:.3f}, RMSE = {test_rmse:.2f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - y_test_pred\n",
        "plt.scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k', s=50)\n",
        "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals (Actual - Predicted)')\n",
        "plt.title('Residual Plot')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUiTd8QZQb7u"
      },
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "In this lab, we:\n",
        "\n",
        "- **Implemented KNN Regression from scratch**: Built a complete `MyKNNRegressor` class that predicts continuous values by averaging the K nearest neighbors\n",
        "- **Explored the bias-variance tradeoff**: Observed how small K values lead to overfitting (high variance, low bias) while large K values lead to underfitting (low variance, high bias)\n",
        "- **Visualized K progression**: Saw how predictions change as we increase K from 1 to larger values, demonstrating the smoothing effect\n",
        "- **Understood distance metrics**: Learned why Euclidean distance is commonly used and how it's sensitive to feature scales\n",
        "- **Mastered feature scaling**: Discovered that standardization (z-score normalization) is critical for KNN, transforming features to have mean=0 and standard deviation=1\n",
        "- **Applied proper data splitting**: Used train/validation/test splits (60/20/20) to tune hyperparameters without overfitting\n",
        "- **Tuned the hyperparameter K**: Found the optimal K by evaluating validation set performance and observing the U-shaped validation error curve\n",
        "- **Analyzed prediction certainty**: Identified that errors are higher in sparse regions or areas with high ambiguity\n",
        "- **Compared implementations**: Verified our custom KNN regressor matches scikit-learn's implementation\n",
        "- **Evaluated with multiple metrics**: Used MSE, RMSE, MAE, and R\u00b2 to comprehensively assess model performance\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "1. **KNN is a lazy learner**: No training phase - all computation happens during prediction\n",
        "2. **Feature scaling is essential**: Distance-based algorithms require features on similar scales\n",
        "3. **K is crucial**: The choice of K controls the bias-variance tradeoff\n",
        "4. **Computational cost**: KNN can be slow for large datasets since it requires distance calculations to all training points\n",
        "5. **Works best with**: Low-dimensional data (curse of dimensionality affects high-dimensional spaces)\n",
        "\n",
        "**When to use KNN Regression:**\n",
        "- Small to medium-sized datasets\n",
        "- Non-parametric problems (no assumption about data distribution)\n",
        "- When you need an interpretable model\n",
        "- When the decision boundary is irregular\n",
        "\n",
        "**When NOT to use KNN Regression:**\n",
        "- Very large datasets (computational cost)\n",
        "- High-dimensional data (curse of dimensionality)\n",
        "- When training time is critical (preprocessing step)\n",
        "- When you need a simple model equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sznUkaQb7u"
      },
      "source": [
        "### Final Conceptual Question\n",
        "\n",
        "> **Question**: Which of the following statements about KNN regression is TRUE?\n",
        ">\n",
        "> A. KNN performs better with high-dimensional data due to increased information\n",
        ">\n",
        "> B. Increasing K always improves model performance on the validation set\n",
        ">\n",
        "> C. KNN requires feature scaling because distance calculations are sensitive to feature magnitudes\n",
        ">\n",
        "> D. KNN has a training phase where it learns parameters from the data\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: C**\n",
        "\n",
        "**Explanation:**\n",
        "- **A is FALSE**: KNN suffers from the \"curse of dimensionality\" - performance degrades in high-dimensional spaces as distances become less meaningful\n",
        "- **B is FALSE**: There's an optimal K value; increasing K beyond this leads to underfitting and worse performance\n",
        "- **C is TRUE**: Distance-based algorithms like KNN are highly sensitive to feature scales, making standardization essential\n",
        "- **D is FALSE**: KNN is a lazy learner with no training phase; all computation happens during prediction\n",
        "\n",
        "**Key Insight**: KNN regression is a non-parametric, instance-based learning algorithm that makes no assumptions about the underlying data distribution. Its lazy learning nature means all computation happens during prediction, making it simple to understand but potentially slow on large datasets. Feature scaling is critical because KNN relies entirely on distance calculations.\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}