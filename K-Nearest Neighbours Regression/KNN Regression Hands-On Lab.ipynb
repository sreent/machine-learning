{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LiyLga1Qb7f"
      },
      "source": [
        "# K-Nearest Neighbors (KNN) Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6unq0EGGQb7h"
      },
      "source": [
        "In this lab, you will implement a K-Nearest Neighbors regressor from scratch, tune its hyperparameters, and apply it to synthetic and real datasets. Along the way, you'll explore the bias-variance tradeoff, understand the importance of feature scaling, and visualize how different K values affect model predictions. By the end, you will have a clear grasp of how KNN regression works, how to evaluate it using metrics like MSE and RÂ², and why practices like proper data splitting and standardization are critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goLE2YSIHsY9"
      },
      "source": [
        "## Overview of KNN Regression\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAArwAAAFWCAYAAACLjU46AAAMT2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnltSSQgQiICU0JsgIiWAlBBaAOlFEJWQBAglxoSgYkcXFVy7iGBFV0EU2wrIYkNddWVR7K5lsaCysi4W7MqbEECXfeV7831z57//nPnnnHNn7r0DAKNDIJPloloA5Enz5bEhAewJySlsUhcgABpgATbQEwgVMm50dASAZbD9e3lzHSCq9oqjSuuf/f+1aIvECiEASDTE6SKFMA/iHwHAm4UyeT4ARBnkLabny1R4LcS6cuggxNUqnKnGzSqcrsaX+m3iY3kQPwKATBMI5JkAaPZAnl0gzIQ6DBgtcJaKJFKI/SH2zcubKoJ4PsS20AbOyVDpc9K/0cn8m2b6kKZAkDmE1bH0F3KgRCHLFcz8P9Pxv0ternJwDhtYaVny0FhVzDBvj3KmhqswDeJ30vTIKIh1AEBxiajfXoVZWcrQBLU9aitU8GDO4HMG6DhFbhx/gI8VCQLDITaCOEOaGxkxYFOUIQlW2cD8oeWSfH48xPoQV4sVQXEDNifkU2MH572eIedxB/inAnm/Dyr9L8qcBK5aH9PJEvMH9DGnwqz4JIipEAcWSBIjIdaEOFKRExc+YJNamMWLHLSRK2NVsVhCLBdLQwLU+lhZhjw4dsB+d55iMHbsRJaEHzmAL+dnxYeqc4U9Egr6/YexYD1iKTdhUEesmBAxGItIHBikjh0ni6UJcWoe15flB8Sqx+L2stzoAXs8QJwbouLNIY5XFMQNji3Ih4tTrY8Xy/Kj49V+4hXZgrBotT/4fhABeCAQ7j4lrOlgKsgGkrbuhm54p+4JBgIgB5lADBwHmMERSf09UniNA4XgT4jEQDE0LqC/VwwKIP95GKviJEOc+uoIMgb6VCo54DHEeSAc5MJ7Zb+SdMiDRPAIMpJ/eCSAVQhjyIVV1f/v+UH2K8OFTMQAoxyckc0YtCQGEQOJocRgoh1uiPvi3ngEvPrD6oJzcM/BOL7aEx4T2gkPCNcIHYRbUyRF8mFejgcdUD94ID/p3+YHt4aabngA7gPVoTLOwg2BI+4K5+HifnBmN8jyBvxWZYU9TPtvEXzzhAbsKM4UlDKC4k+xHT5S017TbUhFletv86P2NX0o37yhnuHz877Jvgi24cMtsSXYIewsdhI7jzVjDYCNHccasVbsqAoPrbhH/StucLbYfn9yoM7wNfP1yaoyqXCude5y/qTuyxfPyFdtRt5U2Uy5JDMrn82FXwwxmy8VOo1iuzi7uAGg+v6oX2+vYvq/Kwir9Su38HcAfI739fX99JULOw7AAQ/4SjjylbPlwE+LBgDnjgiV8gI1h6suBPjmYMDdZwBMgAWwhfG4AHfgDfxBEAgDUSAeJIPJ0PssuM7lYDqYDRaAYlAKVoJ1oAJsAdtBNdgLDoIG0AxOgp/BBXAJXAO34erpBM9AD3gDPiIIQkLoCBMxQEwRK8QBcUE4iC8ShEQgsUgykoZkIlJEicxGFiKlyGqkAtmG1CAHkCPISeQ80o7cQu4jXchL5AOKoTRUFzVGrdHRKAflouFoPDoJzUSnoYXoInQ5Wo5WoXvQevQkegG9hnagz9BeDGAaGAszwxwxDsbDorAULAOTY3OxEqwMq8LqsCb4nK9gHVg39h4n4kycjTvCFRyKJ+BCfBo+F1+GV+DVeD1+Gr+C38d78C8EOsGI4EDwIvAJEwiZhOmEYkIZYSfhMOEM3EudhDdEIpFFtCF6wL2YTMwmziIuI24i7iOeILYTHxJ7SSSSAcmB5EOKIglI+aRi0gbSHtJx0mVSJ+kdWYNsSnYhB5NTyFJyEbmMvJt8jHyZ/IT8kaJFsaJ4UaIoIspMygrKDkoT5SKlk/KRqk21ofpQ46nZ1AXUcmod9Qz1DvWVhoaGuYanRoyGRGO+RrnGfo1zGvc13tN0aPY0Hi2VpqQtp+2inaDdor2i0+nWdH96Cj2fvpxeQz9Fv0d/p8nUdNLka4o052lWatZrXtZ8zqAwrBhcxmRGIaOMcYhxkdGtRdGy1uJpCbTmalVqHdG6odWrzdQeox2lnae9THu39nntpzokHWudIB2RziKd7TqndB4yMaYFk8cUMhcydzDPMDt1ibo2unzdbN1S3b26bbo9ejp6rnqJejP0KvWO6nWwMJY1i8/KZa1gHWRdZ30YYTyCO0I8YumIuhGXR7zVH6nvry/WL9Hfp39N/4MB2yDIIMdglUGDwV1D3NDeMMZwuuFmwzOG3SN1R3qPFI4sGXlw5G9GqJG9UazRLKPtRq1GvcYmxiHGMuMNxqeMu01YJv4m2SZrTY6ZdJkyTX1NJaZrTY+b/sHWY3PZuexy9ml2j5mRWaiZ0mybWZvZR3Mb8wTzIvN95nctqBYciwyLtRYtFj2WppbjLWdb1lr+ZkWx4lhlWa23Omv11trGOsl6sXWD9VMbfRu+TaFNrc0dW7qtn+002yrbq3ZEO45djt0mu0v2qL2bfZZ9pf1FB9TB3UHisMmhfRRhlOco6aiqUTccaY5cxwLHWsf7TiynCKcipwan56MtR6eMXjX67Ogvzm7Ouc47nG+P0RkTNqZoTNOYly72LkKXSperY+ljg8fOG9s49oWrg6vYdbPrTTem23i3xW4tbp/dPdzl7nXuXR6WHmkeGz1ucHQ50ZxlnHOeBM8Az3mezZ7vvdy98r0Oev3l7eid473b++k4m3HicTvGPfQx9xH4bPPp8GX7pvlu9e3wM/MT+FX5PfC38Bf57/R/wrXjZnP3cJ8HOAfIAw4HvOV58ebwTgRigSGBJYFtQTpBCUEVQfeCzYMzg2uDe0LcQmaFnAglhIaHrgq9wTfmC/k1/J4wj7A5YafDaeFx4RXhDyLsI+QRTePR8WHj14y/E2kVKY1siAJR/Kg1UXejbaKnRf8UQ4yJjqmMeRw7JnZ27Nk4ZtyUuN1xb+ID4lfE306wTVAmtCQyElMTaxLfJgUmrU7qmDB6wpwJF5INkyXJjSmklMSUnSm9E4MmrpvYmeqWWpx6fZLNpBmTzk82nJw7+egUxhTBlENphLSktN1pnwRRgipBbzo/fWN6j5AnXC98JvIXrRV1iX3Eq8VPMnwyVmc8zfTJXJPZleWXVZbVLeFJKiQvskOzt2S/zYnK2ZXTl5uUuy+PnJeWd0SqI82Rnp5qMnXG1HaZg6xY1jHNa9q6aT3ycPlOBaKYpGjM14U/+q1KW+V3yvsFvgWVBe+mJ04/NEN7hnRG60z7mUtnPikMLvxhFj5LOKtlttnsBbPvz+HO2TYXmZs+t2WexbxF8zrnh8yvXkBdkLPg1yLnotVFrxcmLWxaZLxo/qKH34V8V1usWSwvvrHYe/GWJfgSyZK2pWOXblj6pURU8kupc2lZ6adlwmW/fD/m+/Lv+5ZnLG9b4b5i80riSunK66v8VlWv1l5duPrhmvFr6tey15asfb1uyrrzZa5lW9ZT1yvXd5RHlDdusNywcsOniqyKa5UBlfs2Gm1cuvHtJtGmy5v9N9dtMd5SuuXDVsnWm9tCttVXWVeVbSduL9j+eEfijrM/cH6o2Wm4s3Tn513SXR3VsdWnazxqanYb7V5Ri9Yqa7v2pO65tDdwb2OdY922fax9pfvBfuX+Pw6kHbh+MPxgyyHOobofrX7ceJh5uKQeqZ9Z39OQ1dDRmNzYfiTsSEuTd9Phn5x+2tVs1lx5VO/oimPUY4uO9R0vPN57Qnai+2TmyYctU1pun5pw6urpmNNtZ8LPnPs5+OdTZ7lnj5/zOdd83uv8kV84vzRccL9Q3+rWevhXt18Pt7m31V/0uNh4yfNSU/u49mOX/S6fvBJ45eer/KsXrkVea7+ecP3mjdQbHTdFN5/eyr314reC3z7enn+HcKfkrtbdsntG96p+t/t9X4d7x9H7gfdbH8Q9uP1Q+PDZI8WjT52LHtMflz0xfVLz1OVpc1dw16U/Jv7R+Uz27GN38Z/af258bvv8x7/8/2rtmdDT+UL+ou/lslcGr3a9dn3d0hvde+9N3puPb0veGbyrfs95f/ZD0ocnH6d/In0q/2z3uelL+Jc7fXl9fTKBXND/K4AB1dEmA4CXuwCgJwPAhOdG6kT1+bC/IOozbT8C/wmrz5D9xR2AOvhPH9MN/25uALB/BwDWUJ+RCkA0HYB4T4COHTtUB89y/edOVSHCs8HWyM/peeng3xT1mfQbv4e3QKXqCoa3/wJTpYMOK+MrgAAAAIplWElmTU0AKgAAAAgABAEaAAUAAAABAAAAPgEbAAUAAAABAAAARgEoAAMAAAABAAIAAIdpAAQAAAABAAAATgAAAAAAAACQAAAAAQAAAJAAAAABAAOShgAHAAAAEgAAAHigAgAEAAAAAQAAArygAwAEAAAAAQAAAVYAAAAAQVNDSUkAAABTY3JlZW5zaG90xDnpGQAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAAdZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDYuMC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iPgogICAgICAgICA8ZXhpZjpQaXhlbFlEaW1lbnNpb24+MzQyPC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjcwMDwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlVzZXJDb21tZW50PlNjcmVlbnNob3Q8L2V4aWY6VXNlckNvbW1lbnQ+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgq/YzGzAAAAHGlET1QAAAACAAAAAAAAAKsAAAAoAAAAqwAAAKsAAFntzmyQcwAAQABJREFUeAHsvQd4Xdd1JbyJ3nshQIJgJ8DeexOLSJGiCtWLLcuWHFuJEnsy8znjL/8kdpzJ/8/EzkwmMx5HsRXJ6lSjJJIiKZEUe+8VYAEIggTReyf473XISz2CAN4F8AC8so4EvvfuO/fcc9e579519ll77343tQgLESACRIAIEAEiQASIABHwUgT6kfB66cjytIgAESACRIAIEAEiQAQMAiS8vBCIABEgAkSACBABIkAEvBoBEl6vHl6eHBEgAkSACBABIkAEiAAJL68BIkAEiAARIAJEgAgQAa9GgITXq4eXJ0cEiAARIAJEgAgQASJAwstrgAgQASJABIgAESACRMCrESDh9erh5ckRASJABIgAESACRIAIkPDyGiACRIAIEAEiQASIABHwagRIeL16eHlyRIAIEAEiQASIABEgAiS8vAaIABEgAkSACBABIkAEvBoBEl6vHl6eHBEgAkSACBABIkAEiAAJL68BIkAEiAARIAJEgAgQAa9GgITXq4eXJ0cEiAARIAJEgAgQASJAwstrgAgQASJABIgAESACRMCrESDh9erh5clZCNzUNzdv3pSGpmYprqyRospqqaitl9qGRmm+0SJ+/fpJcFCARIYGS3xEuCTHREhESLDZ3k+/YyECRIAIEAEiQAQ8FwESXs8dO/bcJgJNN25IeU2dZF8tkiz9yysuU8JbI+W1dUp4m6S5+Yb4+YHwBkqUktz4yHBJiYuSYf0TJHNgsqTEREpQYIAhvzYPyWpEgAgQASJABIiAGyFAwutGg8GuuBYBWHQblcwezcmXzUfPSfa1YkN8QYCdFVh8I0KCJCU2SmaNGiyLxo2UuIgw6afEmPZeZ+jxeyJABIgAESAC7oUACa97jQd74yIEWlpuSk5RqWw9kS17s3KlqKJaGm0Q3daHB/GFzCFDLb3LJ2bI2PQUCQsOIultDRQ/EwEiQASIABFwYwRIeN14cNi1riEAC25uUZm8880hOXgxT24o+YW1t6sFFl1YdlOio+TxOROMxTcqNKSrzXE/IkAEiAARIAJEoJcRIOHtZcB5uJ5F4IY6oB2/fE3e3XHYaHYbm5ul61T37r7C2psYHSEPTh0tKyaPllDV/LIQASJABIgAESAC7o8ACa/7jxF7aBOBFrXiZuUXyvu7j8rhC1fEjlbXZtN3qoH0DoiLltUzx8uCMcOMoxs1vXfg4RsiQASIABEgAm6JAAmvWw4LO9VZBKBYKK+plTe3HZBtpy6Y8GOdbcNufX+VNyRHR8qrK+fLuEEpJsKD3X1ZjwgQASJABIgAEeh9BEh4ex9zHrEHEKjX+Lo7T1+Uf9+2X8qqam3LGGCdDQjwVye0fhqP94bASmynBPr7yexRQ+T7S2ZIgoYxY6xeO6ixDhEgAkSACBCBvkGAhLdvcOdRXYgASOqV4nL553XbTZzd5pYWp62DoMZHhcvw5HhJ0ji7/n7+UlheKRevl0hhRZVxdOuoEd1d4sLD5LkFU2XxuBESqKSZhQgQASJABIgAEXBPBEh43XNc2KtOIIDsaV8dz5J/3bTbZE1zZqMF2U1PipUHJmfKjOHpEqDWWkRygIPb/vN58p46vFXV1TvtAdqZkzFEXlIrb5JKHFiIABEgAkSACBAB90SAhNc9x4W96gQCyKL2P7/YrmT1sq3wY0gZ/PS8SUaSgKQUh5Xkwioco4klkGZ499lLJuWwnS4kq3X41RXzZOKQAczEZgcw1iECRIAIEAEi0AcIkPD2Aeg8pOsQQHzdM1euy68/2yYFZZW2tLvj01Pl5aWzTGKKd3YcurWfmoUD/P0NaUV0B7txewN1n+8snKphyjJNQgrXnRlbIgJEgAgQASJABFyFAAmvq5BkO32CwA21zH55+Ky8tf2gVNQ6lyEgrNiKKaPl6bmT5LXNe2XX2YvGYa3lZosgO5szOUTrk/TTiA3zM4fJC/dNE1h7WYgAESACRIAIEAH3Q4CE1/3GhD3qBAKwxr6+Zb+S3jOCSA3OSogmi3hs1niZNXKwbDx6VrW7LRIVFir1jU0mO9s5jeNb29DorJk730PHOyIlQf5cQ5QNVQc4FiJABIgAESACRMD9ECDhdb8xYY86gUBj8w35zefbZNeZS4a8drQrQpBFhYfKs/OnKOFNl+r6BkGK4Ga17AYocS2prpUNR87INyfPS50SYDsFbcZHRcjPVy+WjIHJdnZhHSJABIgAESACRKCXESDh7WXAeTjXIoDICn//4Vdy8EKeU93tLXIaLt+9b7osHD1MytTZ7fMDJ+W0aoAHxEfLI9PHGQ3vP362VS4WFGt79voaHhwkf/PUMhmrSShYiAARIAJEgAgQAfdDgITX/caEPeoEAp0lvLGRYfL8gmkyV8OJfbjnmHypFt1K1f6GqtThmXmTZb4SYeiBtxzPtp2EAoT3vzy5TMalk/B2YuhYlQgQASJABIhAryFAwttrUPNAPYEAJA3/uHar7DkHSUPHJllYeCNVrwtiO0nDiP3vDTvlZO5V46gGZ7bFE0bKE7MmysZjZ+WTPcdtEV60GRsRLj9/fImMpqShJ4aYbRIBIkAEiAAR6DYCJLzdhpAN9CUCcFr7t6/2yqaj5wQJKJyVoIAAeXjGWFk1dYz8buMe2ZuVY7S/AX5+snrmeFk2KUPWKNndpJZfO2mG4bQGZ7WfPLhAhvWn05oz/Pk9ESACRIAIEIG+QICEty9Q5zFdhgASRnxx4JS8t/OIVNrJjqZHnp05RH6weKYcy7kqa3YfNZKG1LholTpMkZTYKPmfmqL41OUCp5pgnAQsw7MzBsuLi2aYfV12YmyICBABIkAEiAARcBkCJLwug5IN9QUCsMKeUFnCP33+jRRVVNuKo5sYHSHfV4IKze2lwjK5WlYhI9Q6C9L79Yls+QDkWXW9HQskbp0t0hI/M3eyrJo2RpDBjYUIEAEiQASIABFwPwRIeN1vTNijTiJQXFUjv1m7TY5Bj2sjtIK/yheGaezc+8YMlyEqR/DX5BHQ/yLN8FYlvCDOduQM6Ga8piP+M00tPG3EIKYW7uS4sToRIAJEgAgQgd5CgIS3t5Dmce4gAFLa2NgoN1R/i9LU1GT+sL1FJQoNDQ13EVfUs7ZBM4u/0NBQsy/e3/Tzl69OXpBPDp2zTVQhRQDxDQsJ1AgNwRp3t1FqNC4viK8d0oyDw2ENRPflJbNMWDPTIf5DBIgAESACRIAIuB0CJLxuNySe3SGLLIK4ouBzXV2dIazl5eVSXV1tPpeVlUmzxtBFAZm1CC32q6mpMdutf1CvtrbW+mgIb0REhHnFxhAlvxXa1K7CBqm76WdLinCnMX1jSLMNy/Bd++iHSE1agRTFK6ZkCpzhWIgAESACRIAIEAH3RICE1z3HxWN6BUILCyystCCl+MO2/Px8Y8Wtr6+XyspKQ24t4gvrLohtZGSk+KmVNSQk5I7F1t/f32wHCbVKgJJJEFyrYN+qqipzHBzLkGiN0JBTe1MOXK2QWs2SZkd/a7XXlVdEdZg8LE1eWjJDBqj217G/XWmP+xABIkAEiAARIAI9hwAJb89h63Utg1yi4BVWVxBd/IHcnjmjCRyU2OIPBaTWKiCr4eHh0r9/f0lISJCoqCjzGUQWRBEkF8QX7/GH7Y4F3zlus45v9Qd9AQkuqa6Tt3Yelf3ZlwXhynqqQA4Rq9rdHy2bLTNGpAsc11iIABEgAkSACBAB90WAhNd9x8ategZSCasqpAew4l66dMkQXcgUQDyhqQVxBZkNCwuTxMREQ25BdEF4QVotYmuRW5ygKy2jLbcdz97dcVjO5F9XEux6Oy/szvFR4bJi8mh5SCMzhGqWtW9t0W41ZOwMESACRIAIEAEicBsBEl5eCm0iAIspiC0I7vXr16W0tFQuX75sLLiQEIDIQpIAgpuUlCTp6elGmgCyaxFa6xUHcCWxbbPDtzc2aea1vdm5Gpf3sOQVlQvi9LqyBEqLTElLlGfmT5UhaQMMie+tc3PlebAtIkAEiAARIAK+hAAJry+NtpNzhaUWRBda29zcXDl//rwhvYWFhcaKGxcXZ6y28fHxEhMTc+cPcgNHcuvkMD36NWy6DarhPapJJT7Zd1xO58HS29JtTS9kDGGBfpJ4s1bSgm5Kiup2hwwZYmQaycnJxooN4gscWIhAawRwXeL3dVNXHVpu6vWoG/STWR3AdWP94TrjBKo1evxMBIgAEeg+AiS83cfQo1sAGbS0uNeuXZOzZ89KSUmJkS/Akgvd7ahRowzRtbS4sOK6+0O5WTW8Z/ML5cvDZ+XgxTwNOXbLUa6zIgcQkMAAfxmcGCdLxw2X9OgQKbhyWSpUypGXlydBQUHG2j1s2DDBhAAkODAw0G0mAB59cXpB5xHPGROwJpUEXb5WKDnXCuT85XzB9XmtqESCVRKTFBcjybGxEhsVIVNGj5Rwjf4RGBBotOHu/jvzgiHiKRABIuAjCJDw+shAO56msTTpgxiOZSC3sOQWFRUJQoVBxgBiO2LECBk6dKghc/gMEudp5YaS+ZKqWjmgTmw7z1yU3OIyqTaxdlXmYCxsbZ8RNLkgGqFBgRIfGS4ThwyQ+8Yq2U2KlWC1ZkPmAewg9cjOzhZMFLANkSqgXZ40aZLA6hscHGy0zbT6to2zN2/FbwxEt7CsXNbv2Cc5V5Xo5uVLlf6+ajQFNr6v1+9xbQQHBuhfoP7GAqR/fJyMGpwms8ePkcyh6ZIQEyX9tA514t58tfDciAAR6A0ESHh7A2U3OgaczxBJAQ5osFAeO3bMfIYed/DgwTJo0CBJS0uT6OhvQ215spVJeYUhF6U1tSZ6wwHV914tq1Ti22gIR6NqfmGFA6FAtAUQj/DgQIkND5Mxg/rLvNFDZUhSnElS4YgDCAsKXmElR5SKI0eOGFwRRxgaZ+A5duxYo3FG6DXH/c3O/McrEcA1UaREd9/Js/LWus1yMf+auU5uXzLmfesTN4S2n8oZ8IW+IinKvEnj5NkHFkt6arLERUXy+mkNGj8TASJABDqBAAlvJ8Dy1Kp4AIPowoqL6AoXLlwwTmhWqDAsxQ8cONBYJ7FEj2gK3kbOQGoRqqy2QS2z5dX6VyVlSoJrlfg23WhRS1s/CVGyGxUWLEm6tNw/NkpiwkM1oYS/IR8djT3whSwEMYcRtQJh2qCBhhYa5BcTCEgeMJmw5CDehm9H+PjSd806gTqRfVE+3rpDvjl0XOr1eoN8obMF1wekNOEa/WT57Gny5P0LZVD/JK/7XXYWF9YnAkSACHQVARLeriLnAfuBiGGZHVIFyBYuXrxoSC+W2iFXAAlLTU29s+zuKyQMuOj/xmkIr3eKmtdgYQMO5u/OF/bfoG1YfIE5yC6s6JA9YBxSUlIM5rCmQxtthWmz3zpruisCuIwwqTx69rz8/tMNcuL8RV1BaDTXWXf6jOswNjJCls6cYqy9A5IS6BjZHUC5LxEgAj6LAAmvFw69RbrgdHby5Ek5ffq0IWAgWBkZGTJu3DizzA7i6yskt6+G2ZKQZGVlyaFDh4z2F8k3MA5Dhgwxjm4Wwe6rPvK43UcAVlxYdn+75jM5du6CSxOfKOc1lt4Vc6bLC6uWSbLqfLEiwUIEiAARIAL2ESDhtY+V29d0tOhCuoCIC9DqYhk9MzPTWBfhgIYkEXSk6t3hhFMbrL4gvpCUYDICiy/+QH4xJvjjBKR3x8UVR2tU6/2J7Evy+tov5eCZc9Koaa5dXXBdROj1sWrBLHnp0RUSHRHOa8XVILM9IkAEvBoBEl4vGV5LugDtKKy60JIiKQSiLSBqAPS6tCT27WBblveKigrjLAiZCYgvNL6I7gAHN8Q3hiWexXMQQASG//7G+3Lg1Lku6XXtnil+vzEqb/jhYw/KQ/NnSWhIsN1dWY8IEAEi4PMIkPB6+CUAEgXrIWQL+INFFw5ocJCCThRJIrzRCc2Thw1jZjkRQldtaasxXuPHj5fhw4ebkGa0wrv/KDeodXf9jr1KeD8wmt2OegwZAn6LsNQizF81MhmqzhcpsHFN2CkgvamJ8fL3f/YDGTt8iCBONAsRIAJEgAg4R4CE1zlGblkDjlGIBVtQUCBHjx41rwh9NXHiRJPm1worhgcki3siAJIDyzzi+ebk5BjHQkR7APEdOXKkDB48WKwsdu55Br7dK1DUguIS+dv/+4YcOpNtnBXbQwRhxkYMHigLJo2XtJQkjfzhr/F46+R41kXZceSElFVWtbfrPduDNT70k0sXGksvklSwEAEiQASIgHMESHidY+R2NUB2IVk4fPiw0ekiWQQ0ulOmTDEJD7gk7nZD5rRDILqQN5w6dUr2799vdNczZswwjm3QXVOO4hTCXq+ACctHX22X//vR51Ja0TFhjQgLlf/nh9+RWeNGa9a/BqlrqJfEmGi18tbLm19skjVffaNWf/vhyxCi7JUnH5b7Z03t9fPmAYkAESACnogACa8HjRoesIjtevnyZTl48KCJpQuP/5kzZxqiS2c0DxrMNrqK8YU85cSJE0aeggQh0PZC5oDwcdBkU+bQBnB9tAnj9Te/fUO+2n/IxNttrxsBKmNA1rRf/emLci7nivzuoy+MdXdUepr8yeOrNN3wdfkfb31oklWgTTsFiSi+99ByeXr5fZowhZpvO5ixDhEgAr6NAAmvB4w/HoJIaoClbzg64Q/EB05OCDFGhzQPGESbXcRY4w9abGTBQ0QHWPMRwmz27NmGAFOmYhPMHqyGRCYFxaXyl7/5rWRfvmJ0uO0dLlDTUU8dmyE/f/EZ2XLgiPzLe58a5zZocf/i2cckUhOc/H+vvye5SnztEl6Q3JXzZspPn3tMojRiAwsRIAJEgAh0jAAJb8f49Pm3lnwB5OfcuXMC+YLl2ISQVoily+J9CID4wNpbWlpq4vcixBycEZEsBGHMkLyCxLfvxv2GZufbevCI/JNaZgtKyjokqhinFCW3v/nLH0tldY289sl6uZCXL1NGj5I/0YgLJ85fkv/17idGx2uX8GLCOzVzpPznHzzLDGx9dxnwyESACHgQAiS8bjxYlif/nj17jIwBjmjQdSICg2XVdePus2suQAAECPF7ka4Y4eZg5YdVH5Oe2NhYRuBwAcZdaQJ62/c3b5M/aFa18qrqDgkv2g8NDpIXH3lAVmk4Meh9LxcUyuDU/rpfi/zrR+tkpzqudSYFMUj0EN3/b370gowdNpiTn64MIvchAkTApxAg4XXD4basewhXZUVgGDNmjImnC80udZxuOGg93CVY+kF6MflBZA7oeSFxwOSHGfN6GPw2mm/SNMKvfbxe3tu4RR3P6tqocfcmOJJOyRwhL69+UMZpOLEbOp74nW/YtV81vZ9LSXmlU9Ls2CIILxzX/lYJ7/gRQ0l4HcHheyJABIhAGwiQ8LYBSl9uwkMQzkpHjhwxZBce+tDqjh49msvYfTkwbnBskF5Y/SFvQYQORHWYM2eOTJgwwWRpc4Mu+kwXEH/3l797UzbtPSSIsNFRQfzdRLXG/+S51TJZZQgnL+RoSMEmyRicZhzOPtj8jXyyZYfUKHG257J262iI/PAPr74ksyaMYTzejgaA3xEBIkAEFAESXje6DEBmCgsLDdlFEgk4Kk2ePNlk4kI8Vlh1WIgAYvfCmW3Xrl0magey6UHmgIgOSGjA0vMIIH3wb95aI2u37TbJIzo6ImLlPrNiiayYPV12HTsp72zYInXqhDp+5DB5etl9khwfK3//b2/LsXPnjeW3o7as73AnSIiN0cgP31ct8EgSXgsYvhIBIkAE2kGAhLcdYHpzM6y6iMIAp7Tjx49LTU2NcVCCXtfKlNab/eGx3B8BK+kInNnOnDkj0HdPnTpVRo0aRdLbC8MHScPrazcqef1KKmtq2z0iJqnx0VHyf37+E+OU9ht1csvKzTPyhSCdnNw/a4r86VOPyOufbZTPt+kEpqGx3bYcv0C71PA6IsL3RIAIEIGOESDh7RifHv/WWqbevXu3WapGLN3p06eb9LJhYWHU6/b4CHjuAayJEpJVQNsbFBQks2bNMlEcmKGtZ8cVTmt/XLfZJI0A4cVYtFVATBPVEvvGL38mWRq+7B/+8K4UlpQa6QKkDnMnjpP/9L2nVNKwy+iBIWuwU5BSeFLGcPn5D54zzm84DgsRIAJEgAi0jwAJb/vY9Pg3kDDA6x5WXTioIfLC/PnzjXUX5IWFCNhBAKsDkMAgigPi9yJcnRWzl1n37CDY+TqYqJ48nyN/+7s3OoyfCxqKOLn/6YWnJWNImmzYvV+2HzwusBDHavKIRxbN1exrmfJrtfxu3X9YIJWwU5CqeP7k8druU0YSQcJrBzXWIQJEwJcRIOHto9GHRejq1auyY8cOyc3NNRpMpAaGDpNRGPpoUDz4sCBgILs7d+40EofBgwcba2///v1N6DIPPjW37Dp+v9c08cRf/8vvjRNaRyHFkHhiwqjh8uMnVsnQASkmLFldY4PEaaSNoMAA2XX0lIYm+1yuFpW0ayluDUKoxt9+8eHl8gMNdUay2xodfiYCRIAI3IsACe+9mPT4Fjgd5eTkmPTAiLGKkGPwtGfq2B6H3qsPANIL/TdWDBDlA9cTJA5wfoTEgcW1CCD5xN///m3ZuOeAOqE1dNg49LqZQwbJJA1NlhATLSDBtWqZz79eJAdPZ0mexuVFqDK7JT0lWV556mFZOmOK3V1YjwgQASLg0wiQ8Pby8IPsIjUwLHEIZzRx4kSTRADaXVpqenkwvPBwsDwiGx/CliGGM5zZSHp7ZqCB9Q5NGPHf33jflnUWml2kBA7T33pggL/Uq4MaiLIVk9duL7ECtHDKBPnL7z4pKQlxdndjPSJABIiATyNAwtuLww+CC63u1q1bTfas++67z1h2mR64FwfBRw4FXe/Bgwdl3759JiPbokWLTJIKymVcewFUqcPa//lgrXz09Y5OZUrrTi+iVRP8F8+ullULZgu0vCxEgAgQASLgHAESXucYuaQGwkiB7CJ2KpzVJk2aZKQMcFRjIQKuRgDWR8gbYOWFvCEyMlLmzZtnSC9j9boO7RbF+ejZ8/Krf3urQ+c1Vx0RFuLxI4Zo/N0fSH9ad10FK9shAkTABxAg4e2FQYaMAbFS9+/fL3gPyy6SBcDaRhlDLwyAjx4CpBeWXpBeWHtBehHbGbF6ael13UVRW1dvMq79ds1nUlRW7rqGW7WEUGSjNDvbS4+ulNkTx0gwk4y0QogfiQARIALtI0DC2z423f4GhMNKBYulZYQag5Vt2LBhTA7QbXTZgB0EcA1idSErK8tEBEGYspkzZ5pU1bT02kHQeR1gXFpRpbKG7eavpLxSYPl1ZfH395MRaQPkew8tl/umTTRaYE6WXYkw2yICRMDbESDh7aERxkMQ1lx4zB84cECSk5NNmuDU1FRDfHvosGyWCLSJAEhvXl6eHDp0SIqKimTu3LlGUsPoDW3C1emNILjFZRWaQGKHfKh63vLKqk5FXejogIjoMHLQQPnOg0tlzsSxEhYSzJWhjgDjd0SACBCBNhAg4W0DFFdsgmUXmt0tW7aY2JqPP/64JCQk8EHlCnDZRpcQwCTs2rVr8u677xpHNkhrBg0axDi9XULz3p2Ab0V1jWZM22qIb0lFpdxsuWmyqt1b2/kWWHAhPRmVPlBeWLVMFkwZr3F7A53vyBpEgAgQASJwDwIkvPdA0v0NFtlF6DFY1uAhj1ioXELuPrZsoXsI4Hq8cOGCbNu2TRAKb8GCBSazH6/N7uFq7a2cVyrVWfDTrbtky4Ejcin/mlTbTBdstYFXkN3E2GjJGDxIvvvg/TJm2GAJDiLZdcSI74kAESACnUGAhLczaNmoCxkDLLsgu4iHCrI7cuRIE/ifmjsbALJKjyNgOVHiGkWUkDlz5sjQoUPpyOYi5GHpbdakFAXFJbJ53yHZf/KsRnAoVJ1vhdmOw6COVZB+WPSffvpfkJLamIgIGZaWKstnTZP7pk8yRNc4uFo78JUIEAEiQAQ6jQAJb6cha38HZLqCc5BFduERP378eAkJCaGUoX3Y+E0vIwCyBUvvsWPHTJi8+Ph4o+lFOmJGb3DdYEDXi9jbl5XsXtZMatl5+XLu0mWB1KFS4/cWlpYbMhsXHSl+/fw03NhQSY6PlWEDUw3hHZCYoOOhNFitvSxEgAgQASLQPQRIeLuH3529IWO4dOmSIbvV1dUydepUk1QiLCzsTh2+IQLuhACuU2RkO3HihAlZtmzZMklMTCTp7YFBwiQDqYRLyqvMa31jk9TU1ZloC6G3ndCS42IlPDRYndI4Qe6BIWCTRIAI+DgCJLwuuABgxSkoKJCvv/5aKisr75BdaCRZiIA7IwDZDSI34G/gwIEmbB5JrzuPGPtGBIgAESACXUGAhLcrqLXaB2GeEI3hypUrRg85efJkhh5rhRE/ui8CyMgGSy+yAGZmZsrixYslQnWkLESACBABIkAEvAUBEt5ujiQcgPbu3WssZNOmTZOJEycKZAzU3XUTWO7eawhgub2qqsrIcXJzc40Ux7qOe60TPBARIAJEgAgQgR5EgIS3G+A2NDSYlMHIopaUlCRLliwxljGS3W6Ayl37BAGQXshysFKBFYuFCxfK2LFjTXSRPukQD0oEiAARIAJEwIUIkPB2EUwQBDipffbZZ3fimWZkZHSxNe5GBPoeAWjRYeHdsGGDiSzywAMPSEpKClcr+n5o2AMiQASIABHoJgIkvF0AEGQXusd169YZqxisYSC7wcHBXWiNuxAB90EA4cpOnjwpBw8evOPEBj0vVy3cZ4zYEyJABIgAEeg8AiS8ncfMJJSAkw/imCLO7vTp00l2u4Ajd3FPBBBib/PmzXLq1CmBA+asWbPMKoZ79pa9IgJEgAgQASLgHAESXucY3VUDul1YwHbs2GF0u6tXrzZklxawu2DiBw9GACsYV69eNU5s0PUiW+CoUaMYecSDx5RdJwJEgAj4OgIkvJ24Aizd7ldffWUe/rNnz5bhw4czUH8nMGRVz0AAVt68vDwTWxrph++77z4zwWMmNs8YP/aSCBABIkAE7kaAhPduPDr8BOsukkucP39e7r//fhk5ciTJboeI8UtPRgBObMePH5ft27dLamqqrFy50kgbuJrhyaPKvhMBIkAEfBMBEl6b4w5nHoRsysrKMsH5YfEKCAiwuTerEQHPQwArGojP+80338jFixdNmDLoeZku2/PGkj0mAkSACPg6AiS8Nq6AlpYWE43hvffek7S0NJNNLSUltf09+4no/yxEwOMRAOktLS01EUnKyspk2bJlRs9LK6/HDy1PgAgQASLgUwiQ8DoZbjzwCwsLjQNPTk6uPPzoI5KYMkAqG5qlurFJ6ptuSHPLTbmp7QT49ZPgAH8JDwqQmJAgidBXf91GcuAEZH7t1ghA2nD27FnZuHGjDBs2TJYuXUppg1uPGDtHBIgAESACrREg4W2NSKvPSB28e88eOXDkqKQMHiaDMsdLnfhJrRLdhuYb0nSjRVqUFKP49et3h/SGBirpDQ2S/pGhkhgeIiFKhEF+WYiAJyJQV1cnyCgISc+8efNkxIgRlPR44kCyz0SACBABH0WAhLeDgYeU4dr1Qtm6Z5/UBEVIaOIA6ae63Ru3+G0He976yl8JcFCAnyQp4R2ZGC2JESES6O9HuYNT5FjB3RDASkdxcbGRNgQGBhorb0JCAp023W2g2B8iQASIABFoEwES3jZhubWxoqZWDmXnyKWKevELQ7Ypvw5qt/+V8l6VNwTKsPhIGa5/kDxQ5tA+XvzGPRGA4yYSriAG9ZQpU0zCFWRhYyECRIAIEAEi4O4IkPC2M0I1qs89VVAqWYUV0nwTXmjdkyNg70C19qbHRMjo5GjV+AZ3t8l2es7NRKBnEICVt6KiwmRhQ0IKhCkbMmQIJ289AzdbJQJEgAgQARciQMLbCkyoFRpVm3vsWqlcKK6UBrv6hVbttPcxwM9P0mLDZNrABKPrhe6XhQh4CgKQ+SBE2Zdffmli865atUogcWAhAkSACBABIuDOCJDwthodOKGdLiyXM2rZbVDHNJty3VatdPwx0L+fpMdGyKTUOAlTqQMpb8d48Vv3QgCOnJs3b5bc3FxBtsHRo0eT9LrXELE3RIAIEAEi0AoBEl4HQG5oeLErFTVyKL9EquqbOkV2sdyLYlebC2e2iSlxMjIhSgLUkY2FCHgSAleuXDFZB5F9cMWKFTJggDp0crXCk4aQfSUCRIAI+BQCJLwOw12pJPdQfrHkltU4bHX+tlmdeYquXpGaynIZODxDQkJDbWl+kzRqw1SVNuCVhQh4EgKQNhw9etQ4sI0fP17mzp1LK68nDSD7SgSIABHwMQRIeG8P+A210F4sqZJDV4qlvrnF/mWg++WdPyfr3/hXiUlMlAe+87JExcbbIrwIUZaRGCXj1dKL9yxEwJMQqKyslN27d5sshIsXL5aBAwfSyutJA8i+EgEiQAR8CAES3tuDXdvYLPvz1LpbXi231Qm2LoPG+jr5es1bcuSbr2TVD/5MMqfOlACbTjzQ7iI278xBiRIXFmzreKxEBNwFAWRgy8/Pl/Xr10tmZqbMnDlTgoN5HbvL+LAfRIAIEAEi8C0CJLyKBdS3xdX1suXCNalTRzW75aYu6xarlOF3/+U/ytCxE+TRP/kLCQkL75SVK0gtuyC86XERgkQVLETAkxCAlReEt7q6Wh555BGJj4/v1PXvSefKvhIBIkAEiIDnIkDCq2PXpMQ1q6hCjuSXSbO+t1PgpFZVViKf/+G3cj0vV1Z892VJG5EhjQ31Eh2nGaj8/e00Y0juKJU1jFNZQ2igvX1sNcxKRKAXEGhubpZjx47J9u3bZdq0aTJ58mQJCwvrhSPzEESACBABIkAE7CNAwqtY1TU1y96c65JXWS8tNvUMcNo5tnOrvPXffiGLHn9OFj/xvJw+sFsunjquOt6XJDwq2tYoIA5vMmQN6YkSFRLEEGW2UGMld0EAE7/S0lLZtGmTlJeXC+LyMmKDu4wO+0EEiAARIAIWAiS8ikRNQ5NsOntZKprsRd3FQ76ytFg++Of/JiUFV+U7P/tbSUxNky0fQsv7tbzyD/8s0QmJFsYdvkLEEBEcIAuHpRgdL0UNHcLFL90QAUz+Dh48aKy8SDk8f/588be5wuGGp8MuEQEiQASIgBci4POEFxS3vLpWNmdflVqb8l3IFvasXys7v/hIlj79gkyYu0j6+fWTr9//oxxW57U//X//l0ZsSLJ9uQSrjnfJiFRJUEsvCa9t2FjRTRDABLCsrEw+/PBDCQgIMHF5+/fv7ya9YzeIABEgAkSACGjwLH1Y2TNreilaOP3DJ0/LmTp/afYLcHqWqH8t54K8+0//VYJDQuWpv/gr46jW3NQo33zyvhzf/Y28/It/lMQBAzVaQ5DT9lABzmrLRqVKYnioOvzY2oWViIBbIYDsaxs3bpQLFy5IRkaGLFmyhFZetxohdoYIEAEi4NsI+DThBXmFd/lnGzZKY8pw6Rca4fRqQGSGs4f2yUe//Y1UqXYxIjbOeKXfvNkiFcVFmnyiQkZNni7Lnn1RMjREmZ3sU4jUsHSkWnjDaeF1OgCs4JYI4Ld09epVE7EB4cqg5U1NTbV1/bvlCbFTRIAIEAEi4FUI+DThhYf5AdUe7j98VBInz5GmgFCn6YTxYC+9XiAXThyWhtpatZHjeugnN5qb5PT+PZKbdVpWqNPamJlzJaH/AKcJKLA7Nbxe9Zvy2ZMB0d2yZYscOXJE5syZI7NmzRI/PyZU8dkLgidOBIgAEXAjBHyW8IK4lpSUyIYNG+Smf4AkTZghRfU3BRnXnBXsC0uv/nsriK/u0KTphb/+4I9yZPvXquH9Z9XwJtuybjFKgzO0+b2nIIDfRW5urqxbt85kXVu6dKmEapptO6scnnKO7CcRIAJEgAh4JgI+S3jhWZ6dnW0I76jRoyV51Hg5U1JrYvJ2ZSibGhvkK3VaQ8a1V0B4E+w5rUG/O/J2emHG4e0K8tzHnRCARAgRG06fPi3Lly+XwYMH08rrTgPEvhABIkAEfBQBnyW8jWqRxYN5165dJkNURFKK7MxRDW4nMq05XjNwWtu3aZ3R9z755z+TyJhYx6/bfR8U4Ccz0hJlcKxmWtNIDyxEwJMRwETy0qVL8sEHH5hEFJA2wMrLQgSIABEgAkSgLxHwWcKbn58vX331lURERMjixYslUFMC788rkctl1U51vO0NGBzXbrbc1BBlfraWcUFvE5F0QlMLx4UFt9cstxMBj0EAsobi4mJZu3at6fOjjz5q0g17zAmwo0SACBABIuCVCPgk4YVzzb59+0yg/Hnz5hlLlL/GD71YWi2HrpRIfbPNgLzdvCQCNToD0gpP0LTCeM9CBLwBAaye7N+/X06ePCn333+/pKenM0SZNwwsz4EIEAEi4MEI+CThRczQL7/8Uq5cuSIPPPCAeSDDsaayvlEO5cPKW9NlK6/da8Gy7k4dEC9JkVzytYsb67k/ApA15OTkmBWUlJQUWbRokYSHh7t/x9lDIkAEiAAR8FoEfI7wYsm1pqZG3n77bRMnFI41gYGBZoARoSGvvMZYeas13bDzeA1dvy5g0Z2YGqcW3mgJoHa360ByT7dDAL+x8vJy4xAKecNzzz1HWYPbjRI7RASIABHwLQR8jvBiufXQoUNy9OhRmTx5ssyYMeOuEW9QOcOZwgo5V1Qh9erA1hOkF4kmBqmT2sSUWI3Be4ts39UJfiACHo4AZEOQNRw4cEAWLlwoY8aMoazBw8eU3ScCRIAIeDICPkd46+rqTJzQWk0aAWe1AQM0OYRDAcFtUtJ75GqpanqrpKG5xeHb7r9FJIa0mHATmSFEIzQwRmn3MWUL7olAUVGRfPjhhxIbGysrV66UyMhI9+woe0UEiAARIAJej4BPEV4stSJkEh7CU6ZMkQULFkiAOqu1LiC9NY1NauWtlPPFlS6x9EKzCxlDulp2M5OiJTY0WMlu6yPzMxHwHgTq6+vl448/Num7H3zwQSMh8p6z45kQASJABIiAJyHgU4QXzjTbt283ERrgPT5hwoR2g+IrN5bapma5XF6tpLdKKtShrVlDjnWlwKobGRSoZDdchsZHSpTKGGjZ7QqS3MeTEEDqbkRq2Lt3r5EPTZ8+3ZO6z74SASJABIiAFyHgU4QXukJYd5H+9KWXXjJLrc6IJxzZQHZPFZRJrjq0KWdWXa8S31v/t3spwHrbT/9D6uDE8GAZr3rdxIhQOqi1i5jnfIHJEK6Bu1/v7j+M9+Ya0H9uvb/1enct7/9UVVUlb775pvTv319Wr17NiZ73DznPkAgQASLglgj4DOGFnAGhkjZv3myILsKRIemEnYJ9G260SEltvVytqNPXBmP9bdRtTfrXctvy66eWXERcCPL3F6QJjgkNktSoMEkKD5GQAH+1JlPDYAdvd66D8S6ta5CiqjrJq6iRaxW15nqobmiWRp1QYQIVqmMdFRJkkooMiAmTAXoNYLIDB0WkkvalAs38F198YWQN0PEmJiaS9PrSBcBzJQJEgAi4CQI+Q3ghZzh9+rTs3LlTpk6dKuPGjZPg4M5lN4NFr0X/qVOpQ5lafRG6DO+N1EG/g3QhRIluuMoXYpTwRAQHKMGBY5qbjDa70WUEMMYlNfVytrBcTl0vl/Oq767TJXtcE5bF/07jOt6WPRcToNiwINVtx8jo5FgZqWHoQgMDfOaaQMxrSBqOHDli4vGOHj26XRnRHfz4hggQASJABIiAixHwGcILOQP0u8ePH5fnn3/eWHn9NAVwV4pyHGU5hua0ubvht7eXstuswI0egwAIbbNOlg5rBr59eYWSo9n4ahtBdNsf/9Ynh+sB1v14TR8NwjtncLIMjos027x9LgSczp8/L5999pnMnTvXZDXs6u+uNa78TASIABEgAkTALgI+Q3hLS0tl48aNgnBkILydte7aBZT1vAcBWPNLahpkd06BHM4vlcLqOrnRRcdFoAJyG6CROobHR8mcIckytn+sT1h7CwoKZO3atSZKA7KuhYWFUdbgPT8TngkRIAJEwCMQ8AnCCyvT1atXTTrh5ORkWbZs2Z3sah4xSuxkryOAa6astlG+PHdF9l4ulAYXJiGBI2OC6rpXZqbJhNR4CQvy7/Xz680DVlRUyJYtWyQ/P18effRRQ3ydOYv2Zv94LCJABIgAEfB+BHyG8GZnZxvnGSyrIgavvzqWsRCB9hCoqGuU9WevyH4luzUqYXB1gbXXkN7RaTJ9UKI6O3ZNXuPqfvVEew0NDXLw4EHZs2ePPP7445Kenk4Lb08AzTaJABEgAkSgXQR8gvBCxoAUpydOnJBVq1bJoEGD+MBt95Lw7S+gz4ZGd3fOddl0Ll8q1TnRaLZ7ABZYepGIZPW4dBmh2l589sYCh9GsrCz59NNPZfny5SbNcGAgU2p741jznHwXAayKVWvCJqi+woMC2pzEwx+itvGGBN528EakIzh+h2n9QDVCtb4Dos06zXwKKRkiH+G1QT/D8ReJnDpbcHwcDwaGYERO6uI9F8+EBm0H0ZsiNfpOl9vR80Mb1vl1tR0Lh1v4Npv20EeFWSNEBUiQZnWFkz2O0/p5hshBwAIO950teFYWqzM3xiIpMrTLUYjqdQW1uKbOxPJM1ohGaM+6NtBnFAQFQEAAOIJ3tfgE4S0uLpZt27YJLE1IOJGQkEDC29Urxsv3w48LkRjeOXzBhBuDjrejclNvoOav1W0EURr66U0Vfx2VIP1hj9LMe89PGa6RPbwz+x4eWiUlJbJhwwYJDQ2VJUuWSExMTEew8DsiQAQ8DAGEbFx7KldKNWwnHHMRlcaRT+JOinCOm7PzJS0mXGZrnZMa337XpetaN8asdIHQOBYQIayyXa+ul0UjUuViSaWc0Sg59w1PlYHaRmeoD27l1yprZeelAhmkhoYJqXGGODsez+57PBcO5hXJGX1WPDZuiAk5aXdfx3qV9U1yQJ2hQe6AB8JZduacHNsCvtf1/LaevyZFSkJRQGInD4iX4YlRsu50nvFJMV84/BOmdaakJcpEldc5jpdDlTbfInLRsaslsk2PNy0tQWZp/7syCQGWGNPNamDK0Otg3tD+gj6d0Gtj58UCEwULNB3ZaZePGiiJSqy7ipFPEF6kE4Z1acSIEbJw4UIJD9cfSmdGts3h5kZvQwA3jCq9AX14/JLeZItMCDpn51h08ZzkHdkrzQ31d11T/oFBMnjqXIlJGyJ+Hchn8MOFNeTB0YNk/rD+bVpFnPXBE76vrq42MbBBfJFmGIkoWIgAEfAeBGB5/Zedp+ViaZWSp1h5bPxQQ1KsRy3ur3ll1fLmwWwTreaRsemyU1fSPjyWY2LWPzd5mCE8jrHKa9RiDKKWo/s9N3m4IVj7lAA/O2mYthHTKYIGYnWxpEo+0vs7yPjC4SnGOtuVEYBhxPh3aP//w8Jx5jy70k6REvl1Zy5L842b8oiu9MWrb0dXyRwI/aXSSnn/yEVp0v5hQgCr+BglkbFhIfLGgSxDHvtrXHjHY0To82fywATJ0NCZ1lg5OxeMZYGS609O5kipOnY/peMxTLPIdtZCjXbKdIL0qbZzuaxGHh032Dhzox1MJg7lFZsoSdnFFSa/wYvTR8qgmAjb/Wx9Hl5PeGFdOnv2rHz++ecyf/58QXpThkVqfRnwMxBAVr2T18rk3SMXzI8QP0Zn5dzWdXL88/ckNDpWAkPD7lQPDA6VzCUPSfKoseIXcLfV4k6l22/w485QK+/Tk4ZKcgQiGLSu4fmfkWYYYQERjxeyBsTj5aTT88eVZ0AELAQswgtyEq2WyvvVGjdXo9FguRwF91NHwvuwEt5dhvBeMkQJFsbHJgxR8hhk6uOf1oT35LVS2Z17XZ5RgjWqrwmv+njs1b50m/CeVsKrK4WPKNlzBeH94OglGaJhL5dlDDQWV0wgcnXCAKKfqeR34bAUlQd8u/IIhQDGqDPW2Xqd3Oy4UCBfq7V+3tAUWazW965IIvDM3Z9bKF8oBuNT4mSFGn4gEUGB1bvxtpzlU105yNGJ1PemkfAacNr7Bw9aBL4/duyYWUodOXIkH7TtgeXj2/Ejfk/J7gGdVWJ5zk45sW6N5B7YLjNfeFXi04ffvYveaOyQOvBb/MgfGz9Yl/WSjFbp7oY8/xN0vCC7SPyCVRYkfuHE0/PHlWdABCwEQHj/967Tkl9eI9FKWgP8/OWZyUONnwIm9e0R3s9O5kr/yDDjHAwytnCEErLbs35Hwvu8Wnhh9dt+8RoJrwW6w6tl4QXhHakGlFVKHkFiIT04oROF9WfyVDaRZCQD3XGSxnFyy6rkbZX9QXrwxIShxprs0BVbb2GMLFPn8D/sy9LV1BZj3R2eEH2PwQeTgTXHLkl2UYW8QMLbMbaNjY0mJFJeXp4JRwaHNRYi0BoB3IyRJvj3+8/JFb1h47OzcrPlhhz99G25nnVKZn7nFYkZkO5sl3a/BzFerEtsK/UmBYmDtxUQ3nPnzsnXX38tmZmZsmDBAglQZwoWIkAEvAMBi/DCkQmRZ7ZkX5NJA+Pl/pEDdEk9uF3Cu1EtpVjK3pNzS8v6hFp5B8dFGKtva8ILK9+mrHyzGjZCyVG5pnm/pNvKNYSkJjWV6OAgSdYl+5So0HvkYZ2RNICMVTQ0SnF1gx6jUZ29bjmnIWEQHObQ1pfa720XrqkcLU1alFTCWhmjOtMhurQPK7W1vI+60A5fLr+VtCgxIkQtsFESoXplOHytc7DwxilO0PVeUukFvsNzITIkUFf+QiRZJwXI9pmjZDM9NlIGRIfdOQauID2MkTTcS3hbjERvS/ZVtcQOkCkqX0Df9P87r3avQDwXb6gx6KMTOUZ/vUifWTPTkyRPn5kg19Bm2yXT9er095WO5e7cIpmr+t8F2pYV636ogzwChH3NsYskvHYGqb6+3sgZ6urqZOXKlRIfH29nN9bxMQRwgzugTggfHc8xs047pw/d7pGP3pDyq3ky8ZHnJDwh2VgtA0LUW1U1vHasu47HQSKKpyaqrEFF+d5WgG9hYaH5LcJh7eGHH2YsbG8bZJ6PTyNgEV6sjkFyAOepLLXKPTRmkExVpyhkm2xL0oBoOH8yK8NYhjcoiRynS9sPZA6UKCWvtZqa3NLwfmfKCOPJ/7kSxMfVUQyRHTZlXZHr6ggHNo3P8ORHuMdVY9LvMRzYJbwgjnkV1UrAr0uREl6cF6yMIHsZGk1noTrMRQQHKOHNlw1n8yRF79cge1pFUAukb7GS/P4abQDHPK0OWYj6g1CXKMABTnr3KcFDyMv1qlG2JA1hSqbXqSX2vMpCkKooVKUGOKckbQvOXEfyi80K5DTFc7lKFvCdVdBvaHhbE16MxzdqFQfhRQQEOMahxCgpH5cSq9bZCCNp+LYlq8V7X6E1PqdW9o9Uc4vze2z8EKMTfv/oRSXPYj5H3JYk3Lv3t1uAy/li9PWiwfLJicPMJAHSBkxevqdaXUtiQcL7LW5O39XU1Mibb74pSUlJ5iFLq5JTyHyyAn6AWPLZrDPOOvUMdlq0fkN1lRxc83vJP35QNbwx0qLymdDoeBk6+z5JmzhDgsIhrv9WK+WszVS1THxv2giTdthZXU/73lhMNAEFnEeRae2RRx6RoKBvtXqedj7sLxEgAncjYBFekLcfzR5tVsw+UMtclBIgaC9hqcTqmeW0Zml44Z3/ypxMXR4PNGQPOl34M4D4QsMJAginte8q4VVeqaSvRMZrhIVT6sW/4cwVox+F1RJZLHEf1/8NmbMIk9VLfGfHaQ37w9J4XiNCxCg5hNW1Vq2R6CdSy0OmAac3EPWvVMO6Si28GUmx5th71dn58JViQ2hxfiC5H+hyPKyZIPFJ6qOBKBGo84OZoyRSrbywcDcpZg8pSS9VC+6Haj2F9XqBOjEHKeHFvRNkFJZlkGdErZg6MFGmDEq4I/3AOaLf7RFePNdgjW5W8gu9LiQjCIcWrWOyQhMgAU9nlllt3pzP24fOS4Hi84SS3TFqpMHz8nd7zqAL8pKeE/TbzgqIPsjtPsXrSbXoY/xw3bx96IJG5KiT/6iOgJb2m4TXGZoO3yOl8Pvvvy8DBw40Fl7qBh3A4ds7CGA56n3V7+7UEDn4gTktWr++qlJOrl+jkoaTEpnUX8lunNRVlkv5lRwZOGGajFn+mARHRDltyqqAB8PLMzNMmDJrmze9Qtbw0UcfCTKvPfXUUxIRgQmBHbuCN6HAcyEC3omAI+F9Zc5o41W/WTNVHlRyh9UrOFGB0L158LyJ0tCa8KZEhcsFtWy+rxrUuPAgeWTsYEM4YYiwCC/kALBYwrIJorhFSdxjKocAYYMUzNxP9Pbd1m3FLuHF6CAKA54DINggh3gPHeynJy/LPHXEw/I7CDCswH8+f4yxKmO/Co3bDr0piP13pg6XfJXJQbIAh7SJ2kc/NYCcuV5mwrfBSjtGifM3eg4gvIjUA9kDlvknaSixOUP6G9kDLMJWwbkjVFuwxtW1CKH1XXuEF+cC7fNlnTTA+gxrMfADed+g2Ibrc+fhsYNkqKa8//ZIVqvfvsKJ7IASVISeQ6gzyO8Q4aGmoVl+t/esVrwpL83IMPrtb/e69x36c/xaibHcYzUTjoqYWOC8oAvGZIOE917cnG7BA/bUqVPGaW38+PEmQgMfsE5h88kK+BHC8oA0wrhx2CktzU1SWZBv5AthcRrbWZ00muvr5Nhn78jlw3tk9ot/oVEa1Dmrg7BkjscJ0Zn3j2ZnGuuA43Zver9p0yZBmMAHHnjATEI5AfWm0eW5+DICjoT3T5XwIqZuaW29IbAgW5A5RKlF8S21EI5UaUBrwpumS+tICgFLJGK7LlAHNuhDQQAtwguCBDILq+exq6UmhCSIX6aG1ILuF/F14zUEl+NSvzUmnSG82Af1YaEtU50wLLxXlbyCnEKeAavoV1lX70RpgHYXZBH7wJq6XePHLtMoFdcqazReb4mJVoGlfpBGhOECeUYYr/l6jrvVyALCCzxuPYfOS5Xqh2E9RbSFQSCpet5OLbD63GrLwotzwTMNsWxvEdpb/8Ki+qVKMvZfLpb5KpdYPDLVaHpRv60CHTLCkJ0uKNfJSLqRQkALXKeyk89OXTa7wPEa+uKO/FAwKfhErdgYv6Uq/YDmGZMKWPM3KabQZSMEHVY8I/R6ASbU8LY1Iq223bhxw5BdhCWbMWOGyfBEwtsKJH40CNy60WQp4S2yTXix4031LkWSiTsmBb2zXD6yR45+8kcZOmuRZCx+UAKC7WlyfYHwImLK8ePHZebMmeb3yBTf/AESAe9AoC3CCwKI5ATQeMIDH5ZLaHIRhrEtwgsiW6RxXSGFqFAt57KMAeqsVKma2hojabAILxCDc9xptZYiacEFlR80NLeoI1e4EtIEmajHwf3UsdglvCCHcJaDTweODYIGighSiri5SBCxSnXJjoQXSRFQcAyQYnw3Ty20V5TwntX+DYoNNxZZ8I9+2j5eESd3lBJ/1Ie8ABk34dx39nqFIcRZau3GOSWqJnmyOv9h2b8jfSz63R7hNZ1r9Q/6elTlIfBbQduIA2w52rWqaj7iPNZoaLNC1UwjfJolGcGz85aDHTTOMbJ01AAd6/ZXNguqajUmcLaxfuN8kXwJ+KKdsvoGY8EfrKQZKwKIIayyYRLetgak9TYQXoRBunDhgsyaNUsyMjLMhda6Hj8TAfz4EZLMtqShA8jyTxyUwx++LoOnzZcMjcUbqE5sdoqRNKjzBm6C3lrwW9yyZYtJ771o0SI6rnnrQPO8fA6BtggvQABh3KoOU4eulBhrHnS049VZqi3Ci/q4Fx9SGQSiIMCJC9pVpCyGhteR8CoPMiSpXC2m0H3Cigx9r3JJeXLiUGMddSRwaNeOhhdWWFhdYaVN1UgIsKN8WN0AABmtSURBVB5D61qh0ROQbGKiaosfbIfwgrSt10QSMJyszEgzOmBElkCIMGQIQ39A7kB4YZluUL8PnGejEltEqkgIDzbRHpAACeeE+LknrpYpIb4hDygBRIKI9oqeXqcIL/oKScZGlWbMTE9UjXFah4QX2dtAeqG/xTlYpRGGRY20gIJ4vBM0nnK8nkd7pUpTHMPCjXN0LJhQHNbxq9Hvl6jlFxpuONlBbkgLryNS7bxHDN5PPvlEamtrTUgyZndqByhuNjdZeMsapzVdWrFV9IcIC6+5w96+BSBU2bkt6+T05rUy5YnvS9qkGeLvJPGEdaxUvSG+oN6pWMby1pKfny/r16+XxERdFlyxgo5r3jrQPC+fQ6A9wgsiBmnD6/uzjDRBeZYmP+jfLuEFcIiJjmVvRDfA/pAq3EN4dTsW6q1/QeAQiWCHOoWtULI5Q+UQjtIGR8KbqU5mC4b3v5PkwDRy+x+EIUOSBkQLeE5TvidFhujzQYw1GdbQ1oT3pwvGGSc57A5yD6cuRHd4YdpwE7rrG03Q8II6I4M4Bxg9LmISq7xASS/CjEFHW6WEHimKoVG2yCTODJrdc4UVJtU9MnEiMgPOE7peK1bx7W4bnNqy8AI/hFUz1mVtHO2j7Tolrn80TmK1SqbTjGUcGKH4q9YYEwfHgq9MrIpbVe58Va0a3tf2qYZXK8ARD05raAf1IcNoqx2cP753LJCzvKNGJ1jR/8OCscZCjz7Tac0RpQ7eIwbva6+9JgiD9MQTT/Dh2gFWvv4VltL26xLWx3bDkmn92rISKc27KKEx8RIcFi43VNNbnp8rJ9etMREaZn73zyQyUVPotv7FtwP22P4xGpZsmFeGJbNOGZPPt99+W292N+XZZ581jmvWd3wlAkTAcxFoj/DijEDc9mpGrc9V6wlyhwQT0IEi05oVpQEaXqvg/gDr5ruaJheOXDACOBJecKUitYBCYxusyQ+C1H8CJBkREBAKDZ7/o1UD25aF90N1KoPjFhzdkDjBsYBIBijZA2mGZRaZvxBTF05ou7X/WM6H3hWShq9BrjXcF+7ZWJpHJAZIBE6pFXSOyh7mDE2WXHUMQ6rfWLV4QuKA+MBgm3VqjAMxDFT/ju0qaYBlGvGHU9SiXKKSDizz469eLb+nrpfKPrWgwnoKXWuWhvMaoQ5mwxOj7jo/haxNCy8sqgc1mVJ4kL9akKEF7qcW82Y5rlITWN3hUIgIEZgcHNWwZ8ARkwU70RaAHQjvnSgNM0aZaBnHYKlVPGZpO3ecCR2BbuM9CC+jNLQBjJ1N+MFUVlaakGSpqakMSWYHNB+ugxvoVdWJIfFEfnmtmQF3BAeur9LLF+W4Oqg11ddKUFiEIC5vVeE1E61h7MonJGX0RNvWXUymF+kNDZ66HQn+O+qTJ3zX0NAga9asMZEaHn/8cUlOTvaEbrOPRIAIOEEAhPd3e86qRa5F4+pm3nUfAxmD8xdCUYHcIeUwYuViSR2raj/S+tC0Oha0d1gJGaICJKnlE45McN4yFkptDwQa0gNEfoQ8AEvt1UruRqk+eGXmIBMtwNFIiXs2klQgigJi90KD2toWAQnFtLQksyQPZzn0AfVAnKEPRuxY6GkRoQCkDnGDUUKVOKMuIg2MUbnGcnVYQ7Y5kLjdmlADfYVsISFCE3Bo32GlRdYzLNtna5uwZuMztKsIdQbtLjTIIJ/Qx+KcQHj3q1QCfzM0scfDGsXC0YKNdtuy8AJ3hFCD1hltYp9KtURDugHnwYUacQLHzdOJxTsaJQHZz15S4orvWuPjOD7We0RpgIUX+H5f98M4vKftQALxfaxYqlOaM4c7tGUsvNhPLbw/XThWgm87e9PCayHdwSvAv3btmnm4Ip3w/fffL3SQ6QAwfmVuVtDxIowObgYdFw1boysIFfm5UqZhyGrLSwVyhoj4JInTFMNR/QeY6A0dt3HrW9yUkVp4tXq4zvDS1MIWDlh1+eKLL6SgoEBWr14tlBlZyPCVCHg2AtBanlcnLyxnj1DrY2uSg+0FlXUCpyUs3acqgcSSPrz/EXcWiSMci/I345gGh7RAXRqHlTfktkUW30FykK8JIkpUwwsHNhA5WDARrQFJFRytu2gX+8AZ7bJaXWHhbKugz8hqFq9/sOqif9gPFlzoSfPVKILEDchyhmNe0c8Io1WrRBd9hINZmjqowTqK4+spGyKHTGRX9Txr9fihep5x6uQGmQbq1au1F85xMHT0VwsuUjODLIK8w+IMhzjUhb73clmNcdADFo7ZyHAuOFZbhBfkulRJL9otUWkJCGmIZrlEe+naLiJMoK843tuHz8tFnRT8SH1JhjgJU2bhB+0txh3PsaHqrAZiDwKfrU53IM6YKDgSc2u/1q/oJ6zqIPkZ6qxmSTZIeFsj1cZnhCTLysqSL7/8UqZOnWqc1kh42wCKm+4ggB/ciQKN26xLUKV6E8WNzmnRu0zLjWZpbtIsOvoeERlMqC07U+PbjeOHjdznT6ujBXLKd2JXp91ztwogvBs2bJArVzSd6KOPClZfWIgAEfAOBKx7pqNl1fHMrO+xzaqDbdZ7x7rWe2uftuqA5DWpoQGkCO5gkAE4I1emPatR6yDW6+2D4AVVYDjDH+7p1jZUtfqC48OijT+QRhNxoI0bONpo0nADqIf+WVZjtIWCZw/+89fj4DmC84EMBE0FqaUTxBfHRD04iWFb6/NEX24R3otKuiNMyC9IJmDVhQUaEw4YcpAaGMcJ0li+jpOCKrX6QqMcolZuJMmwK2lA/y040Uc4GK49kWuO98SEoUZyYuGFus4K2kJ9WMth9QUW69QREPIQZGAbpNKXNiB21qz5vp8OhNVXWzt4SiUQ3pMnTxqPcIRAmj59+i0i4iknwH72CQLIY450h7Dy4gbR0wU/7HBNUwknCywttbaK9PTxe7t9OJIicgpCky1btkyw+gLHBBYiQASIABHoOgJ4XOWUVd0x2MBaDI0y4gYjVJuzAnIJBzlYj5EVz5EMO9vX8XtIOyCfMFZ87UNX7u+gpUfzS40mG4QXVnSQdmTsA5nv6hODhNdxpPje5xHADPpsYZkGR79gloF6mvNipo8wZN/VrDzRurTU1R+ypwwcCO/u3bvl6NGjsnTpUoYK9JSBYz+JABFwawRgnilW/Ssc7uBchwIJyKQBCRoqLM58dvYPnneusD8YO6o21NXnGYxNJzV19D7VPrfoMxnnBlnJEtUxx6lspKvtkvA6uwL4fZ8jAGs9vPsLCwsFqaJramp0WUY1U5p7PTIy0oS4SkhIMFE4ujKbdDxB/LCgzUI8XjhTYJkH23qiYAaNLDrQ7sJBoKsz6p7oW0+1ScLbU8iyXSJABHwdARBNJGpAuMxb5FXDl6kcwhOfLSC9RuqBE9ECGYaeSpcsxtZ14dWE98SJE7J161aj3502bRolDdaoe8griC40n2fOnDF67JKSEkN84emP76DJDgkJMaQXzk+jR4+WtLQ0Q4S7Q3zx+0I8RcRHRKgyiPxdXTCLRgrMBzXY9zT1uIWl1xcKJiqQM2zbtk3mzZsnU6ZM6dYNzBcw4zkSASJABIhA9xHwasK7a9cuOXDggDzwwAMyatQoEt7uXy+91gKIUXFxsVn+vnTpklRXVxuS214HAtTrNDY2ViZPnixjx4418V27RXr1QFge2qSZdUB6EQ/RVXJ3zLgTlOwi1/okDXHjzWHIWo8XJiqYwMCZdPbs2SbFcHfGqXX7/EwEiAARIAJEoC0EvJbwgjBt375djhw5Ig8++KCMGDGClqS2rgA33AZSdPXqVdmzZ4+x7GIZ3A7ZBHGKiIiQiRMnyowZMyQ8PLxbY44llWINAo5g5kc0HiTiISL0TlcLdEdwShusTgEIXj4uNdakzeyqHqmr/ejL/Uh4+xJ97z427hG4V8CjvnVEHlx3eCbgHoHJsSuLWUbWtnGMtop1TE7s2kKH24hA7yFAwtt7WPNINhEoLy830TVOnz5tHmA2dzPV8FAJCwuTOXPmGGsvJA/dKaC3SMt4KK9E9uReN4HL6zRcSmcLSC3yi0OrO3+oBvrWWJGeqKvq7Hm3rk/C2xoRfnYFAiCdZWVlsn//fmPcGDp06J3JLq45rBJlZ2cLtP6ultHU19cbJ0xM0tsq0dHRMmHCBImPj7/Tp7bqcRsRIAI9i4BXE97169cbC+FTTz0lAwYM4M2mZ68ll7SOh8fhw4cFchQ4qtmx7LY+MCw8cGZbsWKFefiZuLitK3XiM0hvs8YuhIX3nKatPKPeo+c1GDqIrzqQmj462n2NxVb/gY8q0jjGqXdphuZRz0yONgHWQzXOYesYip3ojkdXdSS8s2bNMvp6Wr48ekjdovO4T5w7d05+9atfyZNPPmlW9fC7x/bLly/LH//4R8nPzzfprOfOnevSZwHkVu+//77AZ8QqOC6ca5FgZfDgwfLqq68yBJ8FDl+JQB8h4NWE9+OPPzY3uxdeeMHM7PsIYx7WJgJ4SCA73tq1a6WoqKjdJUI7zWFJc9y4cUa/HRQU5JIHnHbPBP1GZh/EBUTWneuaOQhZbJBeERlnQHKDNaA3svHEK9FFbnTkP0eWHsQ2hKepIcR2TsIL62CM8/Ly5L333pNJkybJkiVLXDI2XggVT6kTCGAidfbsWfnFL34hTz/9tEklj4nU9evX5aOPPpJDhw6ZCTBiP0P25MpJFmQUILZIZW+Vqqoqo1PH5B0+JM8884xERUW59LjWsfhKBIiAPQRIeO3hxFq9gAA0dnhAwKEJ77tT8EBLTEw0lp6BAwe63GERFl1IHRA2BbpeEDkQYhQ9tJErgNzCkmulSLz1Lf8FOXjjjTeMcyGs8K4kH0TXNxFoTXgfeughE+EFk+cPPvjA3Acee+wxgbygpwsI8MaNGwUGl6SkJPnpT39q7kW8znsaebZPBDpGgIS3Y3z4bS8i0NTUJJChHDt2rFvWXavLsOQg9BVSS7d2YrHq8LX3ESDh7X3Mvf2IjoQXEjasHEAWBbI7ZswYY/XtSNaGCaud4oy0ImQiIgO9++67ZlXxxRdflEGDBrl8wm2nr6xDBIjA3Qh4NeHdsGGD0XVRw3v3oLvrJ8TcfeeddyQ3N9dYTLvbT3hjg+zi4edqz+zu9s2X9yfh9eXR75lzdyS8sOQiLvfrr79ufDe+973vCZzY2tPyYzUJDmfnz59vd2UJRBfWYZDn0NDQNk8C7UDH+4c//EHq6uoEx0W0GN572oSLG4lAryPg1YT3m2++Md6zq1atkuHDh3PptNcvr84dEIQXDwvo7uxaXDo6Ah5w48ePl5UrV5pkFB3V5Xe9gwDG9cKFC/Lpp5+ayciCBQv4u+wd6L36KI6ENzMzUxDpBU5qP/vZz0yYwo5IJ6yyO3fuNFZZrDK1VUB4R44cKT/+8Y9NvO+26uD+9etf/1rw3ImLizNps6FTh5NcTEwMr/O2QOM2ItCLCHg14WUc3l68klxwKDwwoO2EtcUVhBcyBsTkhdNIRw88F3SdTdhEAMSEiSdsgsVqthGwCO/Pf/5zsw9++9j20ksvyeLFiyU4OLhdwol6cJK9cuVKhxZeRH6BpRhOsG0VkGVM5NAWIsxgJaOiosJEIlm9erWxEDuTRLTVLrcRASLgGgRIeF2DI1txAQIgvGvWrDFLi64gvHjITZ8+XWBFJOF1wQC5oAkSXheAyCbuQcAivH/1V39lfuvz58+XU6dOmfevvPKKwOrb0T3Aut9Yr/ccQDeArHZEWLEvSC/6gj9kinzrrbdM/F9oeREbnL4EbSHLbUSgdxAg4e0dnHkUGwjAu3nLli2yd+/ebjut4cGEMECw7kB3xweNjQHohSrQOR4/fly2bdtmHApdnQSgF06Bh3BDBCzC+9d//deyaNEigY43JydHXnvtNeM0ZjmPtUVYsS9CihUWFrZ738F+0O6mpKTYlkfhfnbx4kVBn7DK9Pzzz9ve1w0hZpeIgMcj4LWEFzcxZN1BelqQnrFjx7brtODxo+glJ2A9tBBKCNbejqwtzk4ZDyh4Rz/88MNGc9fWg85ZG/ze9QiABOzevdto65cuXWp0jhwb1+Psay1a9w7E4YWTMsKSYXIFicEXX3xhJlcgwch21rrgXrNv3z4T/7sjDe+QIUMEMd2hx7VTcP+ClvhHP/qROf7LL79spBV29mUdIkAEXI+AVxPekydPGovhzJkzzdJ2e166roeVLXYFATwgkB5006ZNZhkQD6yuFixfIiQZHEZo3e0qiq7fj4TX9ZiyRTGW2bYST5SUlBhnNKwaISEFIra0jrKA+wwc3LKystrV8AJjOKLBCRZSKcfiODF3nLyBPCPLG3TFy5cvp4XXETS+JwJ9gIBXE15ouL7++msh4e2DK6uLh7SWATdv3ix4WMFy09kCsouYm0hqgMDvjg+hzrbF+q5FgITXtXiytVsIOFp4rUxrMHBgO6KC/Pu//7uUlpbK97//fZk8efI9k2CQXtR1JK+tsUV7bemAYcVFhkhIqECmLYc5SCRgYQaRhmWYGt7WiPIzEehdBLya8CK3OjLeIBbrrFmz7rnJ9S7UPJpdBBAm6ODBg0aSAm1dRw+h1m3iYZOcnCz333+/pKWlccxbA9THnx0JL8Zo1KhRnJD08Zh4w+FBVnG//7u/+zt58sknjaTBWtHDNQd5G2J8I5buq6++aibC1vfdPX8c14rOAI0viG9NTY1cunTJSBpAdJFa2NUpjbvbb+7vmQjceh7CgbLj/ptUKm3lU9H9HHe1W6/jo3nGt15LeHFRYNYNr3/ET8TDlUvbnnFRopd4YBw5csRo66qrq22RXjzAYNmdPXu2GXOOt/uNN5Z54bCG5WdY4BHmiRZ49xsnT+uRdb//5JNPTLKH1s6QuO62bt1qrK1IBoH4uG1Za7ty3mgbpBekOi8vz9y7cE3HxsYaCQSkVb5CdjG5wOQDBRjgHuyqiUVXxsbb9mlRXnOhuFJT1vvJwJhwCfL3a/MUqxuaBH9t8d0A3TcqJFCCA/ylrumGVNU3yg1tt3Xx0/GLDg2SEK3nLcVnCC8cZFx1g/OWwXfn88ADDNmK4OUMr35kX8ODBdvxZxXcVHFDxQNlxIgR5gHTGU9qqx2+9g4CcBBat26dmYwiNikyYrEQAVcggPsCCBfuB60nu/gORAx/+B5/rpxooV3IIhB/F3/WPQl6X/TFlcdyBVY90Qbuz59//rm5V1vtI2Yx4hcnJCTI6NGjZeDAgT2CB8YXErjs7GwZNmyYOR7GoDPFeq6461g132iRA3lFsjkrX5IjQ+W5ycMlIjjwnlOsa2qWjefyJaekSgnvt89Kq2KQXo/T0hNlYmq8bNJ6l0oqpfn2JMWqg1d/fbZOGJAg09ISJCwowPErj33v1YQXGbvee+89Y/V79NFHSXg97DK1HlK4kSEZBQLDQxcHiy8eLoGBgWaJEgQX8gWQJ9xcO3uj8zBYPLq7kKvgN1lfXy/PPfecmah49Amx80SgFQLuTpxadddlH/Gb/uUvf2ms3VhVhYHJmgRgxS4xMVGWLVtmfGpaO/51txOYcMBn54MPPjCReeBciMkPjh8WFnbPBKj18bA/EobgD323kotgLHHPApm3007rdl31GZbdK+U1subYJSmsrpNFI1Jl0fBUCWzDwgvC+9ahC5JbCsJ7q2D/msZmadLzhMV2ie5/n/59ciJXzl0vv2Ph1WpSr7g1NLeYtqekxcuDowdJTKgmbnHVyfRhO15NePEDRKpahJF5/PHH7/Gu7UPceehOIoAbF6yDuPHgvWWpwU0VNyf8uevMvJOn6tXVYf16++23zaQEcUld/eDzavB4ckTAjRHA8xZh4ZBdDimdseoGwojt58+fN0k4YJD4yU9+YkJGuvJ+jecBJHBwTkRYOoQh3abSKRhI8OyHvKSj4+HZAkdpkOYf/vCHd0LPYfuOHTvk9OnTRhveV07QtUpit2RflU1q3R0SF2Gsu4kRoW2SUJDbivomaVKLsA6AtCiJrVDZwqcncyS/olaGxUfJw2MHyaDYCJU9gNzeuLVyqtdWlcogvs7Ol3NFlRKrsodHxw2WjOQYgQzCG4rXEl4MDsjRxx9/bGZoCAuDi5WFCBCBvkMAyQDwYIHWGjIjWOlZiAAR8HwELMKLSS2cB+G8hwLSC3kaHPvgU4NseNBYO0o9LKs46lvE1HGb43a8t4pVxyK8SE0PwgsL79GjR022O8ThR1/QrlXf2t96Rd+RFQ/SuZ/+9KcmBB3qg/CeOHHCEHaEtENoOjv9s47T1jHb2t/aZvXH8RUE9mxhhVpjcwxxfWriUBnbP1b1u5DKONa8973uKiW19bLx7BU5frVUEiKCZXlGmmS2IrGwBNeqBXjXpQLZrIQ3Qu/Li0akyIz05HZ1wvcezf23eDXhxZIGYrpilrdw4UJJT0+/c7G6/9Cwh0TA+xCAFebAgQMyYcIE89Cjrt77xphn5JsItEd4gQZW5eA0+Nvf/lZ+/OMfy7Rp00wECxBISAWQhrmqqspYYpEcBIQRMdkRHxmSAsghIF2zpAZoE2QUMjfUA+HFcx5JRiCVQmQmtId9YejCfqgDeRz2gTEMx0a7kMBhG+QQCF2HlSd8h+NhBQqRgtAWov9YK1I4dkFBgTkm2kXd1NRUQ+LxGVZu4IHVZTjP4/xCQkLMRB9to2B/9B2Tf0QPaa/A6rr2ZK4czCuWqSoxWDpqoFwuqzZW15GJ0RLegb4Wlt0dFwtk+4UCiVS97wOZA2WS6nL9/e5myvVq5UX7G87kGUc21Js3tL9XOawB3/8fAAD//5YBh88AADyiSURBVO2dB3hc1bmuf2vUe5dlyZbcG3LD2MbYGGODHVNMhxNIIKGl8BBOknvz8JwkJwknOSfnJDmQnHsvBJ5QkkDoxWBj4wI27t24d9myZcnqfaSRuP+3zDZCljR7pBl5yrdhPDN7dln7XXtrf/tf3/pXv891kiCd2traZM+ePbJhwwYZN26cTJkyRfr16xekR8vDIgH/J7Bt2zbZsmWLTJgwQSZNmiTh4eH+X2iWkARIwC2BpqYm+eUvfykNDQ3yxBNPSGJi4vl1nE6nvP/++/L3v/9dHn/8cUlPT5e//e1vMnPmTMF6n376qUCKzJ8/X2bMmCF79+6VxYsXy4kTJ8w9OyoqSm644Qa59NJLJTY2VioqKmTFihWyadMmwX0eU319vVRVVckjjzwil1xyiaxevVqKi4vl1ltvNWU5ePCgvPvuu1JUVCTR0dESEREhQ4YMkWHDhsnatWuNVmhpaZGcnByJj4+X++67T/Lz82XdunWmPNhOdna2VFdXy8cff2zmY59YJy0tTS6//HKZO3euOBwOczw7d+4020IZcYw4PuwL2xkwYIA8++yzcuzYMZk3b55cffXVEhYWdp6X9aGltU12nq6Qd3Yfl9iIcLl9/GBJiY2SZ9bvF4dqmdsnDJahaV9yttbDe7Ou++nRM7L88GmpamiWMVlJMn9krgxKTZAIR5hYSgj7+Ky4Qt7dUyildU2SERctCy/JkzGZyRIdGX5+ufbbDtTP/YJZ8OIEKywslKVLl8qIESNk1qxZnZ5UgVp5LDcJBBqBlStXyuHDh2XOnDkyePBgXo+BVoEsLwl0QcASvBCEP/nJTyQhIcGIPIhdCNfXX3/diNPHHnvMCMBf/epXRvhBsEJgQgxOmzZNYmJi5KWXXpKamhqZPXu2+Q3i9cyZM/Kd73zHiNRVq1bJe++9ZwTw2LFjzd8RPEhDnD788MMyevRoefvtt81+sQ7K8Nxzz5n9XnnllZKbmyuNjY1GrA4aNEhOnTolixYtkrq6OrnlllskLi7ObAPC+IMPPpDNmzfLd7/7XSNg8TcM2x45cqRMnjzZPLRDsB89elTuvPNOM2/JkiVGXGdkZJiHexwb/u4tX75cbrrpJrn++usFyxw5ckSuuuoqcxwdg3GIRBbXNMirO47KGX2/YnCWXDsiV+pbXPLH1bvNg8AdKnjHZKVcUCNYt6rBKS9sPiSFlXXSqlooXsVrZkKMjM1Klql5mZIUHWnWK9flluw7KVtOlkmbLgcxnBEfrdtN1n32l9SYyKAJFAa14EVt4gT+61//KllZWbJw4ULz9GVqmf+QAAn0OYFXXnnF3MjuvfdeE2Xp8wJwhyRAAj4hAMH7i1/8Qg4cOCCXXXaZREZGGqEJoXry5ElJSkqSu+++24hYiENEehGUuuOOO+S6664zkVuIXwjMN954w4hbtMoi8oko6TPPPGOio1OnTpWnn37arItoLqLFWG/79u3y4osvGtEJMWoJ3gcffFA2btwo77zzjnzrW9/qNJoK8YuIa0lJifz4xz+WlJQUI/IwH5Fp7P973/uemY/oNcQploOuwDHgeFA+iPxvfvObZn+IUCNiDQENFhD9Tz75pPTv318eeughI+RRETg+RIU7Tq1tn8uyA0Wy4tBpyU2Kk9vG50t2YpxUNjrlj2v2mMXvmDBEBSzK2nFtkSYVxh8fUfaV9Ubwnq1rlIrGJnHpdheMGijzRuVKmK7odJ2L8G4/VW4Eb62zRU5V6zq63PVjBsqVQ/pLtEaXg2EKesGLE/att94ydYWnKlx0nEiABPqeQHNzs7z22mvicrnk61//urkJ9H0puEcSIAFfELAE7/79+wVRV9iV8IIFAZFOiGC06iBqClH805/+VCZOnCgQpBCtmGpra+WFF16Qzz77zERBMR+WAVgTIDxhgbjmmmvk97//vbEQ3H777WZ7luDFuoiythe8ELnQAGjt/fnPfy6ZmZkXHD5sGBC8paWl8qMf/UhSU1PNMh0FL44RYhvHAqEOIYsJgTUI9WXLlhlhfOjQIWPdwr7BAlNZWZkJviECjig0hG/HqK5ZUP9BpLVQheprGt2F+ByeniiTctPFEdZP6ptdslSFMKbLNVI7Q6Ow6WpD6Ch6dRPSog8CEK74r7apRRbtOWHsC6PUrgA7RJquh8mly7laz7lb8XntsRKzj8SoCPn2lBEyKCW+y7KaDQTIP0EveHGC4kkP7wsWLDBPZAFSNywmCQQNAURBEOnBTQE+uGuvvdZ46ILmAHkgJBDiBHCPhYcXVgTL0gBBiBeimHhZAg+CF8siAoqHXwhj/I2A4ISgxO95eXlmHTwg4zesD6EJAfnrX//aRHsRxMK63QleeHEhhLHMz372M2OZ6FhVdgQvLA3w/0JPQEvAd2tFZlFG2Cxgm3j00UdNRBv9Fe6//34jvrE/cMG6EMMQvLBVWDw6lgcide1xFZ37i6RWAwWiTlp4djEZIau+W9gUItV+gCjvtLwMCVc+3U0Q0YfLauTpdfuMneHey0ZInhGyX10LsvdQabW8vvOYenob5eHpowQCGdHgQJ+CXvDi6RD+GnRew012+PDhXZ5kgV6ZLD8J+CsB3LDQQQMREERfEKnBjYoTCZBAcBCwBC/EI/y56LTWlaCDoIU1AFYGREohHPE34uzZs/LnP//ZdEC7+eabTeQX27Be8PoiUvoLtU7cdtttcuONN5oHZ3eCF1YHlA/rwZ/bcbIjeGFpQKQZrVSIMqOzmfU3DK1X8Oe++uqr8v3vf99YHDoKXkSvYatABBy+4u4EL1iUaAeynWozqFGLgQZoz09NKq636XyXit6x/VNkweiBkpscZ0uQHimvkf/z6V6JVT/vtzRyO0Q7sHVWR8fKa+XNz44ZO8T900bKJbofCt7zVeC/H1pbW2Xr1q2mFyY6yhQUFHRawf57BCwZCQQ+AdyQ0Gt5zZo1gk4jyJqCiA0nEiCB4CDQXvB2zNLQ8QiRMQGiGBFa2BKsSCmsAcjesGvXLtNJbMyYMRf8nTh+/Lj85je/MR297rnnHiNg0SntY82cgI5xsBG0tzQgygo7BP7+wK6A3zqKPJT9+eefN5HZH/zgB+dbgmFpQKuU5eHFcTz11FPGmoHtWuIZ3l/sG17lb3/727Jjxw5xJ3jR0oW/ixDNnf0tRES2Qe0L8Nyen/RjVZNTnt1wwGjgmzSbwqTcNGNHqHE2a8TXoWLWIdWN+jncIVH6Ohf5PbetDYWl8v7ek5KXGi93aWQYm46NwDrhprMaIskQ0luLykxmCH3UkIcuHyVD0hIoeM9Xgh9/sG60ePqaPn266QVqXVx+XGwWjQSCigBaWtDkh5vOXXfdJQMHDgyq4+PBkECoE/CG4EWACi2ysCAgdSEyGljpzRBFhcBENBa2B6QEg+iE3xYdwpDyDJ3HfvjDH8qoUaPOd1qDfcDqVIaUZ0hvhs5l2Bf0AaLGEJxIWYbObfj7hKxO+B1iFFmeLMEL/y9sC0ib9o1vfMNkckA0Flkc0NEONgfYNKA3EGhrb2loH+FFmbBvCOWhQ4eatGYdRXhX51OFZlV4ql2WhhEZybL15Fn5UO0P+RqxnTooXVYcLpZkza6AlGWpmsZMiyjHKmq0A1yxenrbZNawbJmp3t/nNh4w6c7GD0g95wPWnSJrwzr18BbXNpgIMlKUwesb+IYGlfNaWe0eH7pCHNjz8dSFFCC4ySI1GUz0nEiABPqGAP7EID8mbhx4R1MlejdzIgESCB4CELyI2kKI2onwWpaG9hFe/K1AlBfRUrQG4V6dn59vxCeirRCT6OgGUfzyyy+bznAQwegfgLy66BD2wAMPnI/w4t4P7y3y+MJOgPRm6EAHOwGEM/zFiDIjbRhShEFIY0LnOghQWC6Q7gwvWBqgIWDHgLiG/QIpzVBm7AeRY3SYQ05eRJSxDgQvxDMmCF6I6n379pn5//jHP0yqMitNWWdRXrNih38swRuu/t07NUqLDA7/rQIYndvQeQ3py5CK7FBZtYn8QqhC5KGcERoBnpiTKlcNGyBxGtV9a9dxOaw2B0SS2wta2BeGqFier5kc8jUaHAx2BmAMCcGLJNVo7sAJDt8Nen7afZoCJE4kQAI9J4A/tIhkoGkQNxs8dDJbSs95ck0S8EcC6LiFPLiIxMKjD5HZ1VRZWWmWRXSzo8UAfy/wYAxbA3LXQkhD+GKwBlih8LCMezlaiyA+0XoEewCEJYQv+ukg6gvbBDqKYYAb5PaF4EQkFn0JsA4ix+gYB5sj/h4hootILqK3EJ/4DeIaHemwXXzGOjjO06dPmzRo+LuGKDCE8Pjx400GCGwH4hl+X8xDijNM4ILjKS8vN/tE+dEJDuVDRNquJmlqaZUtRWeNVQEe3hhNGbbzdLkc0Q5p2YmxKmjTBKnFjlXUmhRmDU6XydQQowI3S/PwogNaYnSE2V+Vpjg7Vl6nndMapE6Xg50BVoh0zcOLzBBYPljELuogJAQvTlCcXBiZBU9TeJqz+zQFSJxIgAR6TgBNd9YoR+g4ilGQEI3hRAIkEDwEIFStF+6v3Qk4azkcfWf3YvyOvxsQu7h/4+8ForGwI2K77X/HchDXEJ6Yb23P2odVFnyHGMU28d4+g4S1TcyHHxgTtml1prO2ax0T9olyYVls31q2/e/YBr5b8/Ad61nbgujGC2naPLFZWtHa89vXD8jq0Kh5dzFoBAQrQrpIRYZR1DDiGvZpflN+ELD6v5msbSEVWbPm44Vv2IHjCdfcwB3Kfm6NwP43JAQvKhtPVmhmQNJqvDw5wQK7ill6Eri4BNAUicgJEsMjCTuiIe1vAhe3dNw7CZAACZBAKBAIGcFr+XhhOodnhxGmUDi9eYz+QADNl+jEAW8fPHG0FPlDrbAMJEACJBBaBEJC8KJK4fn56KOPjBfnPk1E3Z2/KLROAR4tCfiWALxsb775pukogtSA6CHNiQRIgARIgAT6kkDICF54c2CoR7MqeoUi0ktbQ1+eatxXKBKAZw0dSz788EPTkQUdNCyPXSjy4DGTAAmQAAlcHAIhI3hx40UyaIhe3HTxglmcEwmQgO8IoGfy2rVrjejF6ERI90PB6zve3DIJkAAJkEDnBEJG8KLjGoYkXLx4senNiSEJ2bTa+UnBuSTgLQKwEiH3JPy7X/va10xqIXZY8xZdbocESIAESMAugZARvACCNCLvvfeeycOHIQmZ/N7uacLlSMBzAnjIRL5KJHxHond0FqWNyHOOXIMESIAESKD3BEJK8MLWgPGtkY937ty5JpE0m1d7fxJxCyTQGQE8YFrpyC699FIzrHdny3EeCZAACZAACfiaQEgJXkScCgsLBUP6YXQXjKmNZNWcSIAEvE8A+XctO8PChQtNOjLv74VbJAESIAESIAH3BEJO8GKoQQxAAfGLUZ+QrYETCZCAdwng+jp06JAsW7bMPFxeddVVzH3tXcTcGgmQAAmQgAcEQkrwggvSkyHKixsxhjidPn06e417cMJwURKwQwCC95NPPjFpAK+44gqBpYH+XTvkuAwJkAAJkIAvCISc4AVENLW+9NJLJkvDggULJDk52RdsuU0SCFkCyMrwwQcfSF1dndx8882SkpISsix44CRAAiRAAhefQEgKXnSmWbp0qRQVFcm8efMkLy9PmCrp4p+MLEFwEEDn0L1798rKlStl9OjRMmPGDImJiQmOg+NRkAAJkAAJBCSBkBS8aG49ePCguSEXFBTI5MmTOQhFQJ6+LLQ/EmhqajJ2hl27dpnoLgaboJ3BH2uKZSIBEiCB0CEQkoIX1YvoLny8iPZiEArk5GWUN3ROfB6pbwjgYbKiosLYGRDVRQtKYmKib3bGrZIACZAACZCATQIhK3jhMdy4caN53XLLLTJ8+HB2XrN50nAxEuiKAB4g0XqCAV7mz58vaEFhdLcrWpxPAiRAAiTQVwRCVvAiEoVsDUuWLDFid+bMmRIVFdVX3LkfEghKAtXV1bJ69Wo5ceKEoENofn4+W06CsqZ5UCRAAiQQWARCVvCimhDlheAtKyszg1BkZ2czyhtY5y9L62cEiouLZdGiRZKRkSHXXHONxMXFUfD6WR2xOCRAAiQQigRCWvCi+XXdunWydetW05scyfGjo6ND8TzgMZNArwkgxzVsQrie5syZI6NGjeIDZK+pcgMkQAIkQALeIBDSghfpk0pLS00HG/Qsv/7662XQoEGMSHnjzOI2QooALEJoMXnjjTeMNejWW2+VyMjIkGLAgyUBEiABEvBfAiEteFEtiErBc4io1Pjx401kKiwszH9rjCUjAT8kgNaSTz/9VI4cOSIYWQ3RXU4kQAIkQAIk4C8EQl7wIjIF3+G7775r/IY33HADR4Xyl7OT5QgIAriGzp49K2+++aZJ74c0f+Hh4QFRdhaSBEiABEggNAiEvOBFNTudTtm0aZOJ8k6bNk2mTp1KW0NonP88Si8QgDVo7dq1sn79esH1g5HV2EriBbDcBAmQAAmQgNcIUPAqSkSoSkpKZPHixabTGvKHpqSkUPR67TTjhoKVAK6dM2fOyEcffWS8uxC7AwYM4LUTrBXO4yIBEiCBACVAwftFxTU3N8vmzZuND/G6664zWRuYMD9Az2oWu88IwLuL1hH4dydNmmQivPHx8X22f+6IBEiABEiABOwQoOD9ghIiVadPnzbDDWNIVCTNT0hIYKTKzlnEZbol8Ln16/kPOqPfuZlfvFlLBNQ7rpnKykrjf8egLRhGODU1lddMQNUiC0sCJEACoUGAgrddPSM1GaK827dvNynK8vLyOCxqOz78aJ9Am4pBl3pba50ufbVIY4tLXK1t0qqi19Gvn0Q4+klMRLgkREWYV7hmBtHZATXB+75nzx6TyxrRXbyYxzqgqpCFJQESIIGQIUDB266qrSjvBx98IBh1bfbs2Rwpqh0ffnRPAOdQfbNLTtU0yGl91angbXK1SouKXYjgNhW8YSpsHfpPhCNMoh0OiVfRm5MUa17R4Q793f+VL44TmRlef/11QYsIRlXLzc1ldNf9KcIlSIAESIAELgIBCt4O0BG1Qo/z/fv3y2WXXWZy8zKBfgdI/HoBAYhZCN0ztY1SWFknlY3N0tTSqhHd9j6GC1YzzgYI3JgIh6TGRkleSrxkJcRInH7v58fCF/mrN2zYYDIzIKsJrhVGdy+sX84hARIgARLwDwIUvB3qwYpcLVq0yGRvuPbaa2XgwIF+LT46HAK/9jEBiN3qpmbZV1ItxypqxaXf3ejcTksIfRvRL0yGpifIqMwkY3Xwx2gvrpG6ujozqho+I3d1eno6r5FOa5UzSYAESIAE/IEABW8ntYCbOLyJq1atkmHDhhlrA6NXnYDiLBPBLdGo7u4zVYJ3dxFdO8jC1e7QPzFGxvVPkfS4aL+zONTW1hrf7smTJ2XWrFkyZMgQet3tVCyXIQESIAESuGgEKHi7QN/Q0CDLly8X3NTnzJkjI0eOZASrC1ahOhuR3WIVuTtPV0h5vdMrYtdiCY9vpordCTmpkuFHohdWBgzDjTRkELrXX389R1WzKo3vJEACJEACfkuAgreLqsHoUYWFhbJkyRIzXOrVV18tycnJFL1d8Aq12ap1paLRKVuLyr0W2e3IENkcBmhntkk5aZIcE2llMuu4WJ99R8tHWVmZLF261ER0Z86cKTk5Obwm+qwGuCMSIAESIIGeEqDg7YZcfX29Saq/c+dOmTJliumYExER0c0a/CkUCKAbmlMzL+w4VSGHy2s0/Vj3HdN6wyRCI70jM5NlXHaKRGpWh4s5WWn7MNAExO64cePYUe1iVgj3TQIkQAIkYJsABW83qBDRQuoleHkhfm+++WZJSkqSMM2Zyil0CSC/7qnqBtlQeFYaVfjanXA+oTcb5LHpjGYzC0NCZIRMzUuX7MRYk8PX7v68uRysDBiY5ZNPPhFkLUEaMrZ4eJMwt0UCJEACJOBLAhS8bujC2nDw4EEjeuFZRGQrNjbWzVr8OZgJVDe1yLrCEimtbTLi1e6xOhsb5dDOrdLW1irDx18qMXFxuqr7nLsQx/01VdkV+ZkSFxlud3deXQ4d1ZCfuri4WGDvKSgo4IOfVwlzYyRAAiRAAr4kQMHrhi6icmjKXbZsmRw4cEDmzp1rcvM6dMAATqFHAB3VjmrqsY0nysxgEnYJfK4PTkf37pSX/v1fpWD6lXLdfd/9QvDa2wIGpJg2KEPyUuNtSGR727S7FKK7+/btk5UrVxqhi5y7cSrW/TlPsN1j43IkQAIkQAKhQYCC10Y9I8p7+PBhWbx4sck3iqwNWVlZjHDZYBdsi7ToubBJxe6R8lozcprd42usr5P3n39aNq9YIvf//N81wjvZo/MHWRuGpyfKZbnpZpQ2u/v1xnJ40MNgLHj4u+mmmyQ1NZVi1xtguQ0SIAESIIE+I0DBaxM1RmDbvHmzbNmyxQyhikgv/LyMctkEGCSLVWpmhvXq3S2ta7J9RIju7lq3Wpa+/BcZMfEymff1+yQ6FhFS+15w2H2z4mNkel6mJERH9EmUFwIX5z2sDKdOnZJp06bJpEmTmIbMds1zQRIgARIgAX8hQMHrQU0gNy867Rw7dsxkbEAv9aioKA+2wEUDnQBGUtuu2RlqnC22DgWisbrsrLz+P7+TpoY6uft//VwFY4R+rpfkjEyJjIq2tR04fROjI+WygWmSkwixbGu1Hi9kiV3k3MWD3uTJk+XSSy81WRn4kNdjrFyRBEiABEjgIhGg4PUQfGlpqfEyVlRUmJ7qQ4cO9ahp2sPdcXE/I/DZmUrZo6OqNdnMztDS7JRNyxfLspdfkKtvu1umL1gou9Z+LId3bdPv90j6gFzbrQQxEQ6ZMCDVWBt8PeQwfOvw7a5Zs8ZYGObPn2/sPH5WHSwOCZAACZAACdgiQMFrC9OXC7W0tBg/74oVKyQ+Pl7mzZsnmZmZHFr1S0RB/WlL0VnZX2ov9y6sDKePH5FXn/oPSU7P1I5q35HUrGz55J1/yI41K+WuHzwuOUNH2Ba8GHJ4fHaqjO2f7NPhhhHdxYPdW2+9ZeoSKcjy8/NpZQjqM5sHRwIkQALBTYCC18P6hRhA9Ate3l27dsnAgQNl1qxZkpiYaFu4eLhLLu4nBJA/d2NhqRwsq9EOa+4L1aQWmI/ffkU2frhIbnzgERk7bYaeI2Fm3o7VKnj/+XHJHWp/yGrVu2YAinEqen0V4cX5jZzTH3/8sZw4ccKk4RsxYoTJvUsrg/s65xIkQAIkQAL+SYCCtwf1AlGAVE0YkAKjsEHwTpgwQTgKWw9gBtAq0LgbTpTKobPuBS/OkbLiU/LMv/yzJKVnyHX3PiSxiUnSplaIjcsWyZ7N6+XG+x+RkRMnS1SMvbzOELwFOuIaory+ErzwqcO3i1deXp4sXLiQlp0AOkdZVBIgARIggc4JUPB2zsXtXAgajDy1ZMkSk65p+vTpgkgYRa9bdAG9wJaiMrU0VLsdTrhNH4iOH9gtr/z+19KkEdMoHawEEVKcNzXlZVJXXSUDBg+Tr//4ZzJk7DhbTMJ1hL/xA1JkbJZvLA0o27Zt20zHzP79+5uc07DrcCIBEiABEiCBQCdAwduLGnS5XHLkyBHTiQ2J+K+99lrj5+XQw72A6uer7tZOa7ttdFqDeKyrrpTjez8TV0vz+aNq1Qjv7g2r5cjuXTLr5jtlytwFkpiadv737j6g09pE7bQ2TPPxejvCi1zT8O0uX77ctF5cccUVgpEFeS53VyP8jQRIgARIIFAIUPD2sqbQiW337t2yfv16SUtLM5kbUlJS6OftJVd/Xf040pKdrhAML+xuguj9/PM2aT/+sEvPl4/RaQ0e3scel4HD1MOrkVt3E7KQJWlasskD0zUtGaLF7taw/zvKCd8uLDoYRvuGG24wYjc8/OIMY2y/5FySBEiABEiABOwRoOC1x6nbpaqqqmTDhg2yZ88eM+wwhl5lJ7ZukQXsj5WNzTqs8Fk5U9vYo2NAtPeTt1+V7cjSoII3Z8hwWw9HELj9E2Lkcgw8EeW9gScgdsvLy41nF2IXtpwZM2aYoYN7dIBciQRIgARIgAT8kAAFrxcqBc3BEL0YlAI92wsKCmTKlClGNLBnuxcA+9EmXFrXm1TwHjZDC3tesLZWl2zX6O6hnVtlzh33SEZ2rtgJ1/piaGGI3draWtM6gc6Xo0aNYsYRz6uUa5AACZAACQQAAQpeL1USxENNTY18+OGHUlRUZCK9U6dOlYSEBC/tgZvxBwKo56MVdZqt4ay0tKpdoY+m6HCHDiucIQNT4r02rDAe0tatWycHDhwQdFKbPXu2ZGVl2Yo499FhczckQAIkQAIk4BUCFLxewXhuIxBDJ0+elLVr18qZM2dMlBfDsWL4YUZ6vQj6Im+qRv276zU92ZnaJpN1wdfFQXQ3W+0M09XOEBvZe18tzlOI3Y0bNxobDgaVQCc1ZGRgJzVf1ya3TwIkQAIkcDEIUPB6mTry8xYXF8unn35qer0jXRksDpGRkRS9XmZ9sTbn0lEnztQ0yFodhKKppbV9nzSfFAme3cs1ugsPb2+zM0DsOp1O2bx5s3lh4BSco9nZ2RS7Pqk9bpQESIAESMAfCFDw+qAWIHqRo3fNmjVSVlYmV155pYwZM8aIXh/sjpvsYwIYgKLZ1SY7T5frqGu1mpPXd9aGCEeYjM5Ikkt0wIlI/dzbCaMEWllFYF/AuZmRkcGhsXsLluuTAAmQAAn4NQEKXh9VD0TvqVOnZMWKFYLRqxYsWGBGrmKTsY+A9/FmNVAqlY1Ok6LstEZ7W+2MNexhGR2ammFgSpwZWS05RlsIPFy/4+LoXHn48GFZtGiRieheddVVxrvLc7IjKX4nARIgARIINgIUvD6sUQiMQ4cOmewN8PFOnjxZhg0bZjy9PtwtN91HBCB6S+oaVfSWy9k6p7RhhpcmiN2shGiZmJMmabFRvbYyILK7f/9+M5IackfPmTNHhg4dSpuNl+qLmyEBEiABEvBvAhS8Pq4fRHqPHj1qRC8ivfBLXnLJJezI5mPufbV5RHbLG5pkV3GlFGtuXm9EesO1kxoGlyjITpVUjeyG6feeTvDsNjY2yo4dO0wnteTkZCN2BwwYIBxYoqdUuR4JkAAJkECgEaDg7YMaQ0StsLBQtmzZYpL8w8+LPL0xMdoJycYoW31QRO6iFwQQ2a1scJrcvCeq6k1HttYeRHuRjSFG04/la+qxIWkJAhtDbzqpQewiVR4GRNm6datA5CJryKBBg3je9aK+uSoJkAAJkEDgEaDg7aM6g70B2Rvg6S0tLZVJkyYJRmSLj9e8qhhGi1NAE4CZwaV5eU9VN8i+0iqp0tRlyNML0YmpM7ODqXX9J0zdueHaIS01NlLGZCabbAz43puzAvutq6szeXY/++wzycnJkblz55rhr/mQZaqE/5AACZAACYQQAQrePqxsqyMb8vQiXy+ibfD1JiUl9WEpuCtfEkC0t77ZJaXq7S3RPL21zhbz3anWFtgd8DuitrAtRGk0F3l1k6IiJVP9upnx0RITES7w7/Z2qqiokE2bNsm+ffskNzfX5NnF4BIUu70ly/VJgARIgAQCkQAFbx/XGiK9GIkNwxBXVlaadGUYkY2R3j6uCB/vDoHdVq3r2uYWqXO6pNHl0gjwl4I3wgH7QrjER+EVIQ61tvRe5mokWXdcXV0tGzZsMGIXkd0ZM2aYrAxsSfBxpXPzJEACJEACfkuAgvciVA0ivcjPu2rVKpOvd9y4ccbTi2GIKUouQoX4cJfGyqAitGtLQz+vCF0cAh6m8BC1fPly81CF82rChAm0MfiwfrlpEiABEiCBwCBAwXuR6gniBMMPb9++3WRxQEcieHrRix6d2Sh8L1LFBOBuEdVtbm6WI0eOmA5q8IqPHDny/PlEG0MAViqLTAIkQAIk4FUCFLxexenZxiBUqqqqZPXq1XL8+HFB5HfmzJlmKGLk7aXo9YxnKC6Ncwhpx44dO2ZsMjiH0CES0V3aZELxjOAxkwAJkAAJdEaAgrczKn04D4IFIgUjYEH4IlIHsYLObIz09mFFBOCuLGsM0o7t3LnTtA5MnDjRnD+M6gZghbLIJEACJEACPiNAwesztJ5tGLl6T5w4YXL1okk6Ly9PRo8eLUOGDJHIyEjPNsalg54AHoxwnqxZs0Zqa2tNjl08KMEa43A4gv74eYAkQAIkQAIk4AkBCl5PaPl4WUTskDv14MGDJn8qhC562EP4clQsH8MPkM1bLQLIrYv0di7N/oBBTCB20SJAsRsgFclikgAJkAAJ9CkBCt4+xe1+ZxA0TU1NsnfvXtOhDZFfDEWM0dmQr5fC1z3DYFzCEroYtGTXrl2mNQAeb3R0RAe16Ohoer6DseJ5TCRAAiRAAl4hQMHrFYze3YglbjAcMQYPgMhJS0szgwcMHDjQRPHYoc27zP15azgfEMlFx0YMEXz69GkjcuHXzczM5EOQP1cey0YCJEACJOAXBCh4/aIaui5EfX29bNu2TTZv3mwivLA3DBs2TDIyMhjR6xpbUP2CTB6wMOA8gO0FuXUxSh9H6AuqaubBkAAJkAAJ+JAABa8P4Xpj04juNTQ0yKlTp+TAgQPmNXjwYBk/frwZPSs2NpbC1xug/WwbqHf4uZFbF55udGjMzs42KeuGDx9OC4Of1ReLQwIkQAIk4N8EKHj9u35M6SB+8EKkD5Fe5FyFpQGZHNBZKTU1VZi3NwAq0mYRkYHh7NmzJqILoYvBSBDZh50Fn+HjpqXFJkwuRgIkQAIkQAJKgII3gE4DiF6IIXh6IXwR9UWv/IKCAtPMHRcXJ8y/GkAV2qGo6KAICwuiuuvXrze+XaSlmzp1qvFws8NiB2D8SgIkQAIkQAI2CVDw2gTlT4thWOKamhqThxWdmCCAx44da7y9OTk5jPb6U2XZKAseZDBa2qFDh8zQwIju9u/fX0aMGCFDhw41I6bxQcYGSC5CAiRAAiRAAl0QoODtAkwgzIZQQpQXo7Tt379fECGEQEKaKvh8KZL8vxat0dJ2794teCFKD/sCHmDQKY3WBf+vQ5aQBEiABEjA/wlQ8Pp/HXVbQohepKw6efKkGV4W4hcd2dDBCT35EfGF7YHCqVuMffoj6gxCF/5c2BeQfg4R3vz8/POdEenT7dMq4c5IgARIgASCnAAFb5BUMEQvbA7o0V9eXm46tmHkLQhfDFqRnp5uhDCjvhevwiFy4dEtKSmRffv2yZkzZ8zDCOoG9oUBAwYY+wJHS7t4dcQ9kwAJkAAJBCcBCt4gqldEDvFCGjMIKtgdkNEBQgvZHPBCL38MWUzh2zcVb0VzUSfV1dVmlDTYT8AfDyMYPAId0xjR7Zv64F5IgARIgARCkwAFbxDWO0QWOrYhjVlxcbHpCIXor+Xxhc83NzfXdG7D4dPu4P2TwKqDiooKY1lA5B2RXQhbpBdDRHfQoEGCKDwjut7nzy2SAAmQAAmQQHsCFLztaQTZZ0t0lZWVmaZ0DE0Ljy/mQ2yhYxs6RqFJHaKLUd/enwB40EBEHdHco0ePGt4YChhRdXh0MUoeBG98fDwfNHqPm1sgARIgARIgAVsEKHhtYQrshSBw8UKEF0IM0cYdO3YYYRYREWGE2KhRo0wqrOjoaCPEGPW1X+fWgwVyJOOBAtkWwBl+XTxEwLKAXMmZmZnGRw225GufL5ckARIgARIggd4SoODtLcEAWx/iDBkB0LEN4gw5fDGELcQwmtetpnZkekhISGCGhy7q1xK5YIfOZ3ghfy6i6ehACHELlhkZGSaai3RjHDiiC5icTQIkQAIkQAI+JkDB62PA/rx5NL2jCR5N7ps2bTIeU4g1RCWRzgy5YGF3QNQXAtiKSlrv/nxs3i4bBC4mvDc1NZmMGPBFY7CIAwcOmPkpKSkmSo5sC8OHDzcPELSJeLsmuD0SIAESIAES8JwABa/nzIJyDQhfZHWwLA+IACNaiRG/IOTg+cU7XomJiSFje4DAxQtcKisrzQtRceTPra2tNRFcMMrKyjLWhdTU1PMPBkF5ovCgSIAESIAESCAACVDwBmCl+arIlriD9xTRS8v2gOwCiGqiWR42B7zQVG+9LN9voA9wgeO3Op3heGFRgLiF8AcP2BeQXgwdztABDTzy8vJMFBxeaHpzfXVmcrskQAIkQAIk0DsCFLy94xeUa0P4YcI7RB7EH9Kb4QURDBGIpnqIPGQfQJoziGDYIJD1wcop68+ZHyBsrResHThWpHHD8SJPLjy56ISG3/BKS0szAh/DNiPCDdGL47OEblCeCDwoEiABEiABEggSAhS8QVKRvj4MdGrDCx5fNOVjOFy8I/qLTnAQj/gNghfN+4j6wv9rCeD2UeCoqKjzuWe78wN391v747UEevt5+GzNh2BF2SBg8cJnWBQQrYWAh1UBUW0Ie0wQsujABxGPFGKI4kLY4oXjoC/XYOI/JEACJEACJBAwBCh4A6aq/KegEJJWVBTi8cSJE0ZAFhUVGdEIIYnfISAhMBENRdYCCEWIXXyGoIR4RDQY89qLWwhOzLMmLGv9jn07nU4jsK3fO86zRC3KgGUhZlEmRHAhbvEZ4h3lwQuiHBP8yfiMNGKI6OI3lAUvTiRAAiRAAiRAAoFLgII3cOvO70oO4YloL0YXg9CEwITYhE0AkVREgSFG8Rnv8MDCEoF3S9DioBBJbZ8VApFWiE9M2Aa2iXdrsuZB4GKCoMX28R3LQlhDNMOKgKGV8R2CFqIa+0KnM2wfy6A8nEiABEiABEiABIKLAAVvcNXnRT8aiF5rwme8ID4tkQoRaglS2AoQBcZ7+/UQfYVdwpqHqKwlZq2IbPuoK+ZBzGIehDMiylaEGO+I3ELIWlFliNz2wtYS29a7VX6+kwAJkAAJkAAJBAcBCt7gqEe/PwpLvFoFtYQworMQuNaE+ZgHq4S1DgSz9RmiFIK1vThtPw+fYZOwIsIQwVYnOmsf7de15vGdBEiABEiABEggeAlQ8AZv3fLISIAESIAESIAESIAElAAFL08DEiABEiABEiABEiCBoCZAwRvU1cuDIwESIAESIAESIAESoODlOUACJEACJEACJEACJBDUBCh4g7p6eXAkQAIkQAIkQAIkQAIUvDwHSIAESIAESIAESIAEgpoABW9QVy8PjgRIgARIgARIgARIgIKX5wAJkAAJkAAJkAAJkEBQE6DgDerq5cGRAAmQAAmQAAmQAAlQ8PIcIAESkC8HhL4QRr8LZ3EOCZAACZAACQQUAZ8K3jZFgWFiW/VuiqFh9f/zk44AK7iROsLCJEw/hGEGJxIggT4jgOvRpf+4Wlulpe1zadVrVd/MhMsR12S4XpwRYQ6JcPQzwznzKu2z6uGOSIAESIAEvEjAJ4K3TW+iTlebvvRG2tqmN1MVveaG+qXitW6m4Y4widRXVDheDgpfL1YuN0UCnRGA0G1yuaSmqUVqnS3S0OKSxi+uVVynmPp9IXZxTcZGOCQuMkISoyIkQV8OPKFyIgESIAESIIEAIuBVwQuh26xCt77ZdV7sfilxu6aC2yeEL0RvbES4ROsNlhHfrnnxFxLoCQFL6JbXO6Wi0Wmu02Z9GIWfobvrFNcnRG60it8kFbwZ8TES/4XwpfTtSU1wHRIgARIggb4m4DXB69IbZ73TJXUqdvEZN1dPJzSjOvqFSVxUuMRHhmszapinm+DyJEACnRBA5Lba2Synqxv0vaVH1yjELR5EIXyzEmIkIy7aPJxS9HYCnLNIgARIgAT8ikCvBS90LWwLNY3NplnUahLtzVGixTRGI71J0ZFG9EIIcyIBEvCcAK5Pl16fZfVNUlTTII0trerT7cHTaIddw9sLwZubFGcsD7BAcCIBEiABEiABfyXQK8FrxK5aGKqbVOyqD9Dq8OKNg8X9Mzo8XJJjIiRSI0q8nXqDKrcRagTgny+paTRiF5763kvdLwlC9OKhND8l3lgceI1+yYafSIAESIAE/ItArwQvrAvV2vGlTl/evJFaiHADjY10qOiNor3BgsJ3ErBJAJHcktpGKayq105qrTbX8mwxWBzSY6OM6I1VGxInEiABEiABEvBHAj0WvIjmood3dZNTU4/57tAQ6U3UKBIiSbA6cCIBEnBPAK6FKm15OVJeYzqn+eKB1CoFIr0DEmJlUHKc6Xxqzec7CZAACZAACfgLgR4JXtw80TyK3t7w79qdkIsX0zm7n331ihtquvoFkSLp3Lp298jlSCA0CSCie6S8Vsob9IHUC55ddxRjNLPKsLRESdYHU6Ytc0eLv5MACZAACfQ1gR4J3la9gVbojbRBMzK4u5d+ruHf5uZmqaiokLraav3cIlFRUZKSkirJqanicDjcHjOkcZR2YkuPjWQEyS0tLhDqBPBYiWwMhVV10uzmgRTXZ1VFudTX1qot6atxYHREi09MkiS9Vt11SsODaKqK3aHpiSa1YKjXAY+fBEiABEjAvwj0SPCipzeiu/DwupvOlpbIqmVLZd2aT6S0pESamhpVtDpk9CUFcuc37pORo8fYE716R02Pi9IE+PQJumPO30ObAKK7h8pqTK5ddw+krToAxTt/f142rFwun5tep5C9n0tLS4t2FO0ns69bKNffdY+ER0S4herQa3S4Cl6kLLPffuN2s1yABEiABEiABHpNwGPBixhQlSatr9WOanayMuzetVP+/KcnZWBevgwfOVKju9Gyf+8eWf7hYrl8xkz5zqM/lLSMDLcRJBxpvObnTdMOMu6iTb2mwg2QQIASwPVZrinIDqmdAbYjdxOG/t63c5ucOHLYDP+NJpvGhnpZ/t7bUlVWJvfq9XnNTbeJQzOmuJsgcjPjo2VEehJtDe5g8XcSIAESIIE+JeCx4HWpysUNFVFeO1NjQ4NUapNpQmKixMTEGrF69mypvPTcM7J5w3p54j//ICM0yhsW5n6QCYzEBi8vB6SwQ57LhCIBXJ9FmpWhsLr+nIB1B0EFLjy+n3+urTWqlp3aArPkzVflwzf+IZdecaXc873HJC4hwfZDZoIOQTwqM4ktMe6483cSIAESIIE+JeCx4EVzaYXaGdx5A9sfBTqrtY/KVqgA/ttfnpWVS5fKr//wpIweW2BL8ELoJsdE8mbaHi4/k0A7Arg+D6udAZ3VvurIbbdQJx9xjTY7m2S9Whtee+7/yfAxBXLHg9+VHG2Z8WTCKGzIy9tfbQ2cSIAESIAESMBfCHgkeHEDrddUZJU6qlpPR1RDE+qBfXvlyd/+xjB4/Jf/Jnn5g78iiLuCA49gQnSEEb1dLcP5JBDKBOq1I+nekkqpt9kCY7FyqWd3z45t8vx//6c4GxvlkZ/9UkYWTLDl3bW2gfdwbanJToiWoZqxgRMJkAAJkAAJ+AsBjwUvvLvw8Nrx73Y8SIjdstJSee7//kmWLl4kD37/Ubnlzn+S2Ni4jot2+l31rqDJNFU7r3EiARL4KgE8kCI39q7iCu1Qaj++i+huVXmZPPPbf5PVH74vQ0aNkfFTpsmk6VfKuMumSkRk5Fd31M03MxCFXp9jMpO7WYo/kQAJkAAJkEDfEvBY8NZodLdKX/Zvp+cOyKW9wQ8f3C8v/vkZ2b1rh8y+Zp7c8+0HJSMz01Z018KCjmvw8XIiARL4KgFckzU62MSO0xUeX591NTWyYdVyKTldJHU11XJk/15p1kFlbv3Wg3L57Lm2I73ouJaiIyMWZKcwU8NXq4ffSIAESIAELiIBjwVvtYpdvDwRvJbY/Z8//Jds27hRvvHAQ3KXpiRLTtGbIsK2HkxxKngzKHg9IMZFQ4VAbwQv8vG2tKhVqbVVXC0uzdpwUF784x8kMTlFHvrf/yIZ2QNsXau4muGzH5etuXtDBTyPkwRIgARIwO8JeCx4TYRXo0jaCmprcjqdmoZst6Yme0qKTxXJ7Xd/U+bM+5rHkV3sDDdQk5qMgtcWey4UWgRwSdbqtbmzuFIwOExvpurKSnnv5Rc1P+9H8oNf/kaGaSc2O5lUMPx3qqYOvCQrpTe757okQAIkQAIk4FUCHgveOuPh1UiQjRsqvIHHNL/nn373W9m3Z7c8/OhjMv+6GyU6RhPTexjZxVHDH5igEd4UvaFyIgESuJBAnXp4d5dUCbI19GaCxQHpyT566zV57Ff/IaPGT7QleDGscFZ8jObiZae13vDnuiRAAiRAAt4l4JHgxa4btLkTwwq7Wt1HkNBJ7f133pL/euJfzahq9z7wsMRrTk9MprnTQ9GLHuBJMREqet2P+mR2wn9IIMQINOr1eeBsjVSjFcbGsZ9LR+aUSB3u23oIxbzik4XywpO/k+qqCnnkp09I7uAh53/vbrORmjpwUFKc5Cbb64ja3bb4GwmQAAmQAAl4i4DHgreltc0MK2wnggTv7jNqZXjp2adl1txrZMTI0edvmv1UvPYfkC2Xz5wlyeoTtG623R1YpA48gZHWojTXJycSIIELCeD6LKysk1O1DbZsR3W1NfKG5sRua2uVvKHDzSATtdVVsm39Wjm85zOZf9tdct0d/yTRNjOpxEWEm+hukvp4OZEACZAACZCAvxDwWPAi+oOk9sj36c7VgA4wr7z4F3nj5b9JlNoYIiP0JvhFT5Z+/cJkTME4+eb9D0p2Tq4twRsbeW5oYTSbciIBEriQAEZNK61rkiM6tHCLtrC4m1qam2XNssWy6ZNVUqmpydAq09bq0lRkUTJ11tUya8ENkpKWbuv6xL7wQDo6I0nCNdLLiQRIgARIgAT8hYDHghcFh0/QzuATVnNpc7OO+nSBOu4njvBwiY6OFofDfcQWEjclNlIHnoi0NLO/MGQ5SMBvCMDGUN/cIod0tLUa9du7szXgumzVlhhnU5NU6QiIVeXlEhkdJRn9syUuPsHjHLz5KXEyUC0Ndlps/AYaC0ICJEACJBD0BHokeF3abFqmUV6njubk7obqLYIYVjhDE9pH0s7gLaTcTpASQJTX2BpqGjwegALpyVStGsHqiWjFA2mctsCM0OhuIj32QXpm8bBIgARIIHAJ9EjwIliLKFJlg71sDb3Fg+wMiO7ihorPnEiABLongBHXTJRX3/tigs1oSGqCZmiINsML98U+uQ8SIAESIAESsEugR4IXG0dasioVvLA3+DrKC6GL3J707tqtVi4X6gRwfZbWNcqxijpp1hYZX054BMXoh0PTEiSaLTC+RM1tkwAJkAAJ9JBAjwUvRG6z5vqEl7dJrQ2+mHAjRWYGjNwUrb2/Gdv1BWVuM1gJQOierKqXMyp8kb3BFxMaXGBhyEuON9cpW2B8QZnbJAESIAES6C2BHgte7BjWBqQnq1Q/L26o3o70wreL9EZxEQ52gultTXP9kCSA6/N4Za2crXdKa5t3r1AkS0FENy8l3gz3TbEbkqcYD5oESIAEAoJArwQvjhD30CZNdo9E980u74heRHIjNLKLyBFSkfFGGhDnEgvphwQgcRs1hWBRTb0Rvd6K9OIaTYyOkJzEWE1FpplWmCrQD2ufRSIBEiABErAI9FrwYkO4qUL01jld0qj2BvQS7+mEnuHRKnYxmloMI7s9xcj1SOArBBDphae3RHP09uYahdCFuE2NiZLshBhjY/Akm8NXCsUvJEACJEACJNBHBLwieFFWSFw0mWJoU+T/ROoyT2SvdSNN0KgROqk5dGAKJmToo7OAuwkJArg+6zS7yqnqBqloPGdx8OQaRUtLjFoYcpJitZNalEToaIkUuyFx6vAgSYAESCDgCXhN8FokEN1FsykiSujM5tKbLOYh6Isk97jBQtxCzaIVFDdRRIzgBUREF75dWhgsmnwnAe8SwLWIzmxIW4YOp0gviO/mOsW1iitU/8fD5rlrM0yF7bnrM0kHfUEHUlyn4Sp2OZEACZAACZBAoBDwuuC1DhzC1oy0pr5ep95QWzWhPSJMluAN05sobpqRjn76UpGLaJG1Mt9JgAR8SsC6Pp16fSK1YL22zCDrCoQvftPLUh8+HeZBND4qXGI1S0q4XrOM6Pq0WrhxEiABEiABHxHwmeD1UXm5WRIgARIgARIgARIgARLwiAAFr0e4uDAJkAAJkAAJkAAJkECgEaDgDbQaY3lJgARIgARIgARIgAQ8IkDB6xEuLkwCJEACJEACJEACJBBoBCh4A63GWF4SIAESIAESIAESIAGPCFDweoSLC5MACZAACZAACZAACQQaAQreQKsxlpcESIAESIAESIAESMAjAhS8HuHiwiRAAiRAAiRAAiRAAoFGgII30GqM5SUBEiABEiABEiABEvCIAAWvR7i4MAmQAAmQAAmQAAmQQKARoOANtBpjeUmABEiABEiABEiABDwiQMHrES4uTAIkQAIkQAIkQAIkEGgEKHgDrcZYXhIgARIgARIgARIgAY8IUPB6hIsLkwAJkAAJkAAJkAAJBBoBCt5AqzGWlwRIgARIgARIgARIwCMCFLwe4eLCJEACJEACJEACJEACgUaAgjfQaozlJQESIAESIAESIAES8IgABa9HuLgwCZAACZAACZAACZBAoBGg4A20GmN5SYAESIAESIAESIAEPCJAwesRLi5MAiRAAiRAAiRAAiQQaAQoeAOtxlheEiABEiABEiABEiABjwhQ8HqEiwuTAAmQAAmQAAmQAAkEGgEK3kCrMZaXBEiABEiABEiABEjAIwIUvB7h4sIkQAIkQAIkQAIkQAKBRoCCN9BqjOUlARIgARIgARIgARLwiAAFr0e4uDAJkAAJkAAJkAAJkECgEfj//7O/7AArUQEAAAAASUVORK5CYII=)\n",
        "\n",
        "**Algorithm:**\n",
        "1. Given a new data point $\\mathbf{x}_{\\text{new}}$, compute the distance from $\\mathbf{x}_{\\text{new}}$ to every point in the training set.\n",
        "2. Select the K training points closest to $\\mathbf{x}_{\\text{new}}$ (the K nearest neighbors).\n",
        "3. **Average** the target values of these K neighbors to produce the prediction:\n",
        "\n",
        "$$\\hat{y}_{\\text{new}} = \\frac{1}{K} \\sum_{i=1}^{K} y_i$$\n",
        "\n",
        "where $y_i$ are the target values of the K nearest neighbors.\n",
        "\n",
        "**Key Difference from Classification:**\n",
        "- **Classification**: Uses majority voting among K neighbors' class labels\n",
        "- **Regression**: Averages the K neighbors' continuous target values\n",
        "\n",
        "**Example:**\n",
        "If K=3 and the 3 nearest neighbors have target values [2.5, 3.0, 2.8], the prediction would be:\n",
        "$$\\hat{y} = \\frac{2.5 + 3.0 + 2.8}{3} = 2.77$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eN_d8fRQb7k"
      },
      "source": [
        "## Visual K Progression\n",
        "\n",
        "Before diving into implementation, let's visualize how the K parameter affects predictions step by step.### Understanding K Through VisualizationWhen K=1, the algorithm considers only the single nearest neighbor, making predictions highly sensitive to individual data points. As K increases, predictions become smoother by averaging more neighbors, reducing sensitivity to outliers but potentially oversimplifying patterns in the data.**Note on Distance in 1D**: In the visualization below, we use a single feature (1D data). Here, the Euclidean distance between two points $x$ and $x'$ simplifies to the absolute difference: $d(x, x') = |x - x'|$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tU-KI0DQb7k"
      },
      "outputs": [],
      "source": [
        "# Visualize how K affects predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create simple dataset\n",
        "np.random.seed(42)\n",
        "X_simple = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y_simple = np.array([2.1, 2.8, 3.9, 4.1, 5.2, 5.8, 6.1, 7.2, 8.9, 9.1])\n",
        "\n",
        "# Test point\n",
        "x_test = 5.5\n",
        "\n",
        "# Visualize K progression\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "k_values = [1, 2, 3, 4, 5, 10]\n",
        "\n",
        "for idx, k in enumerate(k_values):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "\n",
        "    # Calculate distances\n",
        "    distances = np.abs(X_simple.flatten() - x_test)\n",
        "    nearest_indices = np.argsort(distances)[:k]\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = np.mean(y_simple[nearest_indices])\n",
        "\n",
        "    # Plot\n",
        "    ax.scatter(X_simple, y_simple, c='blue', s=100, alpha=0.6, label='Training data')\n",
        "    ax.scatter(X_simple[nearest_indices], y_simple[nearest_indices],\n",
        "               c='red', s=200, marker='*', label=f'K={k} neighbors', zorder=3)\n",
        "    ax.scatter(x_test, prediction, c='green', s=300, marker='X',\n",
        "               label=f'Prediction={prediction:.2f}', zorder=4)\n",
        "\n",
        "    # Draw circle showing radius\n",
        "    circle = plt.Circle((x_test, prediction), distances[nearest_indices[-1]],\n",
        "                        color='red', fill=False, linestyle='--', alpha=0.3)\n",
        "    ax.add_patch(circle)\n",
        "\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'K = {k}: Prediction = {prediction:.2f}')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Test point X = {x_test}\")\n",
        "for k in k_values:\n",
        "    distances = np.abs(X_simple.flatten() - x_test)\n",
        "    nearest_indices = np.argsort(distances)[:k]\n",
        "    prediction = np.mean(y_simple[nearest_indices])\n",
        "    print(f\"K={k:2d}: Prediction = {prediction:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKgu6j9HsY9"
      },
      "source": [
        "> **Question**: In KNN regression, the parameter K refers to:>> A. The number of features in the dataset  >> B. The number of nearest neighbors whose target values are averaged for prediction  >> C. The number of clusters in the data  >> D. The number of iterations for training<details><summary>Click to reveal answer</summary>**Correct Answer: B****Explanation:**- **A is FALSE**: The number of features is denoted by other variables (like n or p), not K- **B is TRUE**: K specifically refers to the number of nearest neighbors used to make a prediction by averaging their target values- **C is FALSE**: KNN is not a clustering algorithm; K-means clustering uses K for clusters, but this is KNN regression- **D is FALSE**: KNN is a lazy learner with no training iterations; it simply stores the training data</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAZ9wveaHsY9"
      },
      "source": [
        "## Z-Score Standardization\n",
        "\n",
        "Before we explore feature scaling in practice, let's understand the mathematical foundation.\n",
        "\n",
        "### The Z-Score Formula\n",
        "\n",
        "Standardization (Z-score normalization) transforms features to have mean $\\mu = 0$ and standard deviation $\\sigma = 1$:\n",
        "\n",
        "$$Z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the original value\n",
        "- $\\mu$ is the mean of the feature\n",
        "- $\\sigma$ is the standard deviation of the feature\n",
        "\n",
        "### Why Z-Score Standardization Works\n",
        "\n",
        "1. **Centers the data**: Subtracting the mean shifts the distribution to be centered at zero\n",
        "2. **Scales the spread**: Dividing by standard deviation makes the spread consistent across features\n",
        "3. **Preserves distribution shape**: Unlike min-max scaling, z-score maintains the original distribution's shape\n",
        "4. **Handles outliers better**: Less sensitive to extreme values compared to min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmn36k3LQb7l"
      },
      "outputs": [],
      "source": [
        "# Visualize Z-score standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "# Create two features with different scales\n",
        "feature1 = np.random.normal(loc=100, scale=15, size=200)  # Mean=100, SD=15\n",
        "feature2 = np.random.normal(loc=5, scale=2, size=200)     # Mean=5, SD=2\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "feature1_scaled = scaler.fit_transform(feature1.reshape(-1, 1)).flatten()\n",
        "feature2_scaled = scaler.fit_transform(feature2.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Before standardization - Feature 1\n",
        "axes[0, 0].hist(feature1, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(feature1.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature1.mean():.1f}')\n",
        "axes[0, 0].set_xlabel('Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title(f'Feature 1 (Before)\\nÎ¼={feature1.mean():.1f}, Ï={feature1.std():.1f}')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Before standardization - Feature 2\n",
        "axes[0, 1].hist(feature2, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(feature2.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature2.mean():.1f}')\n",
        "axes[0, 1].set_xlabel('Value')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title(f'Feature 2 (Before)\\nÎ¼={feature2.mean():.1f}, Ï={feature2.std():.1f}')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 1\n",
        "axes[1, 0].hist(feature1_scaled, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].axvline(feature1_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature1_scaled.mean():.1f}')\n",
        "axes[1, 0].set_xlabel('Value (Z-score)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'Feature 1 (After)\\nÎ¼={feature1_scaled.mean():.2f}, Ï={feature1_scaled.std():.2f}')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 2\n",
        "axes[1, 1].hist(feature2_scaled, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].axvline(feature2_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature2_scaled.mean():.1f}')\n",
        "axes[1, 1].set_xlabel('Value (Z-score)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title(f'Feature 2 (After)\\nÎ¼={feature2_scaled.mean():.2f}, Ï={feature2_scaled.std():.2f}')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Before Standardization:\")\n",
        "print(f\"  Feature 1: Î¼={feature1.mean():.2f}, Ï={feature1.std():.2f}, range=[{feature1.min():.2f}, {feature1.max():.2f}]\")\n",
        "print(f\"  Feature 2: Î¼={feature2.mean():.2f}, Ï={feature2.std():.2f}, range=[{feature2.min():.2f}, {feature2.max():.2f}]\")\n",
        "print(\"\\nAfter Standardization:\")\n",
        "print(f\"  Feature 1: Î¼={feature1_scaled.mean():.2f}, Ï={feature1_scaled.std():.2f}, range=[{feature1_scaled.min():.2f}, {feature1_scaled.max():.2f}]\")\n",
        "print(f\"  Feature 2: Î¼={feature2_scaled.mean():.2f}, Ï={feature2_scaled.std():.2f}, range=[{feature2_scaled.min():.2f}, {feature2_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJYpmTLnHsY9"
      },
      "source": [
        "> **Question**: You're building a KNN regression model with three features: Age (range 18-65), Income (range $20,000-$150,000), and Credit Score (range 300-850). If you train KNN without scaling these features, what will happen?>> A. The model will automatically normalize features internally, so scaling is unnecessary for KNN algorithms>> B. The Income feature will dominate distance calculations, making Age and Credit Score almost irrelevant to predictions>> C. All features will contribute equally because KNN uses relative distances rather than absolute values>> D. The model will fail to train because KNN requires all features to have the same scale before fitting<details><summary>Click to reveal answer</summary>**Correct Answer: B****Explanation:**This question tests understanding of the **concrete impact** of not scaling, rather than just knowing you should scale.- **B is TRUE**: Income will dominate distance calculations:  - **Age difference**: max distance = |65 - 18| = 47  - **Income difference**: max distance = |$150k - $20k| = $130,000  - **Credit Score difference**: max distance = |850 - 300| = 550  - **Result**: Income differences are ~2,700x larger than Age differences!  - Two people with similar Age and Credit but different Income will be considered \"far apart\"  - Two people with different Age/Credit but similar Income will be considered \"close together\"  - **The model essentially becomes a 1-feature model based on Income alone**- **A is FALSE**: KNN does NOT automatically normalize features:  - KNN computes raw Euclidean (or Manhattan) distances directly on the input features  - No internal preprocessing or automatic scaling happens  - You must explicitly scale features yourself (or use a pipeline)- **C is FALSE**: KNN uses **absolute** distances, not relative:  - Euclidean distance: $d = \\sqrt{(age_1 - age_2)^2 + (income_1 - income_2)^2 + (credit_1 - credit_2)^2}$  - The income term dominates: $(130000)^2 = 16,900,000,000$ vs $(47)^2 = 2,209$  - Income contributes ~7.6 million times more to the squared distance  - There's no \"relative distance\" calculation that would equalize contributions- **D is FALSE**: The model will train successfully but perform poorly:  - KNN will fit without errors - it simply stores the training data  - sklearn.neighbors.KNeighborsRegressor doesn't check or enforce feature scales  - The model will make predictions, but they'll be dominated by the unscaled feature  - You won't get an error - just bad predictions**Key Insight**: Not scaling doesn't break KNN - it makes it effectively ignore small-scale features. This is why scaling is **essential** for KNN, not just a \"nice to have\" optimization.**Real Example**:```python# Without scalingPoint A: [Age=25, Income=$50k, Credit=700]Point B: [Age=26, Income=$51k, Credit=750]  # Close in all featuresPoint C: [Age=50, Income=$50k, Credit=700]  # Very different age, same income/creditDistance AâB: â[(1)Â² + (1000)Â² + (50)Â²] â 1000Distance AâC: â[(25)Â² + (0)Â² + (0)Â²] â 25KNN considers B much farther from A than C, even though B is similar in all features!```</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudocode for KNN Regressor\n",
        "\n",
        "Before implementing the algorithm, let's understand the pseudocode structure (from lecture slides):\n",
        "\n",
        "```\n",
        "# Inputs\n",
        "#   data      â training set of N examples (x, y)\n",
        "#   k         â number of neighbours\n",
        "#   metric    â distance function (e.g., Euclidean, Manhattan)\n",
        "#   X_query   â set of examples to predict\n",
        "\n",
        "# ----- \"fit\" (lazy) -----\n",
        "X_train â data.x\n",
        "y_train â data.y\n",
        "\n",
        "# ----- predict -----\n",
        "Å· â list of length |X_query|  # outputs align 1:1 with X_query\n",
        "\n",
        "FOR i = 1 TO |X_query| DO\n",
        "    x* â X_query[i]\n",
        "    d â distances from x* to all X_train using metric\n",
        "    J â indices of the k smallest values in d\n",
        "    # regression prediction = average of the targets of the k nearest neighbours\n",
        "    Å·[i] â mean(y_train[J])\n",
        "END FOR\n",
        "\n",
        "RETURN Å·\n",
        "```\n",
        "\n",
        "**Key Points:**\n",
        "- KNN is a \"lazy learner\" - the `fit` method just stores the training data\n",
        "- The `predict` method does all the work: compute distances, find K nearest, average their targets\n",
        "- **Regression uses averaging** (not voting like classification)"
      ],
      "metadata": {
        "id": "vZYOgx4NHsY-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWwk1-fnHsY-"
      },
      "source": [
        "## Implementing a Custom KNN Regressor\n",
        "\n",
        "Below is a scaffold of the `MyKNNRegressor` class. Fill in the TODO sections to complete the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PkxAmvXHsY-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "class MyKNNRegressor:\n",
        "    \"\"\"\n",
        "    A simple K-Nearest Neighbors Regressor implementation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_neighbors : int, default=5\n",
        "        Number of neighbors to use for prediction\n",
        "    metric : str, default='euclidean'\n",
        "        Distance metric to use ('euclidean', 'manhattan', etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.metric = metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the KNN regressor by storing the training data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        # TODO: Store the training data\n",
        "        # Hint: KNN is a \"lazy learner\" - it just stores the training data\n",
        "        self.X_train = ___\n",
        "        self.y_train = ___\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for test data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array of shape (n_samples,)\n",
        "            Predicted target values\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for x_test in X:\n",
        "            # TODO: For each test point:\n",
        "            # 1. Compute distances from x_test to all training points\n",
        "            # 2. Find indices of K nearest neighbors\n",
        "            # 3. Get target values of those K neighbors\n",
        "            # 4. Average them to get the prediction\n",
        "\n",
        "            # Step 1: Compute distances\n",
        "            distances = pairwise_distances(\n",
        "                x_test.reshape(1, -1),\n",
        "                self._____,\n",
        "                metric=self.metric\n",
        "            ).ravel()\n",
        "\n",
        "            # Step 2: Find K nearest neighbor indices\n",
        "            k_nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
        "\n",
        "            # Step 3: Get target values of K nearest neighbors\n",
        "            k_nearest_targets = self.y_train[______]\n",
        "\n",
        "            # Step 4: Average the target values\n",
        "            prediction = np.mean(_______)\n",
        "\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "print(\"MyKNNRegressor class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNGRBbBHsY-"
      },
      "source": [
        "Once you have filled in the implementation, let's test our custom regressor on a simple dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODx0DF2HsY_"
      },
      "source": [
        "## A Dataset for Visualization\n",
        "\n",
        "We'll create a synthetic 1D regression dataset to visualize how KNN regression works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdmajwTrHsY_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic 1D data\n",
        "def f(x):\n",
        "    \"\"\"True underlying function: sinusoidal pattern\"\"\"\n",
        "    return np.sin(x) + 0.1 * x\n",
        "\n",
        "# Training data\n",
        "X_train_1d = np.sort(np.random.uniform(0, 10, 100))\n",
        "y_train_1d = f(X_train_1d) + np.random.normal(0, 0.2, 100)  # Add noise\n",
        "\n",
        "# Reshape for sklearn compatibility\n",
        "X_train_1d = X_train_1d.reshape(-1, 1)\n",
        "\n",
        "# Test data (for smooth prediction curve)\n",
        "X_test_1d = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "y_test_1d_true = f(X_test_1d.ravel())\n",
        "\n",
        "print(f\"Training data shape: X={X_train_1d.shape}, y={y_train_1d.shape}\")\n",
        "print(f\"Test data shape: X={X_test_1d.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10nBkhcNHsY_"
      },
      "source": [
        "Let's visualize the training data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual step-by-step prediction (matching lecture slides 11-13)\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Choose a test point (let's pick x=5.0 from our test range)\n",
        "x_test_single = np.array([[5.0]])  # Must be 2D for sklearn\n",
        "print(\"=\"*70)\n",
        "print(\"STEP-BY-STEP: How KNN Predicts for x_test = 5.0\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Calculate distances from test point to ALL training points\n",
        "print(\"\\nSTEP 1: Pairwise Distance Calculation\")\n",
        "print(\"-\" * 70)\n",
        "distances = pairwise_distances(x_test_single, X_train_1d, metric='euclidean').ravel()\n",
        "print(f\"Computed {len(distances)} distances from x_test=5.0 to all training points\")\n",
        "print(f\"Distance array shape: {distances.shape}\")\n",
        "print(f\"First 10 distances: {distances[:10].round(3)}\")\n",
        "\n",
        "# Step 2: Find K nearest neighbors\n",
        "print(\"\\nSTEP 2: Finding K Nearest Neighbors\")\n",
        "print(\"-\" * 70)\n",
        "K = 5\n",
        "k_indices = np.argsort(distances)[:K]  # Indices of K smallest distances\n",
        "k_distances = distances[k_indices]\n",
        "print(f\"K = {K}\")\n",
        "print(f\"Indices of K nearest neighbors: {k_indices}\")\n",
        "print(f\"Their distances: {k_distances.round(3)}\")\n",
        "print(f\"Their x-coordinates: {X_train_1d[k_indices].ravel().round(3)}\")\n",
        "\n",
        "# Step 3: Get target values of K nearest neighbors\n",
        "print(\"\\nSTEP 3: Get Neighbor Targets\")\n",
        "print(\"-\" * 70)\n",
        "k_targets = y_train_1d[k_indices]\n",
        "print(f\"NN's Labels (target values): {k_targets.round(3)}\")\n",
        "\n",
        "# Step 4: Average to get prediction\n",
        "print(\"\\nSTEP 4: Average (THE PREDICTION!)\")\n",
        "print(\"-\" * 70)\n",
        "manual_prediction = np.mean(k_targets)\n",
        "print(f\"Prediction = mean({k_targets.round(3)})\")\n",
        "print(f\"Prediction = {manual_prediction:.3f}\")\n",
        "\n",
        "# Verify with our custom KNN\n",
        "knn_verify = MyKNNRegressor(n_neighbors=K)\n",
        "knn_verify.fit(X_train_1d, y_train_1d)\n",
        "verify_prediction = knn_verify.predict(x_test_single)[0]\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFICATION:\")\n",
        "print(f\"  Manual calculation:  {manual_prediction:.3f}\")\n",
        "print(f\"  MyKNNRegressor:      {verify_prediction:.3f}\")\n",
        "print(f\"  Match: {np.isclose(manual_prediction, verify_prediction)} â\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Visualize this specific prediction\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(X_train_1d, y_train_1d, c='lightblue', alpha=0.6, edgecolor='k', s=50, label='All training data')\n",
        "plt.scatter(X_train_1d[k_indices], k_targets, c='red', s=100, edgecolor='k',\n",
        "           label=f'K={K} nearest neighbors', zorder=5)\n",
        "plt.scatter(x_test_single, manual_prediction, c='green', s=200, marker='*',\n",
        "           edgecolor='k', label='Prediction (average)', zorder=10)\n",
        "plt.axhline(manual_prediction, color='green', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title(f'Step-by-Step KNN Prediction: x_test={x_test_single[0,0]}, K={K}, Prediction={manual_prediction:.3f}')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uVUFYcjhHsY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Prediction Process: Step-by-Step\n",
        "\n",
        "Before we test different K values, let's manually walk through exactly what happens when KNN makes a single prediction. This matches the step-by-step process from the lecture slides (slides 11-13).\n",
        "\n",
        "**The 4 Steps:**\n",
        "1. **Pairwise Distance Calculation**: Compute distance from test point to all training points\n",
        "2. **Finding K Nearest Neighbors**: Sort distances and select K smallest\n",
        "3. **Get Neighbor Targets**: Retrieve the y-values of those K neighbors  \n",
        "4. **Average**: Compute mean of the K target values â this is the prediction!\n",
        "\n",
        "Let's demonstrate this process:"
      ],
      "metadata": {
        "id": "8vHx0M4NHsY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W58gNJ3hHsY_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training data\n",
        "plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, edgecolor='k', s=50, label='Training data')\n",
        "plt.plot(X_test_1d, y_test_1d_true, 'g--', lw=2, label='True function')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Training Data with True Underlying Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvgZY8aJXjwg"
      },
      "source": [
        "> **Question**: You've trained a KNN regressor on 10,000 training samples. When comparing prediction time for different K values, which statement is most accurate?>> A. Increasing K from 5 to 50 will significantly slow down predictions because the algorithm must average more neighbors' values>> B. Prediction time is nearly identical for K=5 and K=50 because the dominant cost is computing distances to all 10,000 training points, not averaging K values>> C. Larger K values (like K=50) are faster because the algorithm can stop searching after finding the first 50 neighbors instead of examining all 10,000 points>> D. K=5 and K=50 have identical prediction times in theory, but K=50 uses more memory during prediction which can slow down performance on large datasets<details><summary>Click to reveal answer</summary>**Correct Answer: B****Explanation:**This question tests understanding of **where computational cost actually comes from** in KNN prediction.**B is TRUE**: Prediction time is dominated by distance computation, not the averaging step:**Prediction complexity breakdown for a single point:**1. **Compute distances to all N training points**: O(N Ã d)   - For N=10,000 and d=5 features: 50,000 distance calculations   - This is the expensive step!   2. **Find K nearest neighbors**: O(N log K) or O(N) depending on algorithm   - For N=10,000: ~10,000 comparisons   - Still expensive, but less than distance computation   3. **Average the K neighbors' target values**: O(K)   - For K=50: just 50 additions and 1 division   - **This is trivial compared to steps 1-2!****Result**: Whether K=5 or K=50, you still compute 10,000 distances. Averaging 5 vs 50 values is negligible.**Concrete timing example:**```python# Typical prediction times (approximate):K=5:  Distance computation: 95% | Finding K neighbors: 4% | Averaging: 1%K=50: Distance computation: 94% | Finding K neighbors: 5% | Averaging: 1%# Total time is nearly identical!```**A is FALSE**: Averaging K values is not the bottleneck:- Averaging 50 numbers: ~50 additions + 1 division = microseconds- Computing 10,000 distances: ~50,000 multiplications/additions = milliseconds- The averaging step is 1000x faster than distance computation- Even if you increase K from 5 to 500, averaging is still negligible**C is FALSE**: KNN cannot stop early - it must examine ALL training points:- To find the K nearest neighbors, you must compare the new point to ALL N training points- You don't know which are the nearest until you've computed all distances- There's no way to know the 50th nearest neighbor without checking all 10,000 points- **Exception**: Advanced data structures like KD-trees can enable early stopping, but:  - They require preprocessing (not \"training-free\" anymore)  - They only help in low dimensions (< 20 features)  - They're not used by default in simple KNN implementations**D is FALSE**: K doesn't significantly affect memory during prediction:- Memory usage during prediction:  - Store N distances: N Ã sizeof(float) â 40KB for N=10,000  - Store K neighbors: K Ã sizeof(float) â 200 bytes for K=50- K=5 vs K=50 changes memory by only 180 bytes - completely negligible- The memory for storing distances (40KB) dominates regardless of K- This tiny memory difference has zero impact on performance**Key Insight**: - **Training set size N** is the primary driver of prediction time: O(N Ã d)- **K value** has minimal impact on prediction time (only affects the cheap averaging step)- This is why KNN struggles with large datasets - every prediction is expensive!**Practical Implications**:- N=1,000: Fast predictions (~1-10ms per point)- N=10,000: Moderate predictions (~10-100ms per point)- N=100,000: Slow predictions (~100ms-1s per point)- N=1,000,000: Impractical for real-time systems (seconds per prediction)**Optimization Note**: If you need faster predictions with large N, consider:- Approximate nearest neighbors (LSH, HNSW)- Dimensionality reduction (PCA before KNN)- Spatial indexing (KD-trees for d<20)- Using a different algorithm entirely (Random Forest, Gradient Boosting)</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyEKj8S3Qb7n"
      },
      "source": [
        "## Model Complexity and Bias-Variance Tradeoff\n",
        "\n",
        "Let's explore how different K values affect model complexity by testing a wider range: K=1, 5, 10, 15, 30, 50.\n",
        "\n",
        "### Understanding the Tradeoff\n",
        "\n",
        "- **Small K (e.g., K=1)**: High complexity, low bias, high variance â **Overfitting**\n",
        "- **Moderate K (e.g., K=5-10)**: Balanced complexity â **Optimal**\n",
        "- **Large K (e.g., K=30-50)**: Low complexity, high bias, low variance â **Underfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy4br3GKQb7n"
      },
      "outputs": [],
      "source": [
        "# Test extended range of K values with 2x3 subplot comparison\n",
        "k_values_extended = [1, 5, 10, 15, 30, 50]\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "for idx, k in enumerate(k_values_extended):\n",
        "    # Train model\n",
        "    knn_model = MyKNNRegressor(n_neighbors=k)\n",
        "    knn_model.fit(X_train_1d, y_train_1d)\n",
        "\n",
        "    # Predictions\n",
        "    X_plot = np.linspace(X_train_1d.min(), X_train_1d.max(), 300).reshape(-1, 1)\n",
        "    y_plot = knn_model.predict(X_plot)\n",
        "\n",
        "    # Create subplot\n",
        "    plt.subplot(2, 3, idx + 1)\n",
        "    plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, s=50, label='Training data')\n",
        "    plt.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'K={k} prediction')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "    # Label overfitting/optimal/underfitting\n",
        "    if k <= 3:\n",
        "        region_label = 'OVERFITTING'\n",
        "        color = 'red'\n",
        "    elif k <= 15:\n",
        "        region_label = 'OPTIMAL'\n",
        "        color = 'green'\n",
        "    else:\n",
        "        region_label = 'UNDERFITTING'\n",
        "        color = 'orange'\n",
        "\n",
        "    plt.title(f'K = {k}\\n[{region_label}]', color=color, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Model Complexity Analysis:\")\n",
        "print(\"  K=1:  Overfitting - follows training data too closely\")\n",
        "print(\"  K=5:  Optimal - good balance\")\n",
        "print(\"  K=10: Optimal - smooth and generalizes well\")\n",
        "print(\"  K=15: Still good - slightly smoother\")\n",
        "print(\"  K=30: Underfitting - too smooth, misses patterns\")\n",
        "print(\"  K=50: Severe underfitting - barely captures the trend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZHRGKrHsY_"
      },
      "source": [
        "> **Question**: Looking at the plots above, which K value shows signs of overfitting?>> A. K=30 (too smooth, doesn't follow the data closely)  >> B. K=1 (too wiggly, follows every noise point)  >> C. K=10 (balanced smoothness)  >> D. All of them equally<details><summary>Click to reveal answer</summary>**Correct Answer: B****Explanation:**- **A is FALSE**: K=30 shows underfitting (high bias) - too smooth and misses important patterns- **B is TRUE**: K=1 shows overfitting (high variance) - the prediction line is too wiggly and captures noise instead of the underlying pattern- **C is FALSE**: K=10 shows a good balance between bias and variance, capturing the trend without fitting noise- **D is FALSE**: Different K values demonstrate different tradeoffs; they are not equally prone to overfitting**Key Insight**: Small K values lead to overfitting (memorizing noise), while large K values lead to underfitting (oversimplifying patterns).</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3MNGMcQb7n"
      },
      "source": [
        "## Regression Line and MSE Formulas### Understanding Prediction ErrorsIn regression, we measure how well our predictions match the actual values. The **error** (also called **residual**) for a single prediction is:$$\\text{Error (Residual)} = y - \\hat{y}$$Where:- $y$ is the actual value- $\\hat{y}$ is the predicted value### Mean Squared Error (MSE)To evaluate overall model performance, we use Mean Squared Error:$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2$$Where:- $N$ is the number of samples- Squaring emphasizes larger errors- Lower MSE indicates better predictions### RÂ² Score (Coefficient of Determination)RÂ² measures how well the model explains the variance in the data, ranging from 0 to 1:$$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^{N} (y^{(i)} - \\bar{y})^2}$$Where:- $\\bar{y}$ is the mean of actual values- **RÂ² = 1**: Perfect predictions (model explains all variance)- **RÂ² = 0**: Model performs no better than predicting the mean- **RÂ² < 0**: Model performs worse than predicting the mean**Interpretation**: RÂ² tells us the proportion of variance in $y$ that is explained by the model. For example, RÂ² = 0.85 means the model explains 85% of the variance in the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_O7BtMZQb7o"
      },
      "outputs": [],
      "source": [
        "# Visualize over-predicted and under-predicted points\n",
        "np.random.seed(42)\n",
        "X_error = np.linspace(0, 10, 20).reshape(-1, 1)\n",
        "y_true = 2 * X_error.flatten() + np.random.randn(20) * 2\n",
        "\n",
        "# Simple predictions (not perfect)\n",
        "y_pred = 2 * X_error.flatten() + np.random.randn(20) * 1.5\n",
        "\n",
        "# Calculate errors\n",
        "errors = y_true - y_pred\n",
        "mse = np.mean(errors**2)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot 1: Predictions vs Actual\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_error, y_true, c='blue', s=100, alpha=0.6, label='Actual values')\n",
        "plt.scatter(X_error, y_pred, c='red', s=100, marker='s', alpha=0.6, label='Predictions')\n",
        "\n",
        "# Draw error lines\n",
        "for i in range(len(X_error)):\n",
        "    color = 'green' if y_pred[i] > y_true[i] else 'orange'\n",
        "    plt.plot([X_error[i], X_error[i]], [y_true[i], y_pred[i]],\n",
        "             color=color, linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Predictions vs Actual Values')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Error distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "colors = ['green' if e < 0 else 'orange' for e in errors]\n",
        "plt.bar(range(len(errors)), errors, color=colors, alpha=0.6)\n",
        "plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Error (Residual)')\n",
        "plt.title(f'Prediction Errors\\nMSE = {mse:.2f}\\nGreen=Over-predicted, Orange=Under-predicted')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {np.sqrt(mse):.2f}\")\n",
        "print(f\"Mean Absolute Error: {np.mean(np.abs(errors)):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkGHoWtlHsY_"
      },
      "source": [
        "## Working with a Real Dataset\n",
        "\n",
        "Now let's work with a real 2D regression dataset and compare our implementation with scikit-learn's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX9c--OnHsY_"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate a 2D regression dataset with 300 samples and 2 features\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_regression(n_samples=_________, n_features=_________,\n",
        "                       n_informative=2, noise=15.0, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yePX5lxHsY_"
      },
      "source": [
        "Visualize the 2D dataset with target values as colors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMkQN3_HHsZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis',\n",
        "                     edgecolor='k', s=50, alpha=0.7)\n",
        "plt.colorbar(scatter, label='Target Value')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('2D Regression Dataset (color represents target value)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR-wkI2AHsZA"
      },
      "source": [
        "## Splitting into Train, Validation, and Test Sets\n",
        "\n",
        "We split the data into:\n",
        "- **Training set (60%)**: To fit the model\n",
        "- **Validation set (20%)**: To tune hyperparameters (K)\n",
        "- **Test set (20%)**: For final unbiased performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYs7A6NhHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Split data into train (60%), validation (20%), and test (20%) sets\n",
        "# Split into train, validation, and test sets (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=_________)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=_________, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzm6uTyVHsZA"
      },
      "source": [
        "After this split, you should see roughly: Train size 180, Validation size 60, Test size 60."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZqv4VLpHsZA"
      },
      "source": [
        "> **Question**: Why do we use a separate validation set instead of tuning the hyperparameters directly on the test set?>> A. The test set is too small  >> B. To prevent overfitting to the test set and get an unbiased final performance estimate  >> C. The validation set trains faster  >> D. It's just a convention with no real benefit<details><summary>Click to reveal answer</summary>**Correct Answer: B****Explanation:**- **A is FALSE**: Test set size is independent of the reason for using a validation set- **B is TRUE**: If we tune hyperparameters on the test set, we \"leak\" information about the test set into our model selection process, leading to overly optimistic performance estimates. The validation set allows us to tune hyperparameters while keeping the test set completely unseen until final evaluation.- **C is FALSE**: Training speed is not affected by which dataset we use for hyperparameter tuning- **D is FALSE**: This is a critical practice, not just convention - it ensures unbiased performance estimation**Proper Workflow**: Train set (fit model) â Validation set (tune K) â Test set (final evaluation, only once)</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHWY7kokQb7o"
      },
      "source": [
        "## Z-Score Standardization\n",
        "\n",
        "Before we explore feature scaling in practice, let's understand the mathematical foundation.\n",
        "\n",
        "### The Z-Score Formula\n",
        "\n",
        "Standardization (Z-score normalization) transforms features to have mean $\\mu = 0$ and standard deviation $\\sigma = 1$:\n",
        "\n",
        "$$Z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the original value\n",
        "- $\\mu$ is the mean of the feature\n",
        "- $\\sigma$ is the standard deviation of the feature\n",
        "\n",
        "### Why Z-Score Standardization Works\n",
        "\n",
        "1. **Centers the data**: Subtracting the mean shifts the distribution to be centered at zero\n",
        "2. **Scales the spread**: Dividing by standard deviation makes the spread consistent across features\n",
        "3. **Preserves distribution shape**: Unlike min-max scaling, z-score maintains the original distribution's shape\n",
        "4. **Handles outliers better**: Less sensitive to extreme values compared to min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qy_k9gIQb7o"
      },
      "outputs": [],
      "source": [
        "# Visualize Z-score standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "# Create two features with different scales\n",
        "feature1 = np.random.normal(loc=100, scale=15, size=200)  # Mean=100, SD=15\n",
        "feature2 = np.random.normal(loc=5, scale=2, size=200)     # Mean=5, SD=2\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "feature1_scaled = scaler.fit_transform(feature1.reshape(-1, 1)).flatten()\n",
        "feature2_scaled = scaler.fit_transform(feature2.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Before standardization - Feature 1\n",
        "axes[0, 0].hist(feature1, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(feature1.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature1.mean():.1f}')\n",
        "axes[0, 0].set_xlabel('Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title(f'Feature 1 (Before)\\nÎ¼={feature1.mean():.1f}, Ï={feature1.std():.1f}')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Before standardization - Feature 2\n",
        "axes[0, 1].hist(feature2, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(feature2.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature2.mean():.1f}')\n",
        "axes[0, 1].set_xlabel('Value')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title(f'Feature 2 (Before)\\nÎ¼={feature2.mean():.1f}, Ï={feature2.std():.1f}')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 1\n",
        "axes[1, 0].hist(feature1_scaled, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].axvline(feature1_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature1_scaled.mean():.1f}')\n",
        "axes[1, 0].set_xlabel('Value (Z-score)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'Feature 1 (After)\\nÎ¼={feature1_scaled.mean():.2f}, Ï={feature1_scaled.std():.2f}')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# After standardization - Feature 2\n",
        "axes[1, 1].hist(feature2_scaled, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].axvline(feature2_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature2_scaled.mean():.1f}')\n",
        "axes[1, 1].set_xlabel('Value (Z-score)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title(f'Feature 2 (After)\\nÎ¼={feature2_scaled.mean():.2f}, Ï={feature2_scaled.std():.2f}')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Before Standardization:\")\n",
        "print(f\"  Feature 1: Î¼={feature1.mean():.2f}, Ï={feature1.std():.2f}, range=[{feature1.min():.2f}, {feature1.max():.2f}]\")\n",
        "print(f\"  Feature 2: Î¼={feature2.mean():.2f}, Ï={feature2.std():.2f}, range=[{feature2.min():.2f}, {feature2.max():.2f}]\")\n",
        "print(\"\\nAfter Standardization:\")\n",
        "print(f\"  Feature 1: Î¼={feature1_scaled.mean():.2f}, Ï={feature1_scaled.std():.2f}, range=[{feature1_scaled.min():.2f}, {feature1_scaled.max():.2f}]\")\n",
        "print(f\"  Feature 2: Î¼={feature2_scaled.mean():.2f}, Ï={feature2_scaled.std():.2f}, range=[{feature2_scaled.min():.2f}, {feature2_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-CTEjFjHsZA"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Let's demonstrate the importance of feature scaling by comparing scaled vs. unscaled features.\n",
        "\n",
        "### 1. Unscaled Features\n",
        "\n",
        "When features are on different scales (e.g., one feature ranges from 0-100, another from 0-1), KNN will be dominated by the feature with the larger scale, since distance calculations are sensitive to magnitude.\n",
        "\n",
        "### 2. Scaled Features\n",
        "\n",
        "After standardization, all features contribute equally to the distance calculation, typically leading to much better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOqf3NHVHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare model performance with and without feature scaling\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Without scaling\n",
        "knn_raw = MyKNNRegressor(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "y_val_pred_raw = knn_raw.predict(X_val)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(_________)\n",
        "X_val_scaled = scaler.transform(_________)\n",
        "\n",
        "knn_scaled = MyKNNRegressor(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_val_pred_scaled = knn_scaled.predict(X_val_scaled)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Without Scaling - MSE: {mean_squared_error(y_val, y_val_pred_raw):.2f}, \"\n",
        "      f\"RÂ²: {r2_score(y_val, y_val_pred_raw):.4f}\")\n",
        "print(f\"With Scaling    - MSE: {mean_squared_error(y_val, y_val_pred_scaled):.2f}, \"\n",
        "      f\"RÂ²: {r2_score(y_val, y_val_pred_scaled):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyrJaBaMXjwi"
      },
      "source": [
        "> **Question**: When implementing feature scaling in a KNN regression pipeline with train/validation/test splits, which approach correctly prevents data leakage?>> A. Fit StandardScaler on the training set only, then use the same fitted scaler to transform validation and test sets>> B. Fit StandardScaler separately on each dataset (train, validation, test) to ensure each is properly normalized>> C. Fit StandardScaler on the combined training and validation sets (since both are used during development), then transform the test set>> D. Fit StandardScaler on all data before splitting to ensure consistent scaling parameters across all sets<details><summary>Click to reveal answer</summary>**Correct Answer: A****Explanation:****Data leakage** in preprocessing occurs when information from validation or test sets influences the model training or hyperparameter tuning process, leading to overly optimistic performance estimates that don't generalize to new data.**A is TRUE**: Fit on training data only, transform all sets with those parameters:- StandardScaler learns two statistics: mean (Î¼) and standard deviation (Ï) from the training set- These parameters are then applied to transform validation and test sets- This simulates real-world deployment: you only have training data to learn scaling parameters- **Correct workflow:**  ```python  scaler = StandardScaler()  X_train_scaled = scaler.fit_transform(X_train)  # Learn Î¼, Ï from train  X_val_scaled = scaler.transform(X_val)          # Apply train's Î¼, Ï  X_test_scaled = scaler.transform(X_test)        # Apply train's Î¼, Ï  ```**B is FALSE**: Fitting separate scalers causes severe data leakage:- Each dataset would be normalized using its own statistics- **Example of the problem:**  - Training set: Feature1 has Î¼=50, Ï=20 (range: 10-90)  - Test set: Feature1 has Î¼=70, Ï=5 (range: 60-80)  - With separate scalers, both get transformed to Î¼=0, Ï=1  - **Result**: The test scaler \"knows\" that test data is in range 60-80, which is information the model shouldn't have  - This artificially makes test predictions look better than they would in production- **Why it's leakage**: In real deployment, you won't know the test data's distribution in advance**C is FALSE**: Fitting on train+validation leaks validation information:- The scaler learns statistics from the validation set- When tuning hyperparameters using the validation set, you're indirectly using validation data characteristics- **Example:**  - Validation set has outliers that shift the mean  - Scaler fitted on train+val learns a shifted mean  - Model tuning uses this shifted scaling, implicitly \"knowing\" about validation outliers- **Result**: Hyperparameter choices are optimized for data the model has partially \"seen\"**D is FALSE**: Fitting on all data before splitting is the worst option:- The scaler learns statistics from test data- **Completely invalidates test set evaluation** - test performance is unrealistically optimistic- Test set must remain completely unseen until final evaluation- **Example:**  - Test set has extreme values that stretch the scaling range  - Model learns to handle this range because scaler included test data  - Test predictions appear artificially good- This is a common beginner mistake that ruins evaluation validity**Why Data Leakage Matters in Scaling:**Leakage can make bad models appear good:```python# WRONG: Scaling before split (leakage)X_scaled = StandardScaler().fit_transform(X)  # Uses ALL dataX_train, X_test, y_train, y_test = train_test_split(X_scaled, y)# Test RMSE: 0.45 (artificially low - leakage!)# CORRECT: Scaling after split (no leakage)X_train, X_test, y_train, y_test = train_test_split(X, y)scaler = StandardScaler().fit(X_train)  # Uses ONLY trainX_train_scaled = scaler.transform(X_train)X_test_scaled = scaler.transform(X_test)# Test RMSE: 0.52 (realistic - no leakage!)```**Real-World Deployment Analogy:**When your model goes to production:- You have **training data** to compute scaling parameters (Î¼, Ï)- New incoming data is **test data** - you don't know its distribution beforehand- You must apply the **training set's scaling parameters** to new data- This is exactly what Option A simulates!**Key Principle**: Any preprocessing parameters (scaling, imputation, encoding) must be learned from training data only and applied consistently to all sets.</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRCEI0OjQb7p"
      },
      "source": [
        "## Where Do Errors Come From?\n",
        "\n",
        "Not all regions of the feature space are equally predictable. Let's visualize areas of **high certainty** vs **high ambiguity**.\n",
        "\n",
        "### Understanding Prediction Certainty\n",
        "\n",
        "- **High Certainty Regions**: Areas with many nearby training points, where neighbors agree\n",
        "- **High Ambiguity Regions**: Sparse areas or regions where neighbors have varying target values\n",
        "\n",
        "KNN errors are typically higher in:\n",
        "1. Sparse regions (few training samples nearby)\n",
        "2. Boundary regions (where the underlying function changes rapidly)\n",
        "3. Noisy regions (where similar inputs have very different outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yBOfTMHQb7p"
      },
      "outputs": [],
      "source": [
        "# Visualize high certainty vs high ambiguity regions\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create dataset with varying density\n",
        "X_dense = np.random.uniform(0, 5, (100, 2))  # Dense region\n",
        "X_sparse = np.random.uniform(7, 10, (20, 2))  # Sparse region\n",
        "X_combined = np.vstack([X_dense, X_sparse])\n",
        "\n",
        "# Create target with noise\n",
        "y_combined = (2 * X_combined[:, 0] + X_combined[:, 1] +\n",
        "              np.random.randn(len(X_combined)) * 0.5)\n",
        "\n",
        "# Train a KNN model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_combined, y_combined)\n",
        "\n",
        "# Create prediction grid\n",
        "x1_range = np.linspace(-1, 11, 100)\n",
        "x2_range = np.linspace(-1, 11, 100)\n",
        "xx1, xx2 = np.meshgrid(x1_range, x2_range)\n",
        "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "\n",
        "# Get predictions\n",
        "predictions = knn.predict(grid_points)\n",
        "\n",
        "# Calculate local density (number of neighbors within radius)\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "nn_model = NearestNeighbors(radius=1.5)\n",
        "nn_model.fit(X_combined)\n",
        "densities = [len(nn_model.radius_neighbors([point], return_distance=False)[0])\n",
        "             for point in grid_points]\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Predictions with training data\n",
        "scatter1 = axes[0].scatter(X_combined[:, 0], X_combined[:, 1],\n",
        "                           c=y_combined, s=100, cmap='viridis',\n",
        "                           edgecolors='black', linewidth=1.5, zorder=3)\n",
        "contour1 = axes[0].contourf(xx1, xx2, predictions.reshape(xx1.shape),\n",
        "                            levels=20, cmap='viridis', alpha=0.3)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('KNN Predictions (K=5)')\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Target Value')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add region labels\n",
        "axes[0].text(2.5, 8, 'High Certainty\\n(Dense Region)',\n",
        "            bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
        "            fontsize=10, ha='center')\n",
        "axes[0].text(8.5, 8, 'High Ambiguity\\n(Sparse Region)',\n",
        "            bbox=dict(boxstyle='round', facecolor='red', alpha=0.3),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "# Plot 2: Prediction certainty (based on density)\n",
        "contour2 = axes[1].contourf(xx1, xx2, np.array(densities).reshape(xx1.shape),\n",
        "                            levels=20, cmap='RdYlGn', alpha=0.8)\n",
        "axes[1].scatter(X_combined[:, 0], X_combined[:, 1],\n",
        "               c='black', s=50, marker='x', zorder=3, label='Training points')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title('Prediction Certainty\\n(Green=High, Red=Low)')\n",
        "plt.colorbar(contour2, ax=axes[1], label='Number of neighbors')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Prediction Certainty Analysis:\")\n",
        "print(f\"  Dense region (0-5): {len(X_dense)} training points\")\n",
        "print(f\"  Sparse region (7-10): {len(X_sparse)} training points\")\n",
        "print(f\"  Maximum neighbors in radius: {max(densities)}\")\n",
        "print(f\"  Minimum neighbors in radius: {min(densities)}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Green regions: High certainty (many nearby training points)\")\n",
        "print(\"  - Red regions: High ambiguity (few nearby training points)\")\n",
        "print(\"  - Errors are typically higher in red (sparse/ambiguous) regions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP--IGDiHsZA"
      },
      "source": [
        "## Tuning the Hyperparameter KLet's find the optimal K by evaluating different values on the validation set.**Choosing K values to test:**- **Rule of thumb**: Start with $K \\approx \\sqrt{n}$ where $n$ is the number of training samples- For our 180 training samples: $\\sqrt{180} \\approx 13$- We'll test a range around this value: K from 1 to 50- Use validation set performance to select the best K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v0wDkNSHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Find optimal K by evaluating on validation set\n",
        "# TODO: Track both training and validation RMSE\n",
        "\n",
        "# Lists to store metrics\n",
        "train_rmse = []\n",
        "val_rmse = []\n",
        "k_range = range(1, 51)\n",
        "\n",
        "# Loop through different K values\n",
        "for k in k_range:\n",
        "    # Train model with current K\n",
        "    knn = MyKNNRegressor(n_neighbors=_________)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = knn.predict(X_train_scaled)\n",
        "    y_val_pred = knn.predict(_________)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "    val_rmse.append(np.sqrt(mean_squared_error(y_val, _________)))\n",
        "\n",
        "# Find best K\n",
        "best_k = list(k_range)[np.argmin(val_rmse)]\n",
        "print(f\"\\nBest K: {best_k}\")\n",
        "print(f\"Best Validation RMSE: {min(val_rmse):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPHayoNaHsZA"
      },
      "source": [
        "Now, let's plot the RMSE vs. K to visualize the bias-variance tradeoff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nZ3yJGUHsZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(k_range), train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=6)\n",
        "plt.plot(list(k_range), val_rmse, 's-', label='Validation RMSE', linewidth=2, markersize=6)\n",
        "plt.axvline(best_k, linestyle='--', color='red', linewidth=2, label=f'Best K={best_k}')\n",
        "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
        "plt.ylabel('RMSE', fontsize=12)\n",
        "plt.title('Bias-Variance Tradeoff: RMSE vs K', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Small K: Low training RMSE (fits training data closely) but higher validation RMSE (overfitting)\")\n",
        "print(\"- Large K: Training and validation RMSE converge (underfitting - too much smoothing)\")\n",
        "print(f\"- Optimal K={best_k}: Best balance between bias and variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U9eWmHXQb7u"
      },
      "source": [
        "### 3D Regression Surface Visualization\n",
        "\n",
        "Let's visualize how the regression surface changes with different K values in 3D space.\n",
        "This helps us understand:\n",
        "- **K=5**: Optimal balance, smooth surface that captures the trend\n",
        "- **K=10**: Slightly smoother, may miss some local variations\n",
        "- **K=30**: Over-smoothed, underfitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocQc5N06Qb7u"
      },
      "outputs": [],
      "source": [
        "# 3D Regression Surface for different K values\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Use only first 2 features for visualization\n",
        "X_2d = X_train_scaled[:, :2]\n",
        "y_2d = y_train\n",
        "\n",
        "# Create grid for surface\n",
        "x1_range = np.linspace(X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5, 30)\n",
        "x2_range = np.linspace(X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5, 30)\n",
        "xx1, xx2 = np.meshgrid(x1_range, x2_range)\n",
        "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "k_values_3d = [5, 10, 30]\n",
        "titles = ['K=5: Optimal', 'K=10: Balanced', 'K=30: Underfitting']\n",
        "\n",
        "for idx, (k, title) in enumerate(zip(k_values_3d, titles)):\n",
        "    # Train model\n",
        "    knn_3d = MyKNNRegressor(n_neighbors=k)\n",
        "    knn_3d.fit(X_2d, y_2d)\n",
        "\n",
        "    # Predict on grid\n",
        "    predictions = knn_3d.predict(grid_points)\n",
        "    zz = predictions.reshape(xx1.shape)\n",
        "\n",
        "    # Create 3D subplot\n",
        "    ax = fig.add_subplot(1, 3, idx + 1, projection='3d')\n",
        "\n",
        "    # Plot surface\n",
        "    surf = ax.plot_surface(xx1, xx2, zz, cmap='viridis', alpha=0.6, edgecolor='none')\n",
        "\n",
        "    # Plot training points\n",
        "    ax.scatter(X_2d[:, 0], X_2d[:, 1], y_2d, c='red', s=20, alpha=0.6, label='Training data')\n",
        "\n",
        "    ax.set_xlabel('Feature 1 (scaled)')\n",
        "    ax.set_ylabel('Feature 2 (scaled)')\n",
        "    ax.set_zlabel('Target Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(elev=20, azim=45)\n",
        "\n",
        "    # Add colorbar\n",
        "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"3D Surface Interpretation:\")\n",
        "print(\"  K=5:  Captures data trends well, some local variations\")\n",
        "print(\"  K=10: Smoother surface, good generalization\")\n",
        "print(\"  K=30: Very smooth, may be too simple (underfitting)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBEjbpWHsZA"
      },
      "source": [
        "## Comparing with Scikit-Learn's Implementation\n",
        "\n",
        "Let's verify our implementation matches scikit-learn's KNeighborsRegressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAmMXv0PHsZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare your implementation with scikit-learn's KNeighborsRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Our implementation\n",
        "our_knn = MyKNNRegressor(n_neighbors=best_k)\n",
        "our_knn.fit(_________, y_train)\n",
        "y_val_pred_ours = our_knn.predict(X_val_scaled)\n",
        "\n",
        "# Scikit-learn's implementation\n",
        "sklearn_knn = KNeighborsRegressor(n_neighbors=_________)\n",
        "sklearn_knn.fit(X_train_scaled, y_train)\n",
        "y_val_pred_sklearn = sklearn_knn.predict(_________)\n",
        "\n",
        "# Compare\n",
        "print(f\"Our Implementation    - RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred_ours)):.2f}\")\n",
        "print(f\"Scikit-learn          - RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn)):.2f}\")\n",
        "print(f\"Difference: {abs(np.sqrt(mean_squared_error(y_val, y_val_pred_ours)) - np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn))):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_-ncFMQHsZB"
      },
      "source": [
        "## Final Evaluation on Test Set\n",
        "\n",
        "Now that we've chosen the best K using the validation set, let's evaluate on the held-out test set to get an unbiased performance estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgkgrOeHHsZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate final model on test set\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Scale test set\n",
        "X_test_scaled = scaler.transform(_________)\n",
        "\n",
        "# Train final model on combined train+val data\n",
        "X_train_val = np.vstack([X_train_scaled, X_val_scaled])\n",
        "y_train_val = np.concatenate([y_train, y_val])\n",
        "\n",
        "final_knn = MyKNNRegressor(n_neighbors=_________)\n",
        "final_knn.fit(X_train_val, y_train_val)\n",
        "y_test_pred = final_knn.predict(_________)\n",
        "\n",
        "# Evaluation metrics\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Test Set Performance:\")\n",
        "print(f\"  RMSE: {test_rmse:.2f}\")\n",
        "print(f\"  MAE:  {test_mae:.2f}\")\n",
        "print(f\"  RÂ²:   {test_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUrVeb6DHsZB"
      },
      "source": [
        "Visualize predictions vs. actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnFAyEEVHsZB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k', s=50)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "         'r--', lw=2, label='Perfect prediction')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'Test Set: Actual vs Predicted\\n(RÂ² = {test_r2:.3f}, RMSE = {test_rmse:.2f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - y_test_pred\n",
        "plt.scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k', s=50)\n",
        "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals (Actual - Predicted)')\n",
        "plt.title('Residual Plot')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUiTd8QZQb7u"
      },
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "In this lab, we:\n",
        "\n",
        "- **Implemented KNN Regression from scratch**: Built a complete `MyKNNRegressor` class that predicts continuous values by averaging the K nearest neighbors\n",
        "- **Explored the bias-variance tradeoff**: Observed how small K values lead to overfitting (high variance, low bias) while large K values lead to underfitting (low variance, high bias)\n",
        "- **Visualized K progression**: Saw how predictions change as we increase K from 1 to larger values, demonstrating the smoothing effect\n",
        "- **Understood distance metrics**: Learned why Euclidean distance is commonly used and how it's sensitive to feature scales\n",
        "- **Mastered feature scaling**: Discovered that standardization (z-score normalization) is critical for KNN, transforming features to have mean=0 and standard deviation=1\n",
        "- **Applied proper data splitting**: Used train/validation/test splits (60/20/20) to tune hyperparameters without overfitting\n",
        "- **Tuned the hyperparameter K**: Found the optimal K by evaluating validation set performance and observing the U-shaped validation error curve\n",
        "- **Analyzed prediction certainty**: Identified that errors are higher in sparse regions or areas with high ambiguity\n",
        "- **Compared implementations**: Verified our custom KNN regressor matches scikit-learn's implementation\n",
        "- **Evaluated with multiple metrics**: Used MSE, RMSE, MAE, and RÂ² to comprehensively assess model performance\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "1. **KNN is a lazy learner**: No training phase - all computation happens during prediction\n",
        "2. **Feature scaling is essential**: Distance-based algorithms require features on similar scales\n",
        "3. **K is crucial**: The choice of K controls the bias-variance tradeoff\n",
        "4. **Computational cost**: KNN can be slow for large datasets since it requires distance calculations to all training points\n",
        "5. **Works best with**: Low-dimensional data (curse of dimensionality affects high-dimensional spaces)\n",
        "\n",
        "**When to use KNN Regression:**\n",
        "- Small to medium-sized datasets\n",
        "- Non-parametric problems (no assumption about data distribution)\n",
        "- When you need an interpretable model\n",
        "- When the decision boundary is irregular\n",
        "\n",
        "**When NOT to use KNN Regression:**\n",
        "- Very large datasets (computational cost)\n",
        "- High-dimensional data (curse of dimensionality)\n",
        "- When training time is critical (preprocessing step)\n",
        "- When you need a simple model equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sznUkaQb7u"
      },
      "source": [
        "### Final Conceptual Question\n",
        "\n",
        "> **Question**: Which of the following statements about KNN regression is TRUE?\n",
        ">\n",
        "> A. KNN performs better with high-dimensional data due to increased information\n",
        ">\n",
        "> B. Increasing K always improves model performance on the validation set\n",
        ">\n",
        "> C. KNN requires feature scaling because distance calculations are sensitive to feature magnitudes\n",
        ">\n",
        "> D. KNN has a training phase where it learns parameters from the data\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answer</summary>\n",
        "\n",
        "**Correct Answer: C**\n",
        "\n",
        "**Explanation:**\n",
        "- **A is FALSE**: KNN suffers from the \"curse of dimensionality\" - performance degrades in high-dimensional spaces as distances become less meaningful\n",
        "- **B is FALSE**: There's an optimal K value; increasing K beyond this leads to underfitting and worse performance\n",
        "- **C is TRUE**: Distance-based algorithms like KNN are highly sensitive to feature scales, making standardization essential\n",
        "- **D is FALSE**: KNN is a lazy learner with no training phase; all computation happens during prediction\n",
        "\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}