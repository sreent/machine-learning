{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Nearest Neighbors (KNN) Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lab, you will implement a K-Nearest Neighbors regressor from scratch, tune its hyperparameters, and apply it to synthetic and real datasets. Along the way, you'll explore the bias-variance tradeoff, understand the importance of feature scaling, and visualize how different K values affect model predictions. By the end, you will have a clear grasp of how KNN regression works, how to evaluate it using metrics like MSE and RÂ², and why practices like proper data splitting and standardization are critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goLE2YSIHsY9"
      },
      "source": [
        "## Overview of KNN Regression\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "**Algorithm:**\n",
        "1. Given a new data point $\\mathbf{x}_{\\text{new}}$, compute the distance from $\\mathbf{x}_{\\text{new}}$ to every point in the training set.\n",
        "2. Select the K training points closest to $\\mathbf{x}_{\\text{new}}$ (the K nearest neighbors).\n",
        "3. **Average** the target values of these K neighbors to produce the prediction:\n",
        "\n",
        "$$\\hat{y}_{\\text{new}} = \\frac{1}{K} \\sum_{i=1}^{K} y_i$$\n",
        "\n",
        "where $y_i$ are the target values of the K nearest neighbors.\n",
        "\n",
        "**Key Difference from Classification:**\n",
        "- **Classification**: Uses majority voting among K neighbors' class labels\n",
        "- **Regression**: Averages the K neighbors' continuous target values\n",
        "\n",
        "**Example:**\n",
        "If K=3 and the 3 nearest neighbors have target values [2.5, 3.0, 2.8], the prediction would be:\n",
        "$$\\hat{y} = \\frac{2.5 + 3.0 + 2.8}{3} = 2.77$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual K Progression\n\nBefore diving into implementation, let's visualize how the K parameter affects predictions step by step.\n\n### Understanding K Through Visualization\n\nWhen K=1, the algorithm considers only the single nearest neighbor, making predictions highly sensitive to individual data points. As K increases, predictions become smoother by averaging more neighbors, reducing sensitivity to outliers but potentially oversimplifying patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how K affects predictions\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create simple dataset\nnp.random.seed(42)\nX_simple = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\ny_simple = np.array([2.1, 2.8, 3.9, 4.1, 5.2, 5.8, 6.1, 7.2, 8.9, 9.1])\n\n# Test point\nx_test = 5.5\n\n# Visualize K progression\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nk_values = [1, 2, 3, 4, 5, 10]\n\nfor idx, k in enumerate(k_values):\n    ax = axes[idx // 3, idx % 3]\n\n    # Calculate distances\n    distances = np.abs(X_simple.flatten() - x_test)\n    nearest_indices = np.argsort(distances)[:k]\n\n    # Make prediction\n    prediction = np.mean(y_simple[nearest_indices])\n\n    # Plot\n    ax.scatter(X_simple, y_simple, c='blue', s=100, alpha=0.6, label='Training data')\n    ax.scatter(X_simple[nearest_indices], y_simple[nearest_indices],\n               c='red', s=200, marker='*', label=f'K={k} neighbors', zorder=3)\n    ax.scatter(x_test, prediction, c='green', s=300, marker='X',\n               label=f'Prediction={prediction:.2f}', zorder=4)\n\n    # Draw circle showing radius\n    circle = plt.Circle((x_test, prediction), distances[nearest_indices[-1]],\n                        color='red', fill=False, linestyle='--', alpha=0.3)\n    ax.add_patch(circle)\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('y')\n    ax.set_title(f'K = {k}: Prediction = {prediction:.2f}')\n    ax.legend(fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Test point X = {x_test}\")\nfor k in k_values:\n    distances = np.abs(X_simple.flatten() - x_test)\n    nearest_indices = np.argsort(distances)[:k]\n    prediction = np.mean(y_simple[nearest_indices])\n    print(f\"K={k:2d}: Prediction = {prediction:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKgu6j9HsY9"
      },
      "source": [
        "> **Question**: In KNN regression, the parameter K refers to:\n",
        ">\n",
        "> A. The number of features in the dataset  \n",
        ">\n",
        "> B. The number of nearest neighbors whose target values are averaged for prediction  \n",
        ">\n",
        "> C. The number of clusters in the data  \n",
        ">\n",
        "> D. The number of iterations for training\n",
        ">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAZ9wveaHsY9"
      },
      "source": [
        "## Distance Metrics and Feature Scaling\n",
        "\n",
        "**Common Distance Metrics:**\n",
        "- **Euclidean (L2)**: $d(\\mathbf{x}, \\mathbf{x}') = \\sqrt{\\sum_{i=1}^{n} (x_i - x_i')^2}$\n",
        "- **Manhattan (L1)**: $d(\\mathbf{x}, \\mathbf{x}') = \\sum_{i=1}^{n} |x_i - x_i'|$\n",
        "- **Minkowski**: Generalization of both (p=1 is Manhattan, p=2 is Euclidean)\n",
        "\n",
        "**Feature Scaling is Critical:**\n",
        "- Features with larger ranges dominate distance calculations\n",
        "- Example: If feature1 ranges from 0-1 and feature2 from 0-1000, feature2 will dominate\n",
        "- **Solution**: Standardize features to have mean=0 and std=1, or normalize to [0,1] range\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "where $\\mu$ is the mean and $\\sigma$ is the standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJYpmTLnHsY9"
      },
      "source": [
        "> **Question**: When using KNN on a dataset with features measured in very different scales (e.g., age in years vs. income in dollars), what should you do?\n",
        ">\n",
        "> A. NothingâKNN handles different scales automatically  \n",
        ">\n",
        "> B. Remove the feature with the largest scale  \n",
        ">\n",
        "> C. Standardize or normalize all features to comparable ranges  \n",
        ">\n",
        "> D. Use only categorical features\n",
        ">\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudocode for KNN Regressor\n",
        "\n",
        "Before implementing the algorithm, let's understand the pseudocode structure (from lecture slides):\n",
        "\n",
        "```\n",
        "# Inputs\n",
        "#   data      â training set of N examples (x, y)\n",
        "#   k         â number of neighbours\n",
        "#   metric    â distance function (e.g., Euclidean, Manhattan)\n",
        "#   X_query   â set of examples to predict\n",
        "\n",
        "# ----- \"fit\" (lazy) -----\n",
        "X_train â data.x\n",
        "y_train â data.y\n",
        "\n",
        "# ----- predict -----\n",
        "Å· â list of length |X_query|  # outputs align 1:1 with X_query\n",
        "\n",
        "FOR i = 1 TO |X_query| DO\n",
        "    x* â X_query[i]\n",
        "    d â distances from x* to all X_train using metric\n",
        "    J â indices of the k smallest values in d\n",
        "    # regression prediction = average of the targets of the k nearest neighbours\n",
        "    Å·[i] â mean(y_train[J])\n",
        "END FOR\n",
        "\n",
        "RETURN Å·\n",
        "```\n",
        "\n",
        "**Key Points:**\n",
        "- KNN is a \"lazy learner\" - the `fit` method just stores the training data\n",
        "- The `predict` method does all the work: compute distances, find K nearest, average their targets\n",
        "- **Regression uses averaging** (not voting like classification)"
      ],
      "metadata": {
        "id": "vZYOgx4NHsY-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWwk1-fnHsY-"
      },
      "source": [
        "## Implementing a Custom KNN Regressor\n",
        "\n",
        "Below is a scaffold of the `MyKNNRegressor` class. Fill in the TODO sections to complete the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PkxAmvXHsY-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "class MyKNNRegressor:\n",
        "    \"\"\"\n",
        "    A simple K-Nearest Neighbors Regressor implementation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_neighbors : int, default=5\n",
        "        Number of neighbors to use for prediction\n",
        "    metric : str, default='euclidean'\n",
        "        Distance metric to use ('euclidean', 'manhattan', etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.metric = metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the KNN regressor by storing the training data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target values\n",
        "        \"\"\"\n",
        "        # TODO: Store the training data\n",
        "        # Hint: KNN is a \"lazy learner\" - it just stores the training data\n",
        "        self.X_train = ___\n",
        "        self.y_train = ___\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for test data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array of shape (n_samples,)\n",
        "            Predicted target values\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for x_test in X:\n",
        "            # TODO: For each test point:\n",
        "            # 1. Compute distances from x_test to all training points\n",
        "            # 2. Find indices of K nearest neighbors\n",
        "            # 3. Get target values of those K neighbors\n",
        "            # 4. Average them to get the prediction\n",
        "\n",
        "            # Step 1: Compute distances\n",
        "            distances = pairwise_distances(\n",
        "                x_test.reshape(1, -1),\n",
        "                self._____,\n",
        "                metric=self.metric\n",
        "            ).ravel()\n",
        "\n",
        "            # Step 2: Find K nearest neighbor indices\n",
        "            k_nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
        "\n",
        "            # Step 3: Get target values of K nearest neighbors\n",
        "            k_nearest_targets = self.y_train[______]\n",
        "\n",
        "            # Step 4: Average the target values\n",
        "            prediction = np.mean(_______)\n",
        "\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "print(\"MyKNNRegressor class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNGRBbBHsY-"
      },
      "source": [
        "Once you have filled in the implementation, let's test our custom regressor on a simple dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODx0DF2HsY_"
      },
      "source": [
        "## A Dataset for Visualization\n",
        "\n",
        "We'll create a synthetic 1D regression dataset to visualize how KNN regression works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdmajwTrHsY_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic 1D data\n",
        "def f(x):\n",
        "    \"\"\"True underlying function: sinusoidal pattern\"\"\"\n",
        "    return np.sin(x) + 0.1 * x\n",
        "\n",
        "# Training data\n",
        "X_train_1d = np.sort(np.random.uniform(0, 10, 100))\n",
        "y_train_1d = f(X_train_1d) + np.random.normal(0, 0.2, 100)  # Add noise\n",
        "\n",
        "# Reshape for sklearn compatibility\n",
        "X_train_1d = X_train_1d.reshape(-1, 1)\n",
        "\n",
        "# Test data (for smooth prediction curve)\n",
        "X_test_1d = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "y_test_1d_true = f(X_test_1d.ravel())\n",
        "\n",
        "print(f\"Training data shape: X={X_train_1d.shape}, y={y_train_1d.shape}\")\n",
        "print(f\"Test data shape: X={X_test_1d.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10nBkhcNHsY_"
      },
      "source": [
        "Let's visualize the training data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual step-by-step prediction (matching lecture slides 11-13)\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Choose a test point (let's pick x=5.0 from our test range)\n",
        "x_test_single = np.array([[5.0]])  # Must be 2D for sklearn\n",
        "print(\"=\"*70)\n",
        "print(\"STEP-BY-STEP: How KNN Predicts for x_test = 5.0\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Calculate distances from test point to ALL training points\n",
        "print(\"\\nSTEP 1: Pairwise Distance Calculation\")\n",
        "print(\"-\" * 70)\n",
        "distances = pairwise_distances(x_test_single, X_train_1d, metric='euclidean').ravel()\n",
        "print(f\"Computed {len(distances)} distances from x_test=5.0 to all training points\")\n",
        "print(f\"Distance array shape: {distances.shape}\")\n",
        "print(f\"First 10 distances: {distances[:10].round(3)}\")\n",
        "\n",
        "# Step 2: Find K nearest neighbors\n",
        "print(\"\\nSTEP 2: Finding K Nearest Neighbors\")\n",
        "print(\"-\" * 70)\n",
        "K = 5\n",
        "k_indices = np.argsort(distances)[:K]  # Indices of K smallest distances\n",
        "k_distances = distances[k_indices]\n",
        "print(f\"K = {K}\")\n",
        "print(f\"Indices of K nearest neighbors: {k_indices}\")\n",
        "print(f\"Their distances: {k_distances.round(3)}\")\n",
        "print(f\"Their x-coordinates: {X_train_1d[k_indices].ravel().round(3)}\")\n",
        "\n",
        "# Step 3: Get target values of K nearest neighbors\n",
        "print(\"\\nSTEP 3: Get Neighbor Targets\")\n",
        "print(\"-\" * 70)\n",
        "k_targets = y_train_1d[k_indices]\n",
        "print(f\"NN's Labels (target values): {k_targets.round(3)}\")\n",
        "\n",
        "# Step 4: Average to get prediction\n",
        "print(\"\\nSTEP 4: Average (THE PREDICTION!)\")\n",
        "print(\"-\" * 70)\n",
        "manual_prediction = np.mean(k_targets)\n",
        "print(f\"Prediction = mean({k_targets.round(3)})\")\n",
        "print(f\"Prediction = {manual_prediction:.3f}\")\n",
        "\n",
        "# Verify with our custom KNN\n",
        "knn_verify = MyKNNRegressor(n_neighbors=K)\n",
        "knn_verify.fit(X_train_1d, y_train_1d)\n",
        "verify_prediction = knn_verify.predict(x_test_single)[0]\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFICATION:\")\n",
        "print(f\"  Manual calculation:  {manual_prediction:.3f}\")\n",
        "print(f\"  MyKNNRegressor:      {verify_prediction:.3f}\")\n",
        "print(f\"  Match: {np.isclose(manual_prediction, verify_prediction)} â\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Visualize this specific prediction\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(X_train_1d, y_train_1d, c='lightblue', alpha=0.6, edgecolor='k', s=50, label='All training data')\n",
        "plt.scatter(X_train_1d[k_indices], k_targets, c='red', s=100, edgecolor='k',\n",
        "           label=f'K={K} nearest neighbors', zorder=5)\n",
        "plt.scatter(x_test_single, manual_prediction, c='green', s=200, marker='*',\n",
        "           edgecolor='k', label='Prediction (average)', zorder=10)\n",
        "plt.axhline(manual_prediction, color='green', linestyle='--', alpha=0.3)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title(f'Step-by-Step KNN Prediction: x_test={x_test_single[0,0]}, K={K}, Prediction={manual_prediction:.3f}')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uVUFYcjhHsY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Prediction Process: Step-by-Step\n",
        "\n",
        "Before we test different K values, let's manually walk through exactly what happens when KNN makes a single prediction. This matches the step-by-step process from the lecture slides (slides 11-13).\n",
        "\n",
        "**The 4 Steps:**\n",
        "1. **Pairwise Distance Calculation**: Compute distance from test point to all training points\n",
        "2. **Finding K Nearest Neighbors**: Sort distances and select K smallest\n",
        "3. **Get Neighbor Targets**: Retrieve the y-values of those K neighbors  \n",
        "4. **Average**: Compute mean of the K target values â this is the prediction!\n",
        "\n",
        "Let's demonstrate this process:"
      ],
      "metadata": {
        "id": "8vHx0M4NHsY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W58gNJ3hHsY_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training data\n",
        "plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, edgecolor='k', s=50, label='Training data')\n",
        "plt.plot(X_test_1d, y_test_1d_true, 'g--', lw=2, label='True function')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Training Data with True Underlying Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Complexity and Bias-Variance Tradeoff\n\nLet's explore how different K values affect model complexity by testing a wider range: K=1, 5, 10, 15, 30, 50.\n\n### Understanding the Tradeoff\n\n- **Small K (e.g., K=1)**: High complexity, low bias, high variance â **Overfitting**\n- **Moderate K (e.g., K=5-10)**: Balanced complexity â **Optimal**\n- **Large K (e.g., K=30-50)**: Low complexity, high bias, low variance â **Underfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test extended range of K values with 2x3 subplot comparison\nk_values_extended = [1, 5, 10, 15, 30, 50]\n\nplt.figure(figsize=(18, 12))\n\nfor idx, k in enumerate(k_values_extended):\n    # Train model\n    knn_model = MyKNNRegressor(n_neighbors=k)\n    knn_model.fit(X_train_1d, y_train_1d)\n\n    # Predictions\n    X_plot = np.linspace(X_train_1d.min(), X_train_1d.max(), 300).reshape(-1, 1)\n    y_plot = knn_model.predict(X_plot)\n\n    # Create subplot\n    plt.subplot(2, 3, idx + 1)\n    plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, s=50, label='Training data')\n    plt.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'K={k} prediction')\n    plt.xlabel('X')\n    plt.ylabel('y')\n\n    # Label overfitting/optimal/underfitting\n    if k <= 3:\n        region_label = 'OVERFITTING'\n        color = 'red'\n    elif k <= 15:\n        region_label = 'OPTIMAL'\n        color = 'green'\n    else:\n        region_label = 'UNDERFITTING'\n        color = 'orange'\n\n    plt.title(f'K = {k}\\n[{region_label}]', color=color, fontweight='bold')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Model Complexity Analysis:\")\nprint(\"  K=1:  Overfitting - follows training data too closely\")\nprint(\"  K=5:  Optimal - good balance\")\nprint(\"  K=10: Optimal - smooth and generalizes well\")\nprint(\"  K=15: Still good - slightly smoother\")\nprint(\"  K=30: Underfitting - too smooth, misses patterns\")\nprint(\"  K=50: Severe underfitting - barely captures the trend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZHRGKrHsY_"
      },
      "source": [
        "> **Question**: Looking at the plots above, which K value shows signs of overfitting?\n",
        ">\n",
        "> A. K=30 (too smooth, doesn't follow the data closely)  \n",
        ">\n",
        "> B. K=1 (too wiggly, follows every noise point)  \n",
        ">\n",
        "> C. K=10 (balanced smoothness)  \n",
        ">\n",
        "> D. All of them equally\n",
        ">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression Line and MSE Formulas\n\n### Understanding Prediction Errors\n\nIn regression, we measure how well our predictions match the actual values. The **error** (also called **residual**) for a single prediction is:\n\n$$\\text{Error (Residue)} = y - \\hat{y}$$\n\nWhere:\n- $y$ is the actual value\n- $\\hat{y}$ is the predicted value\n\n### Mean Squared Error (MSE)\n\nTo evaluate overall model performance, we use Mean Squared Error:\n\n$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2$$\n\nWhere:\n- $N$ is the number of samples\n- Squaring emphasizes larger errors\n- Lower MSE indicates better predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize over-predicted and under-predicted points\nnp.random.seed(42)\nX_error = np.linspace(0, 10, 20).reshape(-1, 1)\ny_true = 2 * X_error.flatten() + np.random.randn(20) * 2\n\n# Simple predictions (not perfect)\ny_pred = 2 * X_error.flatten() + np.random.randn(20) * 1.5\n\n# Calculate errors\nerrors = y_true - y_pred\nmse = np.mean(errors**2)\n\n# Visualize\nplt.figure(figsize=(12, 6))\n\n# Plot 1: Predictions vs Actual\nplt.subplot(1, 2, 1)\nplt.scatter(X_error, y_true, c='blue', s=100, alpha=0.6, label='Actual values')\nplt.scatter(X_error, y_pred, c='red', s=100, marker='s', alpha=0.6, label='Predictions')\n\n# Draw error lines\nfor i in range(len(X_error)):\n    color = 'green' if y_pred[i] > y_true[i] else 'orange'\n    plt.plot([X_error[i], X_error[i]], [y_true[i], y_pred[i]],\n             color=color, linestyle='--', alpha=0.5)\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Predictions vs Actual Values')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Error distribution\nplt.subplot(1, 2, 2)\ncolors = ['green' if e < 0 else 'orange' for e in errors]\nplt.bar(range(len(errors)), errors, color=colors, alpha=0.6)\nplt.axhline(y=0, color='black', linestyle='-', linewidth=1)\nplt.xlabel('Sample Index')\nplt.ylabel('Error (Residual)')\nplt.title(f'Prediction Errors\\nMSE = {mse:.2f}\\nGreen=Over-predicted, Orange=Under-predicted')\nplt.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Mean Squared Error: {mse:.2f}\")\nprint(f\"Root Mean Squared Error: {np.sqrt(mse):.2f}\")\nprint(f\"Mean Absolute Error: {np.mean(np.abs(errors)):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkGHoWtlHsY_"
      },
      "source": [
        "## Working with a Real Dataset\n",
        "\n",
        "Now let's work with a real 2D regression dataset and compare our implementation with scikit-learn's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX9c--OnHsY_"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate a 2D regression dataset with 300 samples and 2 features\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_regression(n_samples=_________, n_features=_________,\n",
        "                       n_informative=2, noise=15.0, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yePX5lxHsY_"
      },
      "source": [
        "Visualize the 2D dataset with target values as colors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMkQN3_HHsZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis',\n",
        "                     edgecolor='k', s=50, alpha=0.7)\n",
        "plt.colorbar(scatter, label='Target Value')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('2D Regression Dataset (color represents target value)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR-wkI2AHsZA"
      },
      "source": [
        "## Splitting into Train, Validation, and Test Sets\n",
        "\n",
        "We split the data into:\n",
        "- **Training set (60%)**: To fit the model\n",
        "- **Validation set (20%)**: To tune hyperparameters (K)\n",
        "- **Test set (20%)**: For final unbiased performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYs7A6NhHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Split data into train (60%), validation (20%), and test (20%) sets\n",
        "# Split into train, validation, and test sets (60/20/20)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=_________)\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=_________, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzm6uTyVHsZA"
      },
      "source": [
        "After this split, you should see roughly: Train size 180, Validation size 60, Test size 60."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZqv4VLpHsZA"
      },
      "source": [
        "> **Question**: Why do we use a separate validation set instead of tuning the hyperparameters directly on the test set?\n",
        ">\n",
        "> A. The test set is too small  \n",
        ">\n",
        "> B. To prevent overfitting to the test set and get an unbiased final performance estimate  \n",
        ">\n",
        "> C. The validation set trains faster  \n",
        ">\n",
        "> D. It's just a convention with no real benefit\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Z-Score Standardization\n\nBefore we explore feature scaling in practice, let's understand the mathematical foundation.\n\n### The Z-Score Formula\n\nStandardization (Z-score normalization) transforms features to have mean $\\mu = 0$ and standard deviation $\\sigma = 1$:\n\n$$Z = \\frac{x - \\mu}{\\sigma}$$\n\nWhere:\n- $x$ is the original value\n- $\\mu$ is the mean of the feature\n- $\\sigma$ is the standard deviation of the feature\n\n### Why Z-Score Standardization Works\n\n1. **Centers the data**: Subtracting the mean shifts the distribution to be centered at zero\n2. **Scales the spread**: Dividing by standard deviation makes the spread consistent across features\n3. **Preserves distribution shape**: Unlike min-max scaling, z-score maintains the original distribution's shape\n4. **Handles outliers better**: Less sensitive to extreme values compared to min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Z-score standardization\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(42)\n# Create two features with different scales\nfeature1 = np.random.normal(loc=100, scale=15, size=200)  # Mean=100, SD=15\nfeature2 = np.random.normal(loc=5, scale=2, size=200)     # Mean=5, SD=2\n\n# Standardize\nscaler = StandardScaler()\nfeature1_scaled = scaler.fit_transform(feature1.reshape(-1, 1)).flatten()\nfeature2_scaled = scaler.fit_transform(feature2.reshape(-1, 1)).flatten()\n\n# Visualize\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Before standardization - Feature 1\naxes[0, 0].hist(feature1, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\naxes[0, 0].axvline(feature1.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature1.mean():.1f}')\naxes[0, 0].set_xlabel('Value')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title(f'Feature 1 (Before)\\nÎ¼={feature1.mean():.1f}, Ï={feature1.std():.1f}')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Before standardization - Feature 2\naxes[0, 1].hist(feature2, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\naxes[0, 1].axvline(feature2.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature2.mean():.1f}')\naxes[0, 1].set_xlabel('Value')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title(f'Feature 2 (Before)\\nÎ¼={feature2.mean():.1f}, Ï={feature2.std():.1f}')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# After standardization - Feature 1\naxes[1, 0].hist(feature1_scaled, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\naxes[1, 0].axvline(feature1_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature1_scaled.mean():.1f}')\naxes[1, 0].set_xlabel('Value (Z-score)')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title(f'Feature 1 (After)\\nÎ¼={feature1_scaled.mean():.2f}, Ï={feature1_scaled.std():.2f}')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# After standardization - Feature 2\naxes[1, 1].hist(feature2_scaled, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\naxes[1, 1].axvline(feature2_scaled.mean(), color='red', linestyle='--', linewidth=2, label=f'Î¼={feature2_scaled.mean():.1f}')\naxes[1, 1].set_xlabel('Value (Z-score)')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title(f'Feature 2 (After)\\nÎ¼={feature2_scaled.mean():.2f}, Ï={feature2_scaled.std():.2f}')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Before Standardization:\")\nprint(f\"  Feature 1: Î¼={feature1.mean():.2f}, Ï={feature1.std():.2f}, range=[{feature1.min():.2f}, {feature1.max():.2f}]\")\nprint(f\"  Feature 2: Î¼={feature2.mean():.2f}, Ï={feature2.std():.2f}, range=[{feature2.min():.2f}, {feature2.max():.2f}]\")\nprint(\"\\nAfter Standardization:\")\nprint(f\"  Feature 1: Î¼={feature1_scaled.mean():.2f}, Ï={feature1_scaled.std():.2f}, range=[{feature1_scaled.min():.2f}, {feature1_scaled.max():.2f}]\")\nprint(f\"  Feature 2: Î¼={feature2_scaled.mean():.2f}, Ï={feature2_scaled.std():.2f}, range=[{feature2_scaled.min():.2f}, {feature2_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-CTEjFjHsZA"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Let's demonstrate the importance of feature scaling by comparing scaled vs. unscaled features.\n",
        "\n",
        "### 1. Unscaled Features\n",
        "\n",
        "When features are on different scales (e.g., one feature ranges from 0-100, another from 0-1), KNN will be dominated by the feature with the larger scale, since distance calculations are sensitive to magnitude.\n",
        "\n",
        "### 2. Scaled Features\n",
        "\n",
        "After standardization, all features contribute equally to the distance calculation, typically leading to much better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOqf3NHVHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare model performance with and without feature scaling\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Without scaling\n",
        "knn_raw = MyKNNRegressor(n_neighbors=5)\n",
        "knn_raw.fit(X_train, y_train)\n",
        "y_val_pred_raw = knn_raw.predict(X_val)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(_________)\n",
        "X_val_scaled = scaler.transform(_________)\n",
        "\n",
        "knn_scaled = MyKNNRegressor(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_val_pred_scaled = knn_scaled.predict(X_val_scaled)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Without Scaling - MSE: {mean_squared_error(y_val, y_val_pred_raw):.2f}, \"\n",
        "      f\"RÂ²: {r2_score(y_val, y_val_pred_raw):.4f}\")\n",
        "print(f\"With Scaling    - MSE: {mean_squared_error(y_val, y_val_pred_scaled):.2f}, \"\n",
        "      f\"RÂ²: {r2_score(y_val, y_val_pred_scaled):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where Do Errors Come From?\n\nNot all regions of the feature space are equally predictable. Let's visualize areas of **high certainty** vs **high ambiguity**.\n\n### Understanding Prediction Certainty\n\n- **High Certainty Regions**: Areas with many nearby training points, where neighbors agree\n- **High Ambiguity Regions**: Sparse areas or regions where neighbors have varying target values\n\nKNN errors are typically higher in:\n1. Sparse regions (few training samples nearby)\n2. Boundary regions (where the underlying function changes rapidly)\n3. Noisy regions (where similar inputs have very different outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize high certainty vs high ambiguity regions\nnp.random.seed(42)\n\n# Create dataset with varying density\nX_dense = np.random.uniform(0, 5, (100, 2))  # Dense region\nX_sparse = np.random.uniform(7, 10, (20, 2))  # Sparse region\nX_combined = np.vstack([X_dense, X_sparse])\n\n# Create target with noise\ny_combined = (2 * X_combined[:, 0] + X_combined[:, 1] +\n              np.random.randn(len(X_combined)) * 0.5)\n\n# Train a KNN model\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_combined, y_combined)\n\n# Create prediction grid\nx1_range = np.linspace(-1, 11, 100)\nx2_range = np.linspace(-1, 11, 100)\nxx1, xx2 = np.meshgrid(x1_range, x2_range)\ngrid_points = np.c_[xx1.ravel(), xx2.ravel()]\n\n# Get predictions\npredictions = knn.predict(grid_points)\n\n# Calculate local density (number of neighbors within radius)\nfrom sklearn.neighbors import NearestNeighbors\nnn_model = NearestNeighbors(radius=1.5)\nnn_model.fit(X_combined)\ndensities = [len(nn_model.radius_neighbors([point], return_distance=False)[0])\n             for point in grid_points]\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot 1: Predictions with training data\nscatter1 = axes[0].scatter(X_combined[:, 0], X_combined[:, 1],\n                           c=y_combined, s=100, cmap='viridis',\n                           edgecolors='black', linewidth=1.5, zorder=3)\ncontour1 = axes[0].contourf(xx1, xx2, predictions.reshape(xx1.shape),\n                            levels=20, cmap='viridis', alpha=0.3)\naxes[0].set_xlabel('Feature 1')\naxes[0].set_ylabel('Feature 2')\naxes[0].set_title('KNN Predictions (K=5)')\nplt.colorbar(scatter1, ax=axes[0], label='Target Value')\naxes[0].grid(True, alpha=0.3)\n\n# Add region labels\naxes[0].text(2.5, 8, 'High Certainty\\n(Dense Region)',\n            bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n            fontsize=10, ha='center')\naxes[0].text(8.5, 8, 'High Ambiguity\\n(Sparse Region)',\n            bbox=dict(boxstyle='round', facecolor='red', alpha=0.3),\n            fontsize=10, ha='center')\n\n# Plot 2: Prediction certainty (based on density)\ncontour2 = axes[1].contourf(xx1, xx2, np.array(densities).reshape(xx1.shape),\n                            levels=20, cmap='RdYlGn', alpha=0.8)\naxes[1].scatter(X_combined[:, 0], X_combined[:, 1],\n               c='black', s=50, marker='x', zorder=3, label='Training points')\naxes[1].set_xlabel('Feature 1')\naxes[1].set_ylabel('Feature 2')\naxes[1].set_title('Prediction Certainty\\n(Green=High, Red=Low)')\nplt.colorbar(contour2, ax=axes[1], label='Number of neighbors')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Prediction Certainty Analysis:\")\nprint(f\"  Dense region (0-5): {len(X_dense)} training points\")\nprint(f\"  Sparse region (7-10): {len(X_sparse)} training points\")\nprint(f\"  Maximum neighbors in radius: {max(densities)}\")\nprint(f\"  Minimum neighbors in radius: {min(densities)}\")\nprint(\"\\nInterpretation:\")\nprint(\"  - Green regions: High certainty (many nearby training points)\")\nprint(\"  - Red regions: High ambiguity (few nearby training points)\")\nprint(\"  - Errors are typically higher in red (sparse/ambiguous) regions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP--IGDiHsZA"
      },
      "source": [
        "## Tuning the Hyperparameter K\n",
        "\n",
        "Let's find the optimal K by evaluating different values on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v0wDkNSHsZA"
      },
      "outputs": [],
      "source": [
        "# TODO: Find optimal K by evaluating on validation set\n",
        "# TODO: Track both training and validation RMSE\n",
        "\n",
        "# Lists to store metrics\n",
        "train_rmse = []\n",
        "val_rmse = []\n",
        "k_range = range(1, 51)\n",
        "\n",
        "# Loop through different K values\n",
        "for k in k_range:\n",
        "    # Train model with current K\n",
        "    knn = MyKNNRegressor(n_neighbors=_________)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = knn.predict(X_train_scaled)\n",
        "    y_val_pred = knn.predict(_________)\n",
        "    \n",
        "    # Calculate RMSE\n",
        "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "    val_rmse.append(np.sqrt(mean_squared_error(y_val, _________)))\n",
        "\n",
        "# Find best K\n",
        "best_k = list(k_range)[np.argmin(val_rmse)]\n",
        "print(f\"\\nBest K: {best_k}\")\n",
        "print(f\"Best Validation RMSE: {min(val_rmse):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPHayoNaHsZA"
      },
      "source": [
        "Now, let's plot the RMSE vs. K to visualize the bias-variance tradeoff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nZ3yJGUHsZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(k_range), train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=6)\n",
        "plt.plot(list(k_range), val_rmse, 's-', label='Validation RMSE', linewidth=2, markersize=6)\n",
        "plt.axvline(best_k, linestyle='--', color='red', linewidth=2, label=f'Best K={best_k}')\n",
        "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
        "plt.ylabel('RMSE', fontsize=12)\n",
        "plt.title('Bias-Variance Tradeoff: RMSE vs K', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Small K: Low training RMSE (fits training data closely) but higher validation RMSE (overfitting)\")\n",
        "print(\"- Large K: Training and validation RMSE converge (underfitting - too much smoothing)\")\n",
        "print(f\"- Optimal K={best_k}: Best balance between bias and variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3D Regression Surface Visualization\n\nLet's visualize how the regression surface changes with different K values in 3D space.\nThis helps us understand:\n- **K=5**: Optimal balance, smooth surface that captures the trend\n- **K=10**: Slightly smoother, may miss some local variations\n- **K=30**: Over-smoothed, underfitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3D Regression Surface for different K values\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Use only first 2 features for visualization\nX_2d = X_train_scaled[:, :2]\ny_2d = y_train\n\n# Create grid for surface\nx1_range = np.linspace(X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5, 30)\nx2_range = np.linspace(X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5, 30)\nxx1, xx2 = np.meshgrid(x1_range, x2_range)\ngrid_points = np.c_[xx1.ravel(), xx2.ravel()]\n\n# Create figure with subplots\nfig = plt.figure(figsize=(18, 5))\nk_values_3d = [5, 10, 30]\ntitles = ['K=5: Optimal', 'K=10: Balanced', 'K=30: Underfitting']\n\nfor idx, (k, title) in enumerate(zip(k_values_3d, titles)):\n    # Train model\n    knn_3d = MyKNNRegressor(n_neighbors=k)\n    knn_3d.fit(X_2d, y_2d)\n\n    # Predict on grid\n    predictions = knn_3d.predict(grid_points)\n    zz = predictions.reshape(xx1.shape)\n\n    # Create 3D subplot\n    ax = fig.add_subplot(1, 3, idx + 1, projection='3d')\n\n    # Plot surface\n    surf = ax.plot_surface(xx1, xx2, zz, cmap='viridis', alpha=0.6, edgecolor='none')\n\n    # Plot training points\n    ax.scatter(X_2d[:, 0], X_2d[:, 1], y_2d, c='red', s=20, alpha=0.6, label='Training data')\n\n    ax.set_xlabel('Feature 1 (scaled)')\n    ax.set_ylabel('Feature 2 (scaled)')\n    ax.set_zlabel('Target Value')\n    ax.set_title(title)\n    ax.view_init(elev=20, azim=45)\n\n    # Add colorbar\n    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"3D Surface Interpretation:\")\nprint(\"  K=5:  Captures data trends well, some local variations\")\nprint(\"  K=10: Smoother surface, good generalization\")\nprint(\"  K=30: Very smooth, may be too simple (underfitting)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBEjbpWHsZA"
      },
      "source": [
        "## Comparing with Scikit-Learn's Implementation\n",
        "\n",
        "Let's verify our implementation matches scikit-learn's KNeighborsRegressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAmMXv0PHsZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare your implementation with scikit-learn's KNeighborsRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Our implementation\n",
        "our_knn = MyKNNRegressor(n_neighbors=best_k)\n",
        "our_knn.fit(_________, y_train)\n",
        "y_val_pred_ours = our_knn.predict(X_val_scaled)\n",
        "\n",
        "# Scikit-learn's implementation\n",
        "sklearn_knn = KNeighborsRegressor(n_neighbors=_________)\n",
        "sklearn_knn.fit(X_train_scaled, y_train)\n",
        "y_val_pred_sklearn = sklearn_knn.predict(_________)\n",
        "\n",
        "# Compare\n",
        "print(f\"Our Implementation    - RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred_ours)):.2f}\")\n",
        "print(f\"Scikit-learn          - RMSE: {np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn)):.2f}\")\n",
        "print(f\"Difference: {abs(np.sqrt(mean_squared_error(y_val, y_val_pred_ours)) - np.sqrt(mean_squared_error(y_val, y_val_pred_sklearn))):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_-ncFMQHsZB"
      },
      "source": [
        "## Final Evaluation on Test Set\n",
        "\n",
        "Now that we've chosen the best K using the validation set, let's evaluate on the held-out test set to get an unbiased performance estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgkgrOeHHsZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate final model on test set\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Scale test set\n",
        "X_test_scaled = scaler.transform(_________)\n",
        "\n",
        "# Train final model on combined train+val data\n",
        "X_train_val = np.vstack([X_train_scaled, X_val_scaled])\n",
        "y_train_val = np.concatenate([y_train, y_val])\n",
        "\n",
        "final_knn = MyKNNRegressor(n_neighbors=_________)\n",
        "final_knn.fit(X_train_val, y_train_val)\n",
        "y_test_pred = final_knn.predict(_________)\n",
        "\n",
        "# Evaluation metrics\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Test Set Performance:\")\n",
        "print(f\"  RMSE: {test_rmse:.2f}\")\n",
        "print(f\"  MAE:  {test_mae:.2f}\")\n",
        "print(f\"  RÂ²:   {test_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUrVeb6DHsZB"
      },
      "source": [
        "Visualize predictions vs. actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnFAyEEVHsZB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k', s=50)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "         'r--', lw=2, label='Perfect prediction')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'Test Set: Actual vs Predicted\\n(RÂ² = {test_r2:.3f}, RMSE = {test_rmse:.2f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - y_test_pred\n",
        "plt.scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k', s=50)\n",
        "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals (Actual - Predicted)')\n",
        "plt.title('Residual Plot')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Takeaways\n\nIn this lab, we:\n\n- **Implemented KNN Regression from scratch**: Built a complete `MyKNNRegressor` class that predicts continuous values by averaging the K nearest neighbors\n- **Explored the bias-variance tradeoff**: Observed how small K values lead to overfitting (high variance, low bias) while large K values lead to underfitting (low variance, high bias)\n- **Visualized K progression**: Saw how predictions change as we increase K from 1 to larger values, demonstrating the smoothing effect\n- **Understood distance metrics**: Learned why Euclidean distance is commonly used and how it's sensitive to feature scales\n- **Mastered feature scaling**: Discovered that standardization (z-score normalization) is critical for KNN, transforming features to have mean=0 and standard deviation=1\n- **Applied proper data splitting**: Used train/validation/test splits (60/20/20) to tune hyperparameters without overfitting\n- **Tuned the hyperparameter K**: Found the optimal K by evaluating validation set performance and observing the U-shaped validation error curve\n- **Analyzed prediction certainty**: Identified that errors are higher in sparse regions or areas with high ambiguity\n- **Compared implementations**: Verified our custom KNN regressor matches scikit-learn's implementation\n- **Evaluated with multiple metrics**: Used MSE, RMSE, MAE, and RÂ² to comprehensively assess model performance\n\n**Key Insights:**\n\n1. **KNN is a lazy learner**: No training phase - all computation happens during prediction\n2. **Feature scaling is essential**: Distance-based algorithms require features on similar scales\n3. **K is crucial**: The choice of K controls the bias-variance tradeoff\n4. **Computational cost**: KNN can be slow for large datasets since it requires distance calculations to all training points\n5. **Works best with**: Low-dimensional data (curse of dimensionality affects high-dimensional spaces)\n\n**When to use KNN Regression:**\n- Small to medium-sized datasets\n- Non-parametric problems (no assumption about data distribution)\n- When you need an interpretable model\n- When the decision boundary is irregular\n\n**When NOT to use KNN Regression:**\n- Very large datasets (computational cost)\n- High-dimensional data (curse of dimensionality)\n- When training time is critical (preprocessing step)\n- When you need a simple model equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Conceptual Question\n\n> **Question**: Which of the following statements about KNN regression is TRUE?\n>\n> A. KNN performs better with high-dimensional data due to increased information\n>\n> B. Increasing K always improves model performance on the validation set\n>\n> C. KNN requires feature scaling because distance calculations are sensitive to feature magnitudes\n>\n> D. KNN has a training phase where it learns parameters from the data\n\n<details>\n<summary>Click to reveal answer</summary>\n\n**Correct Answer: C**\n\n**Explanation:**\n- **A is FALSE**: KNN suffers from the \"curse of dimensionality\" - performance degrades in high-dimensional spaces as distances become less meaningful\n- **B is FALSE**: There's an optimal K value; increasing K beyond this leads to underfitting and worse performance\n- **C is TRUE**: Distance-based algorithms like KNN are highly sensitive to feature scales, making standardization essential\n- **D is FALSE**: KNN is a lazy learner with no training phase; all computation happens during prediction\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRuv54mqHsZC"
      },
      "source": [
        "## Additional Exercises\n",
        "\n",
        "1. **Try different distance metrics**: Modify `MyKNNRegressor` to use Manhattan distance and compare performance\n",
        "\n",
        "2. **Feature engineering**: Create polynomial features and see if KNN performance improves\n",
        "\n",
        "3. **Cross-validation**: Instead of a single validation split, implement k-fold cross-validation to choose K\n",
        "\n",
        "4. **Computational efficiency**: Measure prediction time for different dataset sizes. How does it scale?\n",
        "\n",
        "5. **Real dataset**: Apply KNN regression to the California Housing dataset and tune all hyperparameters\n",
        "\n",
        "6. **Comparison**: Compare KNN with Linear Regression, Decision Trees, and Random Forest on the same dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Congratulations!** You've successfully implemented and mastered K-Nearest Neighbors regression from scratch. You now understand:\n> - How KNN makes predictions by averaging neighbors\n> - The critical importance of feature scaling\n> - How to tune K using validation sets\n> - The bias-variance tradeoff in action\n>\n> **Next Steps**: Try applying KNN to real-world regression problems, experiment with different distance metrics, and explore weighted KNN where closer neighbors have more influence!\n>\n> Keep learning and experimenting! The best way to master machine learning is through hands-on practice."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}