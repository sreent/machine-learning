{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/K-Nearest%20Neighbours%20Regression/KNN%20Regression%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Regression\n",
    "\n",
    "In this lab, you will implement a K-Nearest Neighbors regressor from scratch, then compare it with scikit-learn's implementation. You'll learn about:\n",
    "- The core KNN regression algorithm\n",
    "- Distance metrics and feature scaling\n",
    "- Bias-variance tradeoff when tuning K\n",
    "- Evaluation metrics for regression tasks\n",
    "\n",
    "Unlike classification where KNN uses **voting** to select a class label, in regression KNN **averages** the target values of the K nearest neighbors to make continuous predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of KNN Regression\n",
    "\n",
    "**Algorithm:**\n",
    "1. Given a new data point $\\mathbf{x}_{\\text{new}}$, compute the distance from $\\mathbf{x}_{\\text{new}}$ to every point in the training set.\n",
    "2. Select the K training points closest to $\\mathbf{x}_{\\text{new}}$ (the K nearest neighbors).\n",
    "3. **Average** the target values of these K neighbors to produce the prediction:\n",
    "\n",
    "$$\\hat{y}_{\\text{new}} = \\frac{1}{K} \\sum_{i=1}^{K} y_i$$\n",
    "\n",
    "where $y_i$ are the target values of the K nearest neighbors.\n",
    "\n",
    "**Key Difference from Classification:**\n",
    "- **Classification**: Uses majority voting among K neighbors' class labels\n",
    "- **Regression**: Averages the K neighbors' continuous target values\n",
    "\n",
    "**Example:**\n",
    "If K=3 and the 3 nearest neighbors have target values [2.5, 3.0, 2.8], the prediction would be:\n",
    "$$\\hat{y} = \\frac{2.5 + 3.0 + 2.8}{3} = 2.77$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: In KNN regression, the parameter K refers to:\n",
    ">\n",
    "> A) The number of features in the dataset  \n",
    ">\n",
    "> B) The number of nearest neighbors whose target values are averaged for prediction  \n",
    ">\n",
    "> C) The number of clusters in the data  \n",
    ">\n",
    "> D) The number of iterations for training\n",
    ">\n",
    "> **Answer**: B. K is the number of nearest neighbors we average to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-off in KNN Regression\n",
    "\n",
    "The choice of K controls the model's complexity:\n",
    "\n",
    "**Small K (e.g., K=1):**\n",
    "- Low bias, high variance\n",
    "- Very flexible, can fit complex patterns\n",
    "- Sensitive to noise and outliers\n",
    "- Risk of overfitting\n",
    "- For K=1, prediction equals the nearest neighbor's value exactly\n",
    "\n",
    "**Large K (e.g., K=100):**\n",
    "- High bias, low variance\n",
    "- Less flexible, smoother predictions\n",
    "- More robust to noise\n",
    "- Risk of underfitting\n",
    "- Predictions become more similar (averaged over many points)\n",
    "\n",
    "**Optimal K:**\n",
    "- Balances bias and variance\n",
    "- Found through validation/cross-validation\n",
    "- Depends on the dataset size and noise level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics and Feature Scaling\n",
    "\n",
    "**Common Distance Metrics:**\n",
    "- **Euclidean (L2)**: $d(\\mathbf{x}, \\mathbf{x}') = \\sqrt{\\sum_{i=1}^{n} (x_i - x_i')^2}$\n",
    "- **Manhattan (L1)**: $d(\\mathbf{x}, \\mathbf{x}') = \\sum_{i=1}^{n} |x_i - x_i'|$\n",
    "- **Minkowski**: Generalization of both (p=1 is Manhattan, p=2 is Euclidean)\n",
    "\n",
    "**Feature Scaling is Critical:**\n",
    "- Features with larger ranges dominate distance calculations\n",
    "- Example: If feature1 ranges from 0-1 and feature2 from 0-1000, feature2 will dominate\n",
    "- **Solution**: Standardize features to have mean=0 and std=1, or normalize to [0,1] range\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: When using KNN on a dataset with features measured in very different scales (e.g., age in years vs. income in dollars), what should you do?\n",
    ">\n",
    "> A) Nothing—KNN handles different scales automatically  \n",
    ">\n",
    "> B) Remove the feature with the largest scale  \n",
    ">\n",
    "> C) Standardize or normalize all features to comparable ranges  \n",
    ">\n",
    "> D) Use only categorical features\n",
    ">\n",
    "> **Answer**: C. Scaling ensures all features contribute fairly to distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Pseudocode for KNN Regressor\n\nBefore implementing the algorithm, let's understand the pseudocode structure (from lecture slides):\n\n```\n# Inputs\n#   data      ← training set of N examples (x, y)\n#   k         ← number of neighbours\n#   metric    ← distance function (e.g., Euclidean, Manhattan)\n#   X_query   ← set of examples to predict\n\n# ----- \"fit\" (lazy) -----\nX_train ← data.x\ny_train ← data.y\n\n# ----- predict -----\nŷ ← list of length |X_query|  # outputs align 1:1 with X_query\n\nFOR i = 1 TO |X_query| DO\n    x* ← X_query[i]\n    d ← distances from x* to all X_train using metric\n    J ← indices of the k smallest values in d\n    # regression prediction = average of the targets of the k nearest neighbours\n    ŷ[i] ← mean(y_train[J])\nEND FOR\n\nRETURN ŷ\n```\n\n**Key Points:**\n- KNN is a \"lazy learner\" - the `fit` method just stores the training data\n- The `predict` method does all the work: compute distances, find K nearest, average their targets\n- **Regression uses averaging** (not voting like classification)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Custom KNN Regressor\n",
    "\n",
    "Below is a scaffold of the `MyKNNRegressor` class. Fill in the TODO sections to complete the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "class MyKNNRegressor:\n",
    "    \"\"\"\n",
    "    A simple K-Nearest Neighbors Regressor implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_neighbors : int, default=5\n",
    "        Number of neighbors to use for prediction\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric to use ('euclidean', 'manhattan', etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the KNN regressor by storing the training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # TODO: Store the training data\n",
    "        # Hint: KNN is a \"lazy learner\" - it just stores the training data\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for test data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array of shape (n_samples,)\n",
    "            Predicted target values\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for x_test in X:\n",
    "            # TODO: For each test point:\n",
    "            # 1. Compute distances from x_test to all training points\n",
    "            # 2. Find indices of K nearest neighbors\n",
    "            # 3. Get target values of those K neighbors\n",
    "            # 4. Average them to get the prediction\n",
    "            \n",
    "            # Step 1: Compute distances\n",
    "            distances = pairwise_distances(\n",
    "                x_test.reshape(1, -1), \n",
    "                self.X_train, \n",
    "                metric=self.metric\n",
    "            ).ravel()\n",
    "            \n",
    "            # Step 2: Find K nearest neighbor indices\n",
    "            k_nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "            \n",
    "            # Step 3: Get target values of K nearest neighbors\n",
    "            k_nearest_targets = self.y_train[k_nearest_indices]\n",
    "            \n",
    "            # Step 4: Average the target values\n",
    "            prediction = np.mean(k_nearest_targets)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "print(\"MyKNNRegressor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have filled in the implementation, let's test our custom regressor on a simple dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset for Visualization\n",
    "\n",
    "We'll create a synthetic 1D regression dataset to visualize how KNN regression works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic 1D data\n",
    "def f(x):\n",
    "    \"\"\"True underlying function: sinusoidal pattern\"\"\"\n",
    "    return np.sin(x) + 0.1 * x\n",
    "\n",
    "# Training data\n",
    "X_train_1d = np.sort(np.random.uniform(0, 10, 100))\n",
    "y_train_1d = f(X_train_1d) + np.random.normal(0, 0.2, 100)  # Add noise\n",
    "\n",
    "# Reshape for sklearn compatibility\n",
    "X_train_1d = X_train_1d.reshape(-1, 1)\n",
    "\n",
    "# Test data (for smooth prediction curve)\n",
    "X_test_1d = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "y_test_1d_true = f(X_test_1d.ravel())\n",
    "\n",
    "print(f\"Training data shape: X={X_train_1d.shape}, y={y_train_1d.shape}\")\n",
    "print(f\"Test data shape: X={X_test_1d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training data:"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Manual step-by-step prediction (matching lecture slides 11-13)\nfrom sklearn.metrics import pairwise_distances\n\n# Choose a test point (let's pick x=5.0 from our test range)\nx_test_single = np.array([[5.0]])  # Must be 2D for sklearn\nprint(\"=\"*70)\nprint(\"STEP-BY-STEP: How KNN Predicts for x_test = 5.0\")\nprint(\"=\"*70)\n\n# Step 1: Calculate distances from test point to ALL training points\nprint(\"\\nSTEP 1: Pairwise Distance Calculation\")\nprint(\"-\" * 70)\ndistances = pairwise_distances(x_test_single, X_train_1d, metric='euclidean').ravel()\nprint(f\"Computed {len(distances)} distances from x_test=5.0 to all training points\")\nprint(f\"Distance array shape: {distances.shape}\")\nprint(f\"First 10 distances: {distances[:10].round(3)}\")\n\n# Step 2: Find K nearest neighbors\nprint(\"\\nSTEP 2: Finding K Nearest Neighbors\")\nprint(\"-\" * 70)\nK = 5\nk_indices = np.argsort(distances)[:K]  # Indices of K smallest distances\nk_distances = distances[k_indices]\nprint(f\"K = {K}\")\nprint(f\"Indices of K nearest neighbors: {k_indices}\")\nprint(f\"Their distances: {k_distances.round(3)}\")\nprint(f\"Their x-coordinates: {X_train_1d[k_indices].ravel().round(3)}\")\n\n# Step 3: Get target values of K nearest neighbors\nprint(\"\\nSTEP 3: Get Neighbor Targets\")\nprint(\"-\" * 70)\nk_targets = y_train_1d[k_indices]\nprint(f\"NN's Labels (target values): {k_targets.round(3)}\")\n\n# Step 4: Average to get prediction\nprint(\"\\nSTEP 4: Average (THE PREDICTION!)\")\nprint(\"-\" * 70)\nmanual_prediction = np.mean(k_targets)\nprint(f\"Prediction = mean({k_targets.round(3)})\")\nprint(f\"Prediction = {manual_prediction:.3f}\")\n\n# Verify with our custom KNN\nknn_verify = MyKNNRegressor(n_neighbors=K)\nknn_verify.fit(X_train_1d, y_train_1d)\nverify_prediction = knn_verify.predict(x_test_single)[0]\n\nprint(f\"\\n\" + \"=\"*70)\nprint(\"VERIFICATION:\")\nprint(f\"  Manual calculation:  {manual_prediction:.3f}\")\nprint(f\"  MyKNNRegressor:      {verify_prediction:.3f}\")\nprint(f\"  Match: {np.isclose(manual_prediction, verify_prediction)} ✓\")\nprint(\"=\"*70)\n\n# Visualize this specific prediction\nplt.figure(figsize=(12, 5))\nplt.scatter(X_train_1d, y_train_1d, c='lightblue', alpha=0.6, edgecolor='k', s=50, label='All training data')\nplt.scatter(X_train_1d[k_indices], k_targets, c='red', s=100, edgecolor='k', \n           label=f'K={K} nearest neighbors', zorder=5)\nplt.scatter(x_test_single, manual_prediction, c='green', s=200, marker='*', \n           edgecolor='k', label='Prediction (average)', zorder=10)\nplt.axhline(manual_prediction, color='green', linestyle='--', alpha=0.3)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(f'Step-by-Step KNN Prediction: x_test={x_test_single[0,0]}, K={K}, Prediction={manual_prediction:.3f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Understanding the Prediction Process: Step-by-Step\n\nBefore we test different K values, let's manually walk through exactly what happens when KNN makes a single prediction. This matches the step-by-step process from the lecture slides (slides 11-13).\n\n**The 4 Steps:**\n1. **Pairwise Distance Calculation**: Compute distance from test point to all training points\n2. **Finding K Nearest Neighbors**: Sort distances and select K smallest\n3. **Get Neighbor Targets**: Retrieve the y-values of those K neighbors  \n4. **Average**: Compute mean of the K target values → this is the prediction!\n\nLet's demonstrate this process:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, edgecolor='k', s=50, label='Training data')\n",
    "plt.plot(X_test_1d, y_test_1d_true, 'g--', lw=2, label='True function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data with True Underlying Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Custom KNN Regressor\n",
    "\n",
    "Let's test our implementation with different K values to see how it affects the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different K values\n",
    "k_values = [1, 3, 10, 30]\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for idx, k in enumerate(k_values, 1):\n",
    "    # Fit our custom KNN regressor\n",
    "    knn = MyKNNRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train_1d, y_train_1d)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = knn.predict(X_test_1d)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(2, 2, idx)\n",
    "    plt.scatter(X_train_1d, y_train_1d, c='blue', alpha=0.6, edgecolor='k', s=30, label='Training data')\n",
    "    plt.plot(X_test_1d, y_test_1d_true, 'g--', lw=1.5, label='True function', alpha=0.7)\n",
    "    plt.plot(X_test_1d, y_pred, 'r-', lw=2, label=f'KNN (K={k})')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'KNN Regression with K={k}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- K=1: Very wiggly, follows training points closely (high variance, low bias)\")\n",
    "print(\"- K=3-10: Smoother, captures general trend while adapting to local patterns\")\n",
    "print(\"- K=30: Very smooth, may miss local variations (low variance, higher bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: Looking at the plots above, which K value shows signs of overfitting?\n",
    ">\n",
    "> A) K=30 (too smooth, doesn't follow the data closely)  \n",
    ">\n",
    "> B) K=1 (too wiggly, follows every noise point)  \n",
    ">\n",
    "> C) K=10 (balanced smoothness)  \n",
    ">\n",
    "> D) All of them equally\n",
    ">\n",
    "> **Answer**: B. K=1 is overfitting—it perfectly fits training noise, creating excessive variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a Real Dataset\n",
    "\n",
    "Now let's work with a real 2D regression dataset and compare our implementation with scikit-learn's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate a 2D regression dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the 2D dataset with target values as colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                     edgecolor='k', s=50, alpha=0.7)\n",
    "plt.colorbar(scatter, label='Target Value')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('2D Regression Dataset (color represents target value)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Validation, and Test Sets\n",
    "\n",
    "We split the data into:\n",
    "- **Training set (60%)**: To fit the model\n",
    "- **Validation set (20%)**: To tune hyperparameters (K)\n",
    "- **Test set (20%)**: For final unbiased performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validation, and test sets (60/20/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Validation size: {X_val.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this split, you should see roughly: Train size 180, Validation size 60, Test size 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: Why do we use a separate validation set instead of tuning the hyperparameters directly on the test set?\n",
    ">\n",
    "> A) The test set is too small  \n",
    ">\n",
    "> B) To prevent overfitting to the test set and get an unbiased final performance estimate  \n",
    ">\n",
    "> C) The validation set trains faster  \n",
    ">\n",
    "> D) It's just a convention with no real benefit\n",
    ">\n",
    "> **Answer**: B. If we tune on the test set, we'll overfit to it and get an overly optimistic performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Let's demonstrate the importance of feature scaling by comparing scaled vs. unscaled performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Without scaling\n",
    "knn_raw = MyKNNRegressor(n_neighbors=5)\n",
    "knn_raw.fit(X_train, y_train)\n",
    "y_val_pred_raw = knn_raw.predict(X_val)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_val, y_val_pred_raw))\n",
    "r2_raw = r2_score(y_val, y_val_pred_raw)\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "knn_scaled = MyKNNRegressor(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_val_pred_scaled = knn_scaled.predict(X_val_scaled)\n",
    "rmse_scaled = np.sqrt(mean_squared_error(y_val, y_val_pred_scaled))\n",
    "r2_scaled = r2_score(y_val, y_val_pred_scaled)\n",
    "\n",
    "print(\"Performance Comparison (K=5):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Without scaling - RMSE: {rmse_raw:.2f}, R²: {r2_raw:.3f}\")\n",
    "print(f\"With scaling    - RMSE: {rmse_scaled:.2f}, R²: {r2_scaled:.3f}\")\n",
    "print(\"=\"*50)\n",
    "if rmse_scaled < rmse_raw:\n",
    "    print(f\"Improvement: {((rmse_raw - rmse_scaled) / rmse_raw * 100):.1f}% reduction in RMSE\")\n",
    "else:\n",
    "    print(\"Note: Scaling impact may vary depending on feature scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Hyperparameter K\n",
    "\n",
    "Let's find the optimal K by evaluating different values on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this section\n",
    "# 1. Create lists to store training and validation metrics\n",
    "# 2. Loop through K values from 1 to 30\n",
    "# 3. For each K, fit the model and compute RMSE on both train and validation sets\n",
    "# 4. Store the results\n",
    "\n",
    "train_rmse = []\n",
    "val_rmse = []\n",
    "k_range = range(1, 31)\n",
    "\n",
    "for k in k_range:\n",
    "    # Fit model\n",
    "    model = MyKNNRegressor(n_neighbors=k)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Compute RMSE\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "    val_rmse.append(np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "\n",
    "# Find best K\n",
    "best_k = list(k_range)[np.argmin(val_rmse)]\n",
    "best_val_rmse = min(val_rmse)\n",
    "\n",
    "print(f\"Best K: {best_k}\")\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the RMSE vs. K to visualize the bias-variance tradeoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(k_range), train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=6)\n",
    "plt.plot(list(k_range), val_rmse, 's-', label='Validation RMSE', linewidth=2, markersize=6)\n",
    "plt.axvline(best_k, linestyle='--', color='red', linewidth=2, label=f'Best K={best_k}')\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('RMSE', fontsize=12)\n",
    "plt.title('Bias-Variance Tradeoff: RMSE vs K', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Small K: Low training RMSE (fits training data closely) but higher validation RMSE (overfitting)\")\n",
    "print(\"- Large K: Training and validation RMSE converge (underfitting - too much smoothing)\")\n",
    "print(f\"- Optimal K={best_k}: Best balance between bias and variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Scikit-Learn's Implementation\n",
    "\n",
    "Let's verify our implementation matches scikit-learn's KNeighborsRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Our implementation\n",
    "our_knn = MyKNNRegressor(n_neighbors=best_k)\n",
    "our_knn.fit(X_train_scaled, y_train)\n",
    "our_pred = our_knn.predict(X_val_scaled)\n",
    "our_rmse = np.sqrt(mean_squared_error(y_val, our_pred))\n",
    "our_r2 = r2_score(y_val, our_pred)\n",
    "\n",
    "# Scikit-learn's implementation\n",
    "sklearn_knn = KNeighborsRegressor(n_neighbors=best_k)\n",
    "sklearn_knn.fit(X_train_scaled, y_train)\n",
    "sklearn_pred = sklearn_knn.predict(X_val_scaled)\n",
    "sklearn_rmse = np.sqrt(mean_squared_error(y_val, sklearn_pred))\n",
    "sklearn_r2 = r2_score(y_val, sklearn_pred)\n",
    "\n",
    "print(\"Validation Set Performance Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Our Implementation  - RMSE: {our_rmse:.3f}, R²: {our_r2:.3f}\")\n",
    "print(f\"Scikit-learn       - RMSE: {sklearn_rmse:.3f}, R²: {sklearn_r2:.3f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Difference in RMSE: {abs(our_rmse - sklearn_rmse):.6f}\")\n",
    "print(\"\\nThe implementations should match very closely!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "\n",
    "Now that we've chosen the best K using the validation set, let's evaluate on the held-out test set to get an unbiased performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Scale test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Retrain on train + validation combined\n",
    "X_train_all = np.vstack([X_train_scaled, X_val_scaled])\n",
    "y_train_all = np.hstack([y_train, y_val])\n",
    "\n",
    "final_knn = MyKNNRegressor(n_neighbors=best_k)\n",
    "final_knn.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = final_knn.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best K: {best_k}\")\n",
    "print(f\"RMSE:  {test_rmse:.3f}\")\n",
    "print(f\"MAE:   {test_mae:.3f}\")\n",
    "print(f\"R²:    {test_r2:.3f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize predictions vs. actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k', s=50)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Test Set: Actual vs Predicted\\n(R² = {test_r2:.3f}, RMSE = {test_rmse:.2f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k', s=50)\n",
    "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance-Weighted KNN (Advanced)\n",
    "\n",
    "In standard KNN regression, all K neighbors contribute equally to the prediction. A more sophisticated approach is to **weight neighbors by their distance** - closer neighbors should have more influence.\n",
    "\n",
    "**Distance-weighted average:**\n",
    "$$\\hat{y} = \\frac{\\sum_{i=1}^{K} w_i \\cdot y_i}{\\sum_{i=1}^{K} w_i}$$\n",
    "\n",
    "where $w_i = \\frac{1}{d_i + \\epsilon}$ (inverse distance weights, $\\epsilon$ prevents division by zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uniform vs distance-weighted KNN using sklearn\n",
    "knn_uniform = KNeighborsRegressor(n_neighbors=best_k, weights='uniform')\n",
    "knn_distance = KNeighborsRegressor(n_neighbors=best_k, weights='distance')\n",
    "\n",
    "knn_uniform.fit(X_train_all, y_train_all)\n",
    "knn_distance.fit(X_train_all, y_train_all)\n",
    "\n",
    "pred_uniform = knn_uniform.predict(X_test_scaled)\n",
    "pred_distance = knn_distance.predict(X_test_scaled)\n",
    "\n",
    "rmse_uniform = np.sqrt(mean_squared_error(y_test, pred_uniform))\n",
    "rmse_distance = np.sqrt(mean_squared_error(y_test, pred_distance))\n",
    "\n",
    "print(\"Comparison: Uniform vs Distance-Weighted KNN\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Uniform weights:  RMSE = {rmse_uniform:.3f}\")\n",
    "print(f\"Distance weights: RMSE = {rmse_distance:.3f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if rmse_distance < rmse_uniform:\n",
    "    improvement = (rmse_uniform - rmse_distance) / rmse_uniform * 100\n",
    "    print(f\"Distance weighting improves RMSE by {improvement:.1f}%\")\n",
    "else:\n",
    "    print(\"Uniform weights perform better on this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement Distance-Weighted KNN\n",
    "\n",
    "**Challenge**: Extend the `MyKNNRegressor` class to support distance-weighted predictions.\n",
    "\n",
    "**Hints:**\n",
    "1. Add a `weights` parameter to `__init__` (either 'uniform' or 'distance')\n",
    "2. In the `predict` method, if weights='distance':\n",
    "   - Get the distances to the K nearest neighbors\n",
    "   - Compute weights as `w_i = 1 / (distance_i + 1e-10)`\n",
    "   - Compute weighted average: `prediction = sum(w_i * y_i) / sum(w_i)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement distance-weighted KNN\n",
    "class MyKNNRegressorWeighted:\n",
    "    \"\"\"\n",
    "    KNN Regressor with support for distance-weighted predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, metric='euclidean', weights='uniform'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.weights = weights  # 'uniform' or 'distance'\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        for x_test in X:\n",
    "            # Compute distances\n",
    "            distances = pairwise_distances(\n",
    "                x_test.reshape(1, -1), \n",
    "                self.X_train, \n",
    "                metric=self.metric\n",
    "            ).ravel()\n",
    "            \n",
    "            # Find K nearest neighbor indices\n",
    "            k_nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "            k_nearest_distances = distances[k_nearest_indices]\n",
    "            k_nearest_targets = self.y_train[k_nearest_indices]\n",
    "            \n",
    "            # TODO: Implement distance weighting\n",
    "            if self.weights == 'uniform':\n",
    "                prediction = np.mean(k_nearest_targets)\n",
    "            elif self.weights == 'distance':\n",
    "                # Compute inverse distance weights\n",
    "                weights = 1 / (k_nearest_distances + 1e-10)\n",
    "                # Weighted average\n",
    "                prediction = np.sum(weights * k_nearest_targets) / np.sum(weights)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Test your implementation\n",
    "my_knn_weighted = MyKNNRegressorWeighted(n_neighbors=best_k, weights='distance')\n",
    "my_knn_weighted.fit(X_train_all, y_train_all)\n",
    "my_pred_weighted = my_knn_weighted.predict(X_test_scaled)\n",
    "my_rmse_weighted = np.sqrt(mean_squared_error(y_test, my_pred_weighted))\n",
    "\n",
    "print(f\"Your distance-weighted KNN RMSE: {my_rmse_weighted:.3f}\")\n",
    "print(f\"Sklearn distance-weighted RMSE:  {rmse_distance:.3f}\")\n",
    "print(f\"Difference: {abs(my_rmse_weighted - rmse_distance):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **KNN Regression Algorithm**: Predicts continuous values by averaging K nearest neighbors' targets\n",
    "\n",
    "2. **Bias-Variance Tradeoff**:\n",
    "   - Small K → High variance, low bias (overfitting)\n",
    "   - Large K → Low variance, high bias (underfitting)\n",
    "   - Optimal K balances both\n",
    "\n",
    "3. **Feature Scaling**: Critical for KNN since it's distance-based\n",
    "\n",
    "4. **Distance Metrics**: Euclidean vs Manhattan vs others\n",
    "\n",
    "5. **Distance Weighting**: Closer neighbors can have more influence\n",
    "\n",
    "6. **Evaluation Metrics**:\n",
    "   - RMSE: Penalizes large errors\n",
    "   - MAE: Average absolute error\n",
    "   - R²: Proportion of variance explained\n",
    "\n",
    "7. **Train/Validation/Test Split**: Essential for proper hyperparameter tuning and unbiased evaluation\n",
    "\n",
    "**When to use KNN Regression:**\n",
    "- Small to medium datasets\n",
    "- Non-linear relationships\n",
    "- Need interpretable predictions (can show similar examples)\n",
    "- Quick prototyping and baseline\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally expensive for large datasets (must compute all distances)\n",
    "- Curse of dimensionality (performance degrades in high dimensions)\n",
    "- Sensitive to irrelevant features\n",
    "- Requires feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises\n",
    "\n",
    "1. **Try different distance metrics**: Modify `MyKNNRegressor` to use Manhattan distance and compare performance\n",
    "\n",
    "2. **Feature engineering**: Create polynomial features and see if KNN performance improves\n",
    "\n",
    "3. **Cross-validation**: Instead of a single validation split, implement k-fold cross-validation to choose K\n",
    "\n",
    "4. **Computational efficiency**: Measure prediction time for different dataset sizes. How does it scale?\n",
    "\n",
    "5. **Real dataset**: Apply KNN regression to the California Housing dataset and tune all hyperparameters\n",
    "\n",
    "6. **Comparison**: Compare KNN with Linear Regression, Decision Trees, and Random Forest on the same dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}