{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Sentiment%20Analysis%20of%20US%20Airline%20Tweets%20-%20A%20Write-Up%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Sentiment Analysis of US Airline Tweets Using Deep Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "Sentiment analysis, a key task in natural language processing (NLP), involves identifying and categorizing opinions expressed in a piece of text to determine the writer's sentiment—whether positive, neutral, or negative. This study focuses on analyzing the sentiment of tweets related to US airlines, aiming to classify them into these three categories. Sentiment analysis on social media data, such as tweets, presents unique challenges due to the informal language, use of slang, emojis, sarcasm, and the brevity of the messages. For instance, a tweet might convey a sentiment that is not straightforward, making it difficult for traditional methods to accurately capture the intended emotional tone.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In today's digital age, social media platforms like Twitter have become crucial channels for customers to voice their opinions and experiences with companies, including airlines. Understanding customer sentiment from these platforms allows airlines to gain real-time insights into customer satisfaction and potential issues. This ability to swiftly analyze and respond to customer feedback is vital, as studies have shown that 67% of customers use social media for customer service. Effective sentiment analysis can enable airlines to proactively address problems, enhance customer satisfaction, and build stronger brand loyalty. The goal of this study is to develop a reliable and scalable method for automating sentiment analysis, tailored to the unique characteristics of social media data, which can help airlines improve their services and gain a competitive edge.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset used in this study is the \"Twitter US Airline Sentiment\" dataset from Kaggle. This dataset consists of 14,640 tweets directed at various US airlines, each labeled as positive, neutral, or negative. The distribution of sentiment labels is approximately 16% positive, 63% negative, and 21% neutral, highlighting a significant class imbalance. The tweets have already undergone basic preprocessing steps, such as the removal of duplicates and irrelevant information. The class imbalance in this dataset poses a challenge for model training, requiring careful selection of evaluation metrics and techniques to ensure that the models perform robustly across all classes.\n",
    "\n",
    "### Constraints and Methodological Focus\n",
    "\n",
    "This assignment is guided by the principles outlined in *Deep Learning with Python* by François Chollet, specifically adhering to \"The Universal Workflow of Machine Learning.\" Within this framework, we are constrained by the requirement to use only Dense layers, Dropout layers, and L1/L2 regularization techniques in our neural network models. We are restricted from using more advanced techniques such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), or methods like Early Stopping. Despite these constraints, our exploration will focus on various dense neural network architectures, including wider, deeper, and regularized models, to determine their effectiveness in sentiment classification. This exploration will provide insights into how different architectural choices impact model performance, particularly in handling the class imbalance present in the dataset.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "The primary objectives of this study are:\n",
    "\n",
    "1. **Data Preprocessing:** To preprocess the text data, converting it into a numerical format that can be effectively utilized by machine learning models. This involves techniques like tokenization, TF-IDF vectorization, and handling class imbalance.\n",
    "\n",
    "2. **Model Development and Exploration:** To develop and train dense neural network models for sentiment classification, exploring various network architectures. This includes experimenting with wider and deeper models, as well as applying Dropout and L1/L2 regularization techniques to understand their impact on model performance.\n",
    "\n",
    "3. **Performance Evaluation:** To rigorously evaluate the performance of these models using a range of metrics, including accuracy, F1 Score, and AUC, with a particular focus on understanding how these models handle the class imbalance in the dataset.\n",
    "\n",
    "4. **Model Optimization:** To investigate and implement techniques such as Dropout and L1/L2 regularization to prevent overfitting and improve the generalization of the models, given the constraints of not using CNNs, RNNs, or Early Stopping.\n",
    "\n",
    "5. **Architectural Insights:** To analyze and compare the performance of different neural network architectures, providing insights into the strengths and limitations of each approach within the given constraints.\n",
    "\n",
    "These objectives aim to provide a comprehensive understanding of the effectiveness of dense neural networks in handling sentiment analysis tasks, while also exploring how different architectural choices impact performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "### Data Loading\n",
    "\n",
    "We begin by mounting Google Drive to access the dataset stored in it. We create the necessary directories to store the dataset and use the `gdown` library to download the dataset from the given URL. This ensures that our data is easily accessible and can be seamlessly integrated into our Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip -q install gdown==4.6.0\n",
    "import gdown\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_path = \"/content/drive/MyDrive/Neural Networks/Twitter US Airline Sentiment/\"\n",
    "    # Create necessary directories to store the dataset\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    base_path = \"./\"\n",
    "\n",
    "# Download the dataset from the given URL\n",
    "URL = \"https://drive.google.com/file/d/15XHy_PdD6Q2aa6n-pnWmSFGCv1oK9vWA/view?usp=sharing\"\n",
    "DOWNLOAD_FILE_PATH = \"https://drive.google.com/uc?export=download&id=\" + URL.split(\"/\")[-2]\n",
    "gdown.download(DOWNLOAD_FILE_PATH, base_path + \"Tweets.csv\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The dataset is loaded into a pandas DataFrame. We use sklearn's `TfidfVectorizer` to convert the text data into numerical form using TF-IDF mode. The sentiment labels are converted into numerical form using label encoding and then one-hot encoded. We chose 5000 features and bigrams for TF-IDF to capture a broad range of important terms and their combinations, providing a richer representation of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the data from the CSV file into a pandas DataFrame\n",
    "file_path = os.path.join(base_path, \"Tweets.csv\")\n",
    "tweets = pd.read_csv(file_path)[['text', 'airline_sentiment']]\n",
    "\n",
    "# Label Encoding to convert sentiment labels into numerical form\n",
    "label_encoder = LabelEncoder()\n",
    "y = to_categorical(label_encoder.fit_transform(tweets['airline_sentiment']))\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['text'], y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Define the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Choosing a Measure of Success\n",
    "\n",
    "### Exploring Class Imbalance\n",
    "\n",
    "Before proceeding with model development, it is crucial to understand the class distribution to address any potential imbalance. The class distribution plot below shows a significant imbalance, with a majority of tweets being negative (63%), followed by neutral (21%), and positive (16%). This imbalance will be taken into account when selecting evaluation metrics and techniques to ensure robust performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = tweets['airline_sentiment'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "**Balanced Accuracy Score** is useful for evaluating the model's performance on each class equally, especially important in the presence of class imbalance. **Precision** and **recall** are essential for understanding the model's ability to correctly classify positive cases (precision) and its coverage of actual positive cases (recall). The **AUC** (Area Under the ROC Curve) measures the model's ability to distinguish between classes. The **F1 Score** provides a balance between precision and recall, useful for overall performance assessment. These metrics provide a comprehensive evaluation of the model's performance, helping us understand its strengths and weaknesses in different aspects of classification. AUC is particularly chosen for this problem as it provides a single scalar value that summarizes the performance across all classification thresholds, making it useful for imbalanced datasets.\n",
    "\n",
    "### Naive Baseline\n",
    "\n",
    "To provide a reference point for model performance, we establish a naive baseline using the most frequent class. Given the class imbalance, the naive baseline would predict every tweet as the majority class, which is \"negative\". The naive baseline results show an accuracy of 0.63, an F1 Score of 0.26, and an AUC of 0.50. These results highlight the need for a more sophisticated model to better capture the sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train_tfidf, y_train.argmax(axis=1))\n",
    "y_dummy_pred = dummy_clf.predict(X_train_tfidf)\n",
    "\n",
    "naive_accuracy = accuracy_score(y_train.argmax(axis=1), y_dummy_pred)\n",
    "naive_f1 = f1_score(y_train.argmax(axis=1), y_dummy_pred, average='macro')\n",
    "naive_auc = roc_auc_score(y_train, to_categorical(y_dummy_pred), average='macro', multi_class='ovo')\n",
    "\n",
    "print(f'Naive Baseline - Accuracy: {naive_accuracy:.2f}, F1 Score: {naive_f1:.2f}, AUC: {naive_auc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights\n",
    "\n",
    "Given the significant class imbalance observed, it is necessary to apply class weights during model training to ensure the model treats each class fairly. Class weights adjust the importance of each class in the loss function, giving more weight to minority classes. This helps the model focus on correctly predicting these underrepresented classes, improving overall performance.\n",
    "\n",
    "The `compute_class_weight` function from sklearn is used to calculate the weights for each class. These weights are then converted into a dictionary format, which can be passed directly to the Keras model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                      classes=np.unique(np.argmax(y_train, axis=1)),\n",
    "                                      y=np.argmax(y_train, axis=1))\n",
    "\n",
    "# Convert the class weights to a dictionary format required by Keras\n",
    "class_weight = dict(enumerate(class_weights))\n",
    "\n",
    "# Print class weights\n",
    "print(f\"Computed Class Weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Deciding on an Evaluation Protocol\n",
    "\n",
    "Given the relatively small size of our dataset, we chose to integrate **K-fold cross-validation** with an internal validation split during model refitting to ensure a robust evaluation.\n",
    "\n",
    "**K-Fold Cross-Validation** is employed to assess the model's generalization ability across different subsets of the data. By dividing the training data into multiple folds and iteratively training and validating on these folds, we obtain a comprehensive estimate of the model's performance. This approach is particularly useful in scenarios with limited data, as it maximizes the use of available information for both training and validation, thereby reducing the risk of overfitting.\n",
    "\n",
    "After identifying the optimal hyperparameters through cross-validation, we refit the model on the entire training dataset. During this refitting process, we use a **10% validation split** from the training data. This internal validation set is employed to monitor the model's performance during training and to detect any potential overfitting through the training history. Observing metrics such as validation loss and accuracy allows us to ensure that the model maintains strong generalization capabilities.\n",
    "\n",
    "By combining K-fold cross-validation with an internal validation split, we achieve a thorough evaluation process. This approach allows us to fine-tune the model effectively while continuously monitoring its performance, ensuring that it is both well-tuned and capable of performing on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Developing a Model that Does Better than a Naive Baseline\n",
    "\n",
    "### Baseline Model\n",
    "\n",
    "We establish a baseline model using a simple dense layer with softmax activation. The model is compiled with categorical crossentropy loss and evaluated using categorical accuracy, F1 score, and AUC. This baseline provides a reference point for evaluating more complex models. A simple model serves as a baseline to understand the minimal performance we can achieve without sophisticated techniques. This helps us gauge the improvement offered by more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC, F1Score\n",
    "\n",
    "# Define model parameters\n",
    "OUTPUT_CLASSES = y_train.shape[1]\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = ['categorical_accuracy', F1Score(name='f1_score', average='macro'), AUC(name='auc', multi_label=True)]\n",
    "\n",
    "# Build baseline model\n",
    "baseline = Sequential([\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(X_train_tfidf.shape[1],))\n",
    "])\n",
    "\n",
    "# Compile the baseline model\n",
    "baseline.compile(optimizer=Adam(learning_rate=0.005),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy', F1Score(name='f1_score', average='macro'), AUC(name='auc', multi_label=True)])\n",
    "\n",
    "# Train baseline model\n",
    "baseline_history = baseline.fit(X_train_tfidf, y_train, batch_size=512, epochs=100,\n",
    "                                validation_split=0.1, verbose=0, class_weight=class_weight)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_scores = baseline.evaluate(X_test_tfidf, y_test, verbose=0)\n",
    "print(f'Baseline Model - Test Accuracy: {baseline_scores[1]:.2f}, F1 Score: {baseline_scores[2]:.2f}, AUC: {baseline_scores[3]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model achieved an accuracy of 0.80, an F1 score of 0.72, and an AUC of 0.91. These results indicate that the simple model performs significantly better than the naive baseline, providing a strong foundation for further model improvements.\n",
    "\n",
    "### Plot Baseline Model Training History\n",
    "\n",
    "We defined a helper function to plot the training and validation loss over epochs. This visualization helps us understand how well the model is learning and if there are signs of overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, monitor='loss'):\n",
    "    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'b.', label=monitor)\n",
    "    plt.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n",
    "    plt.xlim([0, len(loss)])\n",
    "    plt.title('Training and Validation ' + monitor + 's')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(monitor)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(baseline_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training history plot shows a decrease in both training and validation loss, suggesting that the model is learning effectively without significant overfitting. The performance of the baseline model on the validation set demonstrates that it significantly outperforms the naive baseline model (accuracy: 0.63, F1 score: 0.26, AUC: 0.50). This highlights the effectiveness of the baseline model in better capturing the sentiment distribution and providing more reliable classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Scaling Up: Developing a Model that Overfits\n",
    "\n",
    "### More Complex Model\n",
    "\n",
    "We build a model with one or two hidden layers and see if it can overfit the data. This model helps gauge the complexity required to learn the patterns in the data. By monitoring the training and validation loss, we can observe overfitting and decide on regularization techniques to mitigate it. Understanding the complexity required to fit the data is crucial for determining the appropriate model architecture and regularization techniques. This step helps us identify the point where the model becomes too complex and starts overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "# Build overfitting model with one hidden layer\n",
    "overfit = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dense(OUTPUT_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the overfitting model with updated learning rate\n",
    "overfit.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                loss=LOSS,\n",
    "                metrics=METRICS)\n",
    "\n",
    "# Train overfitting model\n",
    "overfit_history = overfit.fit(X_train_tfidf, y_train, batch_size=BATCH_SIZE, epochs=200,\n",
    "                              validation_split=0.1, verbose=0, class_weight=class_weight)\n",
    "\n",
    "# Access validation accuracy and other metrics from history\n",
    "overfit_val_accuracy = overfit_history.history['val_categorical_accuracy'][-1]\n",
    "overfit_val_f1_score = overfit_history.history['val_f1_score'][-1]\n",
    "overfit_val_auc = overfit_history.history['val_auc'][-1]\n",
    "\n",
    "print(f'Validation Accuracy: {overfit_val_accuracy:.2f}, F1 Score: {overfit_val_f1_score:.2f}, AUC: {overfit_val_auc:.2f}')\n",
    "plot_training_history(overfit_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complex model achieved an accuracy of 0.78, an F1 score of 0.73, and an AUC of 0.91 on the validation set. With a learning rate of 0.0001, the training history plot shows that the model is overfitting as the validation loss starts to increase after around 110 epochs, while the training loss continues to decrease. This suggests that the model has learned the training data well but is not generalizing as effectively to the validation data. Regularization techniques will be necessary to mitigate this overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Regularizing Your Model and Tuning Your Hyperparameters\n",
    "\n",
    "### Regularization Techniques\n",
    "\n",
    "We identified that the deeper model with hidden layers overfits the data. To address this, we incorporated regularization techniques. **Dropout** is added to reduce overfitting by randomly setting a fraction of input units to 0 at each update during training, which helps prevent the model from relying too much on any particular neurons. **L2 regularization** is used to add a penalty proportional to the squared value of the weights, discouraging the model from learning overly complex patterns.\n",
    "\n",
    "### Hyperparameter Tuning Using Cross-Validation\n",
    "\n",
    "To find the optimal hyperparameters, we performed cross-validation on the following hyperparameters: dropout rate and L2 regularization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pickle\n",
    "\n",
    "def cross_validation(X, y, cv, model, param_grid, loss, metrics, epochs, callbacks, class_weight, seed=0, force_recompute=False, results_file_path=None):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for hyperparameter tuning with class weights.\n",
    "    \n",
    "    Parameters:\n",
    "    X : numpy.ndarray - The feature matrix.\n",
    "    y : numpy.ndarray - The target labels.\n",
    "    cv : int - The number of folds for cross-validation.\n",
    "    model : keras.Model - The base model to be cloned and tuned.\n",
    "    param_grid : dict - The grid of hyperparameters to search.\n",
    "    loss : str - The loss function to compile the model.\n",
    "    metrics : list - The list of metrics to compile the model.\n",
    "    epochs : int - The number of epochs to train each model.\n",
    "    callbacks : list - The list of callbacks to use during training.\n",
    "    class_weight : dict - The class weights to apply during training.\n",
    "    seed : int - The random seed for reproducibility.\n",
    "    force_recompute : bool - If True, recompute all cross-validation even if results already exist.\n",
    "    results_file_path : str - Path to save or load cross-validation results.\n",
    "    \n",
    "    Returns:\n",
    "    dict - A dictionary with parameter combinations as keys and mean validation scores as values.\n",
    "    \"\"\"\n",
    "    # Ensure results_file_path is specified\n",
    "    if results_file_path is None:\n",
    "        raise ValueError(\"results_file_path must be specified.\")\n",
    "    \n",
    "    # Load existing cross-validation results if they exist and force_recompute is False\n",
    "    if os.path.exists(results_file_path) and not force_recompute:\n",
    "        with open(results_file_path, 'rb') as f:\n",
    "            cvs = pickle.load(f)\n",
    "    else:\n",
    "        cvs = {}\n",
    "    \n",
    "    # Initialize KFold cross-validation with shuffling and a fixed random seed for reproducibility\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Extract hyperparameter names and their possible values\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    # Iterate over all possible combinations of hyperparameters\n",
    "    for params in [dict(zip(keys, v)) for v in itertools.product(*values)]:\n",
    "        params_str = str(params)\n",
    "        if params_str in cvs and not force_recompute:\n",
    "            continue  # Skip if already done and force_recompute is False\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        # Perform K-fold cross-validation\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            # Clone the base model to ensure each fold starts with the same architecture and weights\n",
    "            nn = clone_model(model)\n",
    "            \n",
    "            # Update the dropout rate and L2 regularization factor based on the current hyperparameter combination\n",
    "            for layer in nn.layers:\n",
    "                if isinstance(layer, Dropout):\n",
    "                    layer.rate = params['dropout']\n",
    "                if isinstance(layer, Dense):\n",
    "                    if layer.activation.__name__ != 'softmax':  # Avoid modifying the output layer\n",
    "                        layer.kernel_regularizer = l2(params['alpha'])\n",
    "            \n",
    "            # Compile the cloned model with the specified optimizer, loss function, and metrics\n",
    "            nn.compile(optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "                       loss=loss, metrics=metrics)\n",
    "            \n",
    "            # Train the model on the training split for the current fold with class weights\n",
    "            nn.fit(X[train_index], y[train_index],\n",
    "                   batch_size=params['batch_size'], epochs=epochs,\n",
    "                   validation_data=(X[val_index], y[val_index]),\n",
    "                   callbacks=callbacks, verbose=0, class_weight=class_weight)\n",
    "            \n",
    "            # Evaluate the model on the validation split for the current fold and store the validation score\n",
    "            scores.append(nn.evaluate(X[val_index], y[val_index], verbose=0)[1:])\n",
    "            \n",
    "            # Delete the model to free up memory\n",
    "            del nn\n",
    "        \n",
    "        # Calculate the mean validation score for the current hyperparameter combination\n",
    "        cvs[params_str] = np.array(scores).mean(axis=0)\n",
    "        \n",
    "        # Save the updated cross-validation results to the specified file path\n",
    "        with open(results_file_path, 'wb') as f:\n",
    "            pickle.dump(cvs, f)\n",
    "    \n",
    "    return cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'learning_rate': [0.001],\n",
    "    'batch_size': [512]\n",
    "}\n",
    "\n",
    "# Define model and hyperparameters\n",
    "model = Sequential([\n",
    "    Dense(64, kernel_regularizer=l2(0.01), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save cross-validation results\n",
    "results_file_path = os.path.join(base_path, \"regularized_results.pkl\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validation(X_train_tfidf, y_train, cv=3, model=model, param_grid=param_grid,\n",
    "                             loss=LOSS, metrics=METRICS, epochs=200, callbacks=[], seed=42,\n",
    "                             force_recompute=True, results_file_path=results_file_path, class_weight=class_weight)\n",
    "\n",
    "# Find best parameters\n",
    "best_params = max(cv_scores, key=lambda k: cv_scores[k][2])\n",
    "regularized_best_score = cv_scores[best_params]\n",
    "\n",
    "# Load best_params string into Python dictionary\n",
    "regularized_best_params = eval(best_params)\n",
    "print(f'Best params: {regularized_best_params}, Best score: {regularized_best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the cross-validation results, the best parameters were found to be a dropout rate of 0.9 and an L2 regularization factor of 0.001.\n",
    "\n",
    "Using the best hyperparameters, we retrained the regularized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regularized model with optimal hyperparameters\n",
    "regularized_model = Sequential([\n",
    "    Dense(64, kernel_regularizer=l2(regularized_best_params['alpha']), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(regularized_best_params['dropout']),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the regularized model\n",
    "regularized_model.compile(optimizer=Adam(learning_rate=regularized_best_params['learning_rate']),\n",
    "                          loss=LOSS,\n",
    "                          metrics=METRICS)\n",
    "\n",
    "# Train the regularized model\n",
    "regularized_history = regularized_model.fit(X_train_tfidf, y_train, batch_size=regularized_best_params['batch_size'], epochs=250,\n",
    "                                            validation_split=0.1, callbacks=[], verbose=0, class_weight=class_weight)\n",
    "\n",
    "# Evaluate the regularized model\n",
    "regularized_scores = regularized_model.evaluate(X_test_tfidf, y_test, verbose=0)\n",
    "print(f'Regularized Model - Test Accuracy: {regularized_scores[1]:.2f}, F1 Score: {regularized_scores[2]:.2f}, AUC: {regularized_scores[3]:.2f}')\n",
    "\n",
    "# Plot regularized model training history\n",
    "plot_training_history(regularized_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized model achieved an accuracy of 0.79, an F1 score of 0.74, and an AUC of 0.90 on the test set. The training history plot shows that the validation loss stabilizes and does not increase significantly, indicating that the regularization techniques effectively mitigated overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exploring Different Neural Network Architectures\n",
    "\n",
    "To further explore and optimize the model, we experimented with different architectures:\n",
    "\n",
    "- **Wider models:** These have more units in the hidden layers, allowing them to capture more complex patterns in the data.\n",
    "- **Deeper models:** These include additional hidden layers, enabling the model to learn hierarchical representations of the data.\n",
    "- **Narrower models:** These have fewer units in the hidden layers, promoting simplicity and reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build different models for comparison\n",
    "\n",
    "# Wider model with more units in the hidden layer\n",
    "wider = Sequential([\n",
    "    Dense(128, kernel_regularizer=l2(0.01), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Deeper model with two hidden layers\n",
    "deeper = Sequential([\n",
    "    Dense(64, kernel_regularizer=l2(0.01), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, kernel_regularizer=l2(0.01), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Narrower model with fewer units in the hidden layer\n",
    "narrower = Sequential([\n",
    "    Dense(32, kernel_regularizer=l2(0.01), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating Different Architectures\n",
    "\n",
    "We trained the various model architectures defined above, including wider, deeper, and narrower models, to explore their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save cross-validation results\n",
    "results_file_path = os.path.join(base_path, \"wider_results.pkl\")\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters for the current model\n",
    "cv_scores = cross_validation(X_train_tfidf, y_train, cv=3, model=wider, param_grid=param_grid,\n",
    "                             loss=LOSS, metrics=METRICS, epochs=200, callbacks=[], seed=42,\n",
    "                             force_recompute=False, results_file_path=results_file_path, class_weight=class_weight)\n",
    "\n",
    "# Find best parameters\n",
    "best_params = max(cv_scores, key=lambda k: cv_scores[k][2])\n",
    "wider_best_score = cv_scores[best_params]\n",
    "wider_best_params = eval(best_params)  # Convert string representation back to dictionary\n",
    "\n",
    "print(f'Wider Model - Best params: {wider_best_params}, Best score: {wider_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regularized model with optimal hyperparameters\n",
    "wider_model = Sequential([\n",
    "    Dense(128, kernel_regularizer=l2(wider_best_params['alpha']), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(wider_best_params['dropout']),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the regularized model\n",
    "wider_model.compile(optimizer=Adam(learning_rate=wider_best_params['learning_rate']),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy', F1Score(name='f1_score', average='macro'), AUC(name='auc', multi_label=True)])\n",
    "\n",
    "# Train the regularized model\n",
    "wider_history = wider_model.fit(X_train_tfidf, y_train, batch_size=wider_best_params['batch_size'], epochs=250,\n",
    "                                validation_split=0.1, callbacks=[], verbose=0, class_weight=class_weight)\n",
    "\n",
    "# Evaluate the wider model\n",
    "wider_scores = wider_model.evaluate(X_test_tfidf, y_test, verbose=0)\n",
    "print(f'Wider Model - Test Accuracy: {wider_scores[1]:.2f}, F1 Score: {wider_scores[2]:.2f}, AUC: {wider_scores[3]:.2f}')\n",
    "\n",
    "# Plot wider model training history\n",
    "plot_training_history(wider_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save cross-validation results\n",
    "results_file_path = os.path.join(base_path, \"deeper_results.pkl\")\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters for the current model\n",
    "cv_scores = cross_validation(X_train_tfidf, y_train, cv=3, model=deeper, param_grid=param_grid,\n",
    "                             loss=LOSS, metrics=METRICS, epochs=200, callbacks=[], seed=42,\n",
    "                             force_recompute=True, results_file_path=results_file_path, class_weight=class_weight)\n",
    "\n",
    "# Find best parameters\n",
    "best_params = max(cv_scores, key=lambda k: cv_scores[k][2])\n",
    "deeper_best_score = cv_scores[best_params]\n",
    "deeper_best_params = eval(best_params)  # Convert string representation back to dictionary\n",
    "\n",
    "print(f'Deeper Model - Best params: {deeper_best_params}, Best score: {deeper_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regularized model with optimal hyperparameters\n",
    "deeper_model = Sequential([\n",
    "    Dense(64, kernel_regularizer=l2(deeper_best_params['alpha']), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(deeper_best_params['dropout']),\n",
    "    Dense(64, kernel_regularizer=l2(deeper_best_params['alpha']), activation='relu'),\n",
    "    Dropout(deeper_best_params['dropout']),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the regularized model\n",
    "deeper_model.compile(optimizer=Adam(learning_rate=deeper_best_params['learning_rate']), loss=LOSS, metrics=METRICS)\n",
    "\n",
    "# Train the regularized model\n",
    "deeper_history = deeper_model.fit(X_train_tfidf, y_train, batch_size=deeper_best_params['batch_size'], epochs=250,\n",
    "                                  validation_split=0.1, callbacks=[], verbose=0, class_weight=class_weight)\n",
    "\n",
    "# Evaluate the wider model\n",
    "deeper_scores = deeper_model.evaluate(X_test_tfidf, y_test, verbose=0)\n",
    "print(f'Deeper Model - Test Accuracy: {deeper_scores[1]:.2f}, F1 Score: {deeper_scores[2]:.2f}, AUC: {deeper_scores[3]:.2f}')\n",
    "\n",
    "# Plot deeper model training history\n",
    "plot_training_history(deeper_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save cross-validation results\n",
    "results_file_path = os.path.join(base_path, \"narrower_results.pkl\")\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters for the current model\n",
    "cv_scores = cross_validation(X_train_tfidf, y_train, cv=3, model=narrower, param_grid=param_grid,\n",
    "                             loss=LOSS, metrics=METRICS, epochs=350, callbacks=[], seed=42,\n",
    "                             force_recompute=True, results_file_path=results_file_path, class_weight=class_weight)\n",
    "\n",
    "# Find best parameters\n",
    "best_params = max(cv_scores, key=lambda k: cv_scores[k][2])\n",
    "narrower_best_score = cv_scores[best_params]\n",
    "narrower_best_params = eval(best_params)  # Convert string representation back to dictionary\n",
    "\n",
    "print(f'Narrower Model - Best params: {narrower_best_params}, Best score: {narrower_best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regularized model with optimal hyperparameters\n",
    "narrower_model = Sequential([\n",
    "    Dense(32, kernel_regularizer=l2(narrower_best_params['alpha']), activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(narrower_best_params['dropout']),\n",
    "    Dense(OUTPUT_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the regularized model\n",
    "narrower_model.compile(optimizer=Adam(learning_rate=narrower_best_params['learning_rate']), loss=LOSS, metrics=METRICS)\n",
    "\n",
    "# Train the regularized model\n",
    "narrower_history = narrower_model.fit(X_train_tfidf, y_train, batch_size=narrower_best_params['batch_size'], epochs=350,\n",
    "                                      validation_split=0.1, callbacks=[], verbose=0, class_weight=class_weight)\n",
    "\n",
    "# Evaluate the narrower model\n",
    "narrower_scores = narrower_model.evaluate(X_test_tfidf, y_test, verbose=0)\n",
    "print(f'Narrower Model - Test Accuracy: {narrower_scores[1]:.2f}, F1 Score: {narrower_scores[2]:.2f}, AUC: {narrower_scores[3]:.2f}')\n",
    "\n",
    "# Plot narrower model training history\n",
    "plot_training_history(narrower_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Performance Comparison Table\n",
    "\n",
    "The summary table below shows the performance metrics of different models, including the baseline model and variations such as regularized, wider, deeper, and narrower models. The table presents the accuracy, F1 Score, AUC, and the hyperparameters (Dropout rate and L2 regularization alpha value) used for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extracting the performance metrics and hyperparameters\n",
    "models_performance = {\n",
    "    \"Model\": [\"Baseline\", \"Regularized\", \"Wider\", \"Deeper\", \"Narrower\"],\n",
    "    \"Accuracy\": [baseline_scores[1], regularized_scores[1], wider_scores[1], deeper_scores[1], narrower_scores[1]],\n",
    "    \"F1 Score\": [baseline_scores[2], regularized_scores[2], wider_scores[2], deeper_scores[2], narrower_scores[2]],\n",
    "    \"AUC\": [baseline_scores[3], regularized_scores[3], wider_scores[3], deeper_scores[3], narrower_scores[3]],\n",
    "    \"Dropout\": [np.nan, regularized_best_params['dropout'], wider_best_params['dropout'], deeper_best_params['dropout'], narrower_best_params['dropout']],\n",
    "    \"Alpha\": [np.nan, regularized_best_params['alpha'], wider_best_params['alpha'], deeper_best_params['alpha'], narrower_best_params['alpha']]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame for the performance comparison table\n",
    "models_performance_df = pd.DataFrame(models_performance)\n",
    "print(models_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analysis of Results\n",
    "\n",
    "The performance of various neural network models, including the baseline model and variations such as regularized, wider, deeper, and narrower models, is summarized in the results table. The key metrics—accuracy, F1 Score, and AUC—along with the hyperparameters (Dropout rate and L2 regularization alpha value) used for each model, provide a comprehensive overview of model performance.\n",
    "\n",
    "### Baseline Model\n",
    "\n",
    "- The baseline model achieves an accuracy of 0.786, an F1 Score of 0.727, and an AUC of 0.893.\n",
    "- The training history for the baseline model indicates that it has sufficient statistical power to fit the training data, as shown by the steady decrease in training loss. However, the increase in validation loss after around 20 epochs suggests that the model begins to overfit the data, memorizing patterns specific to the training set that do not generalize well to unseen data.\n",
    "\n",
    "### Regularized Model\n",
    "\n",
    "- The regularized model shows slight improvements over the baseline in all metrics: accuracy (0.794), F1 Score (0.741), and AUC (0.903).\n",
    "- The addition of regularization techniques, such as Dropout and L2 regularization, helps to reduce overfitting, as evidenced by the improved AUC, which measures the model's ability to distinguish between classes.\n",
    "\n",
    "### Wider Model\n",
    "\n",
    "- The wider model achieves the highest AUC of 0.905, indicating its strong ability to discriminate between classes. It also shows improvements in accuracy and F1 Score compared to the baseline.\n",
    "- However, the gains from this architecture are marginal, suggesting that the baseline model already had enough capacity to capture the complexity of the data, and increasing the number of units in the hidden layers did not significantly enhance performance.\n",
    "\n",
    "### Deeper and Narrower Models\n",
    "\n",
    "- Both the deeper and narrower models exhibit performances close to the baseline, with only minor variations in accuracy, F1 Score, and AUC.\n",
    "- The results indicate that adding more layers or reducing the number of units does not substantially improve the model's performance. This is likely because the baseline model already possesses enough statistical power to learn from the data effectively.\n",
    "\n",
    "### Overall Analysis\n",
    "\n",
    "- The baseline model's ability to overfit the data, as shown by the training history plot, highlights its sufficient capacity to learn from the dataset. This capacity limits the extent to which additional complexity in the model architecture (e.g., wider or deeper models) can improve performance.\n",
    "- Regularization plays a key role in enhancing the model's generalization capabilities, as seen in the regularized model's metrics. However, the improvements are marginal, suggesting that the baseline model is already near-optimal for this task within the constraints of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "This study explored various dense neural network architectures for sentiment analysis on a dataset of tweets related to US airlines. The models were evaluated on key metrics such as accuracy, F1 Score, and AUC, with a particular focus on handling class imbalance and preventing overfitting.\n",
    "\n",
    "The **baseline model** demonstrated strong performance, indicating that it already possessed sufficient statistical power to effectively classify the sentiment of tweets. The training history showed evidence of overfitting, with the validation loss increasing after several epochs, suggesting that the model began to memorize the training data rather than generalize from it.\n",
    "\n",
    "Despite experimenting with **wider, deeper, and narrower architectures**, the improvements over the baseline model were marginal. This outcome suggests that the baseline model's architecture was already well-suited to the task, and increasing complexity did not significantly enhance performance. Regularization techniques, such as Dropout and L2 regularization, provided slight improvements by mitigating overfitting and improving the model's ability to generalize.\n",
    "\n",
    "In conclusion, the study highlights that, within the constraints of using only Dense layers, Dropout, and L2 regularization, the baseline model was sufficient for achieving high performance in sentiment classification. The exploration of different architectures confirmed that sometimes simpler models can be just as effective, especially when they already possess the capacity to capture the essential patterns in the data. Regularization remains a valuable tool for enhancing model generalization, particularly in scenarios where overfitting is observed.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Work\n",
    "\n",
    "Future work could focus on enhancing model performance through the implementation of optimization techniques like **Early Stopping** and **learning rate schedulers**, which would prevent overfitting and improve training efficiency. Additionally, increasing the interpretability of the models using techniques such as **LIME** (Local Interpretable Model-agnostic Explanations) or **SHAP** (SHapley Additive exPlanations) would provide valuable insights into the decision-making process, making the models' predictions more transparent and reliable. These improvements would lead to more robust and trustworthy sentiment analysis models, better suited for real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Chollet, F. (2018). *Deep learning with Python*. Manning Publications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
