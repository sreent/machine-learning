{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Sentiment%20Analysis%20of%20US%20Airline%20Tweets%20-%20A%20Write-Up%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Sentiment Analysis of US Airline Tweets Using Deep Neural Network\n\n---\n\n## Introduction\n\n### Problem Definition\n\nSentiment analysis, a key task in natural language processing (NLP), involves identifying and categorising opinions expressed in a piece of text to determine the writer's sentiment—whether positive, neutral, or negative. This study focuses on analysing the sentiment of tweets related to US airlines, aiming to classify them into these three categories. Sentiment analysis on social media data, such as tweets, presents unique challenges due to the informal language, use of slang, emojis, sarcasm, and the brevity of the messages.\n\n### Motivation\n\nIn today's digital age, social media platforms like Twitter have become crucial channels for customers to voice their opinions and experiences with companies, including airlines. Understanding customer sentiment from these platforms allows airlines to gain real-time insights into customer satisfaction and potential issues. Effective sentiment analysis can enable airlines to proactively address problems, enhance customer satisfaction, and build stronger brand loyalty.\n\n### Dataset\n\nThe dataset used in this study is the **Twitter US Airline Sentiment** dataset from Kaggle ([source](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment)). Key characteristics:\n\n| Attribute | Value |\n|-----------|-------|\n| **Total samples** | 14,640 tweets |\n| **Classes** | 3 (Positive, Neutral, Negative) |\n| **Class distribution** | 16% positive, 21% neutral, 63% negative |\n| **Imbalance ratio** | 3.88:1 (majority:minority) |\n\nThe significant class imbalance poses a challenge for model training, requiring careful selection of evaluation metrics and techniques.\n\n### Constraints and Methodological Focus\n\nThis assignment follows the **Universal Workflow of Machine Learning** from *Deep Learning with Python* (Chollet, 2021), Chapter 4.5. We are constrained to use only:\n\n- **Dense layers** (fully connected)\n- **Dropout layers** (regularisation)\n- **L1/L2 regularisation** (weight penalties)\n\nWe are restricted from using CNNs, RNNs, Transformers, or Early Stopping. Despite these constraints, we systematically explore various dense neural network architectures to determine their effectiveness in sentiment classification.\n\n### Objectives\n\n1. **Data Preprocessing:** Convert text data to numerical format using TF-IDF vectorisation\n2. **Model Development:** Train dense neural networks following the Universal ML Workflow\n3. **Performance Evaluation:** Evaluate using F1-Score (primary), Accuracy, and AUC\n4. **Model Optimisation:** Apply Dropout and L2 regularisation to prevent overfitting\n5. **Architectural Insights:** Compare wider, deeper, and narrower architectures\n\n---\n\n## Methodology"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "### Data Loading\n",
    "\n",
    "We begin by mounting Google Drive to access the dataset stored in it. We create the necessary directories to store the dataset and use the `gdown` library to download the dataset from the given URL. This ensures that our data is easily accessible and can be seamlessly integrated into our Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip -q install gdown==4.6.0\n",
    "import gdown\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_path = \"/content/drive/MyDrive/Neural Networks/Twitter US Airline Sentiment/\"\n",
    "    # Create necessary directories to store the dataset\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    base_path = \"./\"\n",
    "\n",
    "# Download the dataset from the given URL\n",
    "URL = \"https://drive.google.com/file/d/15XHy_PdD6Q2aa6n-pnWmSFGCv1oK9vWA/view?usp=sharing\"\n",
    "DOWNLOAD_FILE_PATH = \"https://drive.google.com/uc?export=download&id=\" + URL.split(\"/\")[-2]\n",
    "gdown.download(DOWNLOAD_FILE_PATH, base_path + \"Tweets.csv\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The dataset is loaded into a pandas DataFrame. We use sklearn's `TfidfVectorizer` to convert the text data into numerical form using TF-IDF mode. The sentiment labels are converted into numerical form using label encoding and then one-hot encoded. We chose 5000 features and bigrams for TF-IDF to capture a broad range of important terms and their combinations, providing a richer representation of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the data from the CSV file into a pandas DataFrame\n",
    "file_path = os.path.join(base_path, \"Tweets.csv\")\n",
    "tweets = pd.read_csv(file_path)[['text', 'airline_sentiment']]\n",
    "\n",
    "# Label Encoding to convert sentiment labels into numerical form\n",
    "label_encoder = LabelEncoder()\n",
    "y = to_categorical(label_encoder.fit_transform(tweets['airline_sentiment']))\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['text'], y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Define the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Choosing a Measure of Success\n\n### Exploring Class Imbalance\n\nBefore proceeding with model development, it is crucial to understand the class distribution to address any potential imbalance. The dataset shows a significant imbalance, with a majority of tweets being negative (63%), followed by neutral (21%), and positive (16%). This imbalance will be taken into account when selecting evaluation metrics and techniques to ensure robust performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nclass_counts = tweets['airline_sentiment'].value_counts()\nprint(\"Class Distribution:\")\nprint(class_counts)\nprint(f\"\\nImbalance ratio (majority:minority): {class_counts.max() / class_counts.min():.2f}:1\")\n\n# Plot class distribution\nplt.figure(figsize=(8, 5))\nclass_counts.plot(kind='bar', color=['#d62728', '#7f7f7f', '#2ca02c'], edgecolor='black')\nplt.title('Class Distribution in Twitter US Airline Sentiment Dataset')\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nfor i, v in enumerate(class_counts):\n    plt.text(i, v + 100, f'{v}\\n({v/len(tweets)*100:.1f}%)', ha='center', fontsize=10)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Metrics\n\nWe track multiple metrics to comprehensively evaluate model performance:\n\n| Metric | Purpose | When to Use |\n|--------|---------|-------------|\n| **Accuracy** | Overall correctness | Balanced datasets |\n| **F1-Score (macro)** | Balance of precision & recall across all classes | **Primary metric** for imbalanced data |\n| **AUC** | Discrimination ability across thresholds | Ranking quality; used for hyperparameter tuning |\n\n**Why F1-Score as Primary, AUC for Tuning?**\n\n- **F1-Score** directly measures performance on minority classes - critical for our 3.88:1 imbalanced dataset\n- **AUC** provides smooth gradients during hyperparameter search, making it ideal as a tuning objective\n- We report both metrics for final evaluation\n\n### Naive Baseline\n\nTo provide a reference point for model performance, we establish a naive baseline using the most frequent class. Given the class imbalance, the naive baseline would predict every tweet as the majority class, which is \"negative\". The naive baseline results show an accuracy of 0.63, an F1 Score of 0.26, and an AUC of 0.50. These results highlight the need for a more sophisticated model to better capture the sentiment distribution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train_tfidf, y_train.argmax(axis=1))\n",
    "y_dummy_pred = dummy_clf.predict(X_train_tfidf)\n",
    "\n",
    "naive_accuracy = accuracy_score(y_train.argmax(axis=1), y_dummy_pred)\n",
    "naive_f1 = f1_score(y_train.argmax(axis=1), y_dummy_pred, average='macro')\n",
    "naive_auc = roc_auc_score(y_train, to_categorical(y_dummy_pred), average='macro', multi_class='ovo')\n",
    "\n",
    "print(f'Naive Baseline - Accuracy: {naive_accuracy:.2f}, F1 Score: {naive_f1:.2f}, AUC: {naive_auc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights\n",
    "\n",
    "Given the significant class imbalance observed, it is necessary to apply class weights during model training to ensure the model treats each class fairly. Class weights adjust the importance of each class in the loss function, giving more weight to minority classes. This helps the model focus on correctly predicting these underrepresented classes, improving overall performance.\n",
    "\n",
    "The `compute_class_weight` function from sklearn is used to calculate the weights for each class. These weights are then converted into a dictionary format, which can be passed directly to the Keras model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                      classes=np.unique(np.argmax(y_train, axis=1)),\n",
    "                                      y=np.argmax(y_train, axis=1))\n",
    "\n",
    "# Convert the class weights to a dictionary format required by Keras\n",
    "class_weight = dict(enumerate(class_weights))\n",
    "\n",
    "# Print class weights\n",
    "print(f\"Computed Class Weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\nThe choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n\n### Data Split Strategy (This Notebook)\n\nWith **14,640 samples** (above the 10,000 threshold), we use **Hold-Out validation**:\n\n```\nOriginal Data (14,640 samples) → Hold-Out Selected\n├── Test Set (20%) - Final evaluation only\n└── Training Pool (80%)\n    ├── Training Set (~72%) - Model training\n    └── Validation Set (~8%) - Hyperparameter tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Developing a Model that Does Better than a Naive Baseline\n",
    "\n",
    "### Baseline Model\n",
    "\n",
    "We establish a baseline model using a simple dense layer with softmax activation. The model is compiled with categorical crossentropy loss and evaluated using categorical accuracy, F1 score, and AUC. This baseline provides a reference point for evaluating more complex models. A simple model serves as a baseline to understand the minimal performance we can achieve without sophisticated techniques. This helps us gauge the improvement offered by more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC, F1Score\n\n# Define model parameters\nOUTPUT_CLASSES = y_train.shape[1]\nLOSS = 'categorical_crossentropy'\nMETRICS = ['categorical_accuracy', F1Score(name='f1_score', average='macro'), AUC(name='auc')]\n\n# =============================================================================\n# RESULTS TRACKING - collect all model results programmatically\n# =============================================================================\nmodel_results = []\n\ndef record_results(name, scores, hyperparams=None):\n    \"\"\"Record model evaluation results for comparison table.\"\"\"\n    result = {\n        'Model': name,\n        'Accuracy': scores[1],\n        'F1 Score': scores[2],\n        'AUC': scores[3],\n        'Dropout': hyperparams.get('dropout') if hyperparams else None,\n        'L2 Reg': hyperparams.get('l2_reg') if hyperparams else None,\n        'Learning Rate': hyperparams.get('lr') if hyperparams else None\n    }\n    model_results.append(result)\n    return result\n\n# Build baseline model (Single Layer Perceptron - no hidden layers)\nbaseline = Sequential([\n    Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(X_train_tfidf.shape[1],))\n])\n\n# Compile the baseline model\n# Higher learning rate (0.005) for simple model - converges quickly\nbaseline.compile(optimizer=Adam(learning_rate=0.005),\n                 loss=LOSS,\n                 metrics=METRICS)\n\n# Train baseline model\nbaseline_history = baseline.fit(X_train_tfidf, y_train, batch_size=512, epochs=100,\n                                validation_split=0.1, verbose=0, class_weight=class_weight)\n\n# Evaluate the baseline model\nbaseline_scores = baseline.evaluate(X_test_tfidf, y_test, verbose=0)\nrecord_results('Baseline (SLP)', baseline_scores)\nprint(f'Baseline Model - Test Accuracy: {baseline_scores[1]:.2f}, F1 Score: {baseline_scores[2]:.2f}, AUC: {baseline_scores[3]:.2f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The baseline SLP model significantly outperforms the naive baseline, demonstrating that TF-IDF features capture meaningful sentiment signals even without hidden layers.\n\n### Plot Baseline Model Training History\n\nThe helper function below visualises training and validation loss over epochs, helping us identify convergence and potential overfitting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, monitor='loss'):\n",
    "    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'b.', label=monitor)\n",
    "    plt.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n",
    "    plt.xlim([0, len(loss)])\n",
    "    plt.title('Training and Validation ' + monitor + 's')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(monitor)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(baseline_history, monitor='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The training history shows both training and validation loss decreasing and converging. This is expected for an SLP—it has limited capacity (no hidden layers), so it cannot overfit easily. The model learns a linear decision boundary that generalises well.\n\n**Comparison with Naive Baseline:**\n\nThe SLP dramatically outperforms the naive baseline (Accuracy: 0.63, F1: 0.26, AUC: 0.50), confirming that our TF-IDF features contain useful signal. Next, we add hidden layers to see if we can do better."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Scaling Up: Developing a Model that Overfits\n\n### More Complex Model\n\nWe build a model with one or two hidden layers and see if it can overfit the data. This model helps gauge the complexity required to learn the patterns in the data. By monitoring the training and validation loss, we can observe overfitting and decide on regularisation techniques to mitigate it. Understanding the complexity required to fit the data is crucial for determining the appropriate model architecture and regularisation techniques. This step helps us identify the point where the model becomes too complex and starts overfitting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "BATCH_SIZE = 512\n\n# Build overfitting model with one hidden layer\noverfit = Sequential([\n    Dense(64, activation=\"relu\", input_shape=(X_train_tfidf.shape[1],)),\n    Dense(OUTPUT_CLASSES, activation=\"softmax\")\n])\n\n# Compile the overfitting model\n# Lower learning rate (0.0001) to slow down training and observe overfitting pattern clearly\n# With higher learning rate, the model would overfit too quickly to see the gradual divergence\noverfit.compile(optimizer=Adam(learning_rate=0.0001),\n                loss=LOSS,\n                metrics=METRICS)\n\n# Train overfitting model for 200 epochs to demonstrate overfitting\noverfit_history = overfit.fit(X_train_tfidf, y_train, batch_size=BATCH_SIZE, epochs=200,\n                              validation_split=0.1, verbose=0, class_weight=class_weight)\n\n# Evaluate on test set and record results\noverfit_scores = overfit.evaluate(X_test_tfidf, y_test, verbose=0)\nrecord_results('Overfit (64, no reg)', overfit_scores, {'dropout': 0.0, 'l2_reg': 0.0, 'lr': 0.0001})\n\n# Access validation metrics from history for display\noverfit_val_accuracy = overfit_history.history['val_categorical_accuracy'][-1]\noverfit_val_f1_score = overfit_history.history['val_f1_score'][-1]\noverfit_val_auc = overfit_history.history['val_auc'][-1]\n\nprint(f'Validation Accuracy: {overfit_val_accuracy:.2f}, F1 Score: {overfit_val_f1_score:.2f}, AUC: {overfit_val_auc:.2f}')\nplot_training_history(overfit_history, monitor='loss')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Overfitting Confirmed:** The training history shows the classic overfitting pattern:\n- Training loss continues to decrease (model memorises training data)\n- Validation loss increases after ~110 epochs (model fails to generalise)\n\nThis is exactly what we wanted to see! It confirms that:\n1. A single hidden layer with 64 neurons has **sufficient capacity** to fit the data\n2. **Regularisation is needed** to prevent overfitting\n3. We should NOT add more capacity (wider/deeper)—we should regularise instead\n\n> *\"If your model can overfit, you have enough capacity. The solution is regularisation, not more neurons.\"* — Universal ML Workflow principle"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Regularising Your Model and Tuning Your Hyperparameters\n\n### Regularisation Techniques\n\nWe identified that the model with a hidden layer overfits the data. To address this, we incorporate regularisation techniques:\n\n| Technique | How it works | Effect |\n|-----------|--------------|--------|\n| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation (Srivastava et al., 2014) |\n| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries (Krogh & Hertz, 1992) |\n\n### Hyperparameter Tuning Methods: Choosing the Right Approach\n\nChoosing the right hyperparameter tuning method is crucial for finding optimal model configurations. Each method has distinct advantages and limitations.\n\n#### Method Overview\n\n| Method | How it Works | Resource Allocation |\n|--------|--------------|---------------------|\n| **Grid Search** | Exhaustively tries all specified combinations | Equal epochs for all configurations |\n| **Random Search** | Randomly samples from the hyperparameter space | Equal epochs for all configurations |\n| **Hyperband** | Early stopping of poor performers; focuses resources on promising configs | Adaptive—more epochs for better performers |\n\n#### Why We Choose RandomSearch for This Coursework\n\n**The key constraint:** This coursework does not allow early stopping. This significantly affects our choice of tuning method.\n\n**The problem with Hyperband when early stopping is unavailable:**\n- Hyperband finds hyperparameters optimal for *short* training runs (e.g., 20 epochs)\n- When we retrain for longer (e.g., 150 epochs) without early stopping, the regularisation may be too weak\n- Example: Dropout=0.2 may prevent overfitting at 20 epochs, but be insufficient at 150 epochs\n\n**Why RandomSearch is the better choice:**\n- **No epoch mismatch:** All configurations train for the full 150 epochs\n- **Reliable regularisation:** The selected hyperparameters are optimal for the actual training duration\n- **Simpler workflow:** No need to manually adjust regularisation strength after tuning\n\n| Aspect | Hyperband | RandomSearch (our choice) |\n|--------|-----------|---------------------------|\n| Epoch alignment | Tunes for ~20 epochs | Tunes for full 150 epochs |\n| Regularisation reliability | May be too weak for extended training | Optimal for actual training duration |\n| Computational cost | Lower (early stopping) | Higher (but acceptable for coursework) |\n| Workflow complexity | Requires post-tuning adjustment | Simple—use hyperparameters directly |\n\n### RandomSearch Configuration\n\nWe tune the following hyperparameters using RandomSearch with **15 trials**, each training for **150 epochs**:\n- **L2 regularisation strength** (1e-5 to 1e-2)\n- **Dropout rate** (0.0 to 0.5)\n- **Learning rate** (1e-4 to 1e-2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.layers import Dropout, Dense\nfrom tensorflow.keras.regularizers import l2\n\n# Install and import Keras Tuner\n%pip install -q -U keras-tuner\nimport keras_tuner as kt\n\n# Store dimensions for use in model builder\nINPUT_DIMENSION = X_train_tfidf.shape[1]\n\n# RandomSearch Model Builder\ndef build_model_tunable(hp):\n    \"\"\"\n    Build sentiment analysis model with FIXED architecture (1 hidden layer, 64 neurons).\n    Only tunes regularisation and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # L2 regularisation strength\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n\n    # Fixed architecture: 1 hidden layer with 64 neurons\n    model.add(layers.Dense(64, activation='relu', \n                           kernel_regularizer=regularizers.l2(l2_reg)))\n    \n    # Tunable dropout\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(dropout_rate))\n\n    # Output layer for multi-class classification\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n\n    # Tunable learning rate\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure RandomSearch tuner\n# All configurations train for full EPOCHS_REGULARIZED epochs - no epoch mismatch!\nEPOCHS_REGULARIZED = 150\nMAX_TRIALS = 15\n\ntuner = kt.RandomSearch(\n    build_model_tunable,\n    objective='val_auc',\n    max_trials=MAX_TRIALS,\n    directory='airline_sentiment_randomsearch',\n    project_name='airline_sentiment_tuning',\n    overwrite=True\n)\n\nprint(f\"RandomSearch Configuration:\")\nprint(f\"  - Max trials: {MAX_TRIALS}\")\nprint(f\"  - Epochs per trial: {EPOCHS_REGULARIZED}\")\nprint(f\"  - Objective: val_auc\")\nprint(f\"  - Total training epochs: {MAX_TRIALS} × {EPOCHS_REGULARIZED} = {MAX_TRIALS * EPOCHS_REGULARIZED}\")\n\n# Create validation split from training data for tuning\n# This ensures consistent validation set during hyperparameter search\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train_tfidf, y_train, test_size=0.1, stratify=y_train, random_state=42\n)\n\n# Run RandomSearch - each configuration trains for full 150 epochs\ntuner.search(\n    X_train_split, y_train_split,\n    validation_data=(X_val_split, y_val_split),\n    epochs=EPOCHS_REGULARIZED,  # Full epochs for ALL configurations\n    batch_size=BATCH_SIZE,\n    class_weight=class_weight\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters found by RandomSearch (trained for full 150 epochs):\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\n# Build the best model\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Retrain with Optimised Hyperparameters\n\nNow that we have the best hyperparameters from RandomSearch, we:\n\n1. **Build a fresh model** with the optimised L2 strength, dropout rate, and learning rate\n2. **Retrain from scratch** on the training set\n\n### Why Retrain? (RandomSearch Already Trained for 150 Epochs)\n\nAlthough RandomSearch trained each configuration for 150 epochs, we retrain because:\n\n1. **Fresh weights:** Start from random initialisation for reproducibility\n2. **Consistent evaluation:** Ensures fair comparison with other models in this notebook\n\n### Why Train the Regularised Model for 150 Epochs?\n\nRegularisation slows down learning:\n\n| Technique | Effect on Learning |\n|-----------|-------------------|\n| **Dropout** | Randomly masks neurons each batch → each gradient update uses only partial network information |\n| **L2 penalty** | Penalises large weights → constrains the size of weight updates |\n\nBoth techniques deliberately impede the optimisation process. The model takes smaller, noisier steps toward the solution. This is the *price* we pay for overfitting protection.\n\n| Model | Epochs | Why This Number? |\n|-------|--------|------------------|\n| **SLP (baseline)** | 100 | Simple model, converges quickly |\n| **DNN (no regularisation)** | 200 | Enough to clearly demonstrate overfitting |\n| **DNN (with Dropout + L2)** | 150 | Compensates for slower learning; ensures full convergence |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train with the same number of epochs used during RandomSearch tuning\n# Use the same validation split for consistency\nregularized_history = opt_model.fit(\n    X_train_split, y_train_split,\n    validation_data=(X_val_split, y_val_split),\n    epochs=EPOCHS_REGULARIZED,\n    batch_size=BATCH_SIZE,\n    class_weight=class_weight,\n    verbose=0\n)\n\n# With RandomSearch, hyperparameters are already optimised for 150 epochs\n# No epoch mismatch concern - we can trust the regularisation settings\nbest_epoch = np.argmin(regularized_history.history['val_loss']) + 1\nbest_val_loss = min(regularized_history.history['val_loss'])\nfinal_val_loss = regularized_history.history['val_loss'][-1]\n\nprint(f\"Training complete ({EPOCHS_REGULARIZED} epochs)\")\nprint(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\nprint(f\"Final validation loss: {final_val_loss:.4f}\")\n\n# Evaluate on test set and record results\nregularized_scores = opt_model.evaluate(X_test_tfidf, y_test, verbose=0)\nrecord_results('Regularized (64)', regularized_scores, \n               {'dropout': best_hp.get('dropout'), 'l2_reg': best_hp.get('l2_reg'), 'lr': best_hp.get('lr')})\n\nprint(f'\\nRegularized Model - Test Accuracy: {regularized_scores[1]:.2f}, F1 Score: {regularized_scores[2]:.2f}, AUC: {regularized_scores[3]:.2f}')\n\n# Plot regularized model training history\nplot_training_history(regularized_history, monitor='loss')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Regularisation Works:** The training history shows that validation loss now stabilises instead of increasing. The gap between training and validation loss is smaller, indicating better generalisation.\n\nThe regularised model achieves:\n- Similar or slightly better accuracy than the unregularised DNN\n- Improved F1-Score (better minority class performance)\n- Stable validation metrics (no overfitting)\n\nThis confirms our approach: **regularise the existing architecture rather than adding more capacity**."
  },
  {
   "cell_type": "markdown",
   "source": "### In-Depth Comparison: GridSearch vs RandomSearch vs Hyperband\n\nTo demonstrate comprehensive understanding of hyperparameter tuning methods, we provide a detailed analysis of why **RandomSearch** is the optimal choice for this coursework. This discussion compares all three major approaches and justifies our decision.\n\n#### Method Overview\n\n| Method | Search Strategy | Epoch Handling | Completeness |\n|--------|-----------------|----------------|--------------|\n| **GridSearch** | Exhaustive (all combinations) | Full epochs for all | Guarantees finding optimal within grid |\n| **RandomSearch** | Stochastic (random samples) | Full epochs for all | May miss optimal, but efficient |\n| **Hyperband** | Adaptive (early stopping) | Variable (short for poor configs) | Fast, but epoch mismatch issue |\n\n#### The Critical Constraint: No Early Stopping\n\nThe coursework does not permit early stopping. This single constraint dramatically affects our choice of tuning method:\n\n**Hyperband's Limitation:**\n- Hyperband's efficiency comes from *early stopping* of poor configurations\n- It finds hyperparameters optimal for short training runs (e.g., 20 epochs)\n- When we retrain for 150 epochs without early stopping, these hyperparameters may be suboptimal\n- Example: Dropout=0.2 may prevent overfitting at 20 epochs, but be insufficient at 150 epochs\n\n**Why This Matters:**\n```\nHyperband tunes at 20 epochs → selects Dropout=0.2\nRetrain at 150 epochs without early stopping → model overfits\nThe regularisation was calibrated for short training, not long training\n```\n\nThis is called the **epoch mismatch problem**: the hyperparameters are optimised for one training duration but applied to a different (longer) training duration.\n\n#### GridSearch: Guarantees but Limitations\n\n**Strengths:**\n- Exhaustively searches all specified combinations\n- Guarantees finding the optimal configuration within the grid\n- Reproducible and deterministic\n- Trains all configurations for full epochs (no mismatch)\n\n**Limitations:**\n- Must manually specify discrete values (e.g., Dropout ∈ {0.0, 0.2, 0.3, 0.5})\n- Cannot explore continuous ranges between grid points\n- Exponential cost: 3 hyperparameters × 4 values each = 64 combinations\n- May miss optimal values that fall between grid points\n\n**Example:**\n```\nIf GridSearch tests L2 ∈ {0.001, 0.01} but optimal is 0.005,\nGridSearch will never find it\n```\n\n#### RandomSearch: The Best of Both Worlds\n\n**Strengths:**\n- Samples from continuous ranges (L2 ∈ [1e-5, 1e-2])\n- Can find values that GridSearch would miss\n- More efficient than GridSearch for high-dimensional spaces (Bergstra & Bengio, 2012)\n- Trains all configurations for full epochs (no mismatch)\n- No need to manually specify discrete grid values\n\n**Why RandomSearch Beats GridSearch for This Coursework:**\n1. **Continuous hyperparameters:** L2 regularisation benefits from continuous sampling\n2. **Efficiency:** 15 trials often finds solutions as good as 36+ GridSearch combinations\n3. **Simplicity:** Define ranges, not exhaustive lists\n\n**Why RandomSearch Beats Hyperband for This Coursework:**\n1. **No epoch mismatch:** All 15 trials train for full 150 epochs\n2. **Reliable regularisation:** Selected hyperparameters are calibrated for actual training duration\n3. **No post-tuning adjustment:** Use the hyperparameters directly without modification\n\n#### Computational Cost Analysis\n\n| Method | Configurations | Epochs/Config | Total Epochs | Notes |\n|--------|---------------|---------------|--------------|-------|\n| GridSearch | 36+ (exhaustive) | 150 | 5,400+ | Guarantees grid optimal |\n| **RandomSearch** | **15 (sampled)** | **150** | **2,250** | **Good balance** |\n| Hyperband | 30-50 (adaptive) | 5-20 avg | 300-500 | Fast but epoch mismatch |\n\n#### Decision Framework\n\n| Scenario | Best Choice | Rationale |\n|----------|-------------|-----------|\n| Early stopping available | Hyperband | Most efficient |\n| Small, discrete search space | GridSearch | Guarantees optimal |\n| Continuous hyperparameters, no early stopping | **RandomSearch** | Our situation |\n| Very large search space | Hyperband | Even with mismatch risk |\n| Need reproducibility | GridSearch | Deterministic |\n\n#### Our Final Recommendation\n\n**RandomSearch is the optimal choice for this coursework because:**\n\n1. **Early stopping is not permitted** → Eliminates Hyperband as optimal choice\n2. **Continuous hyperparameter ranges** → RandomSearch can find better values than GridSearch\n3. **Computational cost is acceptable** → 15 trials × 150 epochs is manageable\n4. **No epoch mismatch** → Selected hyperparameters are reliable for 150-epoch training\n5. **Simpler workflow** → No need to manually define grid values or adjust post-tuning\n\n> *\"RandomSearch eliminates the epoch mismatch problem entirely by training all configurations for the full training duration. This ensures the selected regularisation strength is appropriate for the actual 150-epoch training, rather than being calibrated for a shorter Hyperband search.\"*\n\n#### References\n\n- Bergstra, J. and Bengio, Y. (2012) demonstrated that RandomSearch is more efficient than GridSearch for hyperparameter optimisation, especially when some hyperparameters are more important than others.\n- Li, L. et al. (2018) introduced Hyperband, which is efficient but designed for settings where early stopping is available.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Additional Experimentation: Exploring Different Architectures\n\n> **Note:** This section extends beyond the standard Universal ML Workflow (Steps 1-6) to demonstrate **extensive experimentation** with alternative architectures. The goal is to empirically verify that regularisation is more effective than architectural changes for this problem.\n\nTo further explore model optimisation, we experimented with different architectures:\n\n- **Wider models:** More units in hidden layers (128 neurons) to capture more complex patterns\n- **Deeper models:** Additional hidden layers (2 × 64 neurons) for hierarchical representations  \n- **Narrower models:** Fewer units (32 neurons) for simplicity and reduced overfitting risk\n\nEach architecture variant uses **RandomSearch** with the same configuration (15 trials, 150 epochs each) to find optimal regularisation hyperparameters, ensuring fair comparison."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ARCHITECTURE EXPLORATION WITH RANDOMSEARCH\n# =============================================================================\n\ndef build_wider_model(hp):\n    \"\"\"Wider model: 128 neurons in hidden layer.\"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(dropout_rate))\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=LOSS, metrics=METRICS)\n    return model\n\ndef build_deeper_model(hp):\n    \"\"\"Deeper model: 2 hidden layers with 64 neurons each.\"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n    model.add(layers.Dropout(dropout_rate))\n    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n    model.add(layers.Dropout(dropout_rate))\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=LOSS, metrics=METRICS)\n    return model\n\ndef build_narrower_model(hp):\n    \"\"\"Narrower model: 32 neurons in hidden layer.\"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(dropout_rate))\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=LOSS, metrics=METRICS)\n    return model\n\nprint(\"Architecture variants defined:\")\nprint(\"  - Wider:    128 neurons (1 hidden layer)\")\nprint(\"  - Deeper:   64 neurons  (2 hidden layers)\")\nprint(\"  - Narrower: 32 neurons  (1 hidden layer)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training and Evaluating Different Architectures\n\nWe trained the various model architectures defined above, including wider, deeper, and narrower models, using RandomSearch to find optimal hyperparameters for each. All architectures use the same tuning configuration: 15 trials with 150 epochs each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# WIDER MODEL - RandomSearch Tuning\n# =============================================================================\ntuner_wider = kt.RandomSearch(\n    build_wider_model,\n    objective='val_auc',\n    max_trials=MAX_TRIALS,\n    directory='airline_wider_randomsearch',\n    project_name='wider_tuning',\n    overwrite=True\n)\n\ntuner_wider.search(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n                   epochs=EPOCHS_REGULARIZED, batch_size=BATCH_SIZE, class_weight=class_weight)\n\nwider_best_hp = tuner_wider.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Wider Model - Best: L2={wider_best_hp.get('l2_reg'):.6f}, Dropout={wider_best_hp.get('dropout')}, LR={wider_best_hp.get('lr'):.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train wider model with best hyperparameters\nwider_model = tuner_wider.hypermodel.build(wider_best_hp)\nwider_history = wider_model.fit(X_train_split, y_train_split, \n                                 validation_data=(X_val_split, y_val_split), \n                                 epochs=EPOCHS_REGULARIZED, batch_size=BATCH_SIZE,\n                                 class_weight=class_weight, verbose=0)\n\n# Evaluate and record results\nwider_scores = wider_model.evaluate(X_test_tfidf, y_test, verbose=0)\nrecord_results('Wider (128)', wider_scores,\n               {'dropout': wider_best_hp.get('dropout'), 'l2_reg': wider_best_hp.get('l2_reg'), 'lr': wider_best_hp.get('lr')})\n\nprint(f'Wider Model - Test Accuracy: {wider_scores[1]:.2f}, F1 Score: {wider_scores[2]:.2f}, AUC: {wider_scores[3]:.2f}')\nplot_training_history(wider_history, monitor='loss')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DEEPER MODEL - RandomSearch Tuning\n# =============================================================================\ntuner_deeper = kt.RandomSearch(\n    build_deeper_model,\n    objective='val_auc',\n    max_trials=MAX_TRIALS,\n    directory='airline_deeper_randomsearch',\n    project_name='deeper_tuning',\n    overwrite=True\n)\n\ntuner_deeper.search(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n                    epochs=EPOCHS_REGULARIZED, batch_size=BATCH_SIZE, class_weight=class_weight)\n\ndeeper_best_hp = tuner_deeper.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Deeper Model - Best: L2={deeper_best_hp.get('l2_reg'):.6f}, Dropout={deeper_best_hp.get('dropout')}, LR={deeper_best_hp.get('lr'):.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train deeper model with best hyperparameters\ndeeper_model = tuner_deeper.hypermodel.build(deeper_best_hp)\ndeeper_history = deeper_model.fit(X_train_split, y_train_split, \n                                   validation_data=(X_val_split, y_val_split), \n                                   epochs=EPOCHS_REGULARIZED, batch_size=BATCH_SIZE,\n                                   class_weight=class_weight, verbose=0)\n\n# Evaluate and record results\ndeeper_scores = deeper_model.evaluate(X_test_tfidf, y_test, verbose=0)\nrecord_results('Deeper (64×2)', deeper_scores,\n               {'dropout': deeper_best_hp.get('dropout'), 'l2_reg': deeper_best_hp.get('l2_reg'), 'lr': deeper_best_hp.get('lr')})\n\nprint(f'Deeper Model - Test Accuracy: {deeper_scores[1]:.2f}, F1 Score: {deeper_scores[2]:.2f}, AUC: {deeper_scores[3]:.2f}')\nplot_training_history(deeper_history, monitor='loss')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# NARROWER MODEL - RandomSearch Tuning\n# =============================================================================\ntuner_narrower = kt.RandomSearch(\n    build_narrower_model,\n    objective='val_auc',\n    max_trials=MAX_TRIALS,\n    directory='airline_narrower_randomsearch',\n    project_name='narrower_tuning',\n    overwrite=True\n)\n\ntuner_narrower.search(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n                      epochs=EPOCHS_REGULARIZED, batch_size=BATCH_SIZE, class_weight=class_weight)\n\nnarrower_best_hp = tuner_narrower.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Narrower Model - Best: L2={narrower_best_hp.get('l2_reg'):.6f}, Dropout={narrower_best_hp.get('dropout')}, LR={narrower_best_hp.get('lr'):.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train narrower model with best hyperparameters\nnarrower_model = tuner_narrower.hypermodel.build(narrower_best_hp)\nnarrower_history = narrower_model.fit(X_train_split, y_train_split, \n                                       validation_data=(X_val_split, y_val_split), \n                                       epochs=EPOCHS_REGULARIZED, batch_size=BATCH_SIZE,\n                                       class_weight=class_weight, verbose=0)\n\n# Evaluate and record results\nnarrower_scores = narrower_model.evaluate(X_test_tfidf, y_test, verbose=0)\nrecord_results('Narrower (32)', narrower_scores,\n               {'dropout': narrower_best_hp.get('dropout'), 'l2_reg': narrower_best_hp.get('l2_reg'), 'lr': narrower_best_hp.get('lr')})\n\nprint(f'Narrower Model - Test Accuracy: {narrower_scores[1]:.2f}, F1 Score: {narrower_scores[2]:.2f}, AUC: {narrower_scores[3]:.2f}')\nplot_training_history(narrower_history, monitor='loss')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Models Performance Comparison Table\n\nThe summary table below compares all model architectures on the held-out test set. For each model, we report:\n- **Performance metrics:** Accuracy, F1-Score, and AUC\n- **Tuned hyperparameters:** Dropout rate, L2 regularisation strength, and learning rate (found via RandomSearch)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Build comparison table programmatically from recorded results\nmodels_performance_df = pd.DataFrame(model_results)\n\n# Format numeric columns for display\ndisplay_df = models_performance_df.copy()\ndisplay_df['Accuracy'] = display_df['Accuracy'].apply(lambda x: f'{x:.2f}')\ndisplay_df['F1 Score'] = display_df['F1 Score'].apply(lambda x: f'{x:.2f}')\ndisplay_df['AUC'] = display_df['AUC'].apply(lambda x: f'{x:.2f}')\ndisplay_df['Dropout'] = display_df['Dropout'].apply(lambda x: f'{x:.1f}' if pd.notna(x) else '-')\ndisplay_df['L2 Reg'] = display_df['L2 Reg'].apply(lambda x: f'{x:.6f}' if pd.notna(x) else '-')\ndisplay_df['Learning Rate'] = display_df['Learning Rate'].apply(lambda x: f'{x:.6f}' if pd.notna(x) else '-')\n\nprint(f\"Models compared: {len(model_results)}\")\nprint(display_df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "source": "### Detailed Model Analysis\n\nTo better understand model performance, we examine:\n1. **Confusion Matrix** - Per-class prediction accuracy\n2. **Learning Curves** - Training dynamics across multiple metrics\n3. **Sample Predictions** - Qualitative inspection of model outputs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONFUSION MATRIX - Per-class performance visualisation\n# =============================================================================\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Get predictions from the best regularised model\ny_pred_proba = opt_model.predict(X_test_tfidf, verbose=0)\ny_pred = np.argmax(y_pred_proba, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Create confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nclass_names = label_encoder.classes_\n\n# Plot confusion matrix\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Regularised Model (64 neurons)')\nplt.tight_layout()\nplt.show()\n\n# Calculate per-class metrics\nfrom sklearn.metrics import classification_report\nprint(\"\\nPer-Class Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=class_names))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# LEARNING CURVES - Multiple metrics over training\n# =============================================================================\ndef plot_learning_curves(history, title=\"Learning Curves\"):\n    \"\"\"Plot training and validation curves for multiple metrics.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    metrics = [\n        ('loss', 'Loss', 'lower is better'),\n        ('categorical_accuracy', 'Accuracy', 'higher is better'),\n        ('f1_score', 'F1-Score', 'higher is better')\n    ]\n    \n    for ax, (metric, label, note) in zip(axes, metrics):\n        if metric in history.history:\n            train_vals = history.history[metric]\n            val_vals = history.history[f'val_{metric}']\n            epochs = range(1, len(train_vals) + 1)\n            \n            ax.plot(epochs, train_vals, 'b-', label=f'Training', alpha=0.7)\n            ax.plot(epochs, val_vals, 'r-', label=f'Validation', alpha=0.7)\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel(label)\n            ax.set_title(f'{label}\\n({note})')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n    \n    plt.suptitle(title, fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n# Plot learning curves for the regularized model\nplot_learning_curves(regularized_history, \"Regularized Model (64 neurons) - Learning Curves\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SAMPLE PREDICTIONS - Qualitative inspection\n# =============================================================================\n# Get original test tweets (before TF-IDF transformation)\nX_test_original = X_test.reset_index(drop=True)\n\n# Create a DataFrame with predictions\npredictions_df = pd.DataFrame({\n    'Tweet': X_test_original,\n    'True Label': [class_names[i] for i in y_true],\n    'Predicted': [class_names[i] for i in y_pred],\n    'Confidence': [f\"{y_pred_proba[i, y_pred[i]]:.2%}\" for i in range(len(y_pred))],\n    'Correct': ['Yes' if t == p else 'No' for t, p in zip(y_true, y_pred)]\n})\n\n# Show examples from each category\nprint(\"=\" * 80)\nprint(\"SAMPLE PREDICTIONS - 2 examples per sentiment class\")\nprint(\"=\" * 80)\n\nfor sentiment in class_names:\n    print(f\"\\n{'─' * 80}\")\n    print(f\"  {sentiment.upper()} TWEETS (correctly classified)\")\n    print(f\"{'─' * 80}\")\n    \n    # Get correct predictions for this class\n    correct_samples = predictions_df[\n        (predictions_df['True Label'] == sentiment) & \n        (predictions_df['Correct'] == 'Yes')\n    ].head(2)\n    \n    for idx, row in correct_samples.iterrows():\n        tweet_text = row['Tweet'][:100] + '...' if len(row['Tweet']) > 100 else row['Tweet']\n        print(f\"\\n  Tweet: \\\"{tweet_text}\\\"\")\n        print(f\"  Predicted: {row['Predicted']} (Confidence: {row['Confidence']})\")\n\n# Show a few misclassified examples\nprint(f\"\\n{'=' * 80}\")\nprint(\"MISCLASSIFIED EXAMPLES (for error analysis)\")\nprint(\"=\" * 80)\n\nmisclassified = predictions_df[predictions_df['Correct'] == 'No'].head(3)\nfor idx, row in misclassified.iterrows():\n    tweet_text = row['Tweet'][:100] + '...' if len(row['Tweet']) > 100 else row['Tweet']\n    print(f\"\\n  Tweet: \\\"{tweet_text}\\\"\")\n    print(f\"  True: {row['True Label']} | Predicted: {row['Predicted']} ({row['Confidence']})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Observations from Detailed Analysis\n\n**Confusion Matrix Insights:**\n- The model performs best on **negative** tweets (majority class), as expected\n- **Neutral** tweets are often misclassified as negative or positive - this is common as neutral sentiment is inherently ambiguous\n- **Positive** tweets show good precision despite being the minority class, thanks to class weights\n\n**Learning Curves Insights:**\n- All metrics show convergence without significant overfitting (validation curves stable)\n- F1-Score improves steadily, indicating the model learns to handle class imbalance\n- The gap between training and validation metrics is small, confirming good generalisation\n\n**Sample Predictions Insights:**\n- Correctly classified tweets often contain clear sentiment indicators (e.g., \"thank you\", \"worst\", \"love\")\n- Misclassified tweets tend to contain mixed signals, sarcasm, or ambiguous language\n- Confidence scores are generally well-calibrated (higher for clear cases, lower for ambiguous ones)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Analysis of Results\n\nThe performance of various neural network models is summarised in the comparison table above. Key observations:\n\n### Baseline Model (SLP)\n\n- The baseline Single Layer Perceptron (no hidden layers) achieves strong performance, demonstrating that even a simple linear classifier can capture much of the signal in TF-IDF features.\n- Training converges smoothly without overfitting, as expected for a model with limited capacity.\n\n### Overfitting Model (Section 5)\n\n- Adding a hidden layer (64 neurons) increases model capacity.\n- The training history shows clear overfitting: validation loss increases after ~110 epochs while training loss continues to decrease.\n- This confirms the model has **sufficient capacity** to memorise the training data, justifying the need for regularisation.\n- **Note on learning rate:** We used a lower learning rate (0.0001) to slow down training, allowing us to clearly observe the gradual overfitting pattern in the learning curves.\n\n### Regularised Model (64 neurons + Dropout + L2)\n\n- RandomSearch found optimal regularisation hyperparameters by training all configurations for the full 150 epochs.\n- The regularised model shows improved generalisation: validation loss stabilises instead of increasing.\n- F1-Score improves over the baseline, indicating better performance on minority classes.\n- The comparison table now includes the overfitting model, clearly demonstrating the before/after impact of regularisation.\n\n### Architecture Variants\n\n| Architecture | Observation |\n|--------------|-------------|\n| **Wider (128)** | Marginal improvement over 64 neurons; extra capacity not needed |\n| **Deeper (64×2)** | Similar performance; hierarchical features don't help for TF-IDF |\n| **Narrower (32)** | Slight decrease; 32 neurons may underfit slightly |\n\n### Why RandomSearch?\n\nAs discussed in detail in Section 6 (*In-Depth Comparison: GridSearch vs RandomSearch vs Hyperband*), RandomSearch was the optimal choice for this coursework because early stopping is not permitted. This eliminates the epoch mismatch problem that would occur with Hyperband, where hyperparameters tuned for short runs may be suboptimal for extended training.\n\n### Key Insight\n\nThe baseline model already has sufficient statistical power to learn the patterns in TF-IDF features. Adding complexity (wider/deeper) provides diminishing returns. **Regularisation** (Dropout + L2) is more valuable than architectural changes for this task.\n\n> *\"When your baseline can already overfit, the answer is regularisation, not more capacity.\"* — Chollet (2021)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Conclusions\n\nThis study explored dense neural network architectures for sentiment analysis on US airline tweets, following the Universal ML Workflow from Chollet (2021).\n\n### Key Findings\n\n1. **Simple models work well:** A Single Layer Perceptron achieved strong performance, demonstrating that TF-IDF features already capture meaningful sentiment signals.\n\n2. **Overfitting is easy to achieve:** A single hidden layer (64 neurons) was sufficient to overfit the training data, confirming adequate model capacity.\n\n3. **Regularisation > Architecture:** Dropout and L2 regularisation improved generalisation more than architectural changes (wider/deeper/narrower). This aligns with the principle: *\"Regularise, don't expand.\"*\n\n4. **RandomSearch is ideal when early stopping is unavailable:** We chose RandomSearch over Hyperband because:\n   - **No epoch mismatch:** All configurations trained for 150 epochs during tuning\n   - **Reliable hyperparameters:** Selected regularisation is optimal for actual training duration\n   - **Simpler workflow:** No need to adjust regularisation after tuning\n\n5. **Class weights matter:** For this imbalanced dataset (3.88:1 ratio), class weights were essential for learning minority classes effectively.\n\n### Hyperparameter Tuning Recommendations\n\n| Situation | Recommended Approach |\n|-----------|---------------------|\n| Early stopping available | Hyperband (most efficient) |\n| **Early stopping NOT available** | **RandomSearch** (our choice) |\n| Small search space | Grid Search (guarantees optimum) |\n| Very large search space | Hyperband with post-tuning adjustment |\n\n### Limitations\n\n- Constrained to Dense layers only (no CNNs/RNNs/Transformers)\n- TF-IDF loses word order information\n- No text preprocessing applied (lowercasing, URL removal, etc.)\n- No early stopping used (per assignment constraints)\n- RandomSearch is slower than Hyperband (acceptable trade-off for reliability)\n\n### Future Work\n\n- **Text preprocessing** — Explore impact of lowercasing, URL removal, emoji handling\n- **TF-IDF parameters** — Experiment with different max_features and n-gram ranges\n- **Early stopping** and **learning rate scheduling** could improve training efficiency\n- **LIME** or **SHAP** for model interpretability\n- **Pre-trained embeddings** (Word2Vec, BERT) could capture semantic relationships better than TF-IDF\n\n---\n\n## Code Attribution\n\nThe following code patterns and utilities were adapted from external sources:\n\n| Component | Source | Adaptation |\n|-----------|--------|------------|\n| Universal ML Workflow structure | Chollet (2021), Chapter 4.5 | Applied to sentiment analysis |\n| TF-IDF vectorisation | scikit-learn `TfidfVectorizer` | Standard usage with bigrams |\n| Class weight computation | scikit-learn `compute_class_weight` | Standard usage |\n| Keras Sequential API | TensorFlow/Keras documentation | Model architecture |\n| RandomSearch tuning | keras-tuner library | Applied to regularisation search |\n| Training history plotting | DLWP code examples | Adapted for this notebook |\n\n**Original contributions:**\n- Problem framing and dataset selection for airline sentiment\n- Systematic comparison of architecture variants (wider/deeper/narrower)\n- Integration of RandomSearch with class weights for imbalanced data\n- In-depth analysis of why RandomSearch is preferred over Hyperband when early stopping is unavailable\n- Analysis and interpretation of results\n\n---\n\n## References\n\n- Bergstra, J. and Bengio, Y. (2012) 'Random search for hyper-parameter optimization', *Journal of Machine Learning Research*, 13(1), pp. 281–305.\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145.\n\n- Krogh, A. and Hertz, J.A. (1992) 'A simple weight decay can improve generalization', *Advances in Neural Information Processing Systems*, 4, pp. 950–957.\n\n- Li, L. et al. (2018) 'Hyperband: A novel bandit-based approach to hyperparameter optimization', *Journal of Machine Learning Research*, 18(1), pp. 6765–6816.\n\n- Srivastava, N. et al. (2014) 'Dropout: A simple way to prevent neural networks from overfitting', *Journal of Machine Learning Research*, 15(1), pp. 1929–1958.\n\n- Twitter US Airline Sentiment Dataset. Available at: https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment (Accessed: January 2025)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}