{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Imagenette/Imagenette%20-%20TFDS%20Color%20Image%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Imagenette - TFDS Colour Image Example\n\nThis notebook demonstrates the **Universal ML Workflow** for multi-class image classification using TensorFlow Datasets (TFDS).\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Load image datasets from **TensorFlow Datasets (TFDS)**\n- Preprocess colour images: Resize → Grayscale → Flatten → Normalise\n- Handle 10-class image classification with Top-K accuracy\n- Apply Hyperband for hyperparameter tuning on image data\n\n---\n\n## Technique Scope\n\n| Aspect | What We Use | What We Don't Use (Yet) |\n|--------|-------------|------------------------|\n| **Architecture** | Dense layers only | CNNs, pooling, feature extractors |\n| **Regularisation** | L2 + Dropout | Early stopping, data augmentation |\n| **Optimiser** | Adam | SGD with momentum, learning rate schedules |\n| **Tuning** | Hyperband | Bayesian optimisation, neural architecture search |\n\n> **Note**: Dense networks applied to flattened images serve as a baseline. CNNs (Chapter 8) are the standard approach for image classification and would significantly improve performance on this challenging dataset.\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [TensorFlow Datasets - imagenette/160px](https://www.tensorflow.org/datasets/catalog/imagenette) |\n| **Problem Type** | Multi-Class Classification (10 classes) |\n| **Classes** | Tench, English springer, Cassette player, Chain saw, Church, French horn, Garbage truck, Gas pump, Golf ball, Parachute |\n| **Data Balance** | Nearly Balanced |\n| **Total Images** | ~13,000 images |\n| **Preprocessing** | Resize to 32×32 → Grayscale → Flatten (1024 features) |\n\n### Imagenette vs Fashion MNIST\n\n| Aspect | Imagenette | Fashion MNIST |\n|--------|------------|---------------|\n| **Original Images** | 160×160 colour | 28×28 grayscale |\n| **Number of Classes** | 10 | 10 |\n| **Image Content** | Real-world photos | Synthetic clothing |\n| **Difficulty** | Harder (high variation) | Moderate |\n| **Dataset Size** | ~13,000 | 70,000 |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Reuse Philosophy\n",
    "\n",
    "This notebook follows a **\"Same Code, Different Data\"** philosophy. The core ML pipeline remains consistent across different classification tasks:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    UNIVERSAL ML PIPELINE                        │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  Data Loading → Preprocessing → Train/Val/Test Split → Model   │\n",
    "│  → Baseline → Overfitting → Regularisation → Evaluation        │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**What changes:** Data source, preprocessing, number of output classes  \n",
    "**What stays the same:** Model architecture pattern, training loop, evaluation code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem:** Classify images of 10 different objects from the Imagenette dataset - a smaller subset of ImageNet designed for faster experimentation.\n\n**Why this matters:**\n- **Object recognition foundation:** Real-world object classification is the basis for autonomous vehicles, robotics, and visual search\n- **Transfer learning testbed:** Imagenette is used to quickly test architectures before scaling to full ImageNet\n- **Research accessibility:** Enables ML practitioners without GPU clusters to experiment with image classification\n\n**Why Imagenette?** ImageNet has 1000 classes and millions of images, making it slow to iterate on. Imagenette provides a 10-class subset that's large enough to be challenging but small enough for rapid prototyping.\n\n**Why TFDS?** TensorFlow Datasets provides easy access to common ML datasets with consistent APIs, automatic caching, and preprocessing utilities."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Data-Driven Metric Selection\n\n| Criterion | This Dataset | Decision |\n|-----------|--------------|----------|\n| **Class Balance** | ~Equal across 10 classes | Balanced |\n| **Number of Classes** | 10 | Multi-class |\n| **Primary Metric** | Accuracy | Standard for balanced multi-class |\n| **Secondary Metrics** | Top-K Accuracy | Additional insight for multi-class |\n\n**Why these thresholds?**\n- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n- **Balanced data (< 3:1):** Accuracy is meaningful and interpretable\n\n### References\n\n- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modelling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n**Decision:** Since the dataset is balanced, **Accuracy** is the primary metric. **Top-K Accuracy** shows if the correct class was among the model's top K predictions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Data-Driven Protocol Selection\n\n| Criterion | This Dataset | Decision |\n|-----------|--------------|----------|\n| **Sample Size** | ~13,000 images | Large |\n| **Threshold** | > 10,000 | Use Hold-Out |\n| **Protocol** | Train/Validation/Test | 80%/10%/10% split |\n\n**Why 10,000 as a practical threshold?**\n- Below 10,000 samples, hold-out validation has higher variance (Kohavi, 1995)\n- Above 10,000, statistical estimates from hold-out are reliable\n- Deep learning models are expensive to train; K-fold multiplies cost by K (Chollet, 2021)\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n\n**Decision:** With ~13,000 samples, **Hold-Out validation** is appropriate."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries and Load TFDS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Keras Tuner for hyperparameter search\n",
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# RANDOM SEED - Set once, use everywhere\n",
    "# ============================================================\n",
    "SEED = 204\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATASET CONFIGURATION\n# ============================================================\nDATASET = 'imagenette/160px'\nRESIZE = (32, 32)  # Resize to 32x32 - balance between preserving detail and dimensionality\nGRAY_SCALE = True  # Convert to grayscale for simplicity\n\n# ============================================================\n# CLASS NAMES\n# ============================================================\nCLASS_NAMES = [\n    'Tench', 'English springer', 'Cassette player', 'Chain saw', 'Church',\n    'French horn', 'Garbage truck', 'Gas pump', 'Golf ball', 'Parachute'\n]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from TensorFlow Datasets\n",
    "# Combine train and validation splits for custom splitting\n",
    "ds_train = tfds.load(DATASET, split='train', shuffle_files=True)\n",
    "ds_val = tfds.load(DATASET, split='validation', shuffle_files=True)\n",
    "\n",
    "# Process images from both splits\n",
    "images, labels = [], []\n",
    "\n",
    "for ds in [ds_train, ds_val]:\n",
    "    for entry in ds:\n",
    "        image, label = entry['image'].numpy(), entry['label'].numpy()\n",
    "        \n",
    "        # Resize to target size\n",
    "        image = resize(image, (*RESIZE, 3), anti_aliasing=True)\n",
    "        \n",
    "        # Convert to grayscale if specified\n",
    "        if GRAY_SCALE:\n",
    "            image = rgb2gray(image)\n",
    "        \n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(images)\n",
    "y_raw = np.array(labels)\n",
    "\n",
    "# Flatten images: (N, 16, 16) -> (N, 256)\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "# One-hot encode labels\n",
    "y = to_categorical(y_raw)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verify Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "unique, counts = np.unique(y_raw, return_counts=True)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for class_idx, count in zip(unique, counts):\n",
    "    print(f\"  {CLASS_NAMES[class_idx]}: {count} ({100*count/len(y_raw):.1f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = max(counts) / min(counts)\n",
    "print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Decision: {'Use Accuracy (balanced)' if imbalance_ratio < 3 else 'Use F1-Score (imbalanced)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN/TEST SPLIT (90%/10%)\n",
    "# ============================================================\n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_raw,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Also keep raw labels for test set\n",
    "_, _, y_train_full_raw, y_test_raw = train_test_split(\n",
    "    X, y_raw,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_raw,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training + Validation: {X_train_full.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Normalise Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALISE PIXEL VALUES [0, 1]\n",
    "# ============================================================\n",
    "# Note: skimage resize already normalises to [0, 1], but we ensure it\n",
    "X_train_full = X_train_full.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Verify normalisation\n",
    "print(f\"Feature range: [{X_train_full.min():.3f}, {X_train_full.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAIN/VALIDATION SPLIT\n# ============================================================\n# Use 10% of training pool for validation (consistent with other notebooks)\nVALIDATION_SIZE = 0.10\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full,\n    test_size=VALIDATION_SIZE,\n    stratify=y_train_full.argmax(axis=1),\n    random_state=SEED,\n    shuffle=True\n)\n\n# Keep raw labels for train set (for class weights)\ny_train_raw = y_train.argmax(axis=1)\n\nprint(f\"Training: {X_train.shape[0]} samples\")\nprint(f\"Validation: {X_val.shape[0]} samples\")\nprint(f\"Test: {X_test.shape[0]} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Visualise Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display sample images from each class\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle('Sample Images (32×32 Grayscale)', fontsize=14)\n\nfor class_idx in range(10):\n    # Get first sample of this class\n    sample_idx = np.where(y_train_raw == class_idx)[0][0]\n    \n    ax = axes[class_idx // 5, class_idx % 5]\n    # Reshape flattened image back to 2D\n    img = X_train[sample_idx].reshape(RESIZE)\n    ax.imshow(img, cmap='gray')\n    ax.axis('off')\n    ax.set_title(CLASS_NAMES[class_idx], fontsize=9)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "**Baseline for 10-class balanced problem:** 10% accuracy (random guessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MODEL CONFIGURATION\n# ============================================================\nINPUT_DIMENSION = X_train.shape[1]  # 1024 features (32x32)\nOUTPUT_CLASSES = y_train.shape[1]   # 10 classes\n\nOPTIMIZER = 'adam'\nLOSS_FUNC = 'categorical_crossentropy'\nMETRICS = ['accuracy']\n\n# Training configuration\n# Batch Size Selection:\n# - Large datasets (>10,000 samples): Use 512 for efficient GPU utilisation\n# - Small datasets (<10,000 samples): Use 32-64 for better gradient estimates\n# Imagenette has ~13,000 samples → Use batch size 512\nBATCH_SIZE = 512\nEPOCHS_BASELINE = 100\nEPOCHS_REGULARIZED = 150\n\nprint(f\"Input Dimension: {INPUT_DIMENSION}\")\nprint(f\"Output Classes: {OUTPUT_CLASSES}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTABLISH BASELINE\n",
    "# ============================================================\n",
    "# For balanced 10-class classification, random guessing = 10%\n",
    "baseline_accuracy = 1.0 / OUTPUT_CLASSES\n",
    "\n",
    "print(f\"Baseline Accuracy (random guessing): {baseline_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASS WEIGHTS (for balanced training)\n",
    "# ============================================================\n",
    "weights = compute_class_weight('balanced', classes=np.unique(y_train_raw), y=y_train_raw)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "print(\"Class Weights (sample):\")\n",
    "for class_idx in [0, 1, 2]:\n",
    "    print(f\"  {CLASS_NAMES[class_idx]}: {CLASS_WEIGHTS[class_idx]:.4f}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SINGLE LAYER PERCEPTRON (SLP) - Simplest possible model\n",
    "# ============================================================\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(INPUT_DIMENSION,)))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SLP\n",
    "slp_history = slp_model.fit(\n",
    "    X_train, y_train,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS_BASELINE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "slp_val_acc = slp_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(f\"SLP Validation Accuracy: {slp_val_acc:.4f} (baseline: {baseline_accuracy:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# ============================================================\n",
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"Plot training and validation loss/accuracy curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], 'b-', label='Training Loss')\n",
    "    axes[0].plot(history.history['val_loss'], 'r-', label='Validation Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "    axes[1].plot(history.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(slp_history, 'Single Layer Perceptron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding a hidden layer to learn more complex features for distinguishing between 10 diverse object classes.\n\n**No regularisation applied:** We intentionally train this model **without any regularisation** (no dropout, no L2, no early stopping) to observe overfitting behaviour.\n\n---\n\n### Architecture Design Decisions\n\n**Why 64 neurons in the hidden layer?**\n\nThis is a practical starting point that balances capacity and efficiency:\n- **Too few (e.g., 16):** May not have enough capacity to distinguish 10 diverse object classes\n- **Too many (e.g., 512):** Increases overfitting risk and training time without proportional benefit\n- **64 neurons:** A common choice that provides sufficient capacity for most classification tasks\n\n**Why only 1 hidden layer instead of 2-3?**\n\nPer the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n\n1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation\n\n*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTI-LAYER PERCEPTRON (MLP) - Standard architecture\n",
    "# ============================================================\n",
    "HIDDEN_NEURONS = 64\n",
    "\n",
    "mlp_model = Sequential(name='Multi_Layer_Perceptron')\n",
    "mlp_model.add(Dense(HIDDEN_NEURONS, activation='relu', input_shape=(INPUT_DIMENSION,)))\n",
    "mlp_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "mlp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP\n",
    "mlp_history = mlp_model.fit(\n",
    "    X_train, y_train,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS_BASELINE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "mlp_val_acc = mlp_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(f\"MLP Validation Accuracy: {mlp_val_acc:.4f} (baseline: {baseline_accuracy:.2f})\")\n",
    "print(f\"Improvement over SLP: {(mlp_val_acc - slp_val_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(mlp_history, 'Multi-Layer Perceptron (1 Hidden Layer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning with L2 regularisation and Dropout.\n",
    "\n",
    "### Why Hyperband?\n",
    "\n",
    "**Hyperband** is more efficient than grid search because it:\n",
    "1. Starts training many configurations for a few epochs\n",
    "2. Eliminates poor performers early\n",
    "3. Allocates more resources to promising configurations\n",
    "\n",
    "### Regularisation Strategy\n",
    "\n",
    "| Technique | Purpose | How It Works |\n",
    "|-----------|---------|-------------|\n",
    "| **L2 Regularisation** | Prevent large weights | Adds penalty term to loss |\n",
    "| **Dropout** | Prevent co-adaptation | Randomly zeros neurons during training |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPERBAND MODEL BUILDER\n",
    "# ============================================================\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build Imagenette model with FROZEN architecture (1 hidden layer, 64 neurons).\n",
    "    Tunes: L2 regularisation, Dropout rate, Learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    l2_reg = hp.Float('l2_reg', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # Hidden layer with L2 regularisation\n",
    "    model.add(layers.Dense(\n",
    "        HIDDEN_NEURONS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(l2_reg)\n",
    "    ))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE AND RUN HYPERBAND TUNER\n",
    "# ============================================================\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_hyperband,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='imagenette_hyperband',\n",
    "    project_name='imagenette_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Run search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# GET BEST HYPERPARAMETERS AND BEST MODEL DIRECTLY\n# ============================================================\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"Best Hyperparameters found by Hyperband:\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout_rate'):.2f}\")\nprint(f\"  Learning Rate: {best_hp.get('learning_rate'):.6f}\")\n\n# Get the best model directly - already trained at optimal epochs for these hyperparameters\nbest_model = tuner.get_best_models(num_models=1)[0]\nbest_model.summary()"
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Using the Best Model Directly\n\nRather than rebuilding and retraining from scratch, we retrieve the best model directly from the tuner using `tuner.get_best_models()`. This approach avoids the **epoch mismatch problem**:\n\n---\n\n#### The Epoch Mismatch Problem\n\nHyperband uses **successive halving** - most configurations train for few epochs, only top performers get more:\n\n```\nHyperband with max_epochs=50, factor=3:\nRound 1: 81 configs × ~2 epochs  → Keep top 27\nRound 2: 27 configs × ~6 epochs  → Keep top 9\nRound 3:  9 configs × ~17 epochs → Keep top 3\nRound 4:  3 configs × ~50 epochs → Select best\n```\n\nThe best hyperparameters were found optimal at a **specific epoch count** (e.g., 50 epochs). If we rebuild and retrain for a different number of epochs (e.g., 150), the hyperparameters may no longer be optimal - **this is the epoch mismatch problem**.\n\n---\n\n#### Clean Solution: Use Best Model Directly\n\nInstead of rebuilding, we use `tuner.get_best_models(num_models=1)[0]` to retrieve the model that **already achieved the best validation performance** during tuning. This model:\n\n- Has weights trained at the optimal epoch count for its hyperparameters\n- Achieved the best validation accuracy during the Hyperband search\n- Avoids any mismatch between tuning epochs and final epochs\n\n| Approach | Epochs Match? | Issue |\n|----------|---------------|-------|\n| ~~Rebuild + retrain for 150 epochs~~ | ✗ No | Hyperparameters may be suboptimal at 150 epochs |\n| **Use best model directly** | ✓ Yes | Model already trained at optimal epochs |\n\n> *\"Use the model that actually achieved the best performance, not a rebuilt version that might perform differently.\"*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The best model is already trained - evaluate on validation set\nbest_val_acc = best_model.evaluate(X_val, y_val, verbose=0)[1]\nprint(f\"Best Model Validation Accuracy: {best_val_acc:.4f}\")\nprint(f\"Improvement over MLP: {(best_val_acc - mlp_val_acc)*100:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Note: Training history plot is not available when using get_best_models()\n# The best model was retrieved directly from the tuner, which doesn't preserve\n# the training history. To visualise training curves, you would need to either:\n# 1. Use TensorBoard callbacks during tuning, or\n# 2. Retrain the model (but this risks the epoch mismatch problem)\n#\n# For this notebook, we skip the training history plot since we're using\n# the best model directly to ensure optimal performance.\nprint(\"Training history not available when using get_best_models() directly.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Final Evaluation\n\nEvaluate the best model on the held-out test set."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Evaluation\n",
    "\n",
    "Evaluate the best model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TOP-K ACCURACY FUNCTION\n",
    "# ============================================================\n",
    "def top_k_accuracy(y_true, y_pred_proba, k):\n",
    "    \"\"\"\n",
    "    Calculate Top-K accuracy: was the true class in the model's top K predictions?\n",
    "    \n",
    "    Args:\n",
    "        y_true: True class labels (integer indices)\n",
    "        y_pred_proba: Predicted probabilities (N x num_classes)\n",
    "        k: Number of top predictions to consider\n",
    "    \n",
    "    Returns:\n",
    "        Top-K accuracy score\n",
    "    \"\"\"\n",
    "    top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "    correct = sum(y_true[i] in top_k_preds[i] for i in range(len(y_true)))\n",
    "    return correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TEST SET EVALUATION\n# ============================================================\n# Get predictions\ny_pred_proba = best_model.predict(X_test, verbose=0)\ny_pred = y_pred_proba.argmax(axis=1)\n\n# Calculate metrics\ntest_accuracy = accuracy_score(y_test_raw, y_pred)\ntop_2_accuracy = top_k_accuracy(y_test_raw, y_pred_proba, k=2)\ntop_3_accuracy = top_k_accuracy(y_test_raw, y_pred_proba, k=3)\n\nprint(\"=\"*50)\nprint(\"FINAL TEST SET RESULTS\")\nprint(\"=\"*50)\nprint(f\"Top-1 Accuracy: {test_accuracy:.4f} (baseline: {baseline_accuracy:.2f})\")\nprint(f\"Top-2 Accuracy: {top_2_accuracy:.4f}\")\nprint(f\"Top-3 Accuracy: {top_3_accuracy:.4f}\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRIX\n",
    "# ============================================================\n",
    "cm = confusion_matrix(y_test_raw, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS ACCURACY\n",
    "# ============================================================\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"-\"*40)\n",
    "for class_idx in range(OUTPUT_CLASSES):\n",
    "    class_mask = y_test_raw == class_idx\n",
    "    class_correct = (y_pred[class_mask] == class_idx).sum()\n",
    "    class_total = class_mask.sum()\n",
    "    class_acc = class_correct / class_total\n",
    "    print(f\"{CLASS_NAMES[class_idx]:<20}: {class_acc:.2f} ({class_correct}/{class_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Results Summary\n\nThe following dynamically-generated table compares all models trained in this notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RESULTS SUMMARY\n# =============================================================================\n\n# Create results DataFrame\nresults = pd.DataFrame({\n    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'MLP (No Regularisation)', 'MLP (Dropout + L2)', 'MLP (Dropout + L2) - Test'],\n    'Accuracy': [baseline_accuracy, slp_val_acc, mlp_val_acc, best_val_acc, test_accuracy],\n    'Top-3 Acc': [0.3, 0.0, 0.0, 0.0, top_3_accuracy],  # Top-3 computed only for final test\n    'Dataset': ['N/A', 'Validation', 'Validation', 'Validation', 'Test']\n})\n\nprint(\"=\" * 70)\nprint(\"MODEL COMPARISON - RESULTS SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Primary Metric: ACCURACY ({OUTPUT_CLASSES}-class balanced dataset)\")\nprint(\"=\" * 70)\nprint(results.to_string(index=False, float_format='{:.4f}'.format))\nprint(\"=\" * 70)\nprint(f\"\\nKey Observations:\")\nprint(f\"  - All models significantly outperform random baseline ({baseline_accuracy:.2%})\")\nprint(f\"  - Regularisation improves accuracy: {mlp_val_acc:.4f} → {best_val_acc:.4f}\")\nprint(f\"  - Final test accuracy: {test_accuracy:.4f}, Top-3: {top_3_accuracy:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | ~13,000 samples | **Hold-Out** | Kohavi (1995) |\n| **Accuracy vs F1-Score** | > 3:1 imbalance | ~1:1 ratio | **Accuracy** | He and Garcia (2009) |\n| **Batch Size** | > 10,000 samples | ~13,000 samples | **512** | GPU efficiency |\n\n### Lessons Learned\n\n1. **TFDS Simplifies Data Loading:** `tfds.load()` handles download, caching, and parsing automatically, making it easy to access standard benchmarks.\n\n2. **Top-K Accuracy for Multi-Class:** When classes are visually similar (e.g., different objects), Top-K accuracy shows if the correct answer was among the model's top K guesses.\n\n3. **Image Preprocessing Trade-offs:** Resize → Grayscale → Flatten sacrifices spatial information for simplicity. CNNs (Chapter 8) would preserve this structure.\n\n4. **Real-World Images are Harder:** Imagenette contains real photographs with high variation in lighting, backgrounds, and poses - more challenging than synthetic datasets like Fashion MNIST.\n\n5. **Dense Network Limitations:** DNNs treat each pixel as an independent feature, ignoring spatial relationships. This limits performance on image tasks.\n\n6. **Regularisation Prevents Overfitting:** L2 regularisation + Dropout control overfitting without requiring early stopping (Ch. 7).\n\n7. **Batch Size Selection:** With ~13,000 samples (above 10,000 threshold), batch size 512 provides efficient GPU utilisation while maintaining good gradient estimates.\n\n### Next Steps for Better Performance\n\n- **Use CNNs** (Chapter 8) - preserves spatial structure, learns hierarchical features\n- **Higher resolution** - 32×32 still loses significant detail from 160×160 originals\n- **Transfer learning** - use pre-trained models (ResNet, EfficientNet)\n- **Data augmentation** - artificially increase training data variety\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Howard, J. (2019) 'Imagenette: A smaller subset of 10 easily classified classes from Imagenet', *fast.ai*.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Modular Helper Functions\n\nFor cleaner code organisation, you can wrap the model building and training patterns into reusable functions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# MODULAR HELPER FUNCTIONS\n# =============================================================================\n\ndef build_image_classifier(input_dim, num_classes, hidden_units=None, dropout=0.0, l2_reg=0.0,\n                           optimizer='adam', learning_rate=None, name=None):\n    \"\"\"\n    Build a multi-class image classification neural network.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features (flattened image size)\n    num_classes : int\n        Number of output classes\n    hidden_units : list of int, optional\n        Neurons per hidden layer, e.g., [64] or [128, 64]\n    dropout : float\n        Dropout rate (0.0 to 0.5)\n    l2_reg : float\n        L2 regularisation strength\n    learning_rate : float, optional\n        Custom learning rate\n    name : str, optional\n        Model name\n        \n    Returns:\n    --------\n    keras.Sequential : Compiled model ready for training\n    \"\"\"\n    model = Sequential(name=name)\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    hidden_units = hidden_units or []\n    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n    \n    for units in hidden_units:\n        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n        if dropout > 0:\n            model.add(Dropout(dropout))\n    \n    model.add(Dense(num_classes, activation='softmax'))\n    \n    if learning_rate is not None:\n        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n    else:\n        opt = optimizer\n    \n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef train_with_class_weights(model, X_train, y_train, X_val, y_val,\n                              batch_size=512, epochs=100, verbose=0):\n    \"\"\"Train model with automatic class weight computation.\"\"\"\n    y_train_raw = y_train.argmax(axis=1)\n    weights = compute_class_weight('balanced', classes=np.unique(y_train_raw), y=y_train_raw)\n    class_weights = dict(enumerate(weights))\n    \n    return model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=batch_size, \n        epochs=epochs,\n        class_weight=class_weights,\n        verbose=verbose\n    )\n\n\ndef evaluate_multiclass_image(model, X, y_true_onehot, y_true_raw=None):\n    \"\"\"\n    Evaluate multi-class image classification model with Top-K accuracy.\n    \n    Returns:\n    --------\n    dict : Dictionary containing accuracy and top-k accuracy\n    \"\"\"\n    y_pred_proba = model.predict(X, verbose=0)\n    y_pred = y_pred_proba.argmax(axis=1)\n    \n    if y_true_raw is None:\n        y_true_raw = y_true_onehot.argmax(axis=1)\n    \n    # Top-K accuracy\n    def top_k_acc(y_true, y_pred_proba, k):\n        top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n        return sum(y_true[i] in top_k_preds[i] for i in range(len(y_true))) / len(y_true)\n    \n    metrics = {\n        'accuracy': accuracy_score(y_true_raw, y_pred),\n        'top_2_accuracy': top_k_acc(y_true_raw, y_pred_proba, 2),\n        'top_3_accuracy': top_k_acc(y_true_raw, y_pred_proba, 3),\n    }\n    \n    return metrics\n\n\n# =============================================================================\n# USAGE EXAMPLES\n# =============================================================================\n# \n# # Build models\n# slp = build_image_classifier(INPUT_DIMENSION, OUTPUT_CLASSES, name='SLP')\n# mlp = build_image_classifier(INPUT_DIMENSION, OUTPUT_CLASSES, \n#                              hidden_units=[64], name='MLP')\n# mlp_reg = build_image_classifier(INPUT_DIMENSION, OUTPUT_CLASSES,\n#                                  hidden_units=[64], dropout=0.3, l2_reg=0.001,\n#                                  learning_rate=0.001, name='MLP_Regularized')\n# \n# # Train with class weights\n# history = train_with_class_weights(mlp, X_train, y_train, X_val, y_val)\n# \n# # Evaluate with Top-K accuracy\n# metrics = evaluate_multiclass_image(mlp, X_val, y_val)\n# print(f\"Accuracy: {metrics['accuracy']:.4f}, Top-3: {metrics['top_3_accuracy']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}