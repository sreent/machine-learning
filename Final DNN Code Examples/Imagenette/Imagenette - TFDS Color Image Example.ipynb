{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Imagenette/Imagenette%20-%20TFDS%20Color%20Image%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Imagenette - TFDS Color Image Example\n\nThis notebook demonstrates the **Universal ML Workflow** for multi-class image classification using TensorFlow Datasets (TFDS).\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Load image datasets from **TensorFlow Datasets (TFDS)**\n- Preprocess color images: Resize -> Grayscale -> Flatten -> Normalize\n- Handle 10-class image classification\n- Use **Top-N Accuracy** as an additional evaluation metric\n- Apply Hyperband for hyperparameter tuning on image data\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [TensorFlow Datasets - imagenette/160px](https://www.tensorflow.org/datasets/catalog/imagenette) |\n| **Problem Type** | Multi-Class Classification (10 classes) |\n| **Classes** | Tench, English springer, Cassette player, Chain saw, Church, French horn, Garbage truck, Gas pump, Golf ball, Parachute |\n| **Data Balance** | Nearly Balanced |\n| **Original Size** | 160x160 color images |\n| **Preprocessing** | Resize to 64x64 -> Grayscale -> Flatten (4096 features) |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem:** Classify images of 10 different objects from the Imagenette dataset - a smaller subset of ImageNet designed for faster experimentation.\n\n**Why Imagenette?** ImageNet has 1000 classes and millions of images, making it slow to iterate on. Imagenette provides a 10-class subset that's large enough to be challenging but small enough for rapid prototyping.\n\n**Why TFDS?** TensorFlow Datasets provides easy access to common ML datasets with consistent APIs, automatic caching, and preprocessing utilities.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n- **Top-1 Accuracy:** Did the model's top prediction match the true class?\n- **Top-N Accuracy:** Was the true class in the model's top N predictions? (Useful for 10-class problems)\n- **Precision, Recall, AUC:** Per-class and overall performance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\nStandard hold-out test set + validation set + K-fold cross-validation for hyperparameter tuning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries and Load TFDS Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nimport tensorflow_datasets as tfds\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import RMSprop\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport itertools\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'imagenette/160px'\n",
    "RESIZE = (64, 64, 3)\n",
    "GRAY_SCALE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(DATASET, split='all', shuffle_files=True)\n",
    "\n",
    "images, labels = [], []\n",
    "for entry in ds.take(len(ds)) :\n",
    "    image, label = entry['image'], entry['label']\n",
    "    \n",
    "    image, label = image.numpy(), label.numpy()\n",
    "    \n",
    "    image = resize(image, RESIZE, anti_aliasing=True)\n",
    "            \n",
    "    if GRAY_SCALE :\n",
    "        image = rgb2gray(image)\n",
    "        \n",
    "    images.append( image )\n",
    "    labels.append( label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of images to numpy array\n",
    "X = np.array(images)\n",
    "\n",
    "# flatten 2D image array to 1D array\n",
    "X = X.reshape( (X.shape[0], -1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# label -> one-hot encoded vector\n",
    "y = np_utils.to_categorical(label_encoder.transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=labels, \n",
    "                                                    random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise pixels from [0, 255] to [0, 1]\n",
    "X_train, X_test = X_train/255, X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = X_test.shape[0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                 test_size=VALIDATION_SIZE, stratify=y_train.argmax(axis=1),\n",
    "                                                 shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\nBaseline for 10-class balanced problem: 10% accuracy (random guessing)."
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1350., 1350., 1350., 1244., 1350., 1350., 1350., 1350., 1350.,\n",
       "       1350.], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples per each class\n",
    "counts = np.sum(y, axis=0)\n",
    "\n",
    "# the dataset is balanced. one class is slightly less than the others, but minimal.\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10079139913394057"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = max(counts) / sum(counts)\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_CLASSES = y_train.shape[1]\n",
    "\n",
    "OPTIMIZER = 'RMSprop'\n",
    "LOSS_FUNC = 'categorical_crossentropy'\n",
    "METRICS = ['categorical_accuracy', \n",
    "           tf.keras.metrics.Precision(name='precision'), \n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc', multi_label=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learning_rate = 0.1\n\n# Single Layer Perceptron: Input -> Softmax Output (no hidden layers)\nslp_model = Sequential([\n    Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(INPUT_DIMENSION,))\n], name='Single_Layer_Perceptron')\n\nslp_model.compile(optimizer=RMSprop(learning_rate=learning_rate),\n                  loss=LOSS_FUNC, metrics=METRICS)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 128\nEPOCHS = 300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "slp_history = slp_model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n                            validation_data=(X_val, y_val), verbose=0)\nslp_val_score = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(slp_val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(slp_val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(slp_val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(slp_val_score[3]))"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, monitors=['loss', 'AUC']) :\n",
    "\n",
    "  # using the variable axs for multiple Axes\n",
    "  fig, axs = plt.subplots(1, 2, sharex='all', figsize=(15,5))\n",
    " \n",
    "  for ax, monitor in zip(axs.flat, monitors) :\n",
    "    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n",
    "\n",
    "    if monitor == 'loss' :\n",
    "      monitor = monitor.capitalize()\n",
    "\n",
    "    epochs = range(1, len(loss)+1)\n",
    "\n",
    "    ax.plot(epochs, loss, 'b.', label=monitor)\n",
    "    ax.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n",
    "    ax.set_xlim([0, len(loss)])\n",
    "    ax.title.set_text('Training and Validation ' + monitor + 's')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(monitor)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "  _ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "plot_training_history(slp_history, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding capacity to learn complex image patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learning_rate = 0.0025\n\n# Multi-Layer Perceptron: Input -> Hidden(32, relu) -> Softmax Output\nmlp_model = Sequential([\n    Dense(32, activation='relu', input_shape=(INPUT_DIMENSION,)),\n    Dense(OUTPUT_CLASSES, activation='softmax')\n], name='Multi_Layer_Perceptron')\n\nmlp_model.compile(optimizer=RMSprop(learning_rate=learning_rate),\n                  loss=LOSS_FUNC, metrics=METRICS)\n\nmlp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "mlp_history = mlp_model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n                            validation_data=(X_val, y_val), verbose=0)\nmlp_val_score = mlp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_training_history(mlp_history, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(mlp_val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(mlp_val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(mlp_val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(mlp_val_score[3]))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nUsing **Hyperband** for efficient hyperparameter tuning with a frozen architecture.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperband Model Builder for Multi-Class Classification\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Imagenette model with FROZEN architecture (2 layers: 64 -> 32 neurons).\n    Only tunes regularization (Dropout) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # Fixed architecture: 2 hidden layers with 64 and 32 neurons\n    # Layer 1: 64 neurons\n    model.add(layers.Dense(64, activation='relu'))\n    drop_0 = hp.Float('drop_0', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_0))\n\n    # Layer 2: 32 neurons\n    model.add(layers.Dense(32, activation='relu'))\n    drop_1 = hp.Float('drop_1', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_1))\n\n    # Output layer for multi-class classification\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_categorical_accuracy',\n    max_epochs=20,\n    factor=3,\n    directory='imagenette_hyperband',\n    project_name='imagenette_tuning'\n)\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=batch_size\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters and build best model\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  Dropout Layer 1: {best_hp.get('drop_0')}\")\nprint(f\"  Dropout Layer 2: {best_hp.get('drop_1')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr')}\")\n\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the best model\nopt_history = opt_model.fit(X_train, y_train, batch_size=batch_size, epochs=50,\n                            validation_data=(X_val, y_val), verbose=0)\nopt_val_score = opt_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy_score(y, preds, n) :\n",
    "    scores = []\n",
    "    for j in range(preds.shape[0]) :\n",
    "        score = 1 if y[j,:].argmax() in preds[j,:].argsort()[-n:] else 0\n",
    "        \n",
    "        scores.append( score )\n",
    "            \n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = opt_model.predict(X_test, verbose=0)\n\nTOP_N = 3\nwithin = top_n_accuracy_score(y_test, preds, TOP_N)"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Key Takeaways\n\n1. **TFDS simplifies data loading** - `tfds.load()` handles download, caching, and parsing\n2. **Top-N Accuracy** is useful for multi-class problems (model got correct answer in top 3?)\n3. **Image preprocessing:** Resize → Grayscale → Flatten sacrifices spatial info for simplicity\n4. **CNNs** (not covered here) preserve spatial structure and typically perform better on images",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Top N Accuracy (Test Data): 0.60\n"
     ]
    }
   ],
   "source": [
    "print('Within Top N Accuracy (Test Data): {:.2f}'.format(within))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Modular Functions for Multi-Class Classification\n\nThe functions below provide reusable utilities for building and training multi-class classification models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Modular Functions for Multi-Class Classification\n\ndef build_multiclass_model(input_dim, output_classes, hidden_layers=None, \n                           dropout=None, optimizer='adam', \n                           loss='categorical_crossentropy', metrics=['accuracy'], name=None):\n    \"\"\"\n    Build a multi-class classification model.\n    \n    Args:\n        input_dim: Number of input features\n        output_classes: Number of output classes\n        hidden_layers: List of (neurons, activation) tuples for hidden layers, or None for SLP\n        dropout: Dropout rate after each hidden layer (None for no dropout)\n        optimizer: Keras optimizer\n        loss: Loss function\n        metrics: List of metrics\n        name: Model name\n    \n    Returns:\n        Compiled Keras Sequential model\n    \"\"\"\n    model = Sequential(name=name)\n    \n    if hidden_layers:\n        for i, (neurons, activation) in enumerate(hidden_layers):\n            if i == 0:\n                model.add(Dense(neurons, activation=activation, input_shape=(input_dim,)))\n            else:\n                model.add(Dense(neurons, activation=activation))\n            if dropout:\n                model.add(Dropout(dropout))\n    \n    # Output layer with softmax for multi-class\n    if not hidden_layers:\n        model.add(Dense(output_classes, activation='softmax', input_shape=(input_dim,)))\n    else:\n        model.add(Dense(output_classes, activation='softmax'))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=100):\n    \"\"\"\n    Train a model and return history and validation scores.\n    \n    Args:\n        model: Compiled Keras model\n        X_train, y_train: Training data\n        X_val, y_val: Validation data\n        batch_size: Batch size for training\n        epochs: Number of epochs\n    \n    Returns:\n        history: Training history object\n        val_score: Validation metrics (excluding loss)\n    \"\"\"\n    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n                        validation_data=(X_val, y_val), verbose=0)\n    val_score = model.evaluate(X_val, y_val, verbose=0)[1:]\n    return history, val_score\n\n\n# Example usage (commented out):\n# \n# # Build an MLP for multi-class classification\n# model = build_multiclass_model(\n#     input_dim=INPUT_DIMENSION,\n#     output_classes=OUTPUT_CLASSES,\n#     hidden_layers=[(64, 'relu'), (32, 'relu')],\n#     dropout=0.2,\n#     optimizer=RMSprop(learning_rate=0.001),\n#     loss='categorical_crossentropy',\n#     metrics=METRICS,\n#     name='Custom_MLP'\n# )\n# \n# # Train the model\n# history, val_score = train_model(model, X_train, y_train, X_val, y_val,\n#                                  batch_size=128, epochs=100)\n# \n# # Print results\n# print(f'Accuracy: {val_score[0]:.2f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}