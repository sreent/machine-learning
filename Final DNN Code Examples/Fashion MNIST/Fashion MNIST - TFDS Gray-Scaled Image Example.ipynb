{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Fashion%20MNIST/Fashion%20MNIST%20-%20TFDS%20Gray-Scaled%20Image%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Fashion MNIST - TFDS Grayscale Image Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to **multi-class image classification** using grayscale images from TensorFlow Datasets.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Work with **grayscale images** (no colour conversion needed)\n- Handle **10-class image classification**\n- Understand **image preprocessing** for dense neural networks (resize + flatten)\n- Use **TensorFlow Datasets** for standard benchmarks\n- Use **Hyperband** for efficient hyperparameter tuning\n- Apply **Dropout + L2 regularisation** to prevent overfitting\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [TensorFlow Datasets - fashion_mnist](https://www.tensorflow.org/datasets/catalog/fashion_mnist) |\n| **Problem Type** | Multi-Class Classification (10 classes) |\n| **Classes** | T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot |\n| **Data Balance** | Perfectly Balanced (7,000 per class) |\n| **Image Size** | 28×28 grayscale (resized to 32×32) |\n| **Preprocessing** | Resize → Flatten → Normalise |\n\n---\n\n## Technique Scope\n\nThis notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n\n| Technique | Status | Rationale |\n|-----------|--------|-----------|\n| **Dense layers (DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n\n**Note:** For image classification, Convolutional Neural Networks (CNNs) would typically be preferred. Here we use **Dense layers only** to demonstrate the Universal ML Workflow and regularisation techniques. Images are flattened to 1D vectors before being fed to the network.\n\n---\n\n## Why Fashion MNIST?\n\nFashion MNIST is a drop-in replacement for classic MNIST digit classification:\n\n| Aspect | MNIST (Digits) | Fashion MNIST |\n|--------|----------------|---------------|\n| **Classes** | 10 digits (0-9) | 10 clothing items |\n| **Difficulty** | Essentially \"solved\" (~99.8%) | More challenging (~92-94% with CNN) |\n| **Realism** | Handwritten digits | Real product images |\n| **Image size** | 28×28 grayscale | 28×28 grayscale |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\nThe first step in any machine learning project is to clearly define the problem and understand the data.\n\n**Problem Statement:** Classify grayscale images of fashion items into 10 categories.\n\n**Why this matters:**\n- **E-commerce automation:** Fashion classification enables automated product tagging, reducing manual effort for retailers with millions of SKUs\n- **Visual search:** Users can photograph an item and find similar products—requires accurate category classification as a first step\n- **Inventory management:** Automated classification helps track stock across categories\n\n**Why this problem is interesting:**\n- **Standard benchmark:** Fashion MNIST is widely used for evaluating classification algorithms\n- **Balanced classes:** Enables straightforward accuracy comparison\n- **Moderate difficulty:** More challenging than digit MNIST, but achievable with basic techniques\n\n**Data Source:** TensorFlow Datasets provides pre-split train/test sets directly."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Metric Selection Based on Class Imbalance\n\n| Imbalance Ratio | Classification | Primary Metric | Rationale |\n|-----------------|----------------|----------------|-----------|\n| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n\n**Why these thresholds?**\n- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n- **Balanced data (< 3:1):** Accuracy is meaningful and interpretable\n\n### References\n\n- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modelling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n**Fashion MNIST is perfectly balanced** (7,000 samples per class), so **Accuracy** is our primary metric.\n\nWe also report **Top-K Accuracy** - was the correct class among the top K predictions? This is useful for multi-class problems where similar items (e.g., Pullover vs. Coat) may be easily confused."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n\n### Data Split Strategy (This Notebook)\n\n```\nOriginal Data (70,000 samples) → Hold-Out Selected\n├── Test Set (10%) - Final evaluation only\n└── Training Pool (90%)\n    ├── Training Set (~81%) - Model training\n    └── Validation Set (~9%) - Hyperparameter tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries and Set Random Seed\n",
    "\n",
    "We set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Keras Tuner for hyperparameter search\n",
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 204\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load Dataset from TensorFlow Datasets\n",
    "\n",
    "TensorFlow Datasets provides convenient access to standard ML benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dataset configuration\nDATASET = 'fashion_mnist'\nRESIZE = (32, 32)  # Resize 28x28 to 32x32 - balance between detail and dimensionality\n\n# Class names for Fashion MNIST\nCLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nprint(f\"Loading {DATASET} from TensorFlow Datasets...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = tfds.load(DATASET, split='all', shuffle_files=True)\n",
    "\n",
    "# Extract images and labels\n",
    "images, labels = [], []\n",
    "for entry in ds.take(len(ds)):\n",
    "    image, label = entry['image'], entry['label']\n",
    "    \n",
    "    # Convert to numpy (grayscale has 1 channel)\n",
    "    image = image.numpy()[:, :, 0]\n",
    "    label = label.numpy()\n",
    "    \n",
    "    # Resize image to reduce dimensions\n",
    "    image = resize(image, RESIZE, anti_aliasing=True)\n",
    "    \n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "print(f\"Loaded {len(images):,} images\")\n",
    "print(f\"Original size: 28×28, Resized to: {RESIZE[0]}×{RESIZE[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display sample images\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(images[i], cmap='gray')\n    ax.set_title(CLASS_NAMES[labels[i]])\n    ax.axis('off')\nplt.suptitle('Sample Fashion MNIST Images (resized to 32×32)', fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Prepare Features and Labels\n\n**Image Preprocessing for Dense Networks:**\n\n```\n28×28 Image → Resize to 32×32 → Flatten to 1024 features → Normalise [0, 1]\n```\n\n**Why flatten?** Dense layers expect 1D input. Without CNNs (which preserve spatial structure), we treat each pixel as an independent feature.\n\n**Why resize to 32×32?** Expanding from 784 (28×28) to 1024 (32×32) features:\n- Provides a standard power-of-2 dimension for neural networks\n- Preserves all detail from the original images\n- Slight upsampling maintains edge clarity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays and flatten\n",
    "X = np.array(images)\n",
    "X = X.reshape((X.shape[0], -1))  # Flatten: (N, 16, 16) -> (N, 256)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Features per image: {X.shape[1]} ({RESIZE[0]}×{RESIZE[1]} = {RESIZE[0]*RESIZE[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# One-hot encode for multi-class classification\n",
    "y = to_categorical(label_encoder.transform(labels))\n",
    "\n",
    "print(f\"Label shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "TEST_SIZE = 0.10\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=TEST_SIZE, \n    stratify=labels,  # Maintain class balance\n    shuffle=True, random_state=SEED\n)\n\nprint(f\"Training pool: {len(X_train_full):,} samples\")\nprint(f\"Test set: {len(X_test):,} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixel values from [0, 1] (after resize) to ensure consistent range\n",
    "X_train_full = X_train_full / X_train_full.max()\n",
    "X_test = X_test / X_test.max()\n",
    "\n",
    "print(f\"Pixel value range: [{X_train_full.min():.2f}, {X_train_full.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "Before building complex models, we need to establish **baseline performance**.\n",
    "\n",
    "### 5.1 Examine Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "counts = np.sum(y, axis=0)\n",
    "print(\"Samples per class:\")\n",
    "for i, (name, count) in enumerate(zip(CLASS_NAMES, counts)):\n",
    "    print(f\"  {i}: {name}: {int(count):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset size analysis\n",
    "n_samples = len(X)\n",
    "n_classes = y.shape[1]\n",
    "HOLDOUT_THRESHOLD = 10000\n",
    "\n",
    "# Imbalance analysis\n",
    "imbalance_ratio = counts.max() / counts.min()\n",
    "IMBALANCE_THRESHOLD = 3.0\n",
    "\n",
    "use_holdout = n_samples > HOLDOUT_THRESHOLD\n",
    "use_accuracy = imbalance_ratio <= IMBALANCE_THRESHOLD\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA-DRIVEN CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\n",
    "print(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples\")\n",
    "print(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n",
    "\n",
    "print(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\n",
    "print(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1\")\n",
    "print(f\"   Decision: {'Accuracy (balanced)' if use_accuracy else 'F1-Score (imbalanced)'}\")\n",
    "\n",
    "print(f\"\\n3. NUMBER OF CLASSES: {n_classes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "PRIMARY_METRIC = 'accuracy' if use_accuracy else 'f1'\n",
    "print(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy (random guessing with balanced classes)\n",
    "baseline = 1.0 / n_classes\n",
    "\n",
    "print(f\"Baseline accuracy (random guess): {baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "VALIDATION_SIZE = 0.10  # 10% of training pool\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, \n    test_size=VALIDATION_SIZE, \n    stratify=y_train_full.argmax(axis=1),\n    shuffle=True, random_state=SEED\n)\n\nprint(f\"Training set: {X_train.shape[0]:,} samples\")\nprint(f\"Validation set: {X_val.shape[0]:,} samples\")\nprint(f\"Test set: {X_test.shape[0]:,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_CLASSES = y_train.shape[1]\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNC = 'categorical_crossentropy'  # Multi-class classification\n",
    "\n",
    "# Training metrics\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "print(f\"Input dimension: {INPUT_DIMENSION}\")\n",
    "print(f\"Output classes: {OUTPUT_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Layer Perceptron (no hidden layers) - Baseline\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAINING CONFIGURATION\n# =============================================================================\n\n# Batch Size Selection:\n# - Large datasets (>10,000 samples): Use 512 for efficient GPU utilisation\n# - Small datasets (<10,000 samples): Use 32-64 for better gradient estimates\n# Fashion MNIST has 70,000 samples → Use batch size 512\nBATCH_SIZE = 512\n\n# Epoch strategy:\n# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2 (more epochs needed\n#   because regularisation slows convergence)\n\nEPOCHS_BASELINE = 100\nEPOCHS_REGULARIZED = 150"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Single-Layer Perceptron\n",
    "history_slp = slp_model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "val_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SLP validation metrics\n",
    "preds_slp_val = slp_model.predict(X_val, verbose=0).argmax(axis=1)\n",
    "acc_slp_val = accuracy_score(y_val.argmax(axis=1), preds_slp_val)\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(acc_slp_val, baseline))\n",
    "print(f'\\nAccuracy: {acc_slp_val:.2%}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs.\n",
    "    Plots: (1) Loss, (2) Accuracy\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    title_suffix = f' ({title})' if title else ''\n",
    "\n",
    "    # Plot 1: Loss\n",
    "    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[0].set_title(f'Loss{title_suffix}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Accuracy\n",
    "    axs[1].plot(epochs, history.history['accuracy'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[1].plot(epochs, history.history['val_accuracy'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[1].set_title(f'Accuracy{title_suffix}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SLP training history\n",
    "plot_training_history(history_slp, title='SLP Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nThe next step is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n\n**No regularisation applied:** We intentionally train this model **without any regularisation** (no dropout, no L2, no early stopping) to observe overfitting behaviour.\n\n---\n\n### Architecture Design Decisions\n\n**Why 64 neurons in the hidden layer?**\n\nThis is a practical starting point that balances capacity and efficiency:\n- **Too few (e.g., 16):** May not have enough capacity to learn complex visual patterns\n- **Too many (e.g., 512):** Increases overfitting risk and training time without proportional benefit\n- **64 neurons:** A common choice that provides sufficient capacity for most classification tasks on flattened images\n\n**Why only 1 hidden layer instead of 2-3?**\n\nPer the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n\n1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation, not to add more capacity\n\nIf this 1-layer model *couldn't* overfit (training and validation loss both plateau high), we would then add more layers. But since it does overfit, the architecture is adequate.\n\n*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*\n\n### 6.1 Build a Deep Neural Network (DNN)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network (1 hidden layer, no regularisation)\n",
    "dnn_model = Sequential(name='Deep_Neural_Network')\n",
    "dnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "dnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Deep Neural Network\n",
    "history_dnn = dnn_model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val), \n",
    "    verbose=0\n",
    ")\n",
    "val_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DNN training history\n",
    "plot_training_history(history_dnn, title='DNN - No Regularisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DNN validation metrics\n",
    "preds_dnn_val = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\n",
    "acc_dnn_val = accuracy_score(y_val.argmax(axis=1), preds_dnn_val)\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(acc_dnn_val, baseline))\n",
    "print(f'\\nAccuracy: {acc_dnn_val:.2%}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Now we address overfitting by adding **Dropout + L2 regularisation**.\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning.\n",
    "\n",
    "### 7.1 Hyperband Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperband Model Builder\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build Fashion MNIST model with FIXED architecture (1 hidden layer, 64 neurons).\n",
    "    Only tunes regularisation and learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "\n",
    "    # L2 regularisation strength\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
    "\n",
    "    # Fixed architecture: 1 hidden layer with 64 neurons\n",
    "    model.add(layers.Dense(64, activation='relu', \n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer for multi-class classification\n",
    "    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hyperband tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_hyperband,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='fashion_mnist_hyperband',\n",
    "    project_name='fashion_mnist_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"Tuning objective: val_accuracy\")\n",
    "\n",
    "# Run Hyperband search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters found by Hyperband:\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\n# =============================================================================\n# CRITICAL: Extract the number of epochs from the best trial\n# =============================================================================\n# Hyperband trains different configurations for different numbers of epochs.\n# The best trial achieved its performance at a SPECIFIC epoch count.\n# We must retrain for the SAME number of epochs to avoid mismatch.\n\nbest_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\nbest_epochs = best_trial.best_step + 1  # best_step is 0-indexed\n\nprint(f\"\\n>>> Best trial was trained for {best_epochs} epochs <<<\")\nprint(f\"    (This is the epoch count we'll use for retraining)\")\n\n# Build a fresh model with the best hyperparameters\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.2 Retraining with Full Data and Matched Epochs\n\nAfter finding the best hyperparameters, we retrain with two important considerations:\n\n1. **Matched Epochs:** Use the exact number of epochs that produced the best validation score during Hyperband tuning\n2. **Combined Training Data:** Merge training and validation sets to maximise the data available for the final model\n\n---\n\n#### Why Combine Training and Validation Sets?\n\nOnce hyperparameter tuning is complete, the validation set has served its purpose (model selection). For the final model:\n\n| Approach | Training Data | Benefit |\n|----------|--------------|---------|\n| Keep validation separate | ~81% of original | Can monitor overfitting during retrain |\n| **Combine train + validation** | **~90% of original** | **Maximises data for final model** |\n\nWe combine because:\n- **More data = better generalisation:** The model learns from more examples\n- **Validation set's job is done:** It was used for hyperparameter selection, not needed for final training\n- **Standard practice:** This is the recommended approach in production ML pipelines (Chollet, 2021)\n\n---\n\n#### The Epoch Mismatch Problem\n\nHyperband finds hyperparameters optimal at a **specific epoch count**. If we retrain for a different number of epochs, the hyperparameters may no longer be optimal.\n\nWe extract `best_trial.best_step + 1` to match the exact epoch count where Hyperband found the best validation score.\n\n> *\"Combine your data to maximise learning, match your epochs to honour what Hyperband discovered.\"*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RETRAIN WITH FULL DATA AND MATCHED EPOCHS\n# =============================================================================\n\n# Combine training and validation sets for final model\nX_combined = np.vstack([X_train, X_val])\ny_combined = np.vstack([y_train, y_val])  # One-hot encoded labels\n\nprint(f\"Training data: {X_train.shape[0]:,} samples\")\nprint(f\"Validation data: {X_val.shape[0]:,} samples\")\nprint(f\"Combined data: {X_combined.shape[0]:,} samples\")\nprint(f\"  → {(X_combined.shape[0] / X_train.shape[0] - 1) * 100:.1f}% more training data\")\n\nprint(f\"\\nRetraining with best hyperparameters for {best_epochs} epochs...\")\nprint(f\"(Matching the epoch count from Hyperband's best trial)\")\n\nhistory_opt = opt_model.fit(\n    X_combined, y_combined,  # Train on combined data\n    epochs=best_epochs,       # CRITICAL: Use matched epochs!\n    batch_size=BATCH_SIZE,\n    verbose=0\n    # Note: No validation_data - we've merged it into training\n)\n\nprint(f\"\\nTraining complete on combined dataset.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PLOT TRAINING METRICS (Training only - no validation data)\n# =============================================================================\ndef plot_training_only(history, title=None):\n    \"\"\"\n    Plot training metrics over epochs (no validation).\n    Used when training on combined train+val data.\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history.history['loss']) + 1)\n    title_suffix = f' ({title})' if title else ''\n\n    # Plot 1: Loss\n    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n    axs[0].set_title(f'Training Loss{title_suffix}')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(alpha=0.3)\n\n    # Plot 2: Accuracy\n    axs[1].plot(epochs, history.history['accuracy'], 'b-', label='Training', linewidth=1.5)\n    axs[1].set_title(f'Training Accuracy{title_suffix}')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot training history for the final model (trained on combined data)\nplot_training_only(history_opt, title=f'Final Model - Combined Data ({best_epochs} epochs)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# VALIDATION SET STATUS\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"VALIDATION SET STATUS\")\nprint(\"=\" * 60)\nprint(\"\\nThe validation set has been merged into training data.\")\nprint(\"This maximises the data available for the final model.\")\nprint(\"\\nValidation metrics from hyperparameter tuning (before merge):\")\nprint(f\"  - These were used to select the best hyperparameters\")\nprint(f\"  - The final model is evaluated on the TEST set only\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Final Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "preds_test = opt_model.predict(X_test, verbose=0)\n",
    "preds_test_labels = preds_test.argmax(axis=1)\n",
    "y_test_labels = y_test.argmax(axis=1)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_labels, preds_test_labels)\n",
    "test_precision = precision_score(y_test_labels, preds_test_labels, average='macro')\n",
    "test_recall = recall_score(y_test_labels, preds_test_labels, average='macro')\n",
    "test_f1 = f1_score(y_test_labels, preds_test_labels, average='macro')\n",
    "\n",
    "print('=' * 50)\n",
    "print('FINAL TEST SET RESULTS')\n",
    "print('=' * 50)\n",
    "print(f'Accuracy (Test): {test_accuracy:.4f}  ← Primary Metric')\n",
    "print(f'Precision (Test, macro): {test_precision:.4f}')\n",
    "print(f'Recall (Test, macro): {test_recall:.4f}')\n",
    "print(f'F1-Score (Test, macro): {test_f1:.4f}')\n",
    "print(f'\\nBaseline: {baseline:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test_labels, preds_test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d', xticks_rotation=45)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K Accuracy\n",
    "def top_k_accuracy(y_true, y_pred_proba, k):\n",
    "    \"\"\"Calculate top-K accuracy: correct class in top K predictions.\"\"\"\n",
    "    top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "    correct = sum(y_true[i] in top_k_preds[i] for i in range(len(y_true)))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "print(\"Top-K Accuracy (Test Set):\")\n",
    "for k in [1, 2, 3]:\n",
    "    top_k = top_k_accuracy(y_test_labels, preds_test, k)\n",
    "    print(f\"  Top-{k}: {top_k:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Results summary\nresults = pd.DataFrame({\n    'Model': ['Random Baseline', 'SLP (No Hidden)', 'DNN (No Regularisation)', 'DNN (Dropout + L2) - Test'],\n    'Accuracy': [baseline, acc_slp_val, acc_dnn_val, test_accuracy],\n    'Dataset': ['N/A', 'Validation', 'Validation', 'Test']\n})\n\nprint(\"=\" * 70)\nprint(\"MODEL COMPARISON - RESULTS SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Primary Metric: ACCURACY (balanced dataset, {n_classes} classes)\")\nprint(\"=\" * 70)\nprint(results.to_string(index=False, float_format='{:.4f}'.format))\nprint(\"=\" * 70)\nprint(f\"\\nKey Observations:\")\nprint(f\"  - All models significantly outperform random baseline ({baseline:.2%})\")\nprint(f\"  - Final model trained on combined train+val data ({X_combined.shape[0]:,} samples)\")\nprint(f\"  - Test accuracy: {test_accuracy:.2%}, Top-3: {top_k_accuracy(y_test_labels, preds_test, 3):.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | 70,000 samples | Hold-Out | Kohavi (1995) |\n| **Accuracy vs F1-Score** | > 3:1 imbalance | 1.00:1 ratio | Accuracy | He and Garcia (2009) |\n| **Batch Size** | > 10,000 samples | 70,000 samples | 512 | Efficient GPU utilisation |\n\n### Lessons Learned\n\n1. **Image Preprocessing for DNNs:** Without CNNs, images must be flattened to 1D vectors. This sacrifices spatial relationships but enables use of dense layers.\n\n2. **Grayscale Simplifies Processing:** No colour conversion needed—single channel per pixel reduces dimensionality (3x fewer features than RGB).\n\n3. **Balanced Classes → Accuracy:** With 10 equal-sized classes, accuracy is a valid primary metric. No class weights needed.\n\n4. **Top-K Accuracy for Multi-Class:** When similar items may be confused (Pullover vs. Coat), Top-K shows how often the correct class is in the top K predictions.\n\n5. **Capacity Before Regularisation:** Build a model that overfits first (Section 6). If it can't overfit, add more capacity. Only then regularise (Section 7).\n\n6. **Maximise Data for Final Model:** After hyperparameter tuning, we combine training and validation sets for the final model. The validation set's job is done (model selection), so we use all available data to maximise learning.\n\n7. **Regularisation Enables Longer Training:** With proper regularisation, we can train for more epochs without overfitting risk. *\"Regularisation buys you the freedom to train longer.\"*\n\n8. **Technique Scope:** Dense layers only (Ch. 1-4). CNNs (Ch. 8) would achieve ~92-94% accuracy by preserving spatial structure.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145.\n\n- Xiao, H., Rasul, K. and Vollgraf, R. (2017) 'Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms', *arXiv preprint arXiv:1708.07747*."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Modular Helper Functions\n\nFor cleaner code organisation, you can wrap the model building and training patterns into reusable functions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# MODULAR HELPER FUNCTIONS\n# =============================================================================\n\ndef build_multiclass_classifier(input_dim, num_classes, hidden_units=None, dropout=0.0, l2_reg=0.0,\n                                 optimizer='adam', learning_rate=None, name=None):\n    \"\"\"\n    Build a multi-class image classification neural network.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features (flattened image size)\n    num_classes : int\n        Number of output classes\n    hidden_units : list of int, optional\n        Neurons per hidden layer, e.g., [64] or [128, 64]\n    dropout : float\n        Dropout rate (0.0 to 0.5)\n    l2_reg : float\n        L2 regularisation strength\n    learning_rate : float, optional\n        Custom learning rate\n    name : str, optional\n        Model name\n        \n    Returns:\n    --------\n    keras.Sequential : Compiled model ready for training\n    \"\"\"\n    model = Sequential(name=name)\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    hidden_units = hidden_units or []\n    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n    \n    for units in hidden_units:\n        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n        if dropout > 0:\n            model.add(Dropout(dropout))\n    \n    model.add(Dense(num_classes, activation='softmax'))\n    \n    if learning_rate is not None:\n        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n    else:\n        opt = optimizer\n    \n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val,\n                batch_size=512, epochs=100, verbose=0):\n    \"\"\"Train a model and return training history.\"\"\"\n    return model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=batch_size, \n        epochs=epochs,\n        verbose=verbose\n    )\n\n\ndef evaluate_multiclass(model, X, y_true_onehot, class_names=None):\n    \"\"\"\n    Evaluate multi-class classification model.\n    \n    Returns:\n    --------\n    dict : Dictionary containing accuracy, precision, recall, f1, and top-k accuracy\n    \"\"\"\n    y_pred_proba = model.predict(X, verbose=0)\n    y_pred = y_pred_proba.argmax(axis=1)\n    y_true = y_true_onehot.argmax(axis=1)\n    \n    # Top-K accuracy\n    def top_k_acc(y_true, y_pred_proba, k):\n        top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n        return sum(y_true[i] in top_k_preds[i] for i in range(len(y_true))) / len(y_true)\n    \n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='macro'),\n        'recall': recall_score(y_true, y_pred, average='macro'),\n        'f1': f1_score(y_true, y_pred, average='macro'),\n        'top_2_accuracy': top_k_acc(y_true, y_pred_proba, 2),\n        'top_3_accuracy': top_k_acc(y_true, y_pred_proba, 3),\n    }\n    \n    return metrics\n\n\n# =============================================================================\n# USAGE EXAMPLES\n# =============================================================================\n# \n# # Build models\n# slp = build_multiclass_classifier(INPUT_DIMENSION, OUTPUT_CLASSES, name='SLP')\n# mlp = build_multiclass_classifier(INPUT_DIMENSION, OUTPUT_CLASSES, \n#                                    hidden_units=[64], name='MLP')\n# mlp_reg = build_multiclass_classifier(INPUT_DIMENSION, OUTPUT_CLASSES,\n#                                        hidden_units=[64], dropout=0.3, l2_reg=0.001,\n#                                        learning_rate=0.001, name='MLP_Regularized')\n# \n# # Train\n# history = train_model(mlp, X_train, y_train, X_val, y_val)\n# \n# # Evaluate\n# metrics = evaluate_multiclass(mlp, X_val, y_val)\n# print(f\"Accuracy: {metrics['accuracy']:.4f}, Top-3: {metrics['top_3_accuracy']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}