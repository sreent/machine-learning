{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Fashion%20MNIST/Fashion%20MNIST%20-%20TFDS%20Gray-Scaled%20Image%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Fashion MNIST - TFDS Grayscale Image Example\n",
    "\n",
    "This notebook demonstrates the **Universal ML Workflow** applied to **multi-class image classification** using grayscale images from TensorFlow Datasets.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Work with **grayscale images** (no colour conversion needed)\n",
    "- Handle **10-class image classification**\n",
    "- Understand **image preprocessing** for dense neural networks (resize + flatten)\n",
    "- Use **TensorFlow Datasets** for standard benchmarks\n",
    "- Use **Hyperband** for efficient hyperparameter tuning\n",
    "- Apply **Dropout + L2 regularisation** to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| **Source** | [TensorFlow Datasets - fashion_mnist](https://www.tensorflow.org/datasets/catalog/fashion_mnist) |\n",
    "| **Problem Type** | Multi-Class Classification (10 classes) |\n",
    "| **Classes** | T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot |\n",
    "| **Data Balance** | Perfectly Balanced (7,000 per class) |\n",
    "| **Image Size** | 28×28 grayscale (resized to 16×16) |\n",
    "| **Preprocessing** | Resize → Flatten → Normalise |\n",
    "\n",
    "---\n",
    "\n",
    "## Technique Scope\n",
    "\n",
    "This notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n",
    "\n",
    "| Technique | Status | Rationale |\n",
    "|-----------|--------|-----------|\n",
    "| **Dense layers (DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n",
    "| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n",
    "| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n",
    "| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n",
    "| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n",
    "| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n",
    "\n",
    "**Note:** For image classification, Convolutional Neural Networks (CNNs) would typically be preferred. Here we use **Dense layers only** to demonstrate the Universal ML Workflow and regularisation techniques. Images are flattened to 1D vectors before being fed to the network.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Fashion MNIST?\n",
    "\n",
    "Fashion MNIST is a drop-in replacement for classic MNIST digit classification:\n",
    "\n",
    "| Aspect | MNIST (Digits) | Fashion MNIST |\n",
    "|--------|----------------|---------------|\n",
    "| **Classes** | 10 digits (0-9) | 10 clothing items |\n",
    "| **Difficulty** | Essentially \"solved\" (~99.8%) | More challenging (~92-94% with CNN) |\n",
    "| **Realism** | Handwritten digits | Real product images |\n",
    "| **Image size** | 28×28 grayscale | 28×28 grayscale |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Problem and Assembling a Dataset\n",
    "\n",
    "The first step in any machine learning project is to clearly define the problem and understand the data.\n",
    "\n",
    "**Problem Statement:** Classify grayscale images of fashion items into 10 categories.\n",
    "\n",
    "**Why this problem is interesting:**\n",
    "- **Standard benchmark:** Fashion MNIST is widely used for evaluating classification algorithms\n",
    "- **Balanced classes:** Enables straightforward accuracy comparison\n",
    "- **Moderate difficulty:** More challenging than digit MNIST, but achievable with basic techniques\n",
    "\n",
    "**Data Source:** TensorFlow Datasets provides pre-split train/test sets directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing a Measure of Success\n",
    "\n",
    "### Metric Selection Based on Class Imbalance\n",
    "\n",
    "| Imbalance Ratio | Classification | Primary Metric | Rationale |\n",
    "|-----------------|----------------|----------------|-----------|\n",
    "| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n",
    "| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n",
    "| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n",
    "\n",
    "**Fashion MNIST is perfectly balanced** (7,000 samples per class), so **Accuracy** is our primary metric.\n",
    "\n",
    "We also report **Top-K Accuracy** - was the correct class among the top K predictions? This is useful for multi-class problems where similar items (e.g., Pullover vs. Coat) may be easily confused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n\n### Data Split Strategy (This Notebook)\n\n```\nOriginal Data (70,000 samples) → Hold-Out Selected\n├── Test Set (10%) - Final evaluation only\n└── Training Pool (90%)\n    ├── Training Set (~81%) - Model training\n    └── Validation Set (~9%) - Hyperparameter tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries and Set Random Seed\n",
    "\n",
    "We set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Keras Tuner for hyperparameter search\n",
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 204\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load Dataset from TensorFlow Datasets\n",
    "\n",
    "TensorFlow Datasets provides convenient access to standard ML benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET = 'fashion_mnist'\n",
    "RESIZE = (16, 16)  # Resize 28x28 to 16x16 to reduce features\n",
    "\n",
    "# Class names for Fashion MNIST\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Loading {DATASET} from TensorFlow Datasets...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = tfds.load(DATASET, split='all', shuffle_files=True)\n",
    "\n",
    "# Extract images and labels\n",
    "images, labels = [], []\n",
    "for entry in ds.take(len(ds)):\n",
    "    image, label = entry['image'], entry['label']\n",
    "    \n",
    "    # Convert to numpy (grayscale has 1 channel)\n",
    "    image = image.numpy()[:, :, 0]\n",
    "    label = label.numpy()\n",
    "    \n",
    "    # Resize image to reduce dimensions\n",
    "    image = resize(image, RESIZE, anti_aliasing=True)\n",
    "    \n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "print(f\"Loaded {len(images):,} images\")\n",
    "print(f\"Original size: 28×28, Resized to: {RESIZE[0]}×{RESIZE[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i], cmap='gray')\n",
    "    ax.set_title(CLASS_NAMES[labels[i]])\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Fashion MNIST Images (resized to 16×16)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prepare Features and Labels\n",
    "\n",
    "**Image Preprocessing for Dense Networks:**\n",
    "\n",
    "```\n",
    "28×28 Image → Resize to 16×16 → Flatten to 256 features → Normalise [0, 1]\n",
    "```\n",
    "\n",
    "**Why flatten?** Dense layers expect 1D input. Without CNNs (which preserve spatial structure), we treat each pixel as an independent feature.\n",
    "\n",
    "**Why resize?** Reducing from 784 (28×28) to 256 (16×16) features:\n",
    "- Reduces model parameters and training time\n",
    "- Helps prevent overfitting on small patterns\n",
    "- 16×16 retains enough detail for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays and flatten\n",
    "X = np.array(images)\n",
    "X = X.reshape((X.shape[0], -1))  # Flatten: (N, 16, 16) -> (N, 256)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Features per image: {X.shape[1]} ({RESIZE[0]}×{RESIZE[1]} = {RESIZE[0]*RESIZE[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# One-hot encode for multi-class classification\n",
    "y = to_categorical(label_encoder.transform(labels))\n",
    "\n",
    "print(f\"Label shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "TEST_SIZE = 0.10\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=TEST_SIZE, \n    stratify=labels,  # Maintain class balance\n    shuffle=True, random_state=SEED\n)\n\nprint(f\"Training pool: {len(X_train_full):,} samples\")\nprint(f\"Test set: {len(X_test):,} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixel values from [0, 1] (after resize) to ensure consistent range\n",
    "X_train_full = X_train_full / X_train_full.max()\n",
    "X_test = X_test / X_test.max()\n",
    "\n",
    "print(f\"Pixel value range: [{X_train_full.min():.2f}, {X_train_full.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "Before building complex models, we need to establish **baseline performance**.\n",
    "\n",
    "### 5.1 Examine Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "counts = np.sum(y, axis=0)\n",
    "print(\"Samples per class:\")\n",
    "for i, (name, count) in enumerate(zip(CLASS_NAMES, counts)):\n",
    "    print(f\"  {i}: {name}: {int(count):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset size analysis\n",
    "n_samples = len(X)\n",
    "n_classes = y.shape[1]\n",
    "HOLDOUT_THRESHOLD = 10000\n",
    "\n",
    "# Imbalance analysis\n",
    "imbalance_ratio = counts.max() / counts.min()\n",
    "IMBALANCE_THRESHOLD = 3.0\n",
    "\n",
    "use_holdout = n_samples > HOLDOUT_THRESHOLD\n",
    "use_accuracy = imbalance_ratio <= IMBALANCE_THRESHOLD\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA-DRIVEN CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\n",
    "print(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples\")\n",
    "print(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n",
    "\n",
    "print(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\n",
    "print(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1\")\n",
    "print(f\"   Decision: {'Accuracy (balanced)' if use_accuracy else 'F1-Score (imbalanced)'}\")\n",
    "\n",
    "print(f\"\\n3. NUMBER OF CLASSES: {n_classes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "PRIMARY_METRIC = 'accuracy' if use_accuracy else 'f1'\n",
    "print(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy (random guessing with balanced classes)\n",
    "baseline = 1.0 / n_classes\n",
    "\n",
    "print(f\"Baseline accuracy (random guess): {baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "VALIDATION_SIZE = 0.10  # 10% of training pool\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, \n    test_size=VALIDATION_SIZE, \n    stratify=y_train_full.argmax(axis=1),\n    shuffle=True, random_state=SEED\n)\n\nprint(f\"Training set: {X_train.shape[0]:,} samples\")\nprint(f\"Validation set: {X_val.shape[0]:,} samples\")\nprint(f\"Test set: {X_test.shape[0]:,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_CLASSES = y_train.shape[1]\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNC = 'categorical_crossentropy'  # Multi-class classification\n",
    "\n",
    "# Training metrics\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "print(f\"Input dimension: {INPUT_DIMENSION}\")\n",
    "print(f\"Output classes: {OUTPUT_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Layer Perceptron (no hidden layers) - Baseline\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Epoch strategy:\n",
    "# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n",
    "# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2\n",
    "\n",
    "EPOCHS_BASELINE = 100\n",
    "EPOCHS_REGULARIZED = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Single-Layer Perceptron\n",
    "history_slp = slp_model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "val_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SLP validation metrics\n",
    "preds_slp_val = slp_model.predict(X_val, verbose=0).argmax(axis=1)\n",
    "acc_slp_val = accuracy_score(y_val.argmax(axis=1), preds_slp_val)\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(acc_slp_val, baseline))\n",
    "print(f'\\nAccuracy: {acc_slp_val:.2%}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs.\n",
    "    Plots: (1) Loss, (2) Accuracy\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    title_suffix = f' ({title})' if title else ''\n",
    "\n",
    "    # Plot 1: Loss\n",
    "    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[0].set_title(f'Loss{title_suffix}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Accuracy\n",
    "    axs[1].plot(epochs, history.history['accuracy'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[1].plot(epochs, history.history['val_accuracy'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[1].set_title(f'Accuracy{title_suffix}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SLP training history\n",
    "plot_training_history(history_slp, title='SLP Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling Up: Developing a Model That Overfits\n",
    "\n",
    "The next step is to build a model with **enough capacity to overfit**.\n",
    "\n",
    "**No regularisation applied:** We train without regularisation to observe overfitting.\n",
    "\n",
    "### 6.1 Build a Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network (1 hidden layer, no regularisation)\n",
    "dnn_model = Sequential(name='Deep_Neural_Network')\n",
    "dnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "dnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Deep Neural Network\n",
    "history_dnn = dnn_model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val), \n",
    "    verbose=0\n",
    ")\n",
    "val_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DNN training history\n",
    "plot_training_history(history_dnn, title='DNN - No Regularisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DNN validation metrics\n",
    "preds_dnn_val = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\n",
    "acc_dnn_val = accuracy_score(y_val.argmax(axis=1), preds_dnn_val)\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(acc_dnn_val, baseline))\n",
    "print(f'\\nAccuracy: {acc_dnn_val:.2%}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Now we address overfitting by adding **Dropout + L2 regularisation**.\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning.\n",
    "\n",
    "### 7.1 Hyperband Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperband Model Builder\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build Fashion MNIST model with FIXED architecture (1 hidden layer, 64 neurons).\n",
    "    Only tunes regularisation and learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "\n",
    "    # L2 regularisation strength\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
    "\n",
    "    # Fixed architecture: 1 hidden layer with 64 neurons\n",
    "    model.add(layers.Dense(64, activation='relu', \n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer for multi-class classification\n",
    "    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hyperband tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_hyperband,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='fashion_mnist_hyperband',\n",
    "    project_name='fashion_mnist_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"Tuning objective: val_accuracy\")\n",
    "\n",
    "# Run Hyperband search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\n",
    "print(f\"  Dropout Rate: {best_hp.get('dropout')}\")\n",
    "print(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n",
    "\n",
    "opt_model = tuner.hypermodel.build(best_hp)\n",
    "opt_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Retrain with Optimised Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with extended epochs\n",
    "history_opt = opt_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS_REGULARIZED,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0\n",
    ")\n",
    "val_score_opt = opt_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimised model training history\n",
    "plot_training_history(history_opt, title='DNN - Dropout + L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display optimised model validation metrics\n",
    "preds_opt_val = opt_model.predict(X_val, verbose=0).argmax(axis=1)\n",
    "acc_opt_val = accuracy_score(y_val.argmax(axis=1), preds_opt_val)\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(acc_opt_val, baseline))\n",
    "print(f'\\nAccuracy: {acc_opt_val:.2%}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Final Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "preds_test = opt_model.predict(X_test, verbose=0)\n",
    "preds_test_labels = preds_test.argmax(axis=1)\n",
    "y_test_labels = y_test.argmax(axis=1)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_labels, preds_test_labels)\n",
    "test_precision = precision_score(y_test_labels, preds_test_labels, average='macro')\n",
    "test_recall = recall_score(y_test_labels, preds_test_labels, average='macro')\n",
    "test_f1 = f1_score(y_test_labels, preds_test_labels, average='macro')\n",
    "\n",
    "print('=' * 50)\n",
    "print('FINAL TEST SET RESULTS')\n",
    "print('=' * 50)\n",
    "print(f'Accuracy (Test): {test_accuracy:.4f}  ← Primary Metric')\n",
    "print(f'Precision (Test, macro): {test_precision:.4f}')\n",
    "print(f'Recall (Test, macro): {test_recall:.4f}')\n",
    "print(f'F1-Score (Test, macro): {test_f1:.4f}')\n",
    "print(f'\\nBaseline: {baseline:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test_labels, preds_test_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d', xticks_rotation=45)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K Accuracy\n",
    "def top_k_accuracy(y_true, y_pred_proba, k):\n",
    "    \"\"\"Calculate top-K accuracy: correct class in top K predictions.\"\"\"\n",
    "    top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "    correct = sum(y_true[i] in top_k_preds[i] for i in range(len(y_true)))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "print(\"Top-K Accuracy (Test Set):\")\n",
    "for k in [1, 2, 3]:\n",
    "    top_k = top_k_accuracy(y_test_labels, preds_test, k)\n",
    "    print(f\"  Top-{k}: {top_k:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Baseline', 'SLP (No Hidden)', 'DNN (No Regularisation)', 'DNN (Dropout + L2)', 'DNN (Dropout + L2) - Test'],\n",
    "    'Accuracy': [baseline, acc_slp_val, acc_dnn_val, acc_opt_val, test_accuracy],\n",
    "    'Dataset': ['N/A', 'Validation', 'Validation', 'Validation', 'Test']\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON - RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Primary Metric: ACCURACY (balanced dataset, {n_classes} classes)\")\n",
    "print(\"=\" * 70)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"  - All models significantly outperform random baseline ({baseline:.2%})\")\n",
    "print(f\"  - DNN with regularisation achieves {test_accuracy:.2%} test accuracy\")\n",
    "print(f\"  - Top-3 accuracy: {top_k_accuracy(y_test_labels, preds_test, 3):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### Decision Framework Summary\n",
    "\n",
    "| Decision | Threshold | This Dataset | Choice | Reference |\n",
    "|----------|-----------|--------------|--------|-----------|\n",
    "| **Hold-Out vs K-Fold** | > 10,000 samples | 70,000 samples | Hold-Out | Kohavi (1995) |\n",
    "| **Accuracy vs F1-Score** | > 3:1 imbalance | 1.00:1 ratio | Accuracy | He and Garcia (2009) |\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "1. **Image Preprocessing for DNNs:** Without CNNs, images must be flattened to 1D vectors. Resizing reduces dimensionality.\n",
    "\n",
    "2. **Grayscale Simplifies Processing:** No colour conversion needed, single channel per pixel.\n",
    "\n",
    "3. **Balanced Classes → Accuracy:** With 10 equal-sized classes, accuracy is a valid primary metric.\n",
    "\n",
    "4. **Top-K Accuracy:** Useful for multi-class problems where similar items may be confused.\n",
    "\n",
    "5. **Regularisation Prevents Overfitting:** Dropout + L2 controls overfitting without early stopping.\n",
    "\n",
    "6. **Technique Scope:** Dense layers only (Ch. 1-4). CNNs would perform better but are introduced in Ch. 8.\n",
    "\n",
    "### References\n",
    "\n",
    "- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n",
    "\n",
    "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
    "\n",
    "- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145.\n",
    "\n",
    "- Xiao, H., Rasul, K. and Vollgraf, R. (2017) 'Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms', *arXiv preprint arXiv:1708.07747*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}