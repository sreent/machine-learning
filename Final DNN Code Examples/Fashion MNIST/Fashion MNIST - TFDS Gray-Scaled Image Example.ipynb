{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Fashion%20MNIST/Fashion%20MNIST%20-%20TFDS%20Gray-Scaled%20Image%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Fashion MNIST - TFDS Gray-Scaled Image Example\n\nThis notebook demonstrates the **Universal ML Workflow** for multi-class classification on grayscale images using TensorFlow Datasets.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Work with grayscale images (no color conversion needed)\n- Handle 10-class image classification\n- Understand the Fashion MNIST benchmark dataset\n- Apply the Universal ML Workflow to a standard benchmark\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [TensorFlow Datasets - fashion_mnist](https://www.tensorflow.org/datasets/catalog/fashion_mnist) |\n| **Problem Type** | Multi-Class Classification (10 classes) |\n| **Classes** | T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot |\n| **Data Balance** | Perfectly Balanced (7000 per class) |\n| **Image Size** | 28x28 grayscale |\n| **Preprocessing** | Resize to 16x16 -> Flatten (256 features) |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem:** Classify grayscale images of fashion items into 10 categories.\n\n**Why Fashion MNIST?** A drop-in replacement for classic MNIST, but more challenging and realistic. While digit recognition (MNIST) is essentially \"solved,\" fashion item classification presents more subtle differences between classes.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\nStandard accuracy (balanced classes), plus Top-N accuracy for 10-class problem."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n20% test set, validation set, K-fold cross-validation for tuning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries and Load Dataset\n\nNote: Fashion MNIST images are already grayscale (no color conversion needed)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nimport tensorflow_datasets as tfds\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import RMSprop\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport itertools\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'fashion_mnist'\n",
    "RESIZE = (16, 16)\n",
    "GRAY_SCALE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(DATASET, split='all', shuffle_files=True)\n",
    "\n",
    "images, labels = [], []\n",
    "for entry in ds.take(len(ds)) :\n",
    "    image, label = entry['image'], entry['label']\n",
    "    \n",
    "    image, label = image.numpy()[:,:,0], label.numpy()\n",
    "    \n",
    "    image = resize(image, RESIZE, anti_aliasing=True)\n",
    "        \n",
    "    images.append( image )\n",
    "    labels.append( label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of images to numpy array\n",
    "X = np.array(images)\n",
    "\n",
    "# flatten 2D image array to 1D array\n",
    "X = X.reshape( (X.shape[0], -1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# label -> one-hot encoded vector\n",
    "y = np_utils.to_categorical(label_encoder.transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=labels, \n",
    "                                                    random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise pixels from [0, 255] to [0, 1]\n",
    "X_train, X_test = X_train/255., X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = X_test.shape[0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                 test_size=VALIDATION_SIZE, stratify=y_train.argmax(axis=1),\n",
    "                                                 shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\n10 balanced classes â†’ 10% baseline accuracy."
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7000., 7000., 7000., 7000., 7000., 7000., 7000., 7000., 7000.,\n",
       "       7000.], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples per each class\n",
    "counts = np.sum(y, axis=0)\n",
    "\n",
    "# the dataset is balanced. one class is slightly less than the others, but minimal.\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = max(counts) / sum(counts)\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "source": "INPUT_DIMENSION = X_train.shape[1]\nOUTPUT_CLASSES = y_train.shape[1]\n\nOPTIMIZER = 'RMSprop'\nLOSS_FUNC = 'categorical_crossentropy'\nMETRICS = ['categorical_accuracy', \n           tf.keras.metrics.Precision(name='precision'), \n           tf.keras.metrics.Recall(name='recall'),\n           tf.keras.metrics.AUC(name='auc', multi_label=True)]\n\nbatch_size = 128\nEPOCHS = 400",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learning_rate = 0.25\n\n# SLP: Single Layer Perceptron (no hidden layers)\nslp_model = Sequential([\n    Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(INPUT_DIMENSION,))\n], name='Single_Layer_Perceptron')\n\nslp_model.compile(\n    optimizer=RMSprop(learning_rate=learning_rate),\n    loss=LOSS_FUNC,\n    metrics=METRICS\n)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "source": "history_slp = slp_model.fit(X_train, y_train, \n                            batch_size=batch_size, epochs=EPOCHS, \n                            validation_data=(X_val, y_val), \n                            verbose=0)\nslp_val_score = slp_model.evaluate(X_val, y_val, verbose=0)[1:]  # Skip loss, get metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(slp_val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(slp_val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(slp_val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(slp_val_score[3]))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def plot_training_history(history, monitors=['loss', 'AUC']) :\n\n  # using the variable axs for multiple Axes\n  fig, axs = plt.subplots(1, 2, sharex='all', figsize=(15,5))\n \n  for ax, monitor in zip(axs.flat, monitors) :\n    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n\n    if monitor == 'loss' :\n        monitor = monitor.capitalize()\n\n    epochs = range(1, len(loss)+1)\n\n    ax.plot(epochs, loss, 'b.', label=monitor)\n    ax.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n    ax.set_xlim([0, len(loss)])\n    ax.title.set_text('Training and Validation ' + monitor + 's')\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel(monitor)\n    ax.legend()\n    ax.grid()\n\n  _ = plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plot_training_history(history_slp, monitors=['loss', 'auc'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding hidden layers to capture fashion item patterns."
  },
  {
   "cell_type": "code",
   "source": "learning_rate = 0.01\n\n# MLP: Multi-Layer Perceptron (1 hidden layer with 8 neurons)\nmlp_model = Sequential([\n    Dense(8, activation='relu', input_shape=(INPUT_DIMENSION,)),\n    Dense(OUTPUT_CLASSES, activation='softmax')\n], name='Multi_Layer_Perceptron')\n\nmlp_model.compile(\n    optimizer=RMSprop(learning_rate=learning_rate),\n    loss=LOSS_FUNC,\n    metrics=METRICS\n)\n\nmlp_model.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "history_mlp = mlp_model.fit(X_train, y_train, \n                            batch_size=batch_size, epochs=EPOCHS, \n                            validation_data=(X_val, y_val), \n                            verbose=0)\nmlp_val_score = mlp_model.evaluate(X_val, y_val, verbose=0)[1:]  # Skip loss, get metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plot_training_history(history_mlp, monitors=['loss', 'auc'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(mlp_val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(mlp_val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(mlp_val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(mlp_val_score[3]))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nUsing **Hyperband** for efficient hyperparameter tuning with a frozen architecture.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Hyperband Model Builder for Multi-Class Classification\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Fashion MNIST model with FROZEN architecture (2 layers: 32 -> 16 neurons).\n    Only tunes regularization (Dropout) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # Fixed architecture: 2 hidden layers with 32 and 16 neurons\n    # Layer 1: 32 neurons\n    model.add(layers.Dense(32, activation='relu'))\n    drop_0 = hp.Float('drop_0', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_0))\n\n    # Layer 2: 16 neurons\n    model.add(layers.Dense(16, activation='relu'))\n    drop_1 = hp.Float('drop_1', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_1))\n\n    # Output layer for multi-class classification\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Configure Hyperband tuner\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_categorical_accuracy',\n    max_epochs=20,\n    factor=3,\n    directory='fashion_mnist_hyperband',\n    project_name='fashion_mnist_tuning'\n)\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=batch_size\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Get best hyperparameters and build best model\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  Dropout Layer 1: {best_hp.get('drop_0')}\")\nprint(f\"  Dropout Layer 2: {best_hp.get('drop_1')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr')}\")\n\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train the best model\nhistory_opt = opt_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=50,\n    batch_size=batch_size,\n    verbose=1\n)\n\nopt_val_score = opt_model.evaluate(X_val, y_val, verbose=0)[1:]  # Skip loss, get metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preds = opt_model.predict(X_test, verbose=0)\n\nprint('Accuracy (Test): {:.2f} (baseline={:.2f})'.format(accuracy_score(y_test.argmax(axis=1), \n                                                                      preds.argmax(axis=1)), baseline))\nprint('Precision (Test): {:.2f}'.format(precision_score(y_test.argmax(axis=1), \n                                                        preds.argmax(axis=1),\n                                                        average='macro')))\nprint('Recall (Test): {:.2f}'.format(recall_score(y_test.argmax(axis=1), \n                                                  preds.argmax(axis=1),\n                                                  average='macro')))\nprint('AUC (Test): {:.2f}'.format(roc_auc_score(y_test, \n                                                preds,\n                                                multi_class='ovo',\n                                                average='macro')))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "disp = ConfusionMatrixDisplay( confusion_matrix(y_test.argmax(axis=1), preds.argmax(axis=1)),\n                               display_labels=np.sort(np.unique(labels)) )\n_ = disp.plot()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def top_n_accuracy_score(y, preds, n) :\n    scores = []\n    for j in range(preds.shape[0]) :\n        score = 1 if y[j,:].argmax() in preds[j,:].argsort()[-n:] else 0\n        \n        scores.append( score )\n            \n    return sum(scores) / len(scores)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "TOP_N = 3\nwithin = top_n_accuracy_score(y_test, preds, TOP_N)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Key Takeaways\n\n1. **Grayscale simplifies preprocessing** - no color conversion needed\n2. **Balanced classes** allow standard accuracy as primary metric\n3. **Top-N accuracy** useful for multi-class evaluation (was the correct class in top 3?)\n4. **Fashion MNIST** is harder than digit MNIST but same workflow applies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Top N Accuracy (Test Data): 0.98\n"
     ]
    }
   ],
   "source": [
    "print('Within Top N Accuracy (Test Data): {:.2f}'.format(within))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Making the Code More Modular\n\nThe following helper functions encapsulate model building and training for reuse across projects.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_classification_model(input_dimension, output_classes, hidden_layers=0, hidden_neurons=32, \n                               activation='relu', dropout=None, optimizer='rmsprop', \n                               loss='categorical_crossentropy', metrics=['accuracy'], name=None):\n    \"\"\"\n    Build a classification model with configurable architecture.\n    \n    Parameters:\n    -----------\n    input_dimension : int\n        Number of input features\n    output_classes : int\n        Number of output classes\n    hidden_layers : int\n        Number of hidden layers (0 for SLP, >0 for MLP)\n    hidden_neurons : int or list\n        Neurons per hidden layer (int applies to all, list for per-layer)\n    activation : str\n        Activation function for hidden layers\n    dropout : float or None\n        Dropout rate (None = no dropout)\n    optimizer : str or optimizer instance\n        Optimizer for training\n    loss : str\n        Loss function\n    metrics : list\n        Metrics to track\n    name : str\n        Model name\n    \n    Returns:\n    --------\n    Compiled Keras Sequential model\n    \"\"\"\n    model = Sequential(name=name)\n    \n    # Handle neurons as int or list\n    if isinstance(hidden_neurons, int):\n        neurons_list = [hidden_neurons] * hidden_layers\n    else:\n        neurons_list = hidden_neurons\n    \n    # Add hidden layers\n    for i, neurons in enumerate(neurons_list):\n        if i == 0:\n            model.add(Dense(neurons, activation=activation, input_shape=(input_dimension,)))\n        else:\n            model.add(Dense(neurons, activation=activation))\n        \n        if dropout is not None:\n            model.add(Dropout(dropout))\n    \n    # Output layer\n    if hidden_layers == 0:\n        model.add(Dense(output_classes, activation='softmax', input_shape=(input_dimension,)))\n    else:\n        model.add(Dense(output_classes, activation='softmax'))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=100, \n                callbacks=None, verbose=0):\n    \"\"\"\n    Train a model and return training history and validation scores.\n    \n    Parameters:\n    -----------\n    model : Keras model\n        Compiled model to train\n    X_train, y_train : arrays\n        Training data\n    X_val, y_val : arrays\n        Validation data\n    batch_size : int\n        Batch size for training\n    epochs : int\n        Number of training epochs\n    callbacks : list or None\n        Keras callbacks\n    verbose : int\n        Verbosity level\n    \n    Returns:\n    --------\n    dict with 'history' and 'val_score'\n    \"\"\"\n    history = model.fit(\n        X_train, y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=(X_val, y_val),\n        callbacks=callbacks,\n        verbose=verbose\n    )\n    \n    val_score = model.evaluate(X_val, y_val, verbose=0)[1:]  # Skip loss, get metrics\n    \n    return {\n        'history': history,\n        'val_score': val_score\n    }\n\n\n# Example usage (commented out):\n# ----------------------------------\n# # Build an SLP (no hidden layers)\n# slp = build_classification_model(\n#     input_dimension=256, \n#     output_classes=10,\n#     hidden_layers=0,\n#     optimizer=RMSprop(learning_rate=0.25),\n#     loss='categorical_crossentropy',\n#     metrics=METRICS,\n#     name='SLP_Model'\n# )\n#\n# # Build an MLP (with hidden layers)\n# mlp = build_classification_model(\n#     input_dimension=256,\n#     output_classes=10,\n#     hidden_layers=2,\n#     hidden_neurons=[32, 16],\n#     activation='relu',\n#     dropout=0.2,\n#     optimizer='adam',\n#     loss='categorical_crossentropy',\n#     metrics=METRICS,\n#     name='MLP_Model'\n# )\n#\n# # Train the model\n# result = train_model(mlp, X_train, y_train, X_val, y_val, \n#                      batch_size=128, epochs=100)\n# print(f\"Validation Accuracy: {result['val_score'][0]:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}