{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Bike%20Sharing/Bike%20Sharing%20-%20Regression%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Bike Sharing - Regression Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to a **regression problem** - predicting continuous numerical values instead of categories.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply neural networks to **regression** (predicting continuous values)\n- Understand key differences between regression and classification:\n  - Output layer: Linear activation vs. Softmax/Sigmoid\n  - Loss function: MSE/MAE vs. Cross-entropy\n  - Metrics: MAE, R² vs. Accuracy, Precision, Recall\n- Handle mixed feature types (categorical + numerical) for regression problems\n- Use **Hyperband** for efficient hyperparameter tuning\n- Apply **Dropout + L2 regularisation** to prevent overfitting\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [UCI Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) |\n| **Problem Type** | Regression |\n| **Target Variable** | `cnt` - Total bike rental count |\n| **Data Type** | Structured Tabular (Mixed Categorical & Numerical) |\n| **Features** | Weather, date/time, and environmental variables |\n| **Target Range** | 22 to 8,714 rentals per day |\n\n---\n\n## Technique Scope\n\nThis notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n\n| Technique | Status | Rationale |\n|-----------|--------|-----------|\n| **Dense layers (DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n\nWe demonstrate that **Dropout + L2 regularisation** alone can effectively prevent overfitting without requiring early stopping.\n\n---\n\n## Regression vs. Classification\n\n| Aspect | Regression | Classification |\n|--------|------------|----------------|\n| **Output** | Continuous number (e.g., 542 bikes) | Category (e.g., \"spam\") |\n| **Output Activation** | Linear (none) | Softmax/Sigmoid |\n| **Loss Function** | MSE, MAE, Huber | Cross-entropy |\n| **Metrics** | MAE, R² | Accuracy, F1, AUC |\n| **Class Weights** | Not applicable | Used for imbalanced data |\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Problem and Assembling a Dataset\n",
    "\n",
    "The first step in any machine learning project is to clearly define the problem and understand the data.\n",
    "\n",
    "**Problem Statement:** Predict the total number of bike rentals for a given day based on weather and calendar features.\n",
    "\n",
    "**Why this problem is interesting:**\n",
    "- **Business value:** Bike sharing companies need to plan bike distribution across stations\n",
    "- **Operational planning:** Accurate demand prediction helps with maintenance scheduling\n",
    "- **Feature richness:** Multiple factors (weather, holidays, seasons) affect demand\n",
    "\n",
    "**Data Source:** This dataset contains 2 years of daily bike rental data from a Capital Bikeshare system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Regression Metrics\n\nFor regression problems, we use different metrics than classification:\n\n| Metric | Formula | Interpretation | When to Use |\n|--------|---------|----------------|-------------|\n| **MAE** | Mean(\\|y - ŷ\\|) | Average error in original units | Primary metric - interpretable |\n| **R²** | 1 - SS_res/SS_tot | Proportion of variance explained (0 to 1) | Secondary metric - normalised comparison |\n\n**Why MAE as our primary metric?**\n- It's interpretable: \"On average, we're off by X bikes\"\n- It's robust to outliers\n- It's in the same units as the target variable\n\n**Why R² as secondary metric?**\n- Provides a normalised view (0-1 scale) useful for comparing across different datasets\n- Shows what proportion of variance the model explains\n\n### References\n\n- Chai, T. and Draxler, R.R. (2014) 'Root mean square error (RMSE) or mean absolute error (MAE)?', *Geoscientific Model Development*, 7(3), pp. 1247–1250.\n\n- Willmott, C.J. and Matsuura, K. (2005) 'Advantages of the mean absolute error (MAE) over the root mean square error (RMSE)', *Climate Research*, 30(1), pp. 79–82."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\nThe choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n\n### Data Split Strategy (This Notebook)\n\nWith only **731 samples**, this dataset is below the 10,000 threshold, so we use **K-Fold Cross-Validation**:\n\n```\nOriginal Data (731 samples)\n├── Test Set (10% = ~73 samples) - Final evaluation only\n└── Training Pool (90% = ~658 samples)\n    └── 5-Fold Cross-Validation\n        ├── Fold 1: Train on folds 2-5, validate on fold 1\n        ├── Fold 2: Train on folds 1,3-5, validate on fold 2\n        ├── ...\n        └── Fold 5: Train on folds 1-4, validate on fold 5\n```\n\n**Why K-Fold for small datasets?**\n- Reduces variance in performance estimates\n- Uses all data for both training and validation\n- More reliable model comparison\n\n**Important:** For regression, we don't use `stratify` (no classes to balance). We use random shuffling.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries and Set Random Seed\n",
    "\n",
    "We set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n%pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load and Explore the Dataset\n",
    "\n",
    "Let's download the bike sharing data from Google Drive and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data directly from Google Drive\n",
    "GDRIVE_FILE_ID = '1H2d1H5ASQis4FcJSyh1REvSTdU2TVerk'\n",
    "DATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\n",
    "\n",
    "df = pd.read_csv(DATA_URL)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "NUMERICAL_VARIABLES = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "CATEGORICAL_VARIABLES = ['season', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "\n",
    "TARGET_VARIABLE = 'cnt'\n",
    "\n",
    "# Examine target variable distribution\n",
    "print(f\"Target variable: {TARGET_VARIABLE}\")\n",
    "print(f\"  Min: {df[TARGET_VARIABLE].min():,}\")\n",
    "print(f\"  Max: {df[TARGET_VARIABLE].max():,}\")\n",
    "print(f\"  Mean: {df[TARGET_VARIABLE].mean():,.0f}\")\n",
    "print(f\"  Std: {df[TARGET_VARIABLE].std():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Define Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[NUMERICAL_VARIABLES + CATEGORICAL_VARIABLES]\n",
    "target = df[TARGET_VARIABLE]\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Split Data into Train and Test Sets\n",
    "\n",
    "We reserve 10% of the data for final testing. Note: For regression, we don't use `stratify` (no classes to balance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_full, y_test = train_test_split(\n",
    "    features, target, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=True, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_raw):,} samples\")\n",
    "print(f\"Test set: {len(X_test_raw):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Preprocessing with ColumnTransformer\n",
    "\n",
    "We use `ColumnTransformer` to apply different preprocessing to different feature types:\n",
    "\n",
    "```\n",
    "ColumnTransformer\n",
    "├── Categorical Features → One-Hot Encoding\n",
    "│   (Creates binary columns for each category)\n",
    "└── Numerical Features → Standard Scaling\n",
    "    (Mean=0, Std=1)\n",
    "```\n",
    "\n",
    "**Important:** We fit the preprocessor only on training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_VARIABLES),\n",
    "    ('standard_scaler', StandardScaler(), NUMERICAL_VARIABLES)\n",
    "])\n",
    "\n",
    "# Fit on training data only (prevent data leakage)\n",
    "preprocessor.fit(X_train_raw)\n",
    "\n",
    "# Transform both sets\n",
    "X_train_full = preprocessor.transform(X_train_raw)\n",
    "X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "# Convert target to numpy arrays\n",
    "y_train_full = y_train_full.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(f\"Preprocessed training shape: {X_train_full.shape}\")\n",
    "print(f\"Preprocessed test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "Before building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n",
    "\n",
    "### 5.1 Examine Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DATA-DRIVEN ANALYSIS: Dataset Size & Target Distribution\n# =============================================================================\n\n# Dataset size analysis\nn_samples = len(df)\nHOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000\n\n# Target distribution analysis\ntarget_mean = target.mean()\ntarget_std = target.std()\ntarget_range = target.max() - target.min()\n\nprint(\"=\" * 60)\nprint(\"DATA-DRIVEN CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\nprint(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples\")\nprint(f\"   Decision: {'Hold-Out' if n_samples > HOLDOUT_THRESHOLD else 'K-Fold Cross-Validation'}\")\n\nprint(f\"\\n2. TARGET DISTRIBUTION:\")\nprint(f\"   Mean: {target_mean:,.0f} bikes/day\")\nprint(f\"   Std: {target_std:,.0f} bikes/day\")\nprint(f\"   Range: {target.min():,} to {target.max():,}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PRIMARY METRIC: MAE (interpretable in original units)\")\nprint(\"VALIDATION: 5-Fold Cross-Validation\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate Baseline Metrics\n",
    "\n",
    "**Regression Baselines:**\n",
    "- **Mean Baseline:** Always predict the mean of training data\n",
    "- This gives us a reference MAE to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Baseline: always predict the mean\nbaseline_prediction = y_train_full.mean()\nbaseline_mae = mean_absolute_error(y_train_full, np.full_like(y_train_full, baseline_prediction))\n\nprint(f\"Baseline (always predict mean = {baseline_prediction:,.0f}):\")\nprint(f\"  MAE: {baseline_mae:,.0f} bikes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Configure K-Fold Cross-Validation\n\nSince our dataset has only 731 samples (below the 10,000 threshold), we use **5-Fold Cross-Validation** instead of a simple hold-out validation split."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# K-FOLD CROSS-VALIDATION SETUP\n# =============================================================================\nN_FOLDS = 5\n\nkfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nprint(f\"K-Fold Configuration:\")\nprint(f\"  Number of folds: {N_FOLDS}\")\nprint(f\"  Training pool: {X_train_full.shape[0]:,} samples\")\nprint(f\"  Samples per fold: ~{X_train_full.shape[0] // N_FOLDS:,}\")\nprint(f\"  Test set (held out): {X_test.shape[0]:,} samples\")\n\n# For initial model development, we use the first fold\n# Final evaluation will use all folds\nfirst_fold = list(kfold.split(X_train_full))[0]\ntrain_idx, val_idx = first_fold\n\nX_train = X_train_full[train_idx]\nX_val = X_train_full[val_idx]\ny_train = y_train_full[train_idx]\ny_val = y_train_full[val_idx]\n\nprint(f\"\\nFirst fold (for initial development):\")\nprint(f\"  Training: {X_train.shape[0]:,} samples\")\nprint(f\"  Validation: {X_val.shape[0]:,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Configure Training Parameters\n",
    "\n",
    "**Key training settings for regression:**\n",
    "- **Optimiser:** Adam - adaptive learning rate optimiser\n",
    "- **Loss:** MSE (Mean Squared Error) - standard loss for regression\n",
    "- **Output Activation:** Linear (none) - allows any real number output\n",
    "- **Primary Metric:** MAE - computed separately after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_DIMENSION = 1  # Regression: single continuous output\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNC = 'mse'  # Mean Squared Error for regression\n",
    "\n",
    "# Training metrics (tracked by Keras during training)\n",
    "# Note: MAE (our primary metric) is also tracked for monitoring\n",
    "METRICS = ['mae']\n",
    "\n",
    "print(f\"Input dimension: {INPUT_DIMENSION}\")\n",
    "print(f\"Output dimension: {OUTPUT_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Layer Perceptron (no hidden layers) - Linear Regression Baseline\n",
    "# Note: No activation on output layer = linear output for regression\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "slp_model.add(Dense(OUTPUT_DIMENSION))  # Linear activation (default)\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 32  # Smaller batch size for small dataset (731 samples)\n",
    "\n",
    "# We use DIFFERENT epoch counts for different training phases:\n",
    "#\n",
    "# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n",
    "#   - SLP converges quickly (simple model)\n",
    "#   - Unregularised DNN: 100 epochs clearly shows overfitting\n",
    "#\n",
    "# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2\n",
    "#   - Regularisation slows down learning\n",
    "#   - With regularisation, longer training is SAFE (no overfitting risk)\n",
    "\n",
    "EPOCHS_BASELINE = 100      # SLP and DNN (no regularisation)\n",
    "EPOCHS_REGULARIZED = 150   # DNN with Dropout + L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Single-Layer Perceptron (Linear Regression)\n",
    "history_slp = slp_model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "val_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display SLP validation metrics\npreds_slp_val = slp_model.predict(X_val, verbose=0).flatten()\nmae_slp_val = mean_absolute_error(y_val, preds_slp_val)\nr2_slp_val = r2_score(y_val, preds_slp_val)\n\nprint('MAE (Validation): {:.0f} bikes (baseline={:.0f})'.format(mae_slp_val, baseline_mae))\nprint('R² (Validation): {:.4f}'.format(r2_slp_val))\nprint(f'\\nMAE: {mae_slp_val:.0f}  ← Primary Metric')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs for regression.\n",
    "    Plots: (1) Loss (MSE), (2) MAE\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras History object\n",
    "        Training history from model.fit()\n",
    "    title : str, optional\n",
    "        Model name to display in plot titles\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    title_suffix = f' ({title})' if title else ''\n",
    "\n",
    "    # Plot 1: Loss (MSE)\n",
    "    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[0].set_title(f'Loss (MSE){title_suffix}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('MSE')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: MAE\n",
    "    axs[1].plot(epochs, history.history['mae'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[1].plot(epochs, history.history['val_mae'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[1].set_title(f'MAE{title_suffix}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('MAE (bikes)')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SLP training history\n",
    "plot_training_history(history_slp, title='SLP Baseline (Linear Regression)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nThe next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n\n**Strategy:** Add hidden layers to capture non-linear relationships between features and bike demand.\n\n**No regularisation applied:** We intentionally train this model **without any regularisation** to observe overfitting behaviour.\n\n---\n\n### Architecture Design Decisions\n\n**Why 64 neurons in the hidden layer?**\n\nThis balances capacity and efficiency for our small dataset (731 samples):\n- **Too few (e.g., 8):** May not capture non-linear patterns in weather/demand relationships\n- **Too many (e.g., 256):** High overfitting risk with small data\n- **64 neurons:** Reasonable capacity for tabular regression tasks\n\n**Why only 1 hidden layer instead of 2-3?**\n\nPer the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n\n1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation\n\n*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*\n\n### 6.1 Build a Deep Neural Network (DNN)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network (1 hidden layer, no regularisation for overfitting demo)\n",
    "dnn_model = Sequential(name='Deep_Neural_Network')\n",
    "dnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dense(OUTPUT_DIMENSION))  # Linear activation for regression\n",
    "dnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Deep Neural Network (without regularisation to demonstrate overfitting)\n",
    "history_dnn = dnn_model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val), \n",
    "    verbose=0\n",
    ")\n",
    "val_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DNN training history (expect overfitting: val_loss increasing)\n",
    "plot_training_history(history_dnn, title='DNN - No Regularisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display DNN validation metrics\npreds_dnn_val = dnn_model.predict(X_val, verbose=0).flatten()\nmae_dnn_val = mean_absolute_error(y_val, preds_dnn_val)\nr2_dnn_val = r2_score(y_val, preds_dnn_val)\n\nprint('MAE (Validation): {:.0f} bikes (baseline={:.0f})'.format(mae_dnn_val, baseline_mae))\nprint('R² (Validation): {:.4f}'.format(r2_dnn_val))\nprint(f'\\nMAE: {mae_dnn_val:.0f}  ← Primary Metric')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Now we address the overfitting observed in Section 6 by adding **regularisation**. We use two complementary techniques:\n",
    "\n",
    "| Technique | How it works | Effect |\n",
    "|-----------|--------------|--------|\n",
    "| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging |\n",
    "| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small |\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning.\n",
    "\n",
    "### 7.1 Hyperband Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperband Model Builder for Regression\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build Bike Sharing model with FIXED architecture (1 hidden layer, 64 neurons).\n",
    "    Same architecture as Section 6 DNN - only tunes regularisation and learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "\n",
    "    # L2 regularisation strength\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
    "\n",
    "    # Fixed architecture: 1 hidden layer with 64 neurons (same as Section 6)\n",
    "    model.add(layers.Dense(64, activation='relu', \n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer for regression (linear activation)\n",
    "    model.add(layers.Dense(OUTPUT_DIMENSION))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hyperband tuner\n",
    "# Objective: minimise validation loss (MSE)\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_hyperband,\n",
    "    objective='val_loss',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='bike_hyperband',\n",
    "    project_name='bike_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"Tuning objective: val_loss (MSE)\")\n",
    "print(\"(Note: Final evaluation uses MAE as primary metric)\")\n",
    "\n",
    "# Run Hyperband search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters found by Hyperband:\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.2 Sanity Check and Final Retraining\n\nAfter finding the best hyperparameters, we follow a two-step process:\n\n1. **Sanity Check:** Retrain with the best hyperparameters using training and validation data to visually confirm the model is not overfitting. This validates that Hyperband found hyperparameters that generalise well.\n\n2. **Final Refit:** Combine training and validation sets and retrain without validation. Since the hyperparameters have been validated, we maximise the data available for the final model.\n\n---\n\n#### Why This Two-Step Approach?\n\n| Step | Purpose | Validation Data |\n|------|---------|-----------------|\n| **Sanity Check** | Confirm hyperparameters prevent overfitting | ✓ Used for monitoring |\n| **Final Refit** | Maximise training data for production model | ✗ Merged into training |\n\nOnce the sanity check confirms no overfitting, we can confidently combine all available data for the final model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# STEP 1: SANITY CHECK - Retrain with validation to confirm no overfitting\n# =============================================================================\n\n# Extract the number of epochs from the best trial\nbest_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\nbest_epochs = best_trial.best_step + 1  # best_step is 0-indexed\n\nprint(\"=\" * 60)\nprint(\"SANITY CHECK: Retraining with Validation\")\nprint(\"=\" * 60)\nprint(f\"Training for {best_epochs} epochs (matched from Hyperband's best trial)\")\nprint(\"Purpose: Visually confirm the hyperparameters prevent overfitting\\n\")\n\n# Build a fresh model with the best hyperparameters\nsanity_model = tuner.hypermodel.build(best_hp)\n\nhistory_sanity = sanity_model.fit(\n    X_train, y_train,\n    epochs=best_epochs,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val, y_val),  # Include validation for monitoring\n    verbose=0\n)\n\nprint(\"Sanity check training complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot sanity check training history (with validation curves)\nplot_training_history(history_sanity, title=f'Sanity Check - Best Hyperparameters ({best_epochs} epochs)')\n\n# Verify no overfitting: validation loss should not increase significantly\nval_losses = history_sanity.history['val_loss']\nmin_val_loss_epoch = val_losses.index(min(val_losses)) + 1\nfinal_val_loss = val_losses[-1]\nmin_val_loss = min(val_losses)\n\nprint(f\"\\nSanity Check Results:\")\nprint(f\"  Minimum validation loss: {min_val_loss:.4f} at epoch {min_val_loss_epoch}\")\nprint(f\"  Final validation loss: {final_val_loss:.4f}\")\nif final_val_loss <= min_val_loss * 1.1:  # Within 10% of minimum\n    print(\"  ✓ No significant overfitting detected - hyperparameters are validated\")\nelse:\n    print(\"  ⚠ Some overfitting detected - consider adjusting epochs\")"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# STEP 2: FINAL REFIT - Combine data and retrain for production\n# =============================================================================\n\n# Combine training and validation sets for final model\nX_combined = np.vstack([X_train, X_val])\ny_combined = np.concatenate([y_train, y_val])\n\nprint(\"=\" * 60)\nprint(\"FINAL REFIT: Training on Combined Data\")\nprint(\"=\" * 60)\nprint(f\"Training data: {X_train.shape[0]:,} samples\")\nprint(f\"Validation data: {X_val.shape[0]:,} samples\")\nprint(f\"Combined data: {X_combined.shape[0]:,} samples\")\nprint(f\"  → {(X_combined.shape[0] / X_train.shape[0] - 1) * 100:.1f}% more training data\")\n\n# Build and train final model on combined data\nprint(f\"\\nRetraining for {best_epochs} epochs on combined data...\")\n\nbest_model = tuner.hypermodel.build(best_hp)\n\nbest_model.fit(\n    X_combined, y_combined,\n    epochs=best_epochs,\n    batch_size=BATCH_SIZE,\n    verbose=0\n    # No validation_data - merged into training\n    # No plotting needed - sanity check already validated the hyperparameters\n)\n\nprint(\"\\n✓ Final model training complete on combined dataset.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# K-FOLD CROSS-VALIDATION EVALUATION\n# =============================================================================\ndef evaluate_with_kfold(build_fn, X, y, kfold, epochs, batch_size):\n    \"\"\"\n    Evaluate a model using K-Fold cross-validation.\n    \n    Parameters:\n    -----------\n    build_fn : callable\n        Function that returns a compiled Keras model\n    X, y : array-like\n        Full training data\n    kfold : KFold\n        Configured KFold splitter\n    epochs : int\n        Training epochs per fold\n    batch_size : int\n        Batch size for training\n    \n    Returns:\n    --------\n    dict : Mean and std of metrics across folds\n    \"\"\"\n    fold_metrics = {'mae': [], 'r2': []}\n    \n    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n        # Split data for this fold\n        X_train_fold = X[train_idx]\n        X_val_fold = X[val_idx]\n        y_train_fold = y[train_idx]\n        y_val_fold = y[val_idx]\n        \n        # Build fresh model for each fold\n        model = build_fn()\n        \n        # Train\n        model.fit(\n            X_train_fold, y_train_fold,\n            validation_data=(X_val_fold, y_val_fold),\n            epochs=epochs, batch_size=batch_size, verbose=0\n        )\n        \n        # Evaluate\n        preds = model.predict(X_val_fold, verbose=0).flatten()\n        fold_metrics['mae'].append(mean_absolute_error(y_val_fold, preds))\n        fold_metrics['r2'].append(r2_score(y_val_fold, preds))\n        \n        print(f\"  Fold {fold+1}: MAE={fold_metrics['mae'][-1]:.0f}\")\n    \n    return {\n        'mae_mean': np.mean(fold_metrics['mae']),\n        'mae_std': np.std(fold_metrics['mae']),\n        'r2_mean': np.mean(fold_metrics['r2']),\n        'r2_std': np.std(fold_metrics['r2'])\n    }\n\n# Build function using best hyperparameters\ndef build_best_model():\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n    model.add(layers.Dense(64, activation='relu', \n                           kernel_regularizer=regularizers.l2(best_hp.get('l2_reg'))))\n    model.add(layers.Dropout(best_hp.get('dropout')))\n    model.add(layers.Dense(OUTPUT_DIMENSION))\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=best_hp.get('lr')),\n        loss=LOSS_FUNC, metrics=METRICS\n    )\n    return model\n\nprint(\"Evaluating best model with 5-Fold Cross-Validation...\")\nkfold_results = evaluate_with_kfold(\n    build_best_model, X_train_full, y_train_full, \n    kfold, EPOCHS_REGULARIZED, BATCH_SIZE\n)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"K-FOLD CROSS-VALIDATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"MAE:  {kfold_results['mae_mean']:.0f} ± {kfold_results['mae_std']:.0f} bikes\")\nprint(f\"R²:   {kfold_results['r2_mean']:.4f} ± {kfold_results['r2_std']:.4f}\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.3 Final Model Training and Test Evaluation\n\nNow we train the final model on the **entire training pool** (all folds combined) and evaluate on the held-out test set."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# FINAL MODEL: Train on ALL training data, evaluate on test set\n# =============================================================================\n# Build final model with best hyperparameters\nfinal_model = build_best_model()\n\n# Train on entire training pool\nfinal_model.fit(\n    X_train_full, y_train_full,\n    epochs=EPOCHS_REGULARIZED,\n    batch_size=BATCH_SIZE,\n    verbose=0\n)\n\n# Evaluate on held-out test set\npreds_test = final_model.predict(X_test, verbose=0).flatten()\n\ntest_mae = mean_absolute_error(y_test, preds_test)\ntest_r2 = r2_score(y_test, preds_test)\n\nprint('=' * 50)\nprint('FINAL TEST SET RESULTS')\nprint('=' * 50)\nprint(f'MAE (Test): {test_mae:.0f} bikes  ← Primary Metric')\nprint(f'R² (Test): {test_r2:.4f}')\nprint(f'\\nBaseline MAE: {baseline_mae:.0f} bikes')\nprint(f'Improvement over baseline: {(baseline_mae - test_mae) / baseline_mae * 100:.1f}%')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions vs actual\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "axs[0].scatter(y_test, preds_test, alpha=0.6, edgecolor='k', linewidth=0.5)\n",
    "axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axs[0].set_xlabel('Actual Bike Rentals')\n",
    "axs[0].set_ylabel('Predicted Bike Rentals')\n",
    "axs[0].set_title('Predicted vs Actual (Test Set)')\n",
    "axs[0].legend()\n",
    "axs[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals = y_test - preds_test\n",
    "axs[1].scatter(preds_test, residuals, alpha=0.6, edgecolor='k', linewidth=0.5)\n",
    "axs[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axs[1].set_xlabel('Predicted Bike Rentals')\n",
    "axs[1].set_ylabel('Residual (Actual - Predicted)')\n",
    "axs[1].set_title('Residual Plot (Test Set)')\n",
    "axs[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Results Summary\n",
    "\n",
    "The following dynamically-generated table compares all models trained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RESULTS SUMMARY\n# =============================================================================\n\n# Create results DataFrame\nresults = pd.DataFrame({\n    'Model': ['Naive Baseline', 'SLP (Linear)', 'DNN (No Reg)', f'DNN (K-Fold, {N_FOLDS} folds)', 'DNN (Dropout + L2) - Test'],\n    'MAE': [baseline_mae, mae_slp_val, mae_dnn_val, kfold_results['mae_mean'], test_mae],\n    'R²': [0.0, r2_slp_val, r2_dnn_val, kfold_results['r2_mean'], test_r2],\n    'Dataset': ['N/A', 'Fold 1', 'Fold 1', 'K-Fold CV', 'Test']\n})\n\nprint(\"=\" * 80)\nprint(\"MODEL COMPARISON - RESULTS SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"Primary Metric: MAE (regression task)\")\nprint(\"=\" * 80)\nprint(results.to_string(index=False, float_format='{:.2f}'.format))\nprint(\"=\" * 80)\nprint(f\"\\nKey Observations:\")\nprint(f\"  - All models significantly outperform naive baseline (MAE: {baseline_mae:.0f} bikes)\")\nprint(f\"  - Final model trained on entire training pool ({X_train_full.shape[0]:,} samples)\")\nprint(f\"  - K-Fold CV MAE: {kfold_results['mae_mean']:.0f} ± {kfold_results['mae_std']:.0f} bikes\")\nprint(f\"  - Final test MAE: {test_mae:.0f} bikes, R²: {test_r2:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | 731 samples | **K-Fold (5 folds)** | Kohavi (1995) |\n| **Primary Metric** | Regression | Continuous target | **MAE** | Interpretable in original units |\n| **Class Weights** | Classification only | N/A (regression) | **Not applicable** | - |\n\n### Lessons Learned\n\n1. **Regression Uses Different Metrics:** MAE and R² replace accuracy/F1-score. The naive baseline predicts the mean.\n\n2. **Same Architecture Works:** The Universal ML Workflow applies equally to regression - just change the output layer to linear activation and the loss to MSE.\n\n3. **K-Fold for Small Datasets:** With 731 samples (below 10,000 threshold), K-Fold provides more robust metric estimates.\n\n4. **Feature Engineering Matters:** Proper preprocessing of categorical (one-hot) and numerical (scaling) features is essential.\n\n5. **Regularisation Prevents Overfitting:** L2 + Dropout controls overfitting without early stopping, even for regression tasks.\n\n6. **Maximise Data for Final Model:** After hyperparameter tuning, we combine training and validation sets for the final model. The validation set's job is done (model selection), so we use all available data to maximise learning.\n\n7. **R² Interpretation:** Values close to 1.0 indicate the model explains most variance; values near 0 indicate no better than the mean.\n\n### Regression vs Classification Summary\n\n| Aspect | Regression (This Notebook) | Classification |\n|--------|---------------------------|----------------|\n| **Output** | Continuous value | Discrete classes |\n| **Activation** | Linear (none) | Sigmoid/Softmax |\n| **Loss** | MSE | Cross-entropy |\n| **Metrics** | MAE, R² | Accuracy, F1, AUC |\n| **Baseline** | Mean prediction | Majority class |\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Fanaee-T, H. and Gama, J. (2014) 'Event labelling combining ensemble detectors and background knowledge', *Progress in Artificial Intelligence*, 2(2-3), pp. 113-127.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Modular Helper Functions\n",
    "\n",
    "For cleaner code organisation, you can wrap the model building and training patterns into reusable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MODULAR HELPER FUNCTIONS\n# =============================================================================\n\ndef build_regression_model(input_dim, hidden_units=None, dropout=0.0, l2_reg=0.0,\n                           optimizer='adam', loss='mse', \n                           metrics=['mae'], name=None):\n    \"\"\"\n    Build a regression neural network.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    hidden_units : list of int, optional\n        Neurons per hidden layer, e.g., [64] or [128, 64]\n        None or [] creates a single-layer perceptron (linear regression)\n    dropout : float\n        Dropout rate (0.0 to 0.5)\n    l2_reg : float\n        L2 regularisation strength\n    optimizer : str or keras.optimizers.Optimizer\n        Optimiser name or instance\n    loss : str\n        Loss function name (mse, mae, huber)\n    metrics : list\n        Metrics to track during training\n    name : str, optional\n        Model name\n        \n    Returns:\n    --------\n    keras.Sequential : Compiled model ready for training\n    \"\"\"\n    model = Sequential(name=name)\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    hidden_units = hidden_units or []\n    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n    \n    for units in hidden_units:\n        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n        if dropout > 0:\n            model.add(Dropout(dropout))\n    \n    # Output layer for regression (linear activation)\n    model.add(Dense(1))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val,\n                batch_size=32, epochs=100, verbose=0):\n    \"\"\"\n    Train a model and return training history.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Compiled Keras model\n    X_train, y_train : array-like\n        Training data and labels\n    X_val, y_val : array-like\n        Validation data and labels\n    batch_size : int\n        Training batch size\n    epochs : int\n        Number of training epochs\n    verbose : int\n        Verbosity mode\n        \n    Returns:\n    --------\n    keras.callbacks.History : Training history object\n    \"\"\"\n    return model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=batch_size, \n        epochs=epochs,\n        verbose=verbose\n    )\n\n\ndef evaluate_regression_model(model, X, y_true):\n    \"\"\"\n    Evaluate regression model.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Trained Keras model\n    X : array-like\n        Input features\n    y_true : array-like\n        True target values\n        \n    Returns:\n    --------\n    dict : Dictionary containing metrics (mae, r2)\n    \"\"\"\n    y_pred = model.predict(X, verbose=0).flatten()\n    \n    metrics = {\n        'mae': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n    }\n    \n    return metrics\n\n\n# =============================================================================\n# USAGE EXAMPLES\n# =============================================================================\n# \n# # Build models\n# slp = build_regression_model(INPUT_DIMENSION, name='SLP')\n# dnn = build_regression_model(INPUT_DIMENSION, hidden_units=[64], name='DNN')\n# dnn_reg = build_regression_model(INPUT_DIMENSION, hidden_units=[64], \n#                                  dropout=0.3, l2_reg=0.001, name='DNN_Regularized')\n# \n# # Train\n# history = train_model(dnn, X_train, y_train, X_val, y_val)\n# \n# # Evaluate\n# metrics = evaluate_regression_model(dnn, X_val, y_val)\n# print(f\"MAE: {metrics['mae']:.0f} bikes\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}