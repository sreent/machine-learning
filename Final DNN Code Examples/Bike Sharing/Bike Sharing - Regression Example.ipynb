{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Bike%20Sharing/Bike%20Sharing%20-%20Regression%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Bike Sharing - Regression Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to a **regression problem** - predicting continuous numerical values instead of categories.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply neural networks to **regression** (predicting continuous values)\n- Understand differences between regression and classification:\n  - Output layer: Linear activation vs. Softmax/Sigmoid\n  - Loss function: MSE/MAE vs. Cross-entropy\n  - Metrics: MAE, RMSE, R² vs. Accuracy, Precision, Recall\n- Handle mixed feature types for regression problems\n- Evaluate regression models with appropriate metrics\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [UCI Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) |\n| **Problem Type** | Regression |\n| **Target Variable** | `cnt` - Total bike rental count |\n| **Data Type** | Structured (Mixed Categorical & Numerical) |\n| **Features** | Weather, date/time, and environmental variables |\n\n---\n\n## Regression vs. Classification\n\n| Aspect | Regression | Classification |\n|--------|------------|----------------|\n| **Output** | Continuous number (e.g., 542 bikes) | Category (e.g., \"spam\") |\n| **Output Activation** | Linear (none) | Softmax/Sigmoid |\n| **Loss Function** | MSE, MAE, Huber | Cross-entropy |\n| **Metrics** | MAE, RMSE, R² | Accuracy, F1, AUC |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem Statement:** Predict the total number of bike rentals for a given day based on weather and calendar features.\n\n**Business Context:**\n- Bike sharing companies need to plan bike distribution across stations\n- Accurate demand prediction helps with maintenance scheduling\n- Understanding demand drivers informs business strategy",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n**Regression Metrics:**\n\n| Metric | Formula | Interpretation |\n|--------|---------|----------------|\n| **MAE** | Mean Absolute Error | Average prediction error in original units (bikes) |\n| **RMSE** | Root Mean Squared Error | Penalizes large errors more heavily |\n| **R²** | Coefficient of Determination | Proportion of variance explained (0 to 1) |\n\n**We'll use MAE as primary metric** - it's interpretable (\"on average, we're off by X bikes\")."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n- **Hold-out Test Set (10%)**: Final evaluation\n- **Validation Set**: Monitor training, early stopping\n- **K-Fold Cross-Validation**: Hyperparameter tuning\n\n**Note:** For regression, we don't use `stratify` - instead we shuffle randomly."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport itertools\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bike Sharing.csv', sep=',')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_VARIABLES = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "CATEGORICAL_VARIABLES = ['season', 'holiday', 'weekday', 'workingday', 'weathersit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[NUMERICAL_VARIABLES + CATEGORICAL_VARIABLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VARIABLE = 'cnt'\n",
    "\n",
    "target = df[TARGET_VARIABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=TEST_SIZE, \n",
    "                                                    random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_VARIABLES),\n",
    "    ('standard_scaler', StandardScaler(), NUMERICAL_VARIABLES)])\n",
    "\n",
    "_ = preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = preprocessor.transform(X_train), preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = X_test.shape[0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                 test_size=VALIDATION_SIZE,\n",
    "                                                 shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Developing a model that does better than a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\n**Regression Baselines:**\n- **Mean Baseline:** Always predict the mean of training data\n- **Linear Model:** Simple linear regression as a sanity check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "INPUT_DIMENSION = X_train.shape[1]\n\nLEARNING_RATE = 1\nLOSS_FUNC = 'mean_squared_error'\n\n# Compute baseline (mean predictor)\nbaseline = np.var(y_train) * len(y_train) / len(y_train)  # This is essentially the variance\nbaseline = np.mean((y_train - np.mean(y_train))**2)  # MSE of always predicting the mean"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a simple Single Layer Perceptron (no hidden layers)\n# For regression: linear activation (no activation) on output layer\nslp_model = Sequential([\n    Dense(1, input_shape=(INPUT_DIMENSION,))\n])\n\nslp_model._name = 'Single_Layer_Perceptron'\nslp_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE), \n                  loss=LOSS_FUNC)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 64\nEPOCHS = 100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the SLP model\nhistory_slp = slp_model.fit(\n    X_train, y_train,\n    batch_size=batch_size, \n    epochs=EPOCHS,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\nslp_val_score = slp_model.evaluate(X_val, y_val, verbose=0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Mean Squared Error (Validation): {:.2f} (baseline={:.2f})'.format(slp_val_score, baseline))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_training_history(history, monitor='loss'):\n    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n\n    if monitor == 'loss':\n        monitor = monitor.capitalize()\n\n    epochs = range(1, len(loss)+1)\n\n    plt.plot(epochs, loss, 'b.', label=monitor)\n    plt.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n    plt.xlim([0, len(loss)]) \n                              \n    plt.title('Training and Validation ' + monitor + 's')\n    plt.xlabel('Epochs')\n    plt.ylabel(monitor)\n    plt.legend()\n    plt.grid()\n    \n    _ = plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "outputs": [],
   "source": "plot_training_history(history_slp, monitor='loss')"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding hidden layers to capture non-linear relationships between features and bike demand."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "LEARNING_RATE = 0.01\nEPOCHS = 200\n\n# Build a Multi-Layer Perceptron with one hidden layer\nmlp_model = Sequential([\n    Dense(32, activation='relu', input_shape=(INPUT_DIMENSION,)),\n    Dense(1)  # Linear output for regression\n])\n\nmlp_model._name = 'Multi_Layer_Perceptron'\nmlp_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE), \n                  loss=LOSS_FUNC)\n\nmlp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the MLP model\nhistory_mlp = mlp_model.fit(\n    X_train, y_train,\n    batch_size=batch_size, \n    epochs=EPOCHS,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\nmlp_val_score = mlp_model.evaluate(X_val, y_val, verbose=0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Mean Squared Error (Validation): {:.2f} (baseline={:.2f})'.format(mlp_val_score, baseline))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_training_history(history_mlp, monitor='loss')"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nUsing **Hyperband** for efficient hyperparameter tuning with a frozen architecture.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations\n\nThis \"early stopping\" approach saves compute time while still exploring widely."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Hyperband Model Builder for Regression\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Bike Sharing model with FROZEN architecture (2 layers: 64 -> 32 neurons).\n    Only tunes regularization (Dropout) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # Fixed architecture: 2 hidden layers with 64 and 32 neurons\n    # Layer 1: 64 neurons\n    model.add(layers.Dense(64, activation='relu'))\n    drop_0 = hp.Float('drop_0', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_0))\n\n    # Layer 2: 32 neurons\n    model.add(layers.Dense(32, activation='relu'))\n    drop_1 = hp.Float('drop_1', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_1))\n\n    # Output layer for regression\n    model.add(layers.Dense(1))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_loss',\n    max_epochs=20,\n    factor=3,\n    directory='bike_hyperband',\n    project_name='bike_tuning'\n)\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=batch_size\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  Dropout Layer 1: {best_hp.get('drop_0')}\")\nprint(f\"  Dropout Layer 2: {best_hp.get('drop_1')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr')}\")\n\n# Build and train the best model\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the best model\nhistory_opt = opt_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=50,\n    batch_size=batch_size,\n    verbose=1\n)\n\nopt_val_score = opt_model.evaluate(X_val, y_val, verbose=0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Mean Squared Error (Validation): {:.2f} (baseline={:.2f})'.format(opt_val_score, baseline))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = opt_model.predict(X_test, verbose=0)\n\nprint('Mean Squared Error (Test): {:.2f} (baseline={:.2f})'.format(mean_squared_error(y_test, preds), baseline))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 8. Key Takeaways\n\n1. **Regression Output:** Linear activation allows any real number output\n2. **Loss:** MSE/MAE instead of cross-entropy  \n3. **Metrics:** MAE interpretable; R² shows variance explained"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Appendix: Making the Code More Modular\n\nFor larger projects or when you want to reuse code across multiple notebooks, you can encapsulate model building and training into reusable functions. Here's how to make this code more modular:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modular Model Builder Function\ndef build_regression_model(input_dimension, hidden_layers=0, hidden_neurons=32, \n                           activation='relu', dropout=None, \n                           optimizer='rmsprop', loss='mean_squared_error', \n                           name=None):\n    \"\"\"\n    Build a neural network for regression.\n    \n    Parameters:\n    -----------\n    input_dimension : int\n        Number of input features\n    hidden_layers : int\n        Number of hidden layers (0 = single layer perceptron)\n    hidden_neurons : int\n        Number of neurons per hidden layer\n    activation : str\n        Activation function for hidden layers\n    dropout : float or None\n        Dropout rate (None = no dropout)\n    optimizer : str or optimizer\n        Optimizer for training\n    loss : str\n        Loss function\n    name : str\n        Model name\n    \n    Returns:\n    --------\n    Compiled Keras Sequential model\n    \"\"\"\n    model = Sequential()\n    \n    for layer in range(hidden_layers):\n        if layer == 0:\n            model.add(Dense(hidden_neurons, activation=activation, \n                           input_shape=(input_dimension,)))\n        else:\n            model.add(Dense(hidden_neurons, activation=activation))\n        \n        if dropout is not None:\n            model.add(Dropout(dropout))\n    \n    # Output layer - linear for regression\n    if hidden_layers == 0:\n        model.add(Dense(1, input_shape=(input_dimension,)))\n    else:\n        model.add(Dense(1))\n    \n    if name is not None:\n        model._name = name\n        \n    model.compile(optimizer=optimizer, loss=loss)\n    \n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modular Training Function\ndef train_model(model, X_train, y_train, X_val, y_val,\n                batch_size=32, epochs=100, callbacks=None, verbose=0):\n    \"\"\"\n    Train a Keras model and return training history and validation score.\n    \n    Parameters:\n    -----------\n    model : Keras model\n        Compiled model to train\n    X_train, y_train : arrays\n        Training data\n    X_val, y_val : arrays\n        Validation data\n    batch_size : int\n        Batch size for training\n    epochs : int\n        Number of training epochs\n    callbacks : list\n        Keras callbacks (e.g., EarlyStopping)\n    verbose : int\n        Verbosity level\n    \n    Returns:\n    --------\n    dict with keys: 'model', 'val_score', 'history'\n    \"\"\"\n    if callbacks is None:\n        callbacks = []\n    \n    history = model.fit(\n        X_train, y_train,\n        batch_size=batch_size, \n        epochs=epochs,\n        validation_data=(X_val, y_val),\n        callbacks=callbacks,\n        verbose=verbose\n    )\n    \n    val_score = model.evaluate(X_val, y_val, verbose=0)\n    \n    return {\n        'model': model, \n        'val_score': val_score, \n        'history': history\n    }"
  },
  {
   "cell_type": "code",
   "source": "# Example: Using the modular functions\n# \n# # Build a model\n# model = build_regression_model(\n#     input_dimension=INPUT_DIMENSION,\n#     hidden_layers=2,\n#     hidden_neurons=64,\n#     dropout=0.3,\n#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n#     name='Modular_Model'\n# )\n#\n# # Train the model\n# results = train_model(\n#     model, X_train, y_train, X_val, y_val,\n#     batch_size=64, epochs=100\n# )\n#\n# # Access results\n# print(f\"Validation Score: {results['val_score']}\")\n# plot_training_history(results['history'])",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}