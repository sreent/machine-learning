{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/German%20Credit%20Data/German%20Credit%20Data%20-%20SMOTE%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# German Credit Data - Imbalanced Classification with Lift Analysis\n\nThis notebook demonstrates the **Universal ML Workflow** applied to credit risk classification, with special focus on **Lift Curves** to demonstrate model value even when accuracy improvements are modest.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply neural networks to **imbalanced binary classification**\n- Understand why **accuracy alone can be misleading** for imbalanced datasets\n- Use **Lift Curves** and **Cumulative Gains** to demonstrate model value\n- Handle class imbalance using **SMOTE** (Synthetic Minority Over-sampling)\n- Apply the Universal ML Workflow with **K-Fold Cross-Validation**\n\n---\n\n## Why Lift Curves Matter\n\nIn credit scoring, a model may not dramatically improve accuracy over the naive baseline (predicting all \"Good\"). However, the model's **ranking ability** provides significant business value:\n\n| Metric | What It Shows | Business Value |\n|--------|---------------|----------------|\n| **Accuracy** | Overall correct predictions | Can be misleading with imbalance |\n| **Lift** | How much better than random at each decile | Prioritise high-risk applicants |\n| **Cumulative Gains** | % of positives captured at each threshold | Resource allocation |\n\n**Example:** If the top 30% of applicants (ranked by risk score) contain 60% of the bad credits, the model provides **2x lift** - extremely valuable for credit decisions.\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [UCI German Credit Data](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) |\n| **Problem Type** | Binary Classification (Good/Bad Credit Risk) |\n| **Samples** | 1,000 applicants |\n| **Class Distribution** | 70% Good (700), 30% Bad (300) |\n| **Imbalance Ratio** | 2.33:1 |\n| **Features** | 7 numerical + 13 categorical variables |\n\n---\n\n## Technique Scope\n\nThis notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021):\n\n| Technique | Status | Rationale |\n|-----------|--------|-----------|\n| **Dense layers (DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n| **SMOTE** | ✓ Used | Handles class imbalance |\n| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem Statement:** Classify credit applicants as good or bad credit risks based on their financial and personal attributes.\n\n**Business Context:**\n- **False Negative (FN):** Approving a bad credit → Financial loss from default\n- **False Positive (FP):** Rejecting a good credit → Lost business opportunity\n- **Cost asymmetry:** FN is typically 5-10x more costly than FP\n\n**Key Insight:** Even if overall accuracy is similar to baseline, a model that **ranks bad credits higher** provides significant value through prioritisation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Data-Driven Metric Selection\n\n| Criterion | This Dataset | Decision |\n|-----------|--------------|----------|\n| **Class Balance** | 70% Good, 30% Bad | Imbalanced (2.33:1) |\n| **Imbalance Ratio** | 2.33:1 | Below 3:1 threshold |\n| **Primary Metric** | **AUC** | Standard for credit scoring |\n| **Secondary Metrics** | Lift, Accuracy, Gini | Business interpretation |\n\n### Why AUC (not F1-Score) for Credit Scoring?\n\n| Metric | Why AUC is Preferred |\n|--------|---------------------|\n| **Threshold-independent** | F1 depends on a specific threshold; AUC evaluates across ALL thresholds |\n| **Measures ranking** | Credit scoring is fundamentally a ranking problem - who is riskier? |\n| **Probability interpretation** | AUC = P(random bad credit ranked higher than random good credit) |\n| **Gini relationship** | Gini = 2×AUC - 1; Gini is the industry-standard metric for scorecards |\n\n### Metric Relationships\n\n```\nAUC = 0.50  →  Gini = 0.00  →  Random model (no discrimination)\nAUC = 0.70  →  Gini = 0.40  →  Acceptable scorecard\nAUC = 0.80  →  Gini = 0.60  →  Good scorecard\nAUC = 0.90  →  Gini = 0.80  →  Excellent scorecard\n```\n\n### Lift Curve Interpretation\n\n| Lift Value | Meaning |\n|------------|---------|\n| **Lift = 1.0** | No better than random |\n| **Lift = 2.0** | 2x better than random at that decile |\n| **Lift = 3.0** | 3x better than random at that decile |\n\n**Key Insight:** Even with moderate AUC improvement over baseline, the **Lift Curve** shows the model's practical value for prioritising high-risk applicants."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Data-Driven Protocol Selection\n\n| Criterion | This Dataset | Decision |\n|-----------|--------------|----------|\n| **Sample Size** | 1,000 samples | Below 10,000 threshold |\n| **Recommendation** | K-Fold Cross-Validation | More robust estimates |\n| **K Value** | 5 folds | ~200 samples per fold |\n\n**Critical Point for SMOTE:**\n- **Train** on SMOTE-balanced data (synthetic minority samples added)\n- **Validate/Test** on original imbalanced data (reflects real-world distribution)\n\n```\nOriginal Data (1000 samples: 700 Good, 300 Bad)\n├── Test Set (10% = 100 samples) - Original distribution\n└── Training Pool (90% = 900 samples)\n    └── 5-Fold Stratified Cross-Validation\n        ├── Each fold: Apply SMOTE to training portion only\n        └── Validate on original imbalanced fold\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# SMOTE for handling class imbalance\n%pip install -q imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n%pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\n# ============================================================\n# RANDOM SEED - Set once, use everywhere\n# ============================================================\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOAD DATASET\n# ============================================================\nFILE_PATH = 'http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n\nHEADERS = ['checking_account', 'duration', 'credit_history', 'purpose', 'credit_amount',\n           'savings_account', 'employment', 'installment_rate', 'personal_status',\n           'other_debtors', 'residence_since', 'property', 'age', 'other_installments',\n           'housing', 'existing_credits', 'job', 'num_dependents', 'telephone', \n           'foreign_worker', 'credit_risk']\n\ndf = pd.read_csv(FILE_PATH, sep=\" \", header=None, names=HEADERS)\n\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DEFINE FEATURE TYPES\n# ============================================================\nNUMERICAL_VARIABLES = ['duration', 'credit_amount', 'installment_rate',\n                       'residence_since', 'age', 'existing_credits', 'num_dependents']\n\nCATEGORICAL_VARIABLES = ['checking_account', 'credit_history', 'purpose',\n                         'savings_account', 'employment', 'personal_status',\n                         'other_debtors', 'property', 'other_installments',\n                         'housing', 'job', 'telephone', 'foreign_worker']\n\nTARGET_VARIABLE = 'credit_risk'\n\nprint(f\"Numerical features: {len(NUMERICAL_VARIABLES)}\")\nprint(f\"Categorical features: {len(CATEGORICAL_VARIABLES)}\")\nprint(f\"Total features: {len(NUMERICAL_VARIABLES) + len(CATEGORICAL_VARIABLES)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATA-DRIVEN ANALYSIS: Class Balance and Dataset Size\n# ============================================================\nfeatures = df[NUMERICAL_VARIABLES + CATEGORICAL_VARIABLES]\ntarget = df[TARGET_VARIABLE]\n\n# Target: 1 = Good, 2 = Bad (in original data)\n# Convert to: 0 = Good, 1 = Bad (for binary classification)\ntarget_binary = (target == 2).astype(int)\n\n# Class distribution\nn_good = (target_binary == 0).sum()\nn_bad = (target_binary == 1).sum()\nimbalance_ratio = n_good / n_bad\n\n# Dataset size\nn_samples = len(df)\nHOLDOUT_THRESHOLD = 10000\n\nprint(\"=\" * 60)\nprint(\"DATA-DRIVEN CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\nprint(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples\")\nprint(f\"   Decision: {'Hold-Out' if n_samples > HOLDOUT_THRESHOLD else 'K-Fold Cross-Validation'}\")\n\nprint(f\"\\n2. CLASS DISTRIBUTION:\")\nprint(f\"   Good Credit (0): {n_good} ({100*n_good/n_samples:.1f}%)\")\nprint(f\"   Bad Credit (1):  {n_bad} ({100*n_bad/n_samples:.1f}%)\")\nprint(f\"   Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n\nprint(f\"\\n3. IMBALANCE HANDLING:\")\nprint(f\"   Ratio < 3:1, but using SMOTE to improve minority class learning\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PRIMARY METRIC: AUC (threshold-independent ranking)\")\nprint(\"VALIDATION: 5-Fold Stratified Cross-Validation\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4.2 Train/Test Split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAIN/TEST SPLIT (90%/10%)\n# ============================================================\nTEST_SIZE = 0.10\n\nX_train_raw, X_test_raw, y_train_full, y_test = train_test_split(\n    features, target_binary,\n    test_size=TEST_SIZE,\n    stratify=target_binary,\n    random_state=SEED,\n    shuffle=True\n)\n\nprint(f\"Training pool: {len(X_train_raw):,} samples\")\nprint(f\"Test set: {len(X_test_raw):,} samples\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4.3 Preprocessing with ColumnTransformer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PREPROCESSING PIPELINE\n# ============================================================\npreprocessor = ColumnTransformer([\n    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_VARIABLES),\n    ('standard_scaler', StandardScaler(), NUMERICAL_VARIABLES)\n])\n\n# Fit on training data only (prevent data leakage)\npreprocessor.fit(X_train_raw)\n\n# Transform both sets\nX_train_full = preprocessor.transform(X_train_raw)\nX_test = preprocessor.transform(X_test_raw)\n\n# Convert to numpy arrays\ny_train_full = y_train_full.values\ny_test = y_test.values\n\nprint(f\"Preprocessed training shape: {X_train_full.shape}\")\nprint(f\"Preprocessed test shape: {X_test.shape}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4.4 K-Fold Cross-Validation Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# K-FOLD CROSS-VALIDATION SETUP\n# ============================================================\nN_FOLDS = 5\n\nskfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nprint(f\"K-Fold Configuration:\")\nprint(f\"  Number of folds: {N_FOLDS}\")\nprint(f\"  Training pool: {X_train_full.shape[0]:,} samples\")\nprint(f\"  Samples per fold: ~{X_train_full.shape[0] // N_FOLDS:,}\")\nprint(f\"  Test set (held out): {X_test.shape[0]:,} samples\")\n\n# For initial model development, we use the first fold\nfirst_fold = list(skfold.split(X_train_full, y_train_full))[0]\ntrain_idx, val_idx = first_fold\n\nX_train = X_train_full[train_idx]\nX_val = X_train_full[val_idx]\ny_train = y_train_full[train_idx]\ny_val = y_train_full[val_idx]\n\nprint(f\"\\nFirst fold (for initial development):\")\nprint(f\"  Training: {X_train.shape[0]:,} samples\")\nprint(f\"  Validation: {X_val.shape[0]:,} samples\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 Apply SMOTE to Balance Training Data\n\n**How SMOTE Works:**\n1. For each minority sample, find its k nearest neighbors\n2. Randomly select one neighbor\n3. Create a synthetic sample on the line between original and neighbor\n\n**Critical:** Only apply SMOTE to training data, never to validation or test data!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# APPLY SMOTE TO TRAINING DATA ONLY\n# ============================================================\nsmote = SMOTE(sampling_strategy='auto', random_state=SEED)\n\n# Apply SMOTE to training fold only (NOT validation)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nprint(\"Before SMOTE:\")\nprint(f\"  Good (0): {(y_train == 0).sum()}, Bad (1): {(y_train == 1).sum()}\")\n\nprint(\"\\nAfter SMOTE:\")\nprint(f\"  Good (0): {(y_train_smote == 0).sum()}, Bad (1): {(y_train_smote == 1).sum()}\")\n\nprint(\"\\nValidation set (unchanged - original distribution):\")\nprint(f\"  Good (0): {(y_val == 0).sum()}, Bad (1): {(y_val == 1).sum()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LIFT CURVE AND CUMULATIVE GAINS FUNCTIONS\n# ============================================================\ndef calculate_lift_curve(y_true, y_pred_proba, n_bins=10):\n    \"\"\"\n    Calculate lift curve data.\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True binary labels (0/1)\n    y_pred_proba : array-like\n        Predicted probabilities for positive class\n    n_bins : int\n        Number of deciles/bins\n    \n    Returns:\n    --------\n    dict : Contains deciles, lift values, and cumulative gains\n    \"\"\"\n    # Create dataframe for sorting\n    data = pd.DataFrame({\n        'y_true': y_true,\n        'y_pred': y_pred_proba.flatten()\n    })\n    \n    # Sort by predicted probability (descending - highest risk first)\n    data = data.sort_values('y_pred', ascending=False).reset_index(drop=True)\n    \n    # Calculate metrics for each decile\n    total_positives = data['y_true'].sum()\n    total_samples = len(data)\n    base_rate = total_positives / total_samples\n    \n    deciles = []\n    lifts = []\n    cum_gains = []\n    cum_positives = 0\n    \n    bin_size = total_samples // n_bins\n    \n    for i in range(n_bins):\n        start_idx = i * bin_size\n        end_idx = (i + 1) * bin_size if i < n_bins - 1 else total_samples\n        \n        bin_data = data.iloc[start_idx:end_idx]\n        bin_positives = bin_data['y_true'].sum()\n        bin_rate = bin_positives / len(bin_data)\n        \n        # Lift = bin_rate / base_rate\n        lift = bin_rate / base_rate\n        \n        # Cumulative gains\n        cum_positives += bin_positives\n        cum_gain = cum_positives / total_positives\n        \n        deciles.append(i + 1)\n        lifts.append(lift)\n        cum_gains.append(cum_gain)\n    \n    return {\n        'deciles': deciles,\n        'lifts': lifts,\n        'cum_gains': cum_gains,\n        'base_rate': base_rate,\n        'total_positives': total_positives\n    }\n\n\ndef plot_lift_analysis(y_true, y_pred_proba, title='Model'):\n    \"\"\"\n    Plot Lift Curve and Cumulative Gains side by side.\n    \"\"\"\n    results = calculate_lift_curve(y_true, y_pred_proba)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot 1: Lift Curve\n    ax1 = axes[0]\n    ax1.bar(results['deciles'], results['lifts'], color='steelblue', edgecolor='black')\n    ax1.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Random (Lift=1)')\n    ax1.set_xlabel('Decile (1=Highest Risk)')\n    ax1.set_ylabel('Lift')\n    ax1.set_title(f'Lift Curve - {title}')\n    ax1.set_xticks(results['deciles'])\n    ax1.legend()\n    ax1.grid(axis='y', alpha=0.3)\n    \n    # Plot 2: Cumulative Gains\n    ax2 = axes[1]\n    decile_pct = [d * 10 for d in results['deciles']]\n    cum_gains_pct = [g * 100 for g in results['cum_gains']]\n    \n    ax2.plot(decile_pct, cum_gains_pct, 'b-o', linewidth=2, markersize=8, label='Model')\n    ax2.plot([0, 100], [0, 100], 'r--', linewidth=2, label='Random')\n    ax2.fill_between(decile_pct, cum_gains_pct, [d for d in decile_pct], alpha=0.3)\n    ax2.set_xlabel('% of Population (Sorted by Risk Score)')\n    ax2.set_ylabel('% of Bad Credits Captured')\n    ax2.set_title(f'Cumulative Gains Curve - {title}')\n    ax2.legend()\n    ax2.grid(alpha=0.3)\n    ax2.set_xlim([0, 100])\n    ax2.set_ylim([0, 100])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary\n    print(\"\\nLift Analysis Summary:\")\n    print(\"-\" * 50)\n    print(f\"{'Decile':<10} {'Lift':>10} {'Cum Gain':>15}\")\n    print(\"-\" * 50)\n    for i, (d, l, g) in enumerate(zip(results['deciles'], results['lifts'], results['cum_gains'])):\n        print(f\"{d:<10} {l:>10.2f} {g*100:>14.1f}%\")\n    print(\"-\" * 50)\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# HELPER FUNCTION: Plot Training History\n# ============================================================\ndef plot_training_history(history, title='Training History'):\n    \"\"\"Plot training and validation loss/AUC curves.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Loss\n    axes[0].plot(history.history['loss'], 'b-', label='Training Loss')\n    axes[0].plot(history.history['val_loss'], 'r-', label='Validation Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # AUC\n    axes[1].plot(history.history['auc'], 'b-', label='Training AUC')\n    axes[1].plot(history.history['val_auc'], 'r-', label='Validation AUC')\n    axes[1].set_title('Training and Validation AUC')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('AUC')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.suptitle(title, fontsize=14)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\n**Baseline Metrics:**\n- **Accuracy baseline:** 70% (predict all \"Good\")\n- **AUC baseline:** 0.50 (random ranking)\n- **Lift baseline:** 1.0 (no better than random at any decile)\n\n**Goal:** Beat the AUC baseline and demonstrate positive lift in top deciles."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MODEL CONFIGURATION\n# ============================================================\nINPUT_DIMENSION = X_train_full.shape[1]\nOUTPUT_DIMENSION = 1  # Binary classification\n\nOPTIMIZER = 'adam'\nLOSS_FUNC = 'binary_crossentropy'\nMETRICS = ['accuracy', tf.keras.metrics.AUC(name='auc')]\n\n# ============================================================\n# BATCH SIZE SELECTION\n# ============================================================\n# With only 1,000 samples (<10,000 threshold), we use a smaller batch size.\n# 32 provides noisier gradients that act as regularisation and help\n# prevent overfitting on small datasets. For larger datasets (>10,000),\n# use 256-512 for faster, more stable training.\nBATCH_SIZE = 32\n\nEPOCHS_BASELINE = 100\nEPOCHS_REGULARIZED = 150\n\nprint(f\"Input dimension: {INPUT_DIMENSION}\")\nprint(f\"Output dimension: {OUTPUT_DIMENSION}\")\nprint(f\"Batch size: {BATCH_SIZE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ESTABLISH BASELINES\n# ============================================================\n# Accuracy baseline: predict all \"Good\" (majority class)\naccuracy_baseline = n_good / n_samples\n\n# AUC baseline: random ranking\nauc_baseline = 0.5\n\n# Lift baseline: 1.0 (no better than random)\nlift_baseline = 1.0\n\nprint(\"Baseline Metrics:\")\nprint(f\"  Accuracy (predict all Good): {accuracy_baseline:.2%}\")\nprint(f\"  AUC (random ranking): {auc_baseline:.2f}\")\nprint(f\"  Lift (random): {lift_baseline:.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SINGLE LAYER PERCEPTRON (SLP) - Simplest model\n# ============================================================\nslp_model = Sequential(name='Single_Layer_Perceptron')\nslp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\nslp_model.add(Dense(OUTPUT_DIMENSION, activation='sigmoid'))\nslp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train SLP on SMOTE-balanced data\nslp_history = slp_model.fit(\n    X_train_smote, y_train_smote,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_BASELINE,\n    validation_data=(X_val, y_val),  # Validate on original distribution\n    verbose=0\n)\n\n# Evaluate on original (imbalanced) validation set\nslp_preds = slp_model.predict(X_val, verbose=0)\nslp_auc = roc_auc_score(y_val, slp_preds)\nslp_acc = accuracy_score(y_val, (slp_preds > 0.5).astype(int))\n\nprint(f\"SLP Results (Original Validation Distribution):\")\nprint(f\"  Accuracy: {slp_acc:.2%} (baseline: {accuracy_baseline:.2%})\")\nprint(f\"  AUC: {slp_auc:.4f} (baseline: {auc_baseline:.2f})\")\nprint(f\"  Gini: {2*slp_auc - 1:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot SLP training history\nplot_training_history(slp_history, 'Single Layer Perceptron')"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding a hidden layer to learn more complex patterns in credit risk data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MULTI-LAYER PERCEPTRON (MLP) - Standard architecture\n# ============================================================\nHIDDEN_NEURONS = 64\n\nmlp_model = Sequential(name='Multi_Layer_Perceptron')\nmlp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\nmlp_model.add(Dense(HIDDEN_NEURONS, activation='relu'))\nmlp_model.add(Dense(OUTPUT_DIMENSION, activation='sigmoid'))\nmlp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\nmlp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train MLP on SMOTE-balanced data\nmlp_history = mlp_model.fit(\n    X_train_smote, y_train_smote,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_BASELINE,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# Evaluate on original validation set\nmlp_preds = mlp_model.predict(X_val, verbose=0)\nmlp_auc = roc_auc_score(y_val, mlp_preds)\nmlp_acc = accuracy_score(y_val, (mlp_preds > 0.5).astype(int))\n\nprint(f\"MLP Results (Original Validation Distribution):\")\nprint(f\"  Accuracy: {mlp_acc:.2%} (baseline: {accuracy_baseline:.2%})\")\nprint(f\"  AUC: {mlp_auc:.4f} (baseline: {auc_baseline:.2f})\")\nprint(f\"  Gini: {2*mlp_auc - 1:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot MLP training history\nplot_training_history(mlp_history, 'Multi-Layer Perceptron')"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nUsing **Hyperband** for efficient hyperparameter tuning with a frozen architecture.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# HYPERBAND MODEL BUILDER\n# ============================================================\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build German Credit model with FROZEN architecture (64 neurons).\n    Only tunes regularization (Dropout) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # Fixed architecture: 1 hidden layer with 64 neurons\n    model.add(layers.Dense(HIDDEN_NEURONS, activation='relu'))\n    \n    # Tunable dropout\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(dropout_rate))\n\n    # Output layer for binary classification\n    model.add(layers.Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n\n    # Tunable learning rate\n    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n    )\n    return model",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CONFIGURE AND RUN HYPERBAND TUNER\n# ============================================================\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_auc',\n    max_epochs=30,\n    factor=3,\n    directory='german_credit_hyperband',\n    project_name='german_credit_tuning',\n    overwrite=True\n)\n\n# Run Hyperband search on SMOTE-balanced data\ntuner.search(\n    X_train_smote, y_train_smote,\n    validation_data=(X_val, y_val),  # Validate on original distribution\n    epochs=30,\n    batch_size=BATCH_SIZE,\n    verbose=0\n)\n\nprint(\"Hyperband search complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# GET BEST HYPERPARAMETERS AND BUILD BEST MODEL\n# ============================================================\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"Best Hyperparameters:\")\nprint(f\"  Dropout: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('learning_rate'):.6f}\")\n\n# Build and display best model\nbest_model = tuner.hypermodel.build(best_hp)\nbest_model.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# TRAIN REGULARIZED MODEL\n# ============================================================\nreg_history = best_model.fit(\n    X_train_smote, y_train_smote,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_REGULARIZED,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\n# Evaluate on original validation set\nreg_preds = best_model.predict(X_val, verbose=0)\nreg_auc = roc_auc_score(y_val, reg_preds)\nreg_acc = accuracy_score(y_val, (reg_preds > 0.5).astype(int))\n\nprint(f\"Regularized Model Results (Original Validation Distribution):\")\nprint(f\"  Accuracy: {reg_acc:.2%} (baseline: {accuracy_baseline:.2%})\")\nprint(f\"  AUC: {reg_auc:.4f} (baseline: {auc_baseline:.2f})\")\nprint(f\"  Gini: {2*reg_auc - 1:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Plot regularized model training history\nplot_training_history(reg_history, 'Regularized Model')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FINAL EVALUATION WITH LIFT ANALYSIS\n# ============================================================\n# Use best model predictions for lift analysis\nfinal_preds = best_model.predict(X_val, verbose=0)\nfinal_auc = roc_auc_score(y_val, final_preds)\nfinal_gini = 2 * final_auc - 1\nfinal_acc = accuracy_score(y_val, (final_preds > 0.5).astype(int))\n\nprint(\"=\" * 60)\nprint(\"FINAL EVALUATION - ORIGINAL VALIDATION DISTRIBUTION\")\nprint(\"=\" * 60)\nprint(f\"\\n1. STANDARD METRICS:\")\nprint(f\"   Accuracy: {final_acc:.2%} (baseline: {accuracy_baseline:.2%})\")\nprint(f\"   AUC: {final_auc:.4f} (baseline: {auc_baseline:.2f})\")\nprint(f\"   Gini: {final_gini:.4f}\")\n\nprint(f\"\\n2. LIFT ANALYSIS:\")\nprint(\"   (See charts below)\")\nprint(\"=\" * 60)\n\n# Plot lift analysis\nlift_results = plot_lift_analysis(y_val, final_preds, title='Final Regularized Model')\n\n# Key business insight\ntop_3_decile_gain = lift_results['cum_gains'][2]  # First 3 deciles (30%)\nprint(f\"\\n3. BUSINESS INSIGHT:\")\nprint(f\"   By reviewing the top 30% of applicants (ranked by risk score),\")\nprint(f\"   the model captures {top_3_decile_gain*100:.1f}% of bad credits.\")\nprint(f\"   This is {top_3_decile_gain/0.3:.1f}x better than random selection.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 8. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | 1,000 samples | **K-Fold (5 folds)** | Kohavi (1995) |\n| **Primary Metric** | Credit scoring | Ranking problem | **AUC** | Industry standard |\n| **Imbalance Handling** | 2.33:1 ratio | Moderate imbalance | **SMOTE** | Chawla et al. (2002) |\n\n### Why Lift Curves Matter for Credit Scoring\n\n1. **Accuracy can be misleading:** A model predicting all \"Good\" achieves 70% accuracy but provides zero business value.\n\n2. **Lift shows ranking ability:** Even with modest AUC improvement, significant lift in top deciles demonstrates the model's value for prioritising high-risk applicants.\n\n3. **Business interpretation:** \"By reviewing the top 30% of applicants (ranked by risk score), we capture 60% of bad credits\" is actionable for credit analysts.\n\n4. **Threshold-independent:** Unlike accuracy (threshold=0.5), lift and AUC evaluate the model across all possible thresholds.\n\n### Lessons Learned\n\n1. **Train on SMOTE-balanced, Validate on Imbalanced:** This reflects real-world performance while teaching the model to recognize minority patterns.\n\n2. **AUC is the Standard for Credit Scoring:** It measures ranking ability and relates directly to Gini coefficient (Gini = 2×AUC - 1).\n\n3. **Lift Complements AUC:** While AUC gives a single number, lift curves show WHERE the model excels (typically in top deciles).\n\n4. **K-Fold for Small Datasets:** With only 1,000 samples, K-Fold cross-validation provides more robust performance estimates.\n\n### References\n\n- Chawla, N.V. et al. (2002) 'SMOTE: Synthetic Minority Over-sampling Technique', *Journal of Artificial Intelligence Research*, 16, pp. 321-357.\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145.\n\n- Siddiqi, N. (2017) *Intelligent Credit Scoring: Building and Implementing Better Credit Risk Scorecards*. 2nd edn. Wiley."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}