{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/German%20Credit%20Data/German%20Credit%20Data%20-%20SMOTE%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# German Credit Data - SMOTE Example\n\nThis notebook demonstrates the **Universal ML Workflow** using **SMOTE (Synthetic Minority Over-sampling Technique)** to handle class imbalance - an alternative to class weights.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Understand the difference between **class weights** and **oversampling** approaches to handle imbalance\n- Apply **SMOTE** to generate synthetic samples of the minority class\n- Properly evaluate models when using SMOTE (train on balanced, validate on imbalanced)\n- Handle mixed feature types with One-Hot Encoding and Standardization\n- Compare model performance on balanced vs. imbalanced validation data\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [UCI German Credit Data](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) |\n| **Problem Type** | Binary Classification (Good/Bad Credit Risk) |\n| **Data Balance** | Imbalanced (70% Good, 30% Bad) |\n| **Data Type** | Structured (Mixed Categorical & Numerical) |\n| **Features** | 7 numerical + 13 categorical variables |\n| **Imbalance Handling** | SMOTE oversampling |\n\n---\n\n## SMOTE vs. Class Weights\n\n| Approach | How It Works | Pros | Cons |\n|----------|--------------|------|------|\n| **Class Weights** | Penalize misclassifying minority class more | Simple, no data modification | May not fully solve imbalance |\n| **SMOTE** | Generate synthetic minority samples | Creates more training data | May introduce noise, computationally expensive |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem Statement:** Classify credit applicants as good or bad credit risks based on their financial and personal attributes.\n\n**Business Context:** Banks need to assess credit risk to:\n- Minimize loan defaults\n- Offer appropriate interest rates\n- Make fair lending decisions\n\n**Data Source:** This classic dataset from UCI Machine Learning Repository contains 1000 instances with 20 attributes describing credit applicants.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n| Metric | Description | Why We Use It |\n|--------|-------------|---------------|\n| **Balanced Accuracy** | Average of sensitivity and specificity | Handles imbalanced classes |\n| **Precision** | Accuracy of positive predictions | Important when false positives are costly |\n| **Recall** | Coverage of actual positives | Important when false negatives are costly |\n| **AUC** | Overall discrimination | Model's ranking ability |\n\n**Note on Credit Risk:** In banking, false negatives (approving a bad credit risk) can be more costly than false positives (rejecting a good applicant). This asymmetry should inform model selection."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n**Critical Point for SMOTE:**\n- **Train** on SMOTE-balanced data (synthetic minority samples added)\n- **Validate/Test** on original imbalanced data (reflects real-world distribution)\n\n```\nOriginal Data (1000 samples: 700 Good, 300 Bad)\n├── Test Set (10%) - Original distribution\n└── Training Pool (90%)\n    ├── Apply SMOTE → Balanced (630 Good, 630 Bad)\n    └── Validate on original distribution\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries\n\nNote the import of `SMOTE` from `imblearn` (imbalanced-learn) library - a specialized toolkit for handling imbalanced datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport itertools\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
    "\n",
    "HEADERS = ['Status of existing checking account', 'Duration in month', 'Credit history',\n",
    "           'Purpose','Credit amount', 'Savings account/bonds', 'Present employment since',\n",
    "           'Installment rate in percentage of disposable income', 'Personal status and sex',\n",
    "           'Other debtors / guarantors', 'Present residence since', 'Property', 'Age in years',\n",
    "           'Other installment plans', 'Housing', 'Number of existing credits at this bank',\n",
    "           'Job', 'Number of people being liable to provide maintenance for', 'Telephone', 'Foreign worker', \n",
    "           'Cost Matrix(Risk)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status of existing checking account</th>\n",
       "      <th>Duration in month</th>\n",
       "      <th>Credit history</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Savings account/bonds</th>\n",
       "      <th>Present employment since</th>\n",
       "      <th>Installment rate in percentage of disposable income</th>\n",
       "      <th>Personal status and sex</th>\n",
       "      <th>Other debtors / guarantors</th>\n",
       "      <th>...</th>\n",
       "      <th>Property</th>\n",
       "      <th>Age in years</th>\n",
       "      <th>Other installment plans</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Number of existing credits at this bank</th>\n",
       "      <th>Job</th>\n",
       "      <th>Number of people being liable to provide maintenance for</th>\n",
       "      <th>Telephone</th>\n",
       "      <th>Foreign worker</th>\n",
       "      <th>Cost Matrix(Risk)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>...</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Status of existing checking account  Duration in month Credit history  \\\n",
       "0                                 A11                  6            A34   \n",
       "1                                 A12                 48            A32   \n",
       "2                                 A14                 12            A34   \n",
       "3                                 A11                 42            A32   \n",
       "4                                 A11                 24            A33   \n",
       "\n",
       "  Purpose  Credit amount Savings account/bonds Present employment since  \\\n",
       "0     A43           1169                   A65                      A75   \n",
       "1     A43           5951                   A61                      A73   \n",
       "2     A46           2096                   A61                      A74   \n",
       "3     A42           7882                   A61                      A74   \n",
       "4     A40           4870                   A61                      A73   \n",
       "\n",
       "   Installment rate in percentage of disposable income  \\\n",
       "0                                                  4     \n",
       "1                                                  2     \n",
       "2                                                  2     \n",
       "3                                                  2     \n",
       "4                                                  3     \n",
       "\n",
       "  Personal status and sex Other debtors / guarantors  ...  Property  \\\n",
       "0                     A93                       A101  ...      A121   \n",
       "1                     A92                       A101  ...      A121   \n",
       "2                     A93                       A101  ...      A121   \n",
       "3                     A93                       A103  ...      A122   \n",
       "4                     A93                       A101  ...      A124   \n",
       "\n",
       "  Age in years  Other installment plans Housing  \\\n",
       "0           67                     A143    A152   \n",
       "1           22                     A143    A152   \n",
       "2           49                     A143    A152   \n",
       "3           45                     A143    A153   \n",
       "4           53                     A143    A153   \n",
       "\n",
       "  Number of existing credits at this bank   Job  \\\n",
       "0                                       2  A173   \n",
       "1                                       1  A173   \n",
       "2                                       1  A172   \n",
       "3                                       1  A173   \n",
       "4                                       2  A173   \n",
       "\n",
       "  Number of people being liable to provide maintenance for  Telephone  \\\n",
       "0                                                  1             A192   \n",
       "1                                                  1             A191   \n",
       "2                                                  2             A191   \n",
       "3                                                  2             A191   \n",
       "4                                                  2             A191   \n",
       "\n",
       "  Foreign worker Cost Matrix(Risk)  \n",
       "0           A201                 1  \n",
       "1           A201                 2  \n",
       "2           A201                 1  \n",
       "3           A201                 1  \n",
       "4           A201                 2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(FILE_PATH, sep=\" \", header=None)\n",
    "df.columns = HEADERS\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_VARIABLES = ['Duration in month', 'Credit amount', 'Installment rate in percentage of disposable income',\n",
    "                     'Present residence since', 'Age in years', 'Number of existing credits at this bank', \n",
    "                     'Number of people being liable to provide maintenance for']\n",
    "                     \n",
    "CATEGORICAL_VARIABLES = ['Status of existing checking account', 'Credit history',\n",
    "                         'Purpose', 'Savings account/bonds', 'Present employment since',\n",
    "                         'Personal status and sex', 'Other debtors / guarantors',\n",
    "                         'Property', 'Other installment plans', 'Housing',\n",
    "                         'Job', 'Telephone', 'Foreign worker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[NUMERICAL_VARIABLES + CATEGORICAL_VARIABLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VARIABLE = 'Cost Matrix(Risk)'\n",
    "\n",
    "target = df[TARGET_VARIABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=TEST_SIZE, \n",
    "                                                    random_state=SEED, shuffle=True, stratify=target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_VARIABLES),\n",
    "    ('standard_scaler', StandardScaler(), NUMERICAL_VARIABLES)])\n",
    "\n",
    "_ = preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = preprocessor.transform(X_train), preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Apply SMOTE to Balance Training Data\n\n**How SMOTE Works:**\n1. For each minority sample, find its k nearest neighbors\n2. Randomly select one neighbor\n3. Create a synthetic sample somewhere on the line between the original and neighbor\n\n**Important:** Only apply SMOTE to training data, never to validation or test data!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = X_test.shape[0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=VALIDATION_SIZE, stratify=y_train,\n",
    "                                                  shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy='auto', random_state=SEED)\n",
    "Xs, ys = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train, Xs_val, ys_train, ys_val = train_test_split(Xs, ys, test_size=VALIDATION_SIZE, \n",
    "                                                      random_state=SEED, shuffle=True, \n",
    "                                                      stratify=ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Developing a model that does better than a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\n### 5.1 Baseline Performance"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_train, np.zeros(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_DIMENSION = 1\n",
    "\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS_FUNC = 'binary_crossentropy'\n",
    "METRICS = ['accuracy', \n",
    "           tf.keras.metrics.Precision(name='precision'), \n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create baseline model with 2 hidden layers (inline Sequential)\nslp_model = Sequential(name='Single_Layer_Perceptron')\nslp_model.add(Dense(8, activation='relu', input_shape=(INPUT_DIMENSION,)))\nslp_model.add(Dropout(0.25))\nslp_model.add(Dense(8, activation='relu'))\nslp_model.add(Dropout(0.25))\nslp_model.add(Dense(1, activation='sigmoid'))\nslp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 32\nEPOCHS = 100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train on SMOTE-balanced data (no class_weights needed - data is already balanced)\nhistory = slp_model.fit(Xs_train, ys_train, batch_size=batch_size, epochs=EPOCHS, \n                        validation_data=(Xs_val, ys_val), verbose=0)\nval_score = slp_model.evaluate(Xs_val, ys_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Balanced Validation): {:.2f} (baseline=0.5)'.format(val_score[0]))\nprint('Precision (Balanced Validation): {:.2f}'.format(val_score[1]))\nprint('Recall (Balanced Validation): {:.2f}'.format(val_score[2]))\nprint('AUC (Balanced Validation): {:.2f}'.format(val_score[3]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = slp_model.predict(X_val, verbose=0)\n\nprint('Accuracy (Imbalanced Validation): {:.2f} (baseline=0.7)'.format(accuracy_score(y_val, (preds > 0.5).astype('int32'))))\nprint('Precision (Imbalanced Validation): {:.2f}'.format(precision_score(y_val, (preds > 0.5).astype('int32'))))\nprint('Recall (Imbalanced Validation): {:.2f}'.format(recall_score(y_val, (preds > 0.5).astype('int32'))))\nprint('AUC (Imbalanced Validation): {:.2f}'.format(roc_auc_score(y_val, preds)))"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (Validation): 0.70 (baseline = 0.5)\n"
     ]
    }
   ],
   "source": [
    "print('Balanced Accuracy (Validation): {:.2f} (baseline = 0.5)'.format(balanced_accuracy_score(y_val, (preds > 0.5).astype('int32'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, monitors=['loss', 'AUC']) :\n",
    "\n",
    "  # using the variable axs for multiple Axes\n",
    "  fig, axs = plt.subplots(1, 2, sharex='all', figsize=(15,5))\n",
    " \n",
    "  for ax, monitor in zip(axs.flat, monitors) :\n",
    "    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n",
    "\n",
    "    if monitor == 'loss' :\n",
    "      monitor = monitor.capitalize()\n",
    "\n",
    "    epochs = range(1, len(loss)+1)\n",
    "\n",
    "    ax.plot(epochs, loss, 'b.', label=monitor)\n",
    "    ax.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n",
    "    ax.set_xlim([0, len(loss)])\n",
    "    ax.title.set_text('Training and Validation ' + monitor + 's')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(monitor)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "  _ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "plot_training_history(history, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Scaling up: developing a model that overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Larger model to test for overfitting (3 hidden layers, no dropout)\nmlp_model = Sequential(name='Multi_Layer_Perceptron')\nmlp_model.add(Dense(8, activation='relu', input_shape=(INPUT_DIMENSION,)))\nmlp_model.add(Dense(8, activation='relu'))\nmlp_model.add(Dense(8, activation='relu'))\nmlp_model.add(Dense(1, activation='sigmoid'))\nmlp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\nmlp_model.summary()\n\n# Train MLP on SMOTE-balanced data\nhistory_mlp = mlp_model.fit(Xs_train, ys_train, batch_size=batch_size, epochs=EPOCHS, \n                            validation_data=(Xs_val, ys_val), verbose=0)\nval_score_mlp = mlp_model.evaluate(Xs_val, ys_val, verbose=0)[1:]"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nTesting if a larger model can capture more complex patterns in the SMOTE-balanced data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_training_history(history_mlp, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Balanced Validation): {:.2f} (baseline=0.5)'.format(val_score_mlp[0]))\nprint('Precision (Balanced Validation): {:.2f}'.format(val_score_mlp[1]))\nprint('Recall (Balanced Validation): {:.2f}'.format(val_score_mlp[2]))\nprint('AUC (Balanced Validation): {:.2f}'.format(val_score_mlp[3]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = mlp_model.predict(X_val, verbose=0)\n\nprint('Accuracy (Imbalanced Validation): {:.2f} (baseline=0.7)'.format(accuracy_score(y_val, (preds > 0.5).astype('int32'))))\nprint('Precision (Imbalanced Validation): {:.2f}'.format(precision_score(y_val, (preds > 0.5).astype('int32'))))\nprint('Recall (Imbalanced Validation): {:.2f}'.format(recall_score(y_val, (preds > 0.5).astype('int32'))))\nprint('AUC (Imbalanced Validation): {:.2f}'.format(roc_auc_score(y_val, preds)))\nprint('Balanced Accuracy (Validation): {:.2f} (baseline = 0.5)'.format(balanced_accuracy_score(y_val, (preds > 0.5).astype('int32'))))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nUsing **Hyperband** for efficient hyperparameter tuning with a frozen architecture.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperband Model Builder for Binary Classification (SMOTE)\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build German Credit model with FROZEN architecture (2 layers: 16 -> 8 neurons).\n    Only tunes regularization (Dropout) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # Fixed architecture: 2 hidden layers with 16 and 8 neurons\n    # Layer 1: 16 neurons\n    model.add(layers.Dense(16, activation='relu'))\n    drop_0 = hp.Float('drop_0', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_0))\n\n    # Layer 2: 8 neurons\n    model.add(layers.Dense(8, activation='relu'))\n    drop_1 = hp.Float('drop_1', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_1))\n\n    # Output layer for binary classification\n    model.add(layers.Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_auc',\n    max_epochs=20,\n    factor=3,\n    directory='german_credit_hyperband',\n    project_name='german_credit_tuning'\n)\n\n# Run Hyperband search on SMOTE-balanced data\ntuner.search(\n    Xs_train, ys_train,\n    validation_data=(Xs_val, ys_val),\n    epochs=20,\n    batch_size=batch_size\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters and build best model\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  Dropout Layer 1: {best_hp.get('drop_0')}\")\nprint(f\"  Dropout Layer 2: {best_hp.get('drop_1')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr')}\")\n\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the best model on SMOTE-balanced data\nhistory_opt = opt_model.fit(\n    Xs_train, ys_train,\n    validation_data=(Xs_val, ys_val),\n    epochs=50,\n    batch_size=batch_size,\n    verbose=1\n)\n\ntrained_opt_model = {\n    'model': opt_model,\n    'val_score': opt_model.evaluate(Xs_val, ys_val, verbose=0)[1:],\n    'history': history_opt\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate optimized model on original imbalanced validation data\npreds = opt_model.predict(X_val, verbose=0)\n\nprint('Optimized Model - Imbalanced Validation Results:')\nprint('Accuracy (Imbalanced Validation): {:.2f} (baseline=0.7)'.format(accuracy_score(y_val, (preds > 0.5).astype('int32'))))\nprint('Precision (Imbalanced Validation): {:.2f}'.format(precision_score(y_val, (preds > 0.5).astype('int32'))))\nprint('Recall (Imbalanced Validation): {:.2f}'.format(recall_score(y_val, (preds > 0.5).astype('int32'))))\nprint('AUC (Imbalanced Validation): {:.2f}'.format(roc_auc_score(y_val, preds)))\nprint('Balanced Accuracy (Validation): {:.2f} (baseline=0.5)'.format(balanced_accuracy_score(y_val, (preds > 0.5).astype('int32'))))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 8. Key Takeaways and Observations\n\n### Key Lessons Learned\n\n1. **SMOTE vs. Class Weights:**\n   - SMOTE creates synthetic samples, physically balancing the training set\n   - Class weights adjust the loss function mathematically\n   - Both can be effective; SMOTE works well for small minority classes\n\n2. **Critical: Train on Balanced, Validate on Imbalanced:**\n   - Training on SMOTE-balanced data teaches the model to recognize minority patterns\n   - Validating on original imbalanced data reflects real-world performance\n   - Never SMOTE your validation/test sets!\n\n3. **SMOTE Works Best After Preprocessing:**\n   - Apply SMOTE after one-hot encoding and scaling\n   - This allows SMOTE to create meaningful synthetic samples in feature space\n\n4. **Hyperparameter Tuning Still Matters:**\n   - Even with balanced data, dropout and architecture choices affect performance\n   - Use cross-validation to get reliable estimates\n\n### When to Use SMOTE vs. Class Weights\n\n| Scenario | Recommended Approach |\n|----------|---------------------|\n| Very small minority class (<5%) | SMOTE |\n| Moderate imbalance (20-40%) | Either works |\n| Computational constraints | Class Weights |\n| High-dimensional data | Class Weights (SMOTE can create noise) |"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Modular Helper Functions\n\nThe following modular functions can be used for more complex experiments or reusable workflows. These functions encapsulate the model creation and training logic demonstrated inline above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Modular Helper Functions for DNN Classification\n# =============================================================================\n\ndef create_binary_classifier(input_dim, hidden_layers, neurons_per_layer, \n                             dropout_rate=None, activation='relu',\n                             optimizer='rmsprop', loss='binary_crossentropy', \n                             metrics=None, name=None):\n    \"\"\"\n    Create a binary classification neural network with customizable architecture.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    hidden_layers : int\n        Number of hidden layers\n    neurons_per_layer : int or list\n        Neurons per hidden layer (int for uniform, list for varying)\n    dropout_rate : float, optional\n        Dropout rate after each hidden layer (None = no dropout)\n    activation : str\n        Activation function for hidden layers\n    optimizer : str\n        Optimizer for model compilation\n    loss : str\n        Loss function for model compilation\n    metrics : list\n        Metrics to track during training\n    name : str, optional\n        Model name\n    \n    Returns:\n    --------\n    keras.Sequential : Compiled model\n    \n    Example:\n    --------\n    # model = create_binary_classifier(\n    #     input_dim=61, hidden_layers=2, neurons_per_layer=16,\n    #     dropout_rate=0.25, metrics=['accuracy']\n    # )\n    \"\"\"\n    model = Sequential(name=name)\n    \n    # Handle uniform vs varying neurons per layer\n    if isinstance(neurons_per_layer, int):\n        neurons_list = [neurons_per_layer] * hidden_layers\n    else:\n        neurons_list = neurons_per_layer\n    \n    for i, neurons in enumerate(neurons_list):\n        if i == 0:\n            model.add(Dense(neurons, activation=activation, input_shape=(input_dim,)))\n        else:\n            model.add(Dense(neurons, activation=activation))\n        if dropout_rate is not None:\n            model.add(Dropout(dropout_rate))\n    \n    # Output layer for binary classification\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics or ['accuracy'])\n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, \n                batch_size=32, epochs=100, verbose=0):\n    \"\"\"\n    Train a model and return training history and validation scores.\n    \n    Note: This function does NOT use class_weights - use with SMOTE-balanced data.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Compiled Keras model\n    X_train, y_train : array-like\n        Training data (should be SMOTE-balanced for imbalanced problems)\n    X_val, y_val : array-like\n        Validation data (keep original imbalanced distribution)\n    batch_size : int\n        Training batch size\n    epochs : int\n        Number of training epochs\n    verbose : int\n        Verbosity level (0=silent, 1=progress bar, 2=one line per epoch)\n    \n    Returns:\n    --------\n    tuple : (history, val_scores)\n        - history: Training history object\n        - val_scores: Validation metric scores (excluding loss)\n    \n    Example:\n    --------\n    # history, val_scores = train_model(\n    #     model, Xs_train, ys_train, Xs_val, ys_val,\n    #     batch_size=32, epochs=100\n    # )\n    \"\"\"\n    history = model.fit(\n        X_train, y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=(X_val, y_val),\n        verbose=verbose\n    )\n    val_scores = model.evaluate(X_val, y_val, verbose=0)[1:]\n    return history, val_scores",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}