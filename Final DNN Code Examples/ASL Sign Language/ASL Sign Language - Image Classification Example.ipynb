{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/ASL%20Sign%20Language/ASL%20Sign%20Language%20-%20Image%20Classification%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# ASL Sign Language - Image Classification Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to image classification using American Sign Language (ASL) hand gesture images.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Load and extract image data from a zip archive\n- Preprocess images for neural network input: **Colour → Grayscale → Flatten**\n- Build neural networks for multi-class **image classification**\n- Apply the Universal ML Workflow to computer vision problems\n- Evaluate classification performance with accuracy and confusion matrices\n\n---\n\n## Technique Scope\n\n| Aspect | What We Use | What We Don't Use (Yet) |\n|--------|-------------|------------------------|\n| **Architecture** | Dense layers only | CNNs, pooling, feature extractors |\n| **Regularisation** | L2 + Dropout | Early stopping, data augmentation |\n| **Optimiser** | Adam | SGD with momentum, learning rate schedules |\n| **Tuning** | Hyperband | Bayesian optimisation, neural architecture search |\n\n> **Note**: Dense networks applied to flattened images serve as a baseline. CNNs (Chapter 8) are the standard approach for image classification and would preserve spatial structure.\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | ASL Sign Language Dataset (3 classes: A, B, C) |\n| **Problem Type** | Multi-Class Classification |\n| **Data Balance** | Perfectly Balanced (3,000 samples per class) |\n| **Data Type** | Unstructured (Images) |\n| **Preprocessing** | Resize to 32×32 → Grayscale → Flatten (1024 features) |\n\n### Image Preprocessing Pipeline\n\n```\nOriginal Image        Grayscale          Flattened\n[H x W x 3]    →    [32 x 32]    →    [1024]\n(Colour RGB)        (Single channel)   (1D array for NN)\n```\n\n**Why flatten?** Dense neural networks expect 1D input vectors. We sacrifice spatial relationships for simplicity. (CNNs preserve spatial structure but are covered in Chapter 8.)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Reuse Philosophy\n",
    "\n",
    "This notebook follows a **\"Same Code, Different Data\"** philosophy. The core ML pipeline remains consistent across different classification tasks:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    UNIVERSAL ML PIPELINE                        │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  Data Loading → Preprocessing → Train/Val/Test Split → Model   │\n",
    "│  → Baseline → Overfitting → Regularisation → Evaluation        │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**What changes:** Data source, preprocessing, number of output classes  \n",
    "**What stays the same:** Model architecture pattern, training loop, evaluation code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Problem and Assembling a Dataset\n",
    "\n",
    "**Problem Statement:** Classify images of hand gestures into ASL letters (A, B, or C).\n",
    "\n",
    "**Why this matters:**\n",
    "- ASL recognition can help bridge communication gaps\n",
    "- Image classification is foundational to computer vision\n",
    "- This simplified 3-class problem demonstrates key concepts before tackling the full 26-letter alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing a Measure of Success\n",
    "\n",
    "### Data-Driven Metric Selection\n",
    "\n",
    "| Criterion | This Dataset | Decision |\n",
    "|-----------|--------------|----------|\n",
    "| **Class Balance** | Equal across 3 classes | Balanced |\n",
    "| **Number of Classes** | 3 (A, B, C) | Multi-class |\n",
    "| **Primary Metric** | Accuracy | Standard for balanced multi-class |\n",
    "| **Secondary Metrics** | Precision, Recall, AUC | Per-class performance |\n",
    "\n",
    "**Decision:** Since the dataset is balanced, **Accuracy** is the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Data-Driven Protocol Selection\n\n| Criterion | This Dataset | Decision |\n|-----------|--------------|----------|\n| **Sample Size** | ~9,000 images (3,000 per class) | Below 10,000 threshold |\n| **Threshold** | < 10,000 | Use K-Fold |\n| **Protocol** | 5-Fold Stratified Cross-Validation | Preserves class balance |\n\n**Decision:** With ~9,000 samples (below the 10,000 threshold), **K-Fold Cross-Validation** provides more robust performance estimates.\n\n```\nOriginal Data (~9,000 samples)\n├── Test Set (10% = ~900 samples) - Final evaluation only\n└── Training Pool (90% = ~8,100 samples)\n    └── 5-Fold Stratified Cross-Validation\n        ├── Fold 1: Train on folds 2-5, validate on fold 1\n        ├── Fold 2: Train on folds 1,3-5, validate on fold 2\n        ├── ...\n        └── Fold 5: Train on folds 1-4, validate on fold 5\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport zipfile\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nfrom PIL import Image\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n%pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\n# ============================================================\n# RANDOM SEED - Set once, use everywhere\n# ============================================================\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATASET CONFIGURATION\n# ============================================================\nGDRIVE_FILE_ID = '1Df0wlpvKUSD12RAYihBI-slI1zbz-Vjj'\nDATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\nZIP_FILE = 'asl.zip'\nEXTRACT_DIR = 'asl'\n\n# Image configuration\nRESIZE = (32, 32)  # Resize to 32x32 - balance between preserving detail and dimensionality\nSAMPLE_SIZE = 3000  # Max samples per class\n\n# Class names\nCLASS_NAMES = ['A', 'B', 'C']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Google Drive\n",
    "import gdown\n",
    "\n",
    "if not os.path.exists(ZIP_FILE):\n",
    "    print(f\"Downloading ASL dataset from Google Drive...\")\n",
    "    gdown.download(DATA_URL, ZIP_FILE, quiet=False)\n",
    "else:\n",
    "    print(f\"Dataset already downloaded: {ZIP_FILE}\")\n",
    "\n",
    "# Extract the zip file\n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    print(f\"Extracting {ZIP_FILE}...\")\n",
    "    with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    print(f\"Extracted to: {EXTRACT_DIR}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {EXTRACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path, target_size=(16, 16), grayscale=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to image file\n",
    "        target_size: Target size for resizing (width, height)\n",
    "        grayscale: Convert to grayscale if True\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image as numpy array\n",
    "    \"\"\"\n",
    "    # Load image using PIL\n",
    "    img = Image.open(file_path)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Resize using skimage (consistent with other notebooks)\n",
    "    img_resized = resize(img_array, (*target_size, 3), anti_aliasing=True)\n",
    "    \n",
    "    # Convert to grayscale if specified\n",
    "    if grayscale:\n",
    "        img_resized = rgb2gray(img_resized)\n",
    "    \n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from directory structure\n",
    "images, labels = [], []\n",
    "\n",
    "for class_name in CLASS_NAMES:\n",
    "    class_dir = os.path.join(EXTRACT_DIR, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Warning: Directory not found: {class_dir}\")\n",
    "        continue\n",
    "    \n",
    "    files = os.listdir(class_dir)[:SAMPLE_SIZE]\n",
    "    print(f\"Loading class '{class_name}': {len(files)} images\")\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(class_dir, file)\n",
    "        try:\n",
    "            img = load_image(file_path, target_size=RESIZE, grayscale=True)\n",
    "            images.append(img)\n",
    "            labels.append(class_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal images loaded: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(images)\n",
    "y_labels = np.array(labels)\n",
    "\n",
    "# Flatten images: (N, 16, 16) -> (N, 256)\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "# Create label mapping\n",
    "label_to_idx = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "y_raw = np.array([label_to_idx[label] for label in y_labels])\n",
    "\n",
    "# One-hot encode labels\n",
    "y = to_categorical(y_raw)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verify Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "unique, counts = np.unique(y_raw, return_counts=True)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for class_idx, count in zip(unique, counts):\n",
    "    print(f\"  {CLASS_NAMES[class_idx]}: {count} ({100*count/len(y_raw):.1f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = max(counts) / min(counts)\n",
    "print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Decision: {'Use Accuracy (balanced)' if imbalance_ratio < 3 else 'Use F1-Score (imbalanced)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN/TEST SPLIT (90%/10%)\n",
    "# ============================================================\n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_raw,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Also keep raw labels for test set\n",
    "_, _, y_train_full_raw, y_test_raw = train_test_split(\n",
    "    X, y_raw,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_raw,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training + Validation: {X_train_full.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Normalise Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALISE PIXEL VALUES [0, 1]\n",
    "# ============================================================\n",
    "# Note: skimage resize already normalises to [0, 1], but we ensure it\n",
    "X_train_full = X_train_full.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Verify normalisation\n",
    "print(f\"Feature range: [{X_train_full.min():.3f}, {X_train_full.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.7 Configure K-Fold Cross-Validation\n\nSince our dataset has ~9,000 samples (below the 10,000 threshold), we use **5-Fold Stratified Cross-Validation** instead of a simple hold-out validation split."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# K-FOLD CROSS-VALIDATION SETUP\n# ============================================================\nN_FOLDS = 5\n\n# Use StratifiedKFold to preserve class balance in each fold\nskfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nprint(f\"K-Fold Configuration:\")\nprint(f\"  Number of folds: {N_FOLDS}\")\nprint(f\"  Training pool: {X_train_full.shape[0]:,} samples\")\nprint(f\"  Samples per fold: ~{X_train_full.shape[0] // N_FOLDS:,}\")\nprint(f\"  Test set (held out): {X_test.shape[0]:,} samples\")\n\n# For initial model development, we use the first fold\n# Final evaluation will use all folds\nfirst_fold = list(skfold.split(X_train_full, y_train_full.argmax(axis=1)))[0]\ntrain_idx, val_idx = first_fold\n\nX_train = X_train_full[train_idx]\nX_val = X_train_full[val_idx]\ny_train = y_train_full[train_idx]\ny_val = y_train_full[val_idx]\n\n# Keep raw labels for train set (for class weights)\ny_train_raw = y_train.argmax(axis=1)\n\nprint(f\"\\nFirst fold (for initial development):\")\nprint(f\"  Training: {X_train.shape[0]:,} samples\")\nprint(f\"  Validation: {X_val.shape[0]:,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Visualise Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display sample images from each class\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\nfig.suptitle('Sample Images (32×32 Grayscale)', fontsize=14)\n\nfor class_idx, class_name in enumerate(CLASS_NAMES):\n    # Get first sample of this class\n    sample_idx = np.where(y_train_raw == class_idx)[0][0]\n    \n    ax = axes[class_idx]\n    # Reshape flattened image back to 2D\n    img = X_train[sample_idx].reshape(RESIZE)\n    ax.imshow(img, cmap='gray')\n    ax.axis('off')\n    ax.set_title(f\"Letter '{class_name}'\", fontsize=12)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "**Baseline for 3-class balanced problem:** 33.3% accuracy (random guessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MODEL CONFIGURATION\n# ============================================================\nINPUT_DIMENSION = X_train.shape[1]  # 1024 features (32x32)\nOUTPUT_CLASSES = y_train.shape[1]   # 3 classes\n\nOPTIMIZER = 'adam'\nLOSS_FUNC = 'categorical_crossentropy'\nMETRICS = ['accuracy']\n\n# Training configuration\n# Batch Size Selection:\n# - Large datasets (>10,000 samples): Use 512 for efficient GPU utilisation\n# - Small datasets (<10,000 samples): Use 32-64 for better gradient estimates\n# ASL has ~9,000 samples → Use batch size 64 (below threshold, prioritise gradient quality)\nBATCH_SIZE = 64\nEPOCHS_BASELINE = 100\nEPOCHS_REGULARIZED = 150\n\nprint(f\"Input Dimension: {INPUT_DIMENSION}\")\nprint(f\"Output Classes: {OUTPUT_CLASSES}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTABLISH BASELINE\n",
    "# ============================================================\n",
    "# For balanced 3-class classification, random guessing = 33.3%\n",
    "baseline_accuracy = 1.0 / OUTPUT_CLASSES\n",
    "\n",
    "print(f\"Baseline Accuracy (random guessing): {baseline_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CLASS WEIGHTS - Not needed for balanced dataset\n# ============================================================\n# Note: ASL dataset is perfectly balanced (3,000 samples per class)\n# Class weights are not necessary for balanced datasets.\n# We keep this cell for consistency with other notebooks and\n# to demonstrate when class weights would be used.\n\nprint(\"Class Distribution (balanced - no class weights needed):\")\nfor class_idx, class_name in enumerate(CLASS_NAMES):\n    count = np.sum(y_train_raw == class_idx)\n    print(f\"  {class_name}: {count} ({100*count/len(y_train_raw):.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SINGLE LAYER PERCEPTRON (SLP) - Simplest possible model\n",
    "# ============================================================\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(INPUT_DIMENSION,)))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train SLP\nslp_history = slp_model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_BASELINE,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\nslp_val_acc = slp_model.evaluate(X_val, y_val, verbose=0)[1]\nprint(f\"SLP Validation Accuracy: {slp_val_acc:.4f} (baseline: {baseline_accuracy:.2f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# ============================================================\n",
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"Plot training and validation loss/accuracy curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], 'b-', label='Training Loss')\n",
    "    axes[0].plot(history.history['val_loss'], 'r-', label='Validation Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "    axes[1].plot(history.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(slp_history, 'Single Layer Perceptron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding a hidden layer to learn more complex features for distinguishing hand gestures.\n\n**No regularisation applied:** We intentionally train this model **without any regularisation** (no dropout, no L2, no early stopping) to observe overfitting behaviour.\n\n---\n\n### Architecture Design Decisions\n\n**Why 64 neurons in the hidden layer?**\n\nThis is a practical starting point that balances capacity and efficiency:\n- **Too few (e.g., 16):** May not have enough capacity to distinguish subtle hand gesture differences\n- **Too many (e.g., 512):** Increases overfitting risk and training time without proportional benefit\n- **64 neurons:** A common choice that provides sufficient capacity for most classification tasks\n\n**Why only 1 hidden layer instead of 2-3?**\n\nPer the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n\n1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation\n\n*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTI-LAYER PERCEPTRON (MLP) - Standard architecture\n",
    "# ============================================================\n",
    "HIDDEN_NEURONS = 64\n",
    "\n",
    "mlp_model = Sequential(name='Multi_Layer_Perceptron')\n",
    "mlp_model.add(Dense(HIDDEN_NEURONS, activation='relu', input_shape=(INPUT_DIMENSION,)))\n",
    "mlp_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "mlp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train MLP\nmlp_history = mlp_model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS_BASELINE,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n\nmlp_val_acc = mlp_model.evaluate(X_val, y_val, verbose=0)[1]\nprint(f\"MLP Validation Accuracy: {mlp_val_acc:.4f} (baseline: {baseline_accuracy:.2f})\")\nprint(f\"Improvement over SLP: {(mlp_val_acc - slp_val_acc)*100:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(mlp_history, 'Multi-Layer Perceptron (1 Hidden Layer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning with L2 regularisation and Dropout.\n",
    "\n",
    "### Why Hyperband?\n",
    "\n",
    "**Hyperband** is more efficient than grid search because it:\n",
    "1. Starts training many configurations for a few epochs\n",
    "2. Eliminates poor performers early\n",
    "3. Allocates more resources to promising configurations\n",
    "\n",
    "### Regularisation Strategy\n",
    "\n",
    "| Technique | Purpose | How It Works |\n",
    "|-----------|---------|-------------|\n",
    "| **L2 Regularisation** | Prevent large weights | Adds penalty term to loss |\n",
    "| **Dropout** | Prevent co-adaptation | Randomly zeros neurons during training |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPERBAND MODEL BUILDER\n",
    "# ============================================================\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build ASL model with FROZEN architecture (1 hidden layer, 64 neurons).\n",
    "    Tunes: L2 regularisation, Dropout rate, Learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    l2_reg = hp.Float('l2_reg', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # Hidden layer with L2 regularisation\n",
    "    model.add(layers.Dense(\n",
    "        HIDDEN_NEURONS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(l2_reg)\n",
    "    ))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURE AND RUN HYPERBAND TUNER\n# ============================================================\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_accuracy',\n    max_epochs=50,\n    factor=3,\n    directory='asl_hyperband',\n    project_name='asl_tuning',\n    overwrite=True\n)\n\n# Run search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=50,\n    batch_size=BATCH_SIZE,\n    verbose=0\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GET BEST HYPERPARAMETERS\n",
    "# ============================================================\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\n",
    "print(f\"  Dropout Rate: {best_hp.get('dropout_rate'):.2f}\")\n",
    "print(f\"  Learning Rate: {best_hp.get('learning_rate'):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Why Train the Regularised Model Longer?\n\n**Regularisation slows down learning.** Here's why:\n\n| Technique | Effect on Learning |\n|-----------|-------------------|\n| **Dropout** | Randomly masks neurons each batch → each gradient update uses only partial network information |\n| **L2 penalty** | Penalises large weights → constrains the size of weight updates |\n\nBoth techniques deliberately impede the optimisation process. The model takes smaller, noisier steps toward the solution.\n\n| Model | Epochs | Why This Number? |\n|-------|--------|------------------|\n| **SLP (baseline)** | 100 | Simple model, converges quickly |\n| **DNN (no regularisation)** | 100 | Enough to clearly demonstrate overfitting |\n| **DNN (with Dropout + L2)** | 150 | Compensates for slower learning; ensures full convergence |\n\n> *\"Regularisation adds noise and constraints that slow down learning. In exchange for protection against overfitting, the model needs more iterations to converge.\"*\n\n**The insight:** *Regularisation buys you the freedom to train longer.*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD AND TRAIN BEST MODEL\n",
    "# ============================================================\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the best model with more epochs\nbest_history = best_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=EPOCHS_REGULARIZED,\n    batch_size=BATCH_SIZE,\n    verbose=0\n)\n\nbest_val_acc = best_model.evaluate(X_val, y_val, verbose=0)[1]\nprint(f\"Best Model Validation Accuracy: {best_val_acc:.4f}\")\nprint(f\"Improvement over MLP: {(best_val_acc - mlp_val_acc)*100:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(best_history, 'Regularised Model (L2 + Dropout)')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# K-FOLD CROSS-VALIDATION EVALUATION\n# ============================================================\ndef evaluate_with_kfold(build_fn, X, y, y_raw, skfold, epochs, batch_size):\n    \"\"\"\n    Evaluate a model using Stratified K-Fold cross-validation.\n    \"\"\"\n    fold_metrics = {'accuracy': [], 'precision': [], 'recall': []}\n    \n    for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y_raw)):\n        X_train_fold = X[train_idx]\n        X_val_fold = X[val_idx]\n        y_train_fold = y[train_idx]\n        y_val_fold = y[val_idx]\n        y_val_raw_fold = y_raw[val_idx]\n        \n        # Build fresh model for each fold\n        model = build_fn()\n        \n        # Train (no class weights - dataset is balanced)\n        model.fit(\n            X_train_fold, y_train_fold,\n            validation_data=(X_val_fold, y_val_fold),\n            epochs=epochs, batch_size=batch_size,\n            verbose=0\n        )\n        \n        # Evaluate\n        preds = model.predict(X_val_fold, verbose=0).argmax(axis=1)\n        fold_metrics['accuracy'].append(accuracy_score(y_val_raw_fold, preds))\n        fold_metrics['precision'].append(precision_score(y_val_raw_fold, preds, average='macro'))\n        fold_metrics['recall'].append(recall_score(y_val_raw_fold, preds, average='macro'))\n        \n        print(f\"  Fold {fold+1}: Accuracy={fold_metrics['accuracy'][-1]:.4f}\")\n    \n    return {\n        'acc_mean': np.mean(fold_metrics['accuracy']),\n        'acc_std': np.std(fold_metrics['accuracy']),\n        'prec_mean': np.mean(fold_metrics['precision']),\n        'prec_std': np.std(fold_metrics['precision']),\n        'rec_mean': np.mean(fold_metrics['recall']),\n        'rec_std': np.std(fold_metrics['recall'])\n    }\n\n# Build function using best hyperparameters\ndef build_best_model():\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n    model.add(layers.Dense(HIDDEN_NEURONS, activation='relu',\n                           kernel_regularizer=regularizers.l2(best_hp.get('l2_reg'))))\n    model.add(layers.Dropout(best_hp.get('dropout_rate')))\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=best_hp.get('learning_rate')),\n        loss=LOSS_FUNC, metrics=METRICS\n    )\n    return model\n\nprint(\"Evaluating best model with 5-Fold Stratified Cross-Validation...\")\nkfold_results = evaluate_with_kfold(\n    build_best_model, X_train_full, y_train_full, y_train_full_raw,\n    skfold, EPOCHS_REGULARIZED, BATCH_SIZE\n)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"K-FOLD CROSS-VALIDATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Accuracy:  {kfold_results['acc_mean']:.4f} ± {kfold_results['acc_std']:.4f}\")\nprint(f\"Precision: {kfold_results['prec_mean']:.4f} ± {kfold_results['prec_std']:.4f}\")\nprint(f\"Recall:    {kfold_results['rec_mean']:.4f} ± {kfold_results['rec_std']:.4f}\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Final Evaluation\n\nTrain the final model on the **entire training pool** and evaluate on the held-out test set."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# FINAL MODEL: Train on ALL training data, evaluate on test set\n# ============================================================\n# Build final model with best hyperparameters\nfinal_model = build_best_model()\n\n# Train on entire training pool (no class weights - dataset is balanced)\nfinal_model.fit(\n    X_train_full, y_train_full,\n    epochs=EPOCHS_REGULARIZED,\n    batch_size=BATCH_SIZE,\n    verbose=0\n)\n\n# Evaluate on held-out test set\ny_pred_proba = final_model.predict(X_test, verbose=0)\ny_pred = y_pred_proba.argmax(axis=1)\n\n# Calculate metrics\ntest_accuracy = accuracy_score(y_test_raw, y_pred)\ntest_precision = precision_score(y_test_raw, y_pred, average='macro')\ntest_recall = recall_score(y_test_raw, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n\nprint(\"=\"*50)\nprint(\"FINAL TEST SET RESULTS\")\nprint(\"=\"*50)\nprint(f\"Accuracy:  {test_accuracy:.4f} (baseline: {baseline_accuracy:.2f})\")\nprint(f\"Precision: {test_precision:.4f}\")\nprint(f\"Recall:    {test_recall:.4f}\")\nprint(f\"AUC:       {test_auc:.4f}\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRIX\n",
    "# ============================================================\n",
    "cm = confusion_matrix(y_test_raw, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax, cmap='Blues')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i, :].sum()\n",
    "    print(f\"  {class_name}: {correct}/{total} correct ({100*correct/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MODEL COMPARISON\n# ============================================================\nprint(\"\\nModel Comparison:\")\nprint(\"=\"*60)\nprint(f\"{'Model':<35} {'Accuracy':>15} {'Dataset':>10}\")\nprint(\"-\"*60)\nprint(f\"{'Baseline (random)':<35} {baseline_accuracy:>15.4f} {'N/A':>10}\")\nprint(f\"{'Single Layer Perceptron':<35} {slp_val_acc:>15.4f} {'Fold 1':>10}\")\nprint(f\"{'Multi-Layer Perceptron':<35} {mlp_val_acc:>15.4f} {'Fold 1':>10}\")\nprint(f\"{'Regularised (L2+Dropout) K-Fold':<35} {kfold_results['acc_mean']:.4f} ± {kfold_results['acc_std']:.4f}  {'5-Fold CV':>10}\")\nprint(f\"{'Final Model - Test Set':<35} {test_accuracy:>15.4f} {'Test':>10}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | ~9,000 samples | **K-Fold (5 folds)** | Kohavi (1995) |\n| **Primary Metric** | Balanced multi-class | Equal distribution | Accuracy | Standard choice |\n| **Class Weights** | Imbalance > 3:1 | 1:1 (balanced) | Not needed | He and Garcia (2009) |\n\n### Lessons Learned\n\n1. **K-Fold for Medium Datasets:** With ~9,000 samples (below the 10,000 threshold), K-Fold cross-validation provides more robust performance estimates.\n\n2. **K-Fold Reports Mean ± Std:** Unlike hold-out (single number), K-Fold gives us confidence intervals.\n\n3. **No Class Weights for Balanced Data:** Since each class has exactly 3,000 samples, class weights are unnecessary.\n\n4. **Image Preprocessing:** Resize → Grayscale → Flatten → Normalise converts images to vectors for dense networks.\n\n5. **Flattening loses spatial information:** Dense networks treat pixels as independent features, ignoring spatial relationships.\n\n6. **High accuracy on ASL:** The 3-class problem (A, B, C) is relatively easy for neural networks due to distinct hand shapes.\n\n7. **Final Model Uses All Training Data:** After K-Fold validation, the final model is trained on the entire training pool.\n\n### Next Steps for Better Performance\n\n- **Expand to full alphabet** (26 classes) - more challenging\n- **Use CNNs** (Chapter 8) - preserves spatial structure\n- **Higher resolution** - 32×32 is still relatively small\n- **Data augmentation** - artificial variations for robustness"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}