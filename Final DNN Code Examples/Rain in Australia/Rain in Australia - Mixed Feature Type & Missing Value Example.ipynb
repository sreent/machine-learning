{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Rain%20in%20Australia/Rain%20in%20Australia%20-%20Mixed%20Feature%20Type%20%26%20Missing%20Value%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Rain in Australia - Mixed Feature Type & Missing Value Example\n",
    "\n",
    "This notebook demonstrates the **Universal ML Workflow** applied to a binary classification problem with **mixed feature types** (categorical and numerical) and **missing values**.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Handle **mixed feature types** (categorical + numerical) using `ColumnTransformer`\n",
    "- Apply appropriate preprocessing: **One-Hot Encoding** for categorical, **Standardisation** for numerical\n",
    "- Handle **missing values** using different strategies (kNN imputation for numerical, \"Unknown\" category for categorical)\n",
    "- Address **class imbalance** using class weights during training\n",
    "- Build and train deep neural networks for **binary classification**\n",
    "- Use **Hyperband** for efficient hyperparameter tuning\n",
    "- Apply **Dropout + L2 regularisation** to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| **Source** | [Kaggle Weather Dataset](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) |\n",
    "| **Problem Type** | Binary Classification |\n",
    "| **Target Variable** | RainTomorrow (Yes/No) |\n",
    "| **Data Balance** | Imbalanced (~78% No, ~22% Yes) |\n",
    "| **Data Type** | Structured Tabular (Mixed Categorical & Numerical) |\n",
    "| **Missing Data** | Significant missing values in many columns |\n",
    "| **Features** | 16 numerical + 5 categorical variables |\n",
    "| **Imbalance Handling** | Class Weights during Training |\n",
    "\n",
    "---\n",
    "\n",
    "## Technique Scope\n",
    "\n",
    "This notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n",
    "\n",
    "| Technique | Status | Rationale |\n",
    "|-----------|--------|-----------|\n",
    "| **Dense layers (DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n",
    "| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n",
    "| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n",
    "| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n",
    "| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n",
    "| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n",
    "\n",
    "We demonstrate that **Dropout + L2 regularisation** alone can effectively prevent overfitting without requiring early stopping.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Problem and Assembling a Dataset\n",
    "\n",
    "The first step in any machine learning project is to clearly define the problem and understand the data.\n",
    "\n",
    "**Problem Statement:** Given weather observations from various Australian locations, predict whether it will rain tomorrow.\n",
    "\n",
    "**Why this problem is interesting:**\n",
    "- Requires handling **mixed feature types** - we have both categorical (Location, WindDirection) and numerical (Temperature, Humidity) features\n",
    "- Contains **missing values** that must be handled appropriately\n",
    "- Features **class imbalance** - rain days are less common than non-rain days\n",
    "- Real-world application: agricultural planning, event scheduling, water resource management\n",
    "\n",
    "**Data Source:** This dataset contains about 10 years of daily weather observations from numerous Australian weather stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing a Measure of Success\n",
    "\n",
    "### Metric Selection Based on Class Imbalance\n",
    "\n",
    "The choice of evaluation metric depends on **class imbalance**. We use practical guidelines derived from the literature:\n",
    "\n",
    "| Imbalance Ratio | Classification | Primary Metric | Rationale |\n",
    "|-----------------|----------------|----------------|-----------|\n",
    "| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n",
    "| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n",
    "| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n",
    "\n",
    "**Why these thresholds?**\n",
    "- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n",
    "- **F1-Score**: Harmonic mean of precision and recall, effective for imbalanced data (He and Garcia, 2009)\n",
    "\n",
    "### References\n",
    "\n",
    "- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modelling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n",
    "\n",
    "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
    "\n",
    "*Note: The 3:1 threshold is a practical guideline, not a strict academic standard. The literature suggests metric choice depends on domain-specific costs of errors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\nThe choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n| Deep Learning | Hold-Out (preferred) | Training cost prohibitive for K iterations |\n\n**Why 10,000 as a practical threshold?**\n- Below 10,000 samples, hold-out validation has higher variance (Kohavi, 1995)\n- Above 10,000, statistical estimates from hold-out are reliable\n- Deep learning models are expensive to train; K-fold multiplies cost by K (Chollet, 2021)\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n\n- Pedregosa, F. et al. (2011) 'Scikit-learn: machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830.\n\n*Note: The 10,000 threshold is a practical guideline. For computationally cheap models, K-fold is preferred regardless of size.*\n\n### Data Split Strategy (This Notebook)\n\n```\nOriginal Data (~142,000 samples) → Hold-Out Selected\n├── Test Set (10%) - Final evaluation only\n└── Training Pool (90%)\n    ├── Training Set (~81%) - Model training\n    └── Validation Set (~9%) - Hyperparameter tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries and Set Random Seed\n",
    "\n",
    "We set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Keras Tuner for hyperparameter search\n",
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 204\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load and Explore the Dataset\n",
    "\n",
    "Let's download the weather data from Google Drive and examine its structure. Notice the many NaN (missing) values in several columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data directly from Google Drive\n",
    "GDRIVE_FILE_ID = '1gt0c-jdMPYs_o7SBP67Al7Kg-5Ij3xY5'\n",
    "DATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\n",
    "\n",
    "weather = pd.read_csv(DATA_URL)\n",
    "\n",
    "print(f\"Dataset shape: {weather.shape}\")\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine numerical features\n",
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "NUMERICAL_VARIABLES = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', \n",
    "                       'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n",
    "                       'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm',\n",
    "                       'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n",
    "\n",
    "CATEGORICAL_VARIABLES = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine categorical features\n",
    "weather.drop(NUMERICAL_VARIABLES, axis=1).describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_pct = (weather.isnull().sum() / len(weather) * 100).round(1)\n",
    "print(\"Missing Values (%):\\n\")\n",
    "print(missing_pct[missing_pct > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Clean Data and Define Features\n",
    "\n",
    "We drop rows where the target variable or location is missing, as these cannot be imputed meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where target variable or location is missing\n",
    "weather = weather[~weather['RainTomorrow'].isnull() & ~weather['Location'].isnull()]\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {weather.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "COLUMNS = CATEGORICAL_VARIABLES + NUMERICAL_VARIABLES\n",
    "\n",
    "features = weather[COLUMNS]\n",
    "\n",
    "TARGET_VARIABLE = 'RainTomorrow'\n",
    "target = weather[TARGET_VARIABLE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Split Data into Train and Test Sets\n",
    "\n",
    "We reserve 15% of the data for final testing. The `stratify` parameter ensures that each split maintains the same class proportions as the original dataset - critical for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "TEST_SIZE = 0.10\n\n(features_train, features_test, \n target_train, target_test) = train_test_split(features, target, \n                                                test_size=TEST_SIZE, stratify=target,\n                                                shuffle=True, random_state=SEED)\n\nprint(f\"Training set: {len(features_train):,} samples\")\nprint(f\"Test set: {len(features_test):,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Handle Missing Values\n",
    "\n",
    "**Missing Value Strategies:**\n",
    "\n",
    "| Feature Type | Strategy | Rationale |\n",
    "|--------------|----------|-----------|\n",
    "| **Numerical** | kNN Imputation | Estimates missing values based on similar observations |\n",
    "| **Categorical** | Fill with \"Unknown\" | Creates an explicit category for missing data |\n",
    "\n",
    "**Why kNN Imputation for numerical features?**\n",
    "- Preserves relationships between features better than simple mean/median imputation\n",
    "- Uses information from similar samples to estimate missing values\n",
    "- More sophisticated than assuming all missing values equal the mean\n",
    "\n",
    "**Why \"Unknown\" for categorical features?**\n",
    "- Missing categorical data may carry information (e.g., \"wind direction not recorded\" could correlate with calm conditions)\n",
    "- Creating an explicit category allows the model to learn patterns associated with missingness\n",
    "\n",
    "**Important:** We fit imputers only on training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for numerical features using kNN\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "knn_imputer.fit(features_train[NUMERICAL_VARIABLES])\n",
    "\n",
    "numerical_train = knn_imputer.transform(features_train[NUMERICAL_VARIABLES])\n",
    "numerical_test = knn_imputer.transform(features_test[NUMERICAL_VARIABLES])\n",
    "\n",
    "print(f\"Numerical features imputed: {numerical_train.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values with \"Unknown\"\n",
    "categorical_train = features_train[CATEGORICAL_VARIABLES].fillna('Unknown')\n",
    "categorical_test = features_test[CATEGORICAL_VARIABLES].fillna('Unknown')\n",
    "\n",
    "print(f\"Categorical features: {categorical_train.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Preprocessing Pipeline with ColumnTransformer\n",
    "\n",
    "We use `ColumnTransformer` to apply different preprocessing to different feature types simultaneously:\n",
    "\n",
    "```\n",
    "ColumnTransformer\n",
    "├── Categorical Features → One-Hot Encoding\n",
    "│   (Creates binary columns for each category)\n",
    "└── Numerical Features → Standard Scaling\n",
    "    (Mean=0, Std=1)\n",
    "```\n",
    "\n",
    "This is a common pattern for mixed-type datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why One-Hot Encoding for Categorical Features?\n",
    "\n",
    "| Encoding | How it works | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **One-Hot** | Creates binary column per category | No ordinal assumption, works with any model | High dimensionality for many categories |\n",
    "| **Label Encoding** | Maps categories to integers | Compact representation | Implies false ordinal relationship |\n",
    "| **Target Encoding** | Maps to target mean | Captures target relationship | Risk of overfitting, data leakage |\n",
    "\n",
    "We use **One-Hot Encoding** because:\n",
    "1. Neural networks work well with binary features\n",
    "2. No false ordinal relationships (e.g., \"Sydney\" is not > \"Melbourne\")\n",
    "3. `handle_unknown=\"ignore\"` gracefully handles unseen categories in test data\n",
    "\n",
    "#### Why Standardisation for Numerical Features?\n",
    "\n",
    "Neural networks train faster and more stably when inputs have similar scales:\n",
    "- **Before:** Features range from 0-100 (humidity) to 980-1040 (pressure)\n",
    "- **After:** All features have mean=0 and std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine imputed numerical and filled categorical features\n",
    "combined_train = pd.DataFrame(\n",
    "    data=np.hstack((numerical_train, categorical_train)), \n",
    "    columns=NUMERICAL_VARIABLES + CATEGORICAL_VARIABLES\n",
    ")\n",
    "\n",
    "combined_test = pd.DataFrame(\n",
    "    data=np.hstack((numerical_test, categorical_test)), \n",
    "    columns=NUMERICAL_VARIABLES + CATEGORICAL_VARIABLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_VARIABLES),\n",
    "    ('standard_scaler', StandardScaler(), NUMERICAL_VARIABLES)\n",
    "])\n",
    "\n",
    "# Fit on training data only (prevent data leakage)\n",
    "preprocessor.fit(combined_train)\n",
    "\n",
    "# Transform both sets\n",
    "X_train_full = preprocessor.transform(combined_train)\n",
    "X_test = preprocessor.transform(combined_test)\n",
    "\n",
    "print(f\"Preprocessed training shape: {X_train_full.shape}\")\n",
    "print(f\"Preprocessed test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Encode Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable (Yes=1, No=0)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(target)\n",
    "\n",
    "y_train_full = label_encoder.transform(target_train)\n",
    "y_test = label_encoder.transform(target_test)\n",
    "\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Encoding: No=0, Yes=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "Before building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n",
    "\n",
    "### 5.1 Examine Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = target.value_counts().sort_index()\n",
    "print(\"Class distribution:\")\n",
    "print(counts)\n",
    "print(f\"\\nTotal samples: {counts.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset size analysis (for hold-out vs K-fold decision)\n",
    "n_samples = len(weather)\n",
    "HOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000 (Kohavi, 1995; Chollet, 2021)\n",
    "\n",
    "# Imbalance analysis (for metric selection)\n",
    "majority_class = counts.max()\n",
    "minority_class = counts.min()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "IMBALANCE_THRESHOLD = 3.0  # Use F1-Score if ratio > 3.0 (He & Garcia, 2009)\n",
    "\n",
    "# Determine evaluation strategy and metric\n",
    "use_holdout = n_samples > HOLDOUT_THRESHOLD\n",
    "use_f1 = imbalance_ratio > IMBALANCE_THRESHOLD\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA-DRIVEN CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\n",
    "print(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples (Kohavi, 1995)\")\n",
    "print(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n",
    "\n",
    "print(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\n",
    "print(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1 (He & Garcia, 2009)\")\n",
    "print(f\"   Decision: {'F1-Score (imbalanced)' if use_f1 else 'Accuracy (balanced)'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "PRIMARY_METRIC = 'f1' if use_f1 else 'accuracy'\n",
    "print(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate Baseline Metrics\n",
    "\n",
    "**Naive Baseline (Majority Class):** If we always predict \"No Rain\", we achieve ~78% accuracy. This is our accuracy baseline.\n",
    "\n",
    "**Balanced Accuracy Baseline:** A random classifier would achieve 50% balanced accuracy. This is more meaningful for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy (always predict majority class)\n",
    "baseline = counts['No'] / counts.sum()\n",
    "\n",
    "# Balanced accuracy baseline (random classifier)\n",
    "balanced_accuracy_baseline = 0.5  # For binary classification\n",
    "\n",
    "print(f\"Baseline accuracy (always predict 'No'): {baseline:.2%}\")\n",
    "print(f\"Balanced accuracy baseline (random): {balanced_accuracy_baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create Validation Set\n",
    "\n",
    "We split off a portion of the training data for validation. This will be used to:\n",
    "- Evaluate model performance during hyperparameter tuning\n",
    "- Compare models without touching the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "VALIDATION_SIZE = 0.10\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, \n    test_size=VALIDATION_SIZE, stratify=y_train_full,\n    shuffle=True, random_state=SEED\n)\n\nprint(f\"Training set: {X_train.shape[0]:,} samples\")\nprint(f\"Validation set: {X_val.shape[0]:,} samples\")\nprint(f\"Test set: {X_test.shape[0]:,} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Configure Training Parameters\n",
    "\n",
    "**Key training settings:**\n",
    "- **Optimiser:** Adam - adaptive learning rate optimiser with momentum, widely used for deep learning\n",
    "- **Loss:** Binary cross-entropy - standard loss for binary classification\n",
    "- **Training Metrics:** Accuracy, Precision, Recall, AUC (tracked by Keras during training)\n",
    "- **Primary Metric:** F1-Score - computed separately after training using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_DIMENSION = 1  # Binary classification: single output neuron\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNC = 'binary_crossentropy'\n",
    "\n",
    "# Training metrics (tracked by Keras during training)\n",
    "# Note: F1-Score (our primary metric) is computed separately using sklearn\n",
    "METRICS = ['accuracy', \n",
    "           tf.keras.metrics.Precision(name='precision'), \n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "print(f\"Input dimension: {INPUT_DIMENSION}\")\n",
    "print(f\"Output dimension: {OUTPUT_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Layer Perceptron (no hidden layers) - Baseline\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "slp_model.add(Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# We use DIFFERENT epoch counts for different training phases:\n",
    "#\n",
    "# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n",
    "#   - SLP converges quickly (simple model)\n",
    "#   - Unregularised DNN: 100 epochs clearly shows overfitting (val_loss increasing)\n",
    "#\n",
    "# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2\n",
    "#   - WHY train longer? Regularisation SLOWS DOWN learning:\n",
    "#     * Dropout randomly masks neurons, so each update uses partial information\n",
    "#     * L2 penalty constrains weight updates, preventing large steps\n",
    "#     * The model needs MORE iterations to reach the same level of convergence\n",
    "#   - Without extra epochs, we'd stop before the model reaches its full potential\n",
    "#   - With regularisation, longer training is SAFE (no overfitting risk)\n",
    "#\n",
    "# The trade-off: Regularisation exchanges faster convergence for overfitting protection.\n",
    "# We compensate by allowing more training time.\n",
    "\n",
    "EPOCHS_BASELINE = 100      # SLP and DNN (no regularisation)\n",
    "EPOCHS_REGULARIZED = 150   # DNN with Dropout + L2 (needs more time to converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Handle Class Imbalance with Class Weights\n",
    "\n",
    "To handle imbalanced classes, we compute **class weights** that give more importance to the minority class during training:\n",
    "- **No (majority):** Lower weight\n",
    "- **Yes (minority):** Higher weight\n",
    "\n",
    "This makes errors on the minority class \"cost more\", encouraging the model to learn it better.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Class Weights Instead of Resampling?\n",
    "\n",
    "| Technique | How it works | Pros | Cons |\n",
    "|-----------|--------------|------|------|\n",
    "| **Class Weights** | Adjusts loss function to penalise minority errors more | Simple, no data modification | Doesn't add information |\n",
    "| **Oversampling (SMOTE)** | Creates synthetic minority samples | Adds training data | Risk of overfitting to synthetic data |\n",
    "| **Undersampling** | Removes majority class samples | Balances dataset | Loses potentially useful information |\n",
    "\n",
    "We use **class weights** because:\n",
    "1. **Simplicity:** No need to modify the dataset\n",
    "2. **No synthetic data risk:** SMOTE can create unrealistic samples\n",
    "3. **Efficiency:** Training time unchanged\n",
    "4. **Keras integration:** Native support via `class_weight` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for imbalanced data\n",
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "print(\"Class weights:\")\n",
    "print(f\"  No (0):  {CLASS_WEIGHTS[0]:.4f}\")\n",
    "print(f\"  Yes (1): {CLASS_WEIGHTS[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Single-Layer Perceptron\n",
    "history_slp = slp_model.fit(\n",
    "    X_train, y_train, \n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "val_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SLP validation metrics\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_slp[0], baseline))\n",
    "print('Precision (Validation): {:.2f}'.format(val_score_slp[1]))\n",
    "print('Recall (Validation): {:.2f}'.format(val_score_slp[2]))\n",
    "print('AUC (Validation): {:.2f}'.format(val_score_slp[3]))\n",
    "\n",
    "preds_slp_val = (slp_model.predict(X_val, verbose=0) > 0.5).astype('int32').flatten()\n",
    "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
    "    balanced_accuracy_score(y_val, preds_slp_val), balanced_accuracy_baseline))\n",
    "\n",
    "# Calculate F1-Score (primary metric for imbalanced data)\n",
    "f1_slp_val = f1_score(y_val, preds_slp_val)\n",
    "print(f'F1-Score (Validation): {f1_slp_val:.2f}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs.\n",
    "    Plots: (1) Loss, (2) Accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras History object\n",
    "        Training history from model.fit()\n",
    "    title : str, optional\n",
    "        Model name to display in plot titles (e.g., 'SLP', 'DNN')\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    title_suffix = f' ({title})' if title else ''\n",
    "\n",
    "    # Plot 1: Loss\n",
    "    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[0].set_title(f'Loss{title_suffix}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Accuracy\n",
    "    axs[1].plot(epochs, history.history['accuracy'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[1].plot(epochs, history.history['val_accuracy'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[1].set_title(f'Accuracy{title_suffix}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SLP training history\n",
    "plot_training_history(history_slp, title='SLP Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling Up: Developing a Model That Overfits\n",
    "\n",
    "The next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n",
    "\n",
    "**Strategy:** Add hidden layers and neurons to increase model capacity.\n",
    "\n",
    "**No regularisation applied:** We intentionally train this model **without any regularisation** (no dropout, no L2, no early stopping) to observe overfitting behaviour. In the training plots, you should see:\n",
    "- Training loss continues to decrease\n",
    "- Validation loss starts increasing after some epochs (overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "#### Architecture Design Decisions\n",
    "\n",
    "**Why 64 neurons in the hidden layer?**\n",
    "\n",
    "This is a practical starting point that balances capacity and efficiency:\n",
    "- **Too few (e.g., 16):** May not have enough capacity to learn complex patterns\n",
    "- **Too many (e.g., 512):** Increases overfitting risk and training time\n",
    "- **64 neurons:** A common choice for tabular data that provides sufficient capacity\n",
    "\n",
    "**Why only 1 hidden layer?**\n",
    "\n",
    "Per the **Universal ML Workflow**, the goal is to demonstrate that the model *can* overfit. Once overfitting is observed:\n",
    "1. **Capacity is proven sufficient**\n",
    "2. **No need for more depth**\n",
    "3. **Regularise, don't expand**\n",
    "\n",
    "*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*\n",
    "\n",
    "### 6.1 Build a Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network (1 hidden layer, no regularisation for overfitting demo)\n",
    "dnn_model = Sequential(name='Deep_Neural_Network')\n",
    "dnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n",
    "dnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Deep Neural Network (without regularisation to demonstrate overfitting)\n",
    "history_dnn = dnn_model.fit(\n",
    "    X_train, y_train, \n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "    validation_data=(X_val, y_val), \n",
    "    verbose=0\n",
    ")\n",
    "val_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DNN training history (expect overfitting: val_loss increasing)\n",
    "plot_training_history(history_dnn, title='DNN - No Regularisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DNN validation metrics\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_dnn[0], baseline))\n",
    "print('Precision (Validation): {:.2f}'.format(val_score_dnn[1]))\n",
    "print('Recall (Validation): {:.2f}'.format(val_score_dnn[2]))\n",
    "print('AUC (Validation): {:.2f}'.format(val_score_dnn[3]))\n",
    "\n",
    "preds_dnn_val = (dnn_model.predict(X_val, verbose=0) > 0.5).astype('int32').flatten()\n",
    "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
    "    balanced_accuracy_score(y_val, preds_dnn_val), balanced_accuracy_baseline))\n",
    "\n",
    "# Calculate F1-Score (primary metric for imbalanced data)\n",
    "f1_dnn_val = f1_score(y_val, preds_dnn_val)\n",
    "print(f'F1-Score (Validation): {f1_dnn_val:.2f}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Now we address the overfitting observed in Section 6 by adding **regularisation**. We use two complementary techniques:\n",
    "\n",
    "| Technique | How it works | Effect |\n",
    "|-----------|--------------|--------|\n",
    "| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation |\n",
    "| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries |\n",
    "\n",
    "**Same architecture, different regularisation:** We keep the same 1-layer architecture (64 neurons) as Section 6, so the only difference is regularisation.\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Hyperband?\n",
    "\n",
    "| Method | How it works | Pros | Cons |\n",
    "|--------|--------------|------|------|\n",
    "| **Grid Search** | Tries all combinations exhaustively | Thorough, reproducible | Exponentially expensive |\n",
    "| **Random Search** | Samples random combinations | More efficient than grid | Still trains all configs to completion |\n",
    "| **Hyperband** | Early stopping of poor performers | Very efficient for deep learning | May discard slow starters prematurely |\n",
    "\n",
    "We use **Hyperband** because:\n",
    "1. **Efficiency:** Eliminates poor configurations early\n",
    "2. **Deep learning fit:** Training epochs are a natural \"resource\" to allocate adaptively\n",
    "3. **Keras Tuner integration:** Native support via `kt.Hyperband`\n",
    "\n",
    "### 7.1 Hyperband Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperband Model Builder for Binary Classification\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build Rain in Australia model with FIXED architecture (1 hidden layer, 64 neurons).\n",
    "    Same architecture as Section 6 DNN - only tunes regularisation and learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "\n",
    "    # L2 regularisation strength\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
    "\n",
    "    # Fixed architecture: 1 hidden layer with 64 neurons (same as Section 6)\n",
    "    model.add(layers.Dense(64, activation='relu', \n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer for binary classification\n",
    "    model.add(layers.Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hyperband tuner\n",
    "# ===========================================================================\n",
    "# TUNING OBJECTIVE: AUC for imbalanced data\n",
    "# ===========================================================================\n",
    "# AUC is threshold-independent and handles imbalanced data well.\n",
    "# F1-Score is computed separately for final evaluation.\n",
    "\n",
    "TUNING_OBJECTIVE = 'val_accuracy' if PRIMARY_METRIC == 'accuracy' else 'val_auc'\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_hyperband,\n",
    "    objective=TUNING_OBJECTIVE,\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='rain_australia_hyperband',\n",
    "    project_name='rain_australia_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"Tuning objective: {TUNING_OBJECTIVE}\")\n",
    "print(\"(Note: Final evaluation uses F1-Score as primary metric)\")\n",
    "\n",
    "# Run Hyperband search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters and best model directly\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters found by Hyperband:\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\n# Get the best model directly - already trained at optimal epochs for these hyperparameters\nopt_model = tuner.get_best_models(num_models=1)[0]\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.2 Using the Best Model Directly\n\nRather than rebuilding and retraining from scratch, we retrieve the best model directly from the tuner using `tuner.get_best_models()`. This approach avoids the **epoch mismatch problem**:\n\n---\n\n#### The Epoch Mismatch Problem\n\nHyperband uses **successive halving** - most configurations train for few epochs, only top performers get more:\n\n```\nHyperband with max_epochs=20, factor=3:\nRound 1: 81 configs × ~1 epoch  → Keep top 27\nRound 2: 27 configs × ~2 epochs → Keep top 9\nRound 3:  9 configs × ~7 epochs → Keep top 3\nRound 4:  3 configs × ~20 epochs → Select best\n```\n\nThe best hyperparameters were found optimal at a **specific epoch count** (e.g., 20 epochs). If we rebuild and retrain for a different number of epochs (e.g., 150), the hyperparameters may no longer be optimal - **this is the epoch mismatch problem**.\n\n---\n\n#### Clean Solution: Use Best Model Directly\n\nInstead of rebuilding, we use `tuner.get_best_models(num_models=1)[0]` to retrieve the model that **already achieved the best validation performance** during tuning. This model:\n\n- Has weights trained at the optimal epoch count for its hyperparameters\n- Achieved the best validation AUC during the Hyperband search\n- Avoids any mismatch between tuning epochs and final epochs\n\n| Approach | Epochs Match? | Issue |\n|----------|---------------|-------|\n| ~~Rebuild + retrain for 150 epochs~~ | ✗ No | Hyperparameters may be suboptimal at 150 epochs |\n| **Use best model directly** | ✓ Yes | Model already trained at optimal epochs |\n\n> *\"Use the model that actually achieved the best performance, not a rebuilt version that might perform differently.\"*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The best model is already trained - evaluate on validation set\nval_score_opt = opt_model.evaluate(X_val, y_val, verbose=0)[1:]\nprint(f\"Validation Accuracy: {val_score_opt[0]:.4f}\")\nprint(f\"Validation Precision: {val_score_opt[1]:.4f}\")\nprint(f\"Validation Recall: {val_score_opt[2]:.4f}\")\nprint(f\"Validation AUC: {val_score_opt[3]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Note: Training history plot is not available when using get_best_models()\n# The best model was retrieved directly from the tuner, which doesn't preserve\n# the training history. To visualise training curves, you would need to either:\n# 1. Use TensorBoard callbacks during tuning, or\n# 2. Retrain the model (but this risks the epoch mismatch problem)\n#\n# For this notebook, we skip the training history plot since we're using\n# the best model directly to ensure optimal performance.\nprint(\"Training history not available when using get_best_models() directly.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_opt_val = (opt_model.predict(X_val, verbose=0) > 0.5).astype('int32').flatten()\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_opt[0], baseline))\n",
    "print('Precision (Validation): {:.2f}'.format(val_score_opt[1]))\n",
    "print('Recall (Validation): {:.2f}'.format(val_score_opt[2]))\n",
    "print('AUC (Validation): {:.2f}'.format(val_score_opt[3]))\n",
    "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
    "    balanced_accuracy_score(y_val, preds_opt_val), balanced_accuracy_baseline))\n",
    "\n",
    "# Calculate F1-Score (primary metric for imbalanced data)\n",
    "f1_opt_val = f1_score(y_val, preds_opt_val)\n",
    "print(f'F1-Score (Validation): {f1_opt_val:.2f}  ← Primary Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Final Model Evaluation on Test Set\n",
    "\n",
    "Now we evaluate our best model on the held-out test set that was never used during training or tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "test_score = opt_model.evaluate(X_test, y_test, verbose=0)[1:]\n",
    "preds_test = (opt_model.predict(X_test, verbose=0) > 0.5).astype('int32').flatten()\n",
    "\n",
    "# Calculate F1-Score (our primary metric)\n",
    "test_f1 = f1_score(y_test, preds_test)\n",
    "\n",
    "print('=' * 50)\n",
    "print('FINAL TEST SET RESULTS')\n",
    "print('=' * 50)\n",
    "print(f'F1-Score (Test): {test_f1:.4f}  ← Primary Metric')\n",
    "print(f'Accuracy (Test): {test_score[0]:.4f} (baseline={baseline:.4f})')\n",
    "print(f'Precision (Test): {test_score[1]:.4f}')\n",
    "print(f'Recall (Test): {test_score[2]:.4f}')\n",
    "print(f'AUC (Test): {test_score[3]:.4f}')\n",
    "print(f'Balanced Accuracy (Test): {balanced_accuracy_score(y_test, preds_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for test predictions\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, preds_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Test Set Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print per-class metrics\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_mask = y_test == i\n",
    "    class_recall = (preds_test[class_mask] == i).mean()\n",
    "    print(f\"  {class_name}: {class_recall:.2%} recall ({class_mask.sum():,} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Results Summary\n",
    "\n",
    "The following dynamically-generated table compares all models trained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "# Create results DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'DNN (No Regularisation)', 'DNN (Dropout + L2)', 'DNN (Dropout + L2) - Test'],\n",
    "    'Accuracy': [baseline, val_score_slp[0], val_score_dnn[0], val_score_opt[0], test_score[0]],\n",
    "    'F1-Score': [0.0, f1_slp_val, f1_dnn_val, f1_opt_val, test_f1],\n",
    "    'Dataset': ['N/A', 'Validation', 'Validation', 'Validation', 'Test']\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON - RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Primary Metric: F1-SCORE (imbalance ratio: {imbalance_ratio:.2f}:1)\")\n",
    "print(\"=\" * 70)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"  - All models outperform naive baseline ({baseline:.2%} accuracy)\")\n",
    "print(f\"  - Regularisation improves F1: {f1_dnn_val:.4f} → {f1_opt_val:.4f}\")\n",
    "print(f\"  - Final test F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### Decision Framework Summary\n",
    "\n",
    "| Decision | Threshold | This Dataset | Choice | Reference |\n",
    "|----------|-----------|--------------|--------|-----------|\n",
    "| **Hold-Out vs K-Fold** | > 10,000 samples | 142,193 samples | Hold-Out | Kohavi (1995); Chollet (2021) |\n",
    "| **Accuracy vs F1-Score** | > 3:1 imbalance | 3.51:1 ratio | F1-Score | He and Garcia (2009) |\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "1. **Mixed Feature Types:** Use `ColumnTransformer` to apply different preprocessing to different feature types:\n",
    "   - **Categorical:** One-Hot Encoding (no false ordinal relationships)\n",
    "   - **Numerical:** Standard Scaling (consistent scale for neural networks)\n",
    "\n",
    "2. **Missing Value Handling:** Different strategies for different feature types:\n",
    "   - **Numerical:** kNN imputation preserves feature relationships\n",
    "   - **Categorical:** \"Unknown\" category allows model to learn from missingness patterns\n",
    "\n",
    "3. **Data-Driven Metric Selection:** With imbalance ratio > 3:1, we use F1-Score instead of Accuracy.\n",
    "\n",
    "4. **Class Imbalance Handling:** Class weights during training help the model learn minority class patterns without synthetic data risks.\n",
    "\n",
    "5. **Regularisation Prevents Overfitting:** Combining **Dropout + L2 regularisation** controls overfitting effectively.\n",
    "\n",
    "6. **Regularisation Enables Longer Training:** With proper regularisation, we train for 150 epochs (vs 100 baseline) without overfitting risk.\n",
    "\n",
    "7. **Technique Scope:** We use only techniques from Chapters 1–4 of *Deep Learning with Python* (Chollet, 2021).\n",
    "\n",
    "### References\n",
    "\n",
    "- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n",
    "\n",
    "- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning*. 2nd edn. New York: Springer.\n",
    "\n",
    "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
    "\n",
    "- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145.\n",
    "\n",
    "- Pedregosa, F. et al. (2011) 'Scikit-learn: machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Modular Helper Functions\n",
    "\n",
    "For cleaner code organisation, you can wrap the model building and training patterns into reusable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODULAR HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def build_binary_classifier(input_dim, hidden_units=None, dropout=0.0, l2_reg=0.0,\n",
    "                            optimizer='adam', loss='binary_crossentropy', \n",
    "                            metrics=['accuracy'], name=None):\n",
    "    \"\"\"\n",
    "    Build a binary classification neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features\n",
    "    hidden_units : list of int, optional\n",
    "        Neurons per hidden layer, e.g., [64] or [128, 64]\n",
    "        None or [] creates a single-layer perceptron\n",
    "    dropout : float\n",
    "        Dropout rate (0.0 to 0.5)\n",
    "    l2_reg : float\n",
    "        L2 regularisation strength\n",
    "    optimizer : str or keras.optimizers.Optimizer\n",
    "        Optimiser name or instance\n",
    "    loss : str\n",
    "        Loss function name\n",
    "    metrics : list\n",
    "        Metrics to track during training\n",
    "    name : str, optional\n",
    "        Model name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Sequential : Compiled model ready for training\n",
    "    \"\"\"\n",
    "    model = Sequential(name=name)\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    \n",
    "    hidden_units = hidden_units or []\n",
    "    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n",
    "    \n",
    "    for units in hidden_units:\n",
    "        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    \n",
    "    # Output layer for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val,\n",
    "                class_weights=None, batch_size=512, epochs=100, verbose=0):\n",
    "    \"\"\"\n",
    "    Train a model and return training history.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Compiled Keras model\n",
    "    X_train, y_train : array-like\n",
    "        Training data and labels\n",
    "    X_val, y_val : array-like\n",
    "        Validation data and labels\n",
    "    class_weights : dict, optional\n",
    "        Class weights for imbalanced data\n",
    "    batch_size : int\n",
    "        Training batch size\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    verbose : int\n",
    "        Verbosity mode\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.callbacks.History : Training history object\n",
    "    \"\"\"\n",
    "    return model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_binary_model(model, X, y_true, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate binary classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained Keras model\n",
    "    X : array-like\n",
    "        Input features\n",
    "    y_true : array-like\n",
    "        True labels (0 or 1)\n",
    "    threshold : float\n",
    "        Classification threshold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict(X, verbose=0).flatten()\n",
    "    y_pred = (y_pred_proba > threshold).astype('int32')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'auc': roc_auc_score(y_true, y_pred_proba),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "# \n",
    "# # Build models\n",
    "# slp = build_binary_classifier(INPUT_DIMENSION, name='SLP')\n",
    "# dnn = build_binary_classifier(INPUT_DIMENSION, hidden_units=[64], name='DNN')\n",
    "# dnn_reg = build_binary_classifier(INPUT_DIMENSION, hidden_units=[64], \n",
    "#                                   dropout=0.3, l2_reg=0.001, name='DNN_Regularized')\n",
    "# \n",
    "# # Train\n",
    "# history = train_model(dnn, X_train, y_train, X_val, y_val, \n",
    "#                       class_weights=CLASS_WEIGHTS)\n",
    "# \n",
    "# # Evaluate\n",
    "# metrics = evaluate_binary_model(dnn, X_val, y_val)\n",
    "# print(f\"F1-Score: {metrics['f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}