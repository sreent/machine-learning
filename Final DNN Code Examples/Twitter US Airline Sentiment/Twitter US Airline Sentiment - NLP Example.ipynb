{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Twitter%20US%20Airline%20Sentiment/Twitter%20US%20Airline%20Sentiment%20-%20NLP%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Twitter US Airline Sentiment - NLP Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to a multi-class NLP classification problem using Twitter airline sentiment data.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply the Universal ML Workflow to an NLP text classification problem\n- Convert text data to numerical features using **TF-IDF (Term Frequency-Inverse Document Frequency)** vectorization\n- Handle **imbalanced classes** using class weights during training\n- Build and train deep neural networks for **multi-class classification**\n- Use **Hyperband** for efficient hyperparameter tuning\n- Apply **Dropout + L2 regularization** to prevent overfitting\n- Evaluate model performance using appropriate metrics for imbalanced data (**F1-Score**, Accuracy, Precision, Recall)\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [Kaggle Twitter US Airline Sentiment](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment) |\n| **Problem Type** | Multi-Class Classification (3 classes) |\n| **Data Balance** | Imbalanced (Negative: ~63%, Neutral: ~21%, Positive: ~16%) |\n| **Data Type** | Unstructured Text (Tweets) |\n| **Input Features** | TF-IDF Vectors (5000 features, bigrams) |\n| **Output** | Sentiment: Negative, Neutral, or Positive |\n| **Imbalance Handling** | Class Weights during Training |\n\n---\n\n## Technique Scope\n\nThis notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n\n| Technique | Status | Rationale |\n|-----------|--------|-----------|\n| **Dense layers (MLP/DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n| **Dropout** | ✓ Used | Regularization technique (Ch. 4) |\n| **L2 regularization** | ✓ Used | Weight penalty (Ch. 4) |\n| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n\nWe demonstrate that **Dropout + L2 regularization** alone can effectively prevent overfitting without requiring early stopping.\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\nThe first step in any machine learning project is to clearly define the problem and understand the data.\n\n**Problem Statement:** Given a tweet about a US airline, predict the sentiment (Negative, Neutral, or Positive).\n\n**Why this matters:** Airlines can use sentiment analysis to:\n- Identify unhappy customers quickly and respond to complaints\n- Track brand perception over time\n- Discover common pain points in customer experience\n\n**Data Source:** This dataset contains tweets about major US airlines, collected via Twitter's API and labeled by human annotators.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Metric Selection Based on Class Imbalance\n\nThe choice of evaluation metric depends on **class imbalance**. We use practical guidelines derived from the literature:\n\n| Imbalance Ratio | Classification | Primary Metric | Rationale |\n|-----------------|----------------|----------------|-----------|\n| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n\n**Why these thresholds?**\n- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n- **F1-Score**: Harmonic mean of precision and recall, effective for imbalanced data (He and Garcia, 2009)\n\n### References\n\n- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modeling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n\n- Brownlee, J. (2020) *A gentle introduction to imbalanced classification*. Available at: https://machinelearningmastery.com/what-is-imbalanced-classification/ (Accessed: 20 January 2025).\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Luque, A., Carrasco, A., Martín, A. and de las Heras, A. (2019) 'The impact of class imbalance in classification performance metrics based on the binary confusion matrix', *Pattern Recognition*, 91, pp. 216–231.\n\n*Note: The 3:1 threshold is a practical guideline, not a strict academic standard. The literature suggests metric choice depends on domain-specific costs of errors.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\nThe choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n| Deep Learning | Hold-Out (preferred) | Training cost prohibitive for K iterations |\n\n**Why 10,000 as a practical threshold?**\n- Below 10,000 samples, hold-out validation has higher variance (Kohavi, 1995)\n- Above 10,000, statistical estimates from hold-out are reliable\n- Deep learning models are expensive to train; K-fold multiplies cost by K (Chollet, 2021)\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n\n- Pedregosa, F. et al. (2011) 'Scikit-learn: machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830. Available at: https://scikit-learn.org/stable/modules/cross_validation.html (Accessed: 20 January 2025).\n\n*Note: The 10,000 threshold is a practical guideline. For computationally cheap models, K-fold is preferred regardless of size.*\n\n### Data Split Strategy (This Notebook)\n\n```\nOriginal Data (14,640 samples) → Hold-Out Selected\n├── Test Set (10%) - Final evaluation only\n└── Training Pool (90%)\n    ├── Training Set (81%) - Model training\n    └── Validation Set (9%) - Hyperparameter tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries and Set Random Seed\n\nWe set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n%pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Load and Explore the Dataset\n\nLet's download the Twitter airline sentiment data from Google Drive and examine its structure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data directly from Google Drive\nGDRIVE_FILE_ID = '15XHy_PdD6Q2aa6n-pnWmSFGCv1oK9vWA'\nDATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\n\ntweets = pd.read_csv(DATA_URL)\ntweets = tweets[['text', 'airline_sentiment']]\n\ntweets.head()"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Split Data into Train and Test Sets\n\nWe reserve 10% of the data for final testing. The `stratify` parameter ensures that each split maintains the same class proportions as the original dataset - critical for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "(tweets_train, tweets_test, \n",
    " sentiment_train, sentiment_test) = train_test_split(tweets['text'], tweets['airline_sentiment'], \n",
    "                                                     test_size=TEST_SIZE, stratify=tweets['airline_sentiment'],\n",
    "                                                     shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 Text Vectorization with TF-IDF\n\nNeural networks require numerical input, but tweets are text. We use **TF-IDF (Term Frequency-Inverse Document Frequency)** to convert text to numbers.\n\n**How TF-IDF works:**\n- **TF (Term Frequency):** How often a word appears in a document\n- **IDF (Inverse Document Frequency):** Downweights words that appear in many documents (like \"the\", \"is\")\n- **TF-IDF = TF × IDF:** Words that are frequent in a document but rare overall get high scores\n\n**Our settings:**\n- `max_features=5000`: Keep only the 5000 most important terms\n- `ngram_range=(1, 2)`: Include both single words (unigrams) and word pairs (bigrams) like \"great service\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "NGRAMS = 2\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, NGRAMS), max_features=MAX_FEATURES)\n",
    "tfidf.fit(tweets_train)\n",
    "\n",
    "X_train, X_test = tfidf.transform(tweets_train).toarray(), tfidf.transform(tweets_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 Encode Labels as One-Hot Vectors\n\nFor multi-class classification with softmax output, we need to convert categorical labels to one-hot encoded vectors:\n- Negative → [1, 0, 0]\n- Neutral → [0, 1, 0]\n- Positive → [0, 0, 1]",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "label_encoder = LabelEncoder()\nlabel_encoder.fit(tweets['airline_sentiment'])\n\ny_train = to_categorical(label_encoder.transform(sentiment_train))\ny_test = to_categorical(label_encoder.transform(sentiment_test))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\nBefore building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n\n### 5.1 Examine Class Distribution\n\nLet's look at how the sentiment classes are distributed:"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>3099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  text\n",
       "0          negative  9178\n",
       "1           neutral  3099\n",
       "2          positive  2363"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = tweets.groupby(['airline_sentiment']).count()\n",
    "counts.reset_index(inplace=True)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n# =============================================================================\n\n# Dataset size analysis (for hold-out vs K-fold decision)\nn_samples = len(tweets)\nHOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000 (Kohavi, 1995; Chollet, 2021)\n\n# Imbalance analysis (for metric selection)\nmajority_class = counts['text'].max()\nminority_class = counts['text'].min()\nimbalance_ratio = majority_class / minority_class\nIMBALANCE_THRESHOLD = 3.0  # Use F1-Score if ratio > 3.0 (He & Garcia, 2009)\n\n# Determine evaluation strategy and metric\nuse_holdout = n_samples > HOLDOUT_THRESHOLD\nuse_f1 = imbalance_ratio > IMBALANCE_THRESHOLD\n\nprint(\"=\" * 60)\nprint(\"DATA-DRIVEN CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\nprint(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples (Kohavi, 1995)\")\nprint(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n\nprint(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\nprint(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1 (He & Garcia, 2009)\")\nprint(f\"   Decision: {'F1-Score (imbalanced)' if use_f1 else 'Accuracy (balanced)'}\")\n\nprint(\"\\n\" + \"=\" * 60)\nPRIMARY_METRIC = 'f1' if use_f1 else 'accuracy'\nprint(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 Calculate Baseline Metrics\n\n**Naive Baseline (Majority Class):** If we always predict \"negative\", we get ~63% accuracy. This is our accuracy baseline.\n\n**Balanced Accuracy Baseline:** A random classifier would achieve 33.3% balanced accuracy (1/3 for each class). This is more meaningful for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6269125683060109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = counts[counts['airline_sentiment']=='negative']['text'].values[0] / counts['text'].sum()\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Balanced accuracy baseline (random classifier)\nbalanced_accuracy_baseline = balanced_accuracy_score(y_train.argmax(axis=1), np.zeros(len(y_train)))\n\nprint(f\"Baseline accuracy (majority class): {baseline:.2f}\")\nprint(f\"Balanced accuracy baseline (random): {balanced_accuracy_baseline:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3 Create Validation Set\n\nWe split off a portion of the training data for validation. This will be used to:\n- Evaluate model performance during hyperparameter tuning\n- Compare models without touching the test set",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=VALIDATION_SIZE, stratify=y_train,\n",
    "                                                  shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 Configure Training Parameters\n\n**Key training settings:**\n- **Optimizer:** Adam - adaptive learning rate optimizer with momentum, widely used for deep learning\n- **Loss:** Categorical cross-entropy - standard loss for multi-class classification\n- **Training Metrics:** Accuracy, Precision, Recall, AUC (tracked by Keras during training)\n- **Primary Metric:** F1-Score (macro-averaged) - computed separately after training using sklearn, since Keras doesn't provide a built-in macro-F1 metric",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "INPUT_DIMENSION = X_train.shape[1]\nOUTPUT_CLASSES = y_train.shape[1]\n\nOPTIMIZER = 'adam'\nLOSS_FUNC = 'categorical_crossentropy'\n\n# Training metrics (tracked by Keras during training)\n# Note: F1-Score (our primary metric) is computed separately using sklearn\n# because Keras doesn't provide a built-in macro-averaged F1 metric\nMETRICS = ['categorical_accuracy', \n           tf.keras.metrics.Precision(name='precision'), \n           tf.keras.metrics.Recall(name='recall'),\n           # multi_label=True is about LABEL FORMAT, not task type:\n           # - Our task: multi-CLASS (3 mutually exclusive classes)\n           # - Our labels: one-hot encoded [[1,0,0], [0,1,0], [0,0,1]]\n           # - multi_label=True tells Keras to expect one-hot format\n           # - multi_label=False expects integer indices [0, 1, 2]\n           tf.keras.metrics.AUC(name='auc', multi_label=True)]"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Single_Layer_Perceptron\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 15003     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,003\n",
      "Trainable params: 15,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Single-Layer Perceptron (no hidden layers)\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(INPUT_DIMENSION,)))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "BATCH_SIZE = 512\nEPOCHS = 100"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.5 Handle Class Imbalance with Class Weights\n\nTo handle imbalanced classes, we compute **class weights** that give more importance to minority classes during training:\n- **Negative (majority):** Lower weight (~0.53)\n- **Neutral:** Medium weight (~1.57)\n- **Positive (minority):** Higher weight (~2.06)\n\nThis makes errors on minority classes \"cost more\", encouraging the model to learn them better.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5317352220103514, 1: 1.5748285599031868, 2: 2.064516129032258}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.argmax(y_train, axis=1)\n",
    "weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the Single-Layer Perceptron\nhistory_slp = slp_model.fit(X_train, y_train, \n                            class_weight=CLASS_WEIGHTS,\n                            batch_size=BATCH_SIZE, epochs=EPOCHS, \n                            validation_data=(X_val, y_val),\n                            verbose=0)\nval_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display SLP validation metrics\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_slp[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_slp[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_slp[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_slp[3]))\n\npreds_slp_val = slp_model.predict(X_val, verbose=0).argmax(axis=1)\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_slp_val), balanced_accuracy_baseline))\n\n# Calculate F1-Score (primary metric for imbalanced data)\nf1_slp_val = f1_score(y_val.argmax(axis=1), preds_slp_val, average='macro')\nprint(f'F1-Score (Validation): {f1_slp_val:.2f}  ← Primary Metric')"
  },
  {
   "cell_type": "code",
   "source": "def plot_training_history(history, title=None):\n    \"\"\"\n    Plot training and validation metrics over epochs.\n    Plots: (1) Loss, (2) Accuracy\n    \n    Note: We use Accuracy for training visualization (directly tracked by Keras).\n    F1-Score is computed for final evaluation since it's our primary metric.\n\n    Parameters:\n    -----------\n    history : keras History object\n        Training history from model.fit()\n    title : str, optional\n        Model name to display in plot titles (e.g., 'SLP', 'DNN')\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history.history['loss']) + 1)\n    title_suffix = f' ({title})' if title else ''\n\n    # Plot 1: Loss\n    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n    axs[0].set_title(f'Loss{title_suffix}')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(alpha=0.3)\n\n    # Plot 2: Accuracy (directly tracked by Keras)\n    axs[1].plot(epochs, history.history['categorical_accuracy'], 'b-', label='Training', linewidth=1.5)\n    axs[1].plot(epochs, history.history['val_categorical_accuracy'], 'r-', label='Validation', linewidth=1.5)\n    axs[1].set_title(f'Accuracy{title_suffix}')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "# Plot SLP training history\nplot_training_history(history_slp, title='SLP Baseline')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nThe next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n\n**Strategy:** Add hidden layers and neurons to increase model capacity.\n\n**No regularization applied:** We intentionally train this model **without any regularization** (no dropout, no L2, no early stopping) to observe overfitting behavior. In the training plots, you should see:\n- Training loss continues to decrease\n- Validation loss starts increasing after some epochs (overfitting)\n\nThis demonstrates why regularization (Section 7) is necessary.\n\n### 6.1 Build a Deep Neural Network (DNN)\n\nLet's add a hidden layer with 64 neurons and ReLU activation:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deep Neural Network (1 hidden layer, no dropout for overfitting demo)\ndnn_model = Sequential(name='Deep_Neural_Network')\ndnn_model.add(Dense(64, activation='relu', input_shape=(INPUT_DIMENSION,)))\ndnn_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\ndnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\ndnn_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the Deep Neural Network (without early stopping to demonstrate overfitting)\nhistory_dnn = dnn_model.fit(X_train, y_train, \n                            class_weight=CLASS_WEIGHTS,\n                            batch_size=BATCH_SIZE, epochs=EPOCHS, \n                            validation_data=(X_val, y_val), \n                            verbose=0)\nval_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot DNN training history (expect overfitting: val_loss increasing)\nplot_training_history(history_dnn, title='DNN - No Regularization')"
  },
  {
   "cell_type": "code",
   "source": "# Display DNN validation metrics\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_dnn[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_dnn[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_dnn[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_dnn[3]))\n\npreds_dnn_val = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_dnn_val), balanced_accuracy_baseline))\n\n# Calculate F1-Score (primary metric for imbalanced data)\nf1_dnn_val = f1_score(y_val.argmax(axis=1), preds_dnn_val, average='macro')\nprint(f'F1-Score (Validation): {f1_dnn_val:.2f}  ← Primary Metric')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nNow we address the overfitting observed in Section 6 by adding **regularization**. We use two complementary techniques:\n\n| Technique | How it works | Effect |\n|-----------|--------------|--------|\n| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation |\n| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries |\n\n**Same architecture, different regularization:** We keep the same 1-layer architecture (64 neurons) as Section 6, so the only difference is regularization. This allows a fair comparison.\n\nUsing **Hyperband** for efficient hyperparameter tuning to find optimal regularization strengths.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations\n\n### 7.1 Hyperband Search"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperband Model Builder for Multi-Class Twitter Airline Classification\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Twitter Airline model with FIXED architecture (1 hidden layer, 64 neurons).\n    Same architecture as Section 6 DNN - only tunes regularization and learning rate.\n    Uses Adam optimizer (same as Section 6) for fair comparison.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # L2 regularization strength\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n\n    # Fixed architecture: 1 hidden layer with 64 neurons (same as Section 6)\n    model.add(layers.Dense(64, activation='relu', \n                           kernel_regularizer=regularizers.l2(l2_reg)))\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(dropout_rate))\n\n    # Output layer for multi-class classification\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\n# ===========================================================================\n# TUNING OBJECTIVE: Why AUC instead of F1?\n# ===========================================================================\n# We use F1-Score as our PRIMARY METRIC for final evaluation, but AUC for tuning:\n#\n# 1. Keras limitation: No built-in macro-averaged F1 metric available during training\n#    - F1 requires computing precision/recall across all classes, then averaging\n#    - Keras metrics are computed per-batch, making macro-F1 impractical\n#\n# 2. AUC is a better tuning objective:\n#    - AUC is threshold-independent (evaluates across all decision thresholds)\n#    - F1 is threshold-dependent (requires choosing a cutoff for predictions)\n#    - Both penalize models that ignore minority classes\n#    - Models with high AUC tend to achieve high F1 at optimal thresholds\n#\n# 3. Alternative approaches (not used here):\n#    - Custom Keras metric (complex, slower)\n#    - Validation callback computing F1 each epoch (adds overhead)\n#\n# Final evaluation uses sklearn's f1_score(average='macro') as primary metric.\n# ===========================================================================\n\nTUNING_OBJECTIVE = 'val_categorical_accuracy' if PRIMARY_METRIC == 'accuracy' else 'val_auc'\n\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective=TUNING_OBJECTIVE,\n    max_epochs=20,\n    factor=3,\n    directory='twitter_airline_hyperband',\n    project_name='twitter_airline_tuning',\n    overwrite=True  # Ensure fresh tuning on each run\n)\n\nprint(f\"Tuning objective: {TUNING_OBJECTIVE}\")\nprint(\"(Note: Final evaluation uses F1-Score as primary metric)\")\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=BATCH_SIZE,\n    class_weight=CLASS_WEIGHTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters and build best model\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  L2 Regularization: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Retrain with Optimized Hyperparameters\n\nNow that we have the best hyperparameters from Hyperband search, we:\n\n1. **Build a fresh model** with the optimized L2 strength, dropout rate, and learning rate\n2. **Retrain from scratch** with full epochs (not the limited epochs used during search)\n\n**Why no early stopping?**\n\nWe deliberately avoid early stopping for two reasons:\n\n1. **Scope of techniques:** Early stopping is not covered in Chapters 1–4 of *Deep Learning with Python* (Chollet, 2021), which defines the techniques available for this workflow.\n\n2. **Regularization sufficiency:** With proper regularization (Dropout + L2), the model should **not overfit** even when trained for the full number of epochs:\n   - **Section 6 (no regularization):** Model overfits → validation loss increases\n   - **Section 7 (with regularization):** Model doesn't overfit → validation loss stays low\n\nThe training plots should show that both training and validation loss converge together, demonstrating that **Dropout + L2 alone prevent overfitting**.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the best model (regularization via Dropout + L2, no early stopping needed)\nhistory_opt = opt_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    class_weight=CLASS_WEIGHTS,\n    verbose=1\n)\nval_score_opt = opt_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "source": "# Plot optimized model training history (expect no overfitting: both converge)\nplot_training_history(history_opt, title='DNN - Dropout + L2')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds_opt_val = opt_model.predict(X_val, verbose=0).argmax(axis=1)\n\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_opt[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_opt[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_opt[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_opt[3]))\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_opt_val), balanced_accuracy_baseline))\n\n# Calculate F1-Score (primary metric for imbalanced data)\nf1_opt_val = f1_score(y_val.argmax(axis=1), preds_opt_val, average='macro')\nprint(f'F1-Score (Validation): {f1_opt_val:.2f}  ← Primary Metric')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.3 Final Model Evaluation on Test Set\n\nNow we evaluate our best model on the held-out test set that was never used during training or tuning."
  },
  {
   "cell_type": "code",
   "source": "# Final evaluation on test set\ntest_score = opt_model.evaluate(X_test, y_test, verbose=0)[1:]\npreds_test = opt_model.predict(X_test, verbose=0).argmax(axis=1)\n\n# Calculate F1-Score (our primary metric)\ntest_f1 = f1_score(y_test.argmax(axis=1), preds_test, average='macro')\n\nprint('=' * 50)\nprint('FINAL TEST SET RESULTS')\nprint('=' * 50)\nprint(f'F1-Score (Test): {test_f1:.4f}  ← Primary Metric')\nprint(f'Accuracy (Test): {test_score[0]:.4f} (baseline={baseline:.4f})')\nprint(f'Precision (Test): {test_score[1]:.4f}')\nprint(f'Recall (Test): {test_score[2]:.4f}')\nprint(f'AUC (Test): {test_score[3]:.4f}')\nprint(f'Balanced Accuracy (Test): {balanced_accuracy_score(y_test.argmax(axis=1), preds_test):.4f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display confusion matrix for test predictions\nfig, ax = plt.subplots(figsize=(8, 6))\ncm = confusion_matrix(y_test.argmax(axis=1), preds_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Test Set Predictions')\nplt.tight_layout()\nplt.show()\n\n# Print per-class recall (what % of each class was correctly identified)\nprint(\"\\nPer-Class Recall:\")\nfor i, class_name in enumerate(label_encoder.classes_):\n    class_mask = y_test.argmax(axis=1) == i\n    class_recall = (preds_test[class_mask] == i).mean()\n    print(f\"  {class_name.capitalize()}: {class_recall:.2%} ({class_mask.sum()} samples)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Results Summary\n\nThe following dynamically-generated table compares all models trained in this notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# RESULTS SUMMARY - Using Previously Computed Values\n# =============================================================================\n\n# Reuse predictions and F1 scores computed earlier:\n# - preds_slp_val, f1_slp_val (Section 5: SLP Baseline)\n# - preds_dnn_val, f1_dnn_val (Section 6: DNN No Regularization)\n# - preds_opt_val, f1_opt_val (Section 7: DNN with Dropout + L2)\n# - preds_test, test_f1 (Section 7.3: Test Set Evaluation)\n\n# Create results DataFrame\nresults = pd.DataFrame({\n    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'DNN (No Regularization)', 'DNN (Dropout + L2)', 'DNN (Dropout + L2) - Test'],\n    'Accuracy': [baseline, val_score_slp[0], val_score_dnn[0], val_score_opt[0], test_score[0]],\n    'F1-Score': [0.0, f1_slp_val, f1_dnn_val, f1_opt_val, test_f1],\n    'Dataset': ['N/A', 'Validation', 'Validation', 'Validation', 'Test']\n})\n\nprint(\"=\" * 70)\nprint(\"MODEL COMPARISON - RESULTS SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Primary Metric: F1-SCORE (imbalance ratio: {imbalance_ratio:.2f}:1)\")\nprint(\"=\" * 70)\nprint(results.to_string(index=False, float_format='{:.4f}'.format))\nprint(\"=\" * 70)\nprint(f\"\\nKey Observations:\")\nprint(f\"  - All models outperform naive baseline ({baseline:.2%} accuracy)\")\nprint(f\"  - Regularization improves F1: {f1_dnn_val:.4f} → {f1_opt_val:.4f}\")\nprint(f\"  - Final test F1-Score: {test_f1:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | 14,640 samples | Hold-Out | Kohavi (1995); Chollet (2021) |\n| **Accuracy vs F1-Score** | > 3:1 imbalance | 3.88:1 ratio | F1-Score | He and Garcia (2009) |\n\n### Lessons Learned\n\n1. **Data-Driven Metric Selection:** With imbalance ratio > 3:1, we use F1-Score instead of Accuracy to ensure fair evaluation across all classes.\n\n2. **Data-Driven Evaluation Protocol:** With > 10,000 samples and deep learning, hold-out validation provides reliable estimates while being computationally efficient.\n\n3. **Class Imbalance Handling:** Using class weights during training improves performance on minority classes.\n\n4. **Simple Models Can Work Well:** The SLP achieved competitive F1-Score with good feature engineering (TF-IDF).\n\n5. **Regularization Prevents Overfitting:** The unregularized DNN showed overfitting; combining **Dropout + L2 regularization** controls this effectively.\n\n6. **Technique Scope:** We use only techniques from Chapters 1–4 of *Deep Learning with Python* (Chollet, 2021). Early stopping, while useful, is outside this scope—Dropout and L2 provide sufficient regularization.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning*. 2nd edn. New York: Springer.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Modular Helper Functions\n",
    "\n",
    "For cleaner code organization, you can wrap the model building and training patterns into reusable functions. Below are the modular versions of the code used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MODULAR HELPER FUNCTIONS\n# =============================================================================\n\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\n\ndef build_dnn(input_dim, output_dim, hidden_units=None, dropout=0.0, l2_reg=0.0,\n              optimizer='adam', loss='categorical_crossentropy', \n              metrics=['categorical_accuracy'], name=None):\n    \"\"\"\n    Build a Deep Neural Network with configurable architecture and regularization.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    output_dim : int\n        Number of output classes (1 for binary, >1 for multi-class)\n    hidden_units : list of int, optional\n        Neurons per hidden layer, e.g., [64] or [128, 64, 32]\n        None or [] creates a single-layer perceptron (no hidden layers)\n    dropout : float\n        Dropout rate (0.0 to 0.5) applied after each hidden layer\n    l2_reg : float\n        L2 regularization strength (0.0 for no regularization)\n    optimizer : str or keras.optimizers.Optimizer\n        Optimizer name or instance\n    loss : str\n        Loss function name\n    metrics : list\n        Metrics to track during training\n    name : str, optional\n        Model name for identification\n        \n    Returns:\n    --------\n    keras.Sequential : Compiled model ready for training\n    \n    Examples:\n    ---------\n    # Single-layer perceptron (no hidden layers)\n    slp = build_dnn(5000, 3, name='SLP')\n    \n    # DNN with one hidden layer (64 neurons)\n    dnn = build_dnn(5000, 3, hidden_units=[64], name='DNN')\n    \n    # DNN with regularization\n    dnn_reg = build_dnn(5000, 3, hidden_units=[64], dropout=0.3, l2_reg=0.001)\n    \n    # Deeper network\n    deep = build_dnn(5000, 3, hidden_units=[128, 64, 32], dropout=0.2)\n    \"\"\"\n    model = Sequential(name=name)\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    # Hidden layers\n    hidden_units = hidden_units or []\n    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n    \n    for units in hidden_units:\n        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n        if dropout > 0:\n            model.add(Dropout(dropout))\n    \n    # Output layer\n    output_activation = 'sigmoid' if output_dim == 1 else 'softmax'\n    model.add(Dense(output_dim, activation=output_activation))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val,\n                class_weights=None, batch_size=512, epochs=100, verbose=0):\n    \"\"\"\n    Train a model and return training history.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Compiled Keras model\n    X_train, y_train : array-like\n        Training data and labels\n    X_val, y_val : array-like\n        Validation data and labels\n    class_weights : dict, optional\n        Class weights for imbalanced data\n    batch_size : int\n        Training batch size (default: 512)\n    epochs : int\n        Number of training epochs (default: 100)\n    verbose : int\n        Verbosity mode (0=silent, 1=progress bar)\n        \n    Returns:\n    --------\n    keras.callbacks.History : Training history object\n    \"\"\"\n    return model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        class_weight=class_weights,\n        batch_size=batch_size, \n        epochs=epochs,\n        verbose=verbose\n    )\n\n\ndef evaluate_model(model, X, y_true, class_names=None):\n    \"\"\"\n    Evaluate model and return comprehensive metrics.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Trained Keras model\n    X : array-like\n        Input features\n    y_true : array-like\n        True labels (one-hot encoded)\n    class_names : list, optional\n        Class names for display\n        \n    Returns:\n    --------\n    dict : Dictionary containing all metrics (accuracy, f1_macro, precision, \n           recall, auc, balanced_accuracy)\n    \"\"\"\n    # Get predictions\n    y_pred_proba = model.predict(X, verbose=0)\n    y_pred = y_pred_proba.argmax(axis=1)\n    y_true_labels = y_true.argmax(axis=1)\n    \n    # Calculate metrics (consistent with main notebook)\n    metrics = {\n        'accuracy': accuracy_score(y_true_labels, y_pred),\n        'f1_macro': f1_score(y_true_labels, y_pred, average='macro'),\n        'precision': precision_score(y_true_labels, y_pred, average='macro'),\n        'recall': recall_score(y_true_labels, y_pred, average='macro'),\n        'auc': roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro'),\n        'balanced_accuracy': balanced_accuracy_score(y_true_labels, y_pred),\n    }\n    \n    return metrics\n\n\ndef print_metrics(metrics, dataset_name='Validation', primary_metric='f1_macro'):\n    \"\"\"\n    Print metrics in a formatted way.\n    \n    Parameters:\n    -----------\n    metrics : dict\n        Dictionary of metric names and values\n    dataset_name : str\n        Name of the dataset (e.g., 'Validation', 'Test')\n    primary_metric : str\n        Key of the primary metric to highlight\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"{dataset_name.upper()} METRICS\")\n    print('='*50)\n    \n    for name, value in metrics.items():\n        suffix = '  ← Primary Metric' if name == primary_metric else ''\n        print(f\"{name.replace('_', ' ').title()}: {value:.4f}{suffix}\")\n\n\ndef plot_history(history, title=None):\n    \"\"\"\n    Plot training history (loss and accuracy).\n    \n    Parameters:\n    -----------\n    history : keras.callbacks.History\n        Training history from model.fit()\n    title : str, optional\n        Title suffix for plots\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n    epochs = range(1, len(history.history['loss']) + 1)\n    suffix = f' ({title})' if title else ''\n    \n    # Loss\n    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training')\n    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation')\n    axs[0].set_title(f'Loss{suffix}')\n    axs[0].set_xlabel('Epoch')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(alpha=0.3)\n    \n    # Accuracy\n    acc_key = 'categorical_accuracy' if 'categorical_accuracy' in history.history else 'accuracy'\n    axs[1].plot(epochs, history.history[acc_key], 'b-', label='Training')\n    axs[1].plot(epochs, history.history[f'val_{acc_key}'], 'r-', label='Validation')\n    axs[1].set_title(f'Accuracy{suffix}')\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# =============================================================================\n# USAGE EXAMPLES\n# =============================================================================\n# \n# # Build models\n# slp = build_dnn(INPUT_DIMENSION, OUTPUT_CLASSES, name='SLP')\n# dnn = build_dnn(INPUT_DIMENSION, OUTPUT_CLASSES, hidden_units=[64], name='DNN')\n# dnn_reg = build_dnn(INPUT_DIMENSION, OUTPUT_CLASSES, hidden_units=[64], \n#                     dropout=0.3, l2_reg=0.001, name='DNN_Regularized')\n# \n# # Train\n# history = train_model(dnn, X_train, y_train, X_val, y_val, \n#                       class_weights=CLASS_WEIGHTS)\n# \n# # Evaluate\n# metrics = evaluate_model(dnn, X_val, y_val)\n# print_metrics(metrics, 'Validation')\n# \n# # Plot\n# plot_history(history, 'DNN')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}