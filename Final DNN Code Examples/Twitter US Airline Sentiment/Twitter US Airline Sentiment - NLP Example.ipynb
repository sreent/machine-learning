{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Twitter%20US%20Airline%20Sentiment/Twitter%20US%20Airline%20Sentiment%20-%20NLP%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Twitter US Airline Sentiment - NLP Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to a multi-class NLP classification problem using Twitter airline sentiment data.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply the Universal ML Workflow to an NLP text classification problem\n- Convert text data to numerical features using **TF-IDF (Term Frequency-Inverse Document Frequency)** vectorization\n- Handle **imbalanced classes** using class weights during training\n- Build and train deep neural networks for **multi-class classification**\n- Use **Hyperband** for efficient hyperparameter tuning\n- Evaluate model performance using appropriate metrics for imbalanced data (Balanced Accuracy, Precision, Recall, AUC)\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [Kaggle Twitter US Airline Sentiment](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment) |\n| **Problem Type** | Multi-Class Classification (3 classes) |\n| **Data Balance** | Imbalanced (Negative: ~63%, Neutral: ~21%, Positive: ~16%) |\n| **Data Type** | Unstructured Text (Tweets) |\n| **Input Features** | TF-IDF Vectors (5000 features, bigrams) |\n| **Output** | Sentiment: Negative, Neutral, or Positive |\n| **Imbalance Handling** | Class Weights during Training |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\nThe first step in any machine learning project is to clearly define the problem and understand the data.\n\n**Problem Statement:** Given a tweet about a US airline, predict the sentiment (Negative, Neutral, or Positive).\n\n**Why this matters:** Airlines can use sentiment analysis to:\n- Identify unhappy customers quickly and respond to complaints\n- Track brand perception over time\n- Discover common pain points in customer experience\n\n**Data Source:** This dataset contains tweets about major US airlines, collected via Twitter's API and labeled by human annotators.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Metric Selection Based on Class Imbalance\n\nThe choice of evaluation metric depends on **class imbalance**. We use practical guidelines derived from the literature:\n\n| Imbalance Ratio | Classification | Primary Metric | Rationale |\n|-----------------|----------------|----------------|-----------|\n| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n\n**Why these thresholds?**\n- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n- **F1-Score**: Harmonic mean of precision and recall, effective for imbalanced data (He and Garcia, 2009)\n\n### References\n\n- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modeling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n\n- Brownlee, J. (2020) *A gentle introduction to imbalanced classification*. Available at: https://machinelearningmastery.com/what-is-imbalanced-classification/ (Accessed: 20 January 2025).\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Luque, A., Carrasco, A., Martín, A. and de las Heras, A. (2019) 'The impact of class imbalance in classification performance metrics based on the binary confusion matrix', *Pattern Recognition*, 91, pp. 216–231.\n\n*Note: The 3:1 threshold is a practical guideline, not a strict academic standard. The literature suggests metric choice depends on domain-specific costs of errors.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\nThe choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n| Deep Learning | Hold-Out (preferred) | Training cost prohibitive for K iterations |\n\n**Why 10,000 as a practical threshold?**\n- Below 10,000 samples, hold-out validation has higher variance (Kohavi, 1995)\n- Above 10,000, statistical estimates from hold-out are reliable\n- Deep learning models are expensive to train; K-fold multiplies cost by K (Chollet, 2021)\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n\n- Pedregosa, F. et al. (2011) 'Scikit-learn: machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830. Available at: https://scikit-learn.org/stable/modules/cross_validation.html (Accessed: 20 January 2025).\n\n*Note: The 10,000 threshold is a practical guideline. For computationally cheap models, K-fold is preferred regardless of size.*\n\n### Data Split Strategy (This Notebook)\n\n```\nOriginal Data (14,640 samples) → Hold-Out Selected\n├── Test Set (10%) - Final evaluation only\n└── Training Pool (90%)\n    ├── Training Set (81%) - Model training\n    └── Validation Set (9%) - Early stopping & tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries and Set Random Seed\n\nWe set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Load and Explore the Dataset\n\nLet's load the Twitter airline sentiment data and examine its structure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0                @VirginAmerica What @dhepburn said.           neutral\n",
       "1  @VirginAmerica plus you've added commercials t...          positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('Tweets.csv', sep=',')\n",
    "tweets = tweets[['text', 'airline_sentiment']]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Split Data into Train and Test Sets\n\nWe reserve 10% of the data for final testing. The `stratify` parameter ensures that each split maintains the same class proportions as the original dataset - critical for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "(tweets_train, tweets_test, \n",
    " sentiment_train, sentiment_test) = train_test_split(tweets['text'], tweets['airline_sentiment'], \n",
    "                                                     test_size=TEST_SIZE, stratify=tweets['airline_sentiment'],\n",
    "                                                     shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 Text Vectorization with TF-IDF\n\nNeural networks require numerical input, but tweets are text. We use **TF-IDF (Term Frequency-Inverse Document Frequency)** to convert text to numbers.\n\n**How TF-IDF works:**\n- **TF (Term Frequency):** How often a word appears in a document\n- **IDF (Inverse Document Frequency):** Downweights words that appear in many documents (like \"the\", \"is\")\n- **TF-IDF = TF × IDF:** Words that are frequent in a document but rare overall get high scores\n\n**Our settings:**\n- `max_features=5000`: Keep only the 5000 most important terms\n- `ngram_range=(1, 2)`: Include both single words (unigrams) and word pairs (bigrams) like \"great service\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "NGRAMS = 2\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, NGRAMS), max_features=MAX_FEATURES)\n",
    "tfidf.fit(tweets_train)\n",
    "\n",
    "X_train, X_test = tfidf.transform(tweets_train).toarray(), tfidf.transform(tweets_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 Encode Labels as One-Hot Vectors\n\nFor multi-class classification with softmax output, we need to convert categorical labels to one-hot encoded vectors:\n- Negative → [1, 0, 0]\n- Neutral → [0, 1, 0]\n- Positive → [0, 0, 1]",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "label_encoder = LabelEncoder()\nlabel_encoder.fit(tweets['airline_sentiment'])\n\ny_train = to_categorical(label_encoder.transform(sentiment_train))\ny_test = to_categorical(label_encoder.transform(sentiment_test))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\nBefore building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n\n### 5.1 Examine Class Distribution\n\nLet's look at how the sentiment classes are distributed:"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>3099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  text\n",
       "0          negative  9178\n",
       "1           neutral  3099\n",
       "2          positive  2363"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = tweets.groupby(['airline_sentiment']).count()\n",
    "counts.reset_index(inplace=True)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n# =============================================================================\n\n# Dataset size analysis (for hold-out vs K-fold decision)\nn_samples = len(tweets)\nHOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000 (Kohavi, 1995; Chollet, 2021)\n\n# Imbalance analysis (for metric selection)\nmajority_class = counts['text'].max()\nminority_class = counts['text'].min()\nimbalance_ratio = majority_class / minority_class\nIMBALANCE_THRESHOLD = 3.0  # Use F1-Score if ratio > 3.0 (He & Garcia, 2009)\n\n# Determine evaluation strategy and metric\nuse_holdout = n_samples > HOLDOUT_THRESHOLD\nuse_f1 = imbalance_ratio > IMBALANCE_THRESHOLD\n\nprint(\"=\" * 60)\nprint(\"DATA-DRIVEN CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\nprint(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples (Kohavi, 1995)\")\nprint(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n\nprint(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\nprint(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1 (He & Garcia, 2009)\")\nprint(f\"   Decision: {'F1-Score (imbalanced)' if use_f1 else 'Accuracy (balanced)'}\")\n\nprint(\"\\n\" + \"=\" * 60)\nPRIMARY_METRIC = 'f1' if use_f1 else 'accuracy'\nprint(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 Calculate Baseline Metrics\n\n**Naive Baseline (Majority Class):** If we always predict \"negative\", we get ~63% accuracy. This is our accuracy baseline.\n\n**Balanced Accuracy Baseline:** A random classifier would achieve 33.3% balanced accuracy (1/3 for each class). This is more meaningful for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6269125683060109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = counts[counts['airline_sentiment']=='negative']['text'].values[0] / counts['text'].sum()\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Balanced accuracy baseline (random classifier)\nbalanced_accuracy_baseline = balanced_accuracy_score(y_train.argmax(axis=1), np.zeros(len(y_train)))\n\nprint(f\"Baseline accuracy (majority class): {baseline:.2f}\")\nprint(f\"Balanced accuracy baseline (random): {balanced_accuracy_baseline:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 Create Validation Set\n",
    "\n",
    "We split off a portion of the training data for validation. This will be used to monitor training progress and evaluate model performance without touching the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=VALIDATION_SIZE, stratify=y_train,\n",
    "                                                  shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 Configure Training Parameters\n\n**Key training settings:**\n- **Optimizer:** RMSprop - adaptive learning rate optimizer that works well for most problems\n- **Loss:** Categorical cross-entropy - standard loss for multi-class classification\n- **Early Stopping:** Configured to stop training when validation loss stops improving (patience=10 epochs)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "INPUT_DIMENSION = X_train.shape[1]\nOUTPUT_CLASSES = y_train.shape[1]\n\nOPTIMIZER = 'rmsprop'\nLOSS_FUNC = 'categorical_crossentropy'\nMETRICS = ['categorical_accuracy', \n           tf.keras.metrics.Precision(name='precision'), \n           tf.keras.metrics.Recall(name='recall'),\n           tf.keras.metrics.AUC(name='auc', multi_label=True)]"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Single_Layer_Perceptron\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 15003     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,003\n",
      "Trainable params: 15,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Single-Layer Perceptron (no hidden layers)\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax', input_shape=(INPUT_DIMENSION,)))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 512\nEPOCHS = 100"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.5 Handle Class Imbalance with Class Weights\n\nTo handle imbalanced classes, we compute **class weights** that give more importance to minority classes during training:\n- **Negative (majority):** Lower weight (~0.53)\n- **Neutral:** Medium weight (~1.57)\n- **Positive (minority):** Higher weight (~2.06)\n\nThis makes errors on minority classes \"cost more\", encouraging the model to learn them better.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5317352220103514, 1: 1.5748285599031868, 2: 2.064516129032258}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.argmax(y_train, axis=1)\n",
    "weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the Single-Layer Perceptron\nhistory_slp = slp_model.fit(X_train, y_train, \n                            class_weight=CLASS_WEIGHTS,\n                            batch_size=batch_size, epochs=EPOCHS, \n                            validation_data=(X_val, y_val),\n                            verbose=0)\nval_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_slp[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_slp[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_slp[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_slp[3]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = slp_model.predict(X_val, verbose=0).argmax(axis=1)\n\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds), balanced_accuracy_baseline))"
  },
  {
   "cell_type": "code",
   "source": "def plot_training_history(history, primary_metric='f1'):\n    \"\"\"\n    Plot training and validation metrics over epochs.\n    Always plots: (1) Loss, (2) Primary metric (F1 or Accuracy)\n\n    Parameters:\n    -----------\n    history : keras History object\n        Training history from model.fit()\n    primary_metric : str\n        'f1' for F1-Score (computed from precision/recall) or 'accuracy' for categorical_accuracy\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, sharex='all', figsize=(15, 5))\n    epochs = range(1, len(history.history['loss']) + 1)\n\n    # Plot 1: Loss (always)\n    ax = axs[0]\n    ax.plot(epochs, history.history['loss'], 'b.-', label='Training Loss')\n    ax.plot(epochs, history.history['val_loss'], 'r.-', label='Validation Loss')\n    ax.set_xlim([0, len(epochs)])\n    ax.set_title('Training and Validation Loss')\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('Loss')\n    ax.legend()\n    ax.grid()\n\n    # Plot 2: Primary metric (F1 or Accuracy)\n    ax = axs[1]\n    if primary_metric == 'f1':\n        # Compute F1 from precision and recall: F1 = 2 * (P * R) / (P + R)\n        train_precision = np.array(history.history['precision'])\n        train_recall = np.array(history.history['recall'])\n        train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall + 1e-7)\n\n        val_precision = np.array(history.history['val_precision'])\n        val_recall = np.array(history.history['val_recall'])\n        val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall + 1e-7)\n\n        ax.plot(epochs, train_f1, 'b.-', label='Training F1-Score')\n        ax.plot(epochs, val_f1, 'r.-', label='Validation F1-Score')\n        ax.set_title('Training and Validation F1-Score')\n        ax.set_ylabel('F1-Score')\n    else:\n        # Use categorical accuracy\n        ax.plot(epochs, history.history['categorical_accuracy'], 'b.-', label='Training Accuracy')\n        ax.plot(epochs, history.history['val_categorical_accuracy'], 'r.-', label='Validation Accuracy')\n        ax.set_title('Training and Validation Accuracy')\n        ax.set_ylabel('Accuracy')\n\n    ax.set_xlim([0, len(epochs)])\n    ax.set_xlabel('Epochs')\n    ax.legend()\n    ax.grid()\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "# Plot SLP training history\nplot_training_history(history_slp, primary_metric=PRIMARY_METRIC)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nThe next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n\n**Strategy:** Add hidden layers and neurons to increase model capacity.\n\n**No regularization applied:** We intentionally train this model **without any regularization** (no dropout, no L2, no early stopping) to observe overfitting behavior. In the training plots, you should see:\n- Training loss continues to decrease\n- Validation loss starts increasing after some epochs (overfitting)\n\nThis demonstrates why regularization (Section 7) is necessary.\n\n### 6.1 Build a Deep Neural Network (DNN)\n\nLet's add a hidden layer with 64 neurons and ReLU activation:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deep Neural Network (1 hidden layer, no dropout for overfitting demo)\ndnn_model = Sequential(name='Deep_Neural_Network')\ndnn_model.add(Dense(64, activation='relu', input_shape=(INPUT_DIMENSION,)))\ndnn_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\ndnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\ndnn_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the Deep Neural Network (without early stopping to demonstrate overfitting)\nhistory_dnn = dnn_model.fit(X_train, y_train, \n                            class_weight=CLASS_WEIGHTS,\n                            batch_size=batch_size, epochs=EPOCHS, \n                            validation_data=(X_val, y_val), \n                            verbose=0)\nval_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot DNN training history\nplot_training_history(history_dnn, primary_metric=PRIMARY_METRIC)"
  },
  {
   "cell_type": "code",
   "source": "# Display DNN validation metrics\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_dnn[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_dnn[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_dnn[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_dnn[3]))\n\npreds_dnn = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_dnn), balanced_accuracy_baseline))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nNow we address the overfitting observed in Section 6 by adding **regularization**. We use two complementary techniques:\n\n| Technique | How it works | Effect |\n|-----------|--------------|--------|\n| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation |\n| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries |\n\nUsing **Hyperband** for efficient hyperparameter tuning to find optimal regularization strengths.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations\n\n### 7.1 Hyperband Search"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperband Model Builder for Multi-Class Twitter Airline Classification\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Twitter Airline model with FROZEN architecture (2 layers: 64 -> 32 neurons).\n    Tunes regularization (Dropout + L2) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # L2 regularization strength (shared across layers)\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n\n    # Fixed architecture: 2 hidden layers with 64 and 32 neurons\n    # Layer 1: 64 neurons with L2 regularization\n    model.add(layers.Dense(64, activation='relu', \n                           kernel_regularizer=regularizers.l2(l2_reg)))\n    drop_0 = hp.Float('drop_0', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_0))\n\n    # Layer 2: 32 neurons with L2 regularization\n    model.add(layers.Dense(32, activation='relu',\n                           kernel_regularizer=regularizers.l2(l2_reg)))\n    drop_1 = hp.Float('drop_1', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_1))\n\n    # Output layer for multi-class classification\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\n# Use appropriate objective based on PRIMARY_METRIC\n# Note: For F1, we use AUC as tuning objective (good proxy for imbalanced data)\n# Final evaluation still uses F1-Score as the primary metric\nTUNING_OBJECTIVE = 'val_categorical_accuracy' if PRIMARY_METRIC == 'accuracy' else 'val_auc'\n\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective=TUNING_OBJECTIVE,\n    max_epochs=20,\n    factor=3,\n    directory='twitter_airline_hyperband',\n    project_name='twitter_airline_tuning'\n)\n\nprint(f\"Tuning objective: {TUNING_OBJECTIVE}\")\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=batch_size,\n    class_weight=CLASS_WEIGHTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters and build best model\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  L2 Regularization: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Layer 1: {best_hp.get('drop_0')}\")\nprint(f\"  Dropout Layer 2: {best_hp.get('drop_1')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Retrain with Optimized Hyperparameters\n\nNow that we have the best hyperparameters from Hyperband search, we:\n\n1. **Build a fresh model** with the optimized L2 strength, dropout rates, and learning rate\n2. **Retrain from scratch** with full epochs (not the limited epochs used during search)\n\n**Why no early stopping?**\n\nWith proper regularization (Dropout + L2), the model should **not overfit** even when trained for the full number of epochs. This is the key insight:\n- **Section 6 (no regularization):** Model overfits → validation loss increases\n- **Section 7 (with regularization):** Model doesn't overfit → validation loss stays low\n\nThe training plots should show that both training and validation loss converge together, demonstrating that **Dropout + L2 alone prevent overfitting**.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the best model (regularization via Dropout + L2, no early stopping needed)\nhistory_opt = opt_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=EPOCHS,\n    batch_size=batch_size,\n    class_weight=CLASS_WEIGHTS,\n    verbose=1\n)\nval_score_opt = opt_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "source": "# Plot optimized model training history\nplot_training_history(history_opt, primary_metric=PRIMARY_METRIC)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds_opt = opt_model.predict(X_val, verbose=0)\n\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_opt[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_opt[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_opt[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_opt[3]))\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_opt.argmax(axis=1)), balanced_accuracy_baseline))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.3 Final Model Evaluation on Test Set\n\nNow we evaluate our best model on the held-out test set that was never used during training or tuning."
  },
  {
   "cell_type": "code",
   "source": "# Final evaluation on test set\ntest_score = opt_model.evaluate(X_test, y_test, verbose=0)[1:]\npreds_test = opt_model.predict(X_test, verbose=0)\n\nprint('=' * 50)\nprint('FINAL TEST SET RESULTS')\nprint('=' * 50)\nprint('Accuracy (Test): {:.2f} (baseline={:.2f})'.format(test_score[0], baseline))\nprint('Precision (Test): {:.2f}'.format(test_score[1]))\nprint('Recall (Test): {:.2f}'.format(test_score[2]))\nprint('AUC (Test): {:.2f}'.format(test_score[3]))\nprint('Balanced Accuracy (Test): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_test.argmax(axis=1), preds_test.argmax(axis=1)), balanced_accuracy_baseline))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display confusion matrix for test predictions\nfig, ax = plt.subplots(figsize=(8, 6))\ncm = confusion_matrix(y_test.argmax(axis=1), preds_test.argmax(axis=1))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Test Set Predictions')\nplt.tight_layout()\nplt.show()\n\n# Print per-class performance\nprint(\"\\nPer-Class Performance:\")\nfor i, class_name in enumerate(label_encoder.classes_):\n    class_mask = y_test.argmax(axis=1) == i\n    class_acc = (preds_test.argmax(axis=1)[class_mask] == i).mean()\n    print(f\"  {class_name.capitalize()}: {class_acc:.2%} accuracy ({class_mask.sum()} samples)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Results Summary\n\nThe following dynamically-generated table compares all models trained in this notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# RESULTS SUMMARY - Dynamically Generated\n# =============================================================================\nfrom sklearn.metrics import f1_score\n\n# Calculate metrics for each model\npreds_slp = slp_model.predict(X_val, verbose=0).argmax(axis=1)\npreds_dnn_val = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\npreds_opt_val = opt_model.predict(X_val, verbose=0).argmax(axis=1)\npreds_test_final = preds_test.argmax(axis=1)\ny_val_labels = y_val.argmax(axis=1)\ny_test_labels = y_test.argmax(axis=1)\n\n# Calculate F1 scores (macro-averaged for multi-class)\nf1_slp = f1_score(y_val_labels, preds_slp, average='macro')\nf1_dnn = f1_score(y_val_labels, preds_dnn_val, average='macro')\nf1_opt = f1_score(y_val_labels, preds_opt_val, average='macro')\nf1_test = f1_score(y_test_labels, preds_test_final, average='macro')\n\n# Create results DataFrame\nresults = pd.DataFrame({\n    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'DNN (No Dropout)', 'Optimized (Tuned)', 'Optimized (Test)'],\n    'Accuracy': [baseline, val_score_slp[0], val_score_dnn[0], val_score_opt[0], test_score[0]],\n    'F1-Score': [0.0, f1_slp, f1_dnn, f1_opt, f1_test],\n    'Dataset': ['N/A', 'Validation', 'Validation', 'Validation', 'Test']\n})\n\nprint(\"=\" * 65)\nprint(\"MODEL COMPARISON - RESULTS SUMMARY\")\nprint(\"=\" * 65)\nprint(f\"Primary Metric: {PRIMARY_METRIC.upper()} (imbalance ratio: {imbalance_ratio:.2f}:1)\")\nprint(\"=\" * 65)\nprint(results.to_string(index=False, float_format='{:.4f}'.format))\nprint(\"=\" * 65)\nprint(f\"\\nKey Observations:\")\nprint(f\"  - All models outperform naive baseline ({baseline:.2%} accuracy)\")\nprint(f\"  - Best validation F1-Score: {max(f1_slp, f1_dnn, f1_opt):.4f}\")\nprint(f\"  - Final test F1-Score: {f1_test:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | 14,640 samples | Hold-Out | Kohavi (1995); Chollet (2021) |\n| **Accuracy vs F1-Score** | > 3:1 imbalance | 3.88:1 ratio | F1-Score | He and Garcia (2009) |\n\n### Lessons Learned\n\n1. **Data-Driven Metric Selection:** With imbalance ratio > 3:1, we use F1-Score instead of Accuracy to ensure fair evaluation across all classes.\n\n2. **Data-Driven Evaluation Protocol:** With > 10,000 samples and deep learning, hold-out validation provides reliable estimates while being computationally efficient.\n\n3. **Class Imbalance Handling:** Using class weights during training improves performance on minority classes.\n\n4. **Simple Models Can Work Well:** The SLP achieved competitive F1-Score with good feature engineering (TF-IDF).\n\n5. **Regularization Prevents Overfitting:** The unregularized DNN showed overfitting; combining **Dropout + L2 regularization** controls this without needing early stopping.\n\n6. **Complementary Regularization:** Dropout (ensemble-like effect) and L2 (weight penalty) work together to prevent overfitting while allowing full training.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning*. 2nd edn. New York: Springer.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Modular Helper Functions\n",
    "\n",
    "For cleaner code organization, you can wrap the model building and training patterns into reusable functions. Below are the modular versions of the code used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODULAR HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "# The following functions encapsulate the model building and training patterns\n",
    "# used throughout this notebook. You can use these for cleaner code organization.\n",
    "\n",
    "def deep_neural_network(hidden_layers=0, hidden_neurons=64, activation='relu',\n",
    "                        dropout=0.0, input_dimension=2, output_dimension=1,\n",
    "                        optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                        name=None):\n",
    "    \"\"\"\n",
    "    Create a deep neural network with configurable architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hidden_layers : int\n",
    "        Number of hidden layers (0 for single-layer perceptron)\n",
    "    hidden_neurons : int\n",
    "        Number of neurons per hidden layer\n",
    "    activation : str\n",
    "        Activation function for hidden layers ('relu', 'tanh', etc.)\n",
    "    dropout : float\n",
    "        Dropout rate (0.0 to 1.0) applied after each hidden layer\n",
    "    input_dimension : int\n",
    "        Number of input features\n",
    "    output_dimension : int\n",
    "        Number of output classes (1 for binary, >1 for multi-class)\n",
    "    optimizer : str\n",
    "        Optimizer name ('rmsprop', 'adam', 'sgd', etc.)\n",
    "    loss : str\n",
    "        Loss function name\n",
    "    metrics : list\n",
    "        List of metrics to track during training\n",
    "    name : str, optional\n",
    "        Model name for identification\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Sequential : Compiled model ready for training\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # model = deep_neural_network(\n",
    "    #     hidden_layers=2, hidden_neurons=64, activation='relu', dropout=0.25,\n",
    "    #     input_dimension=5000, output_dimension=3,\n",
    "    #     optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "    #     metrics=['categorical_accuracy'], name='My_Model'\n",
    "    # )\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    for layer in range(hidden_layers):\n",
    "        model.add(Dense(hidden_neurons, activation=activation,\n",
    "                       input_shape=(input_dimension,) if layer == 0 else None))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    output_activation = 'sigmoid' if output_dimension == 1 else 'softmax'\n",
    "    if hidden_layers == 0:\n",
    "        model.add(Dense(output_dimension, activation=output_activation, \n",
    "                       input_shape=(input_dimension,)))\n",
    "    else:\n",
    "        model.add(Dense(output_dimension, activation=output_activation))\n",
    "    \n",
    "    if name is not None:\n",
    "        model._name = name\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_deep_neural_network(model, X_train, y_train, X_val, y_val,\n",
    "                              class_weights=None, batch_size=32, epochs=100):\n",
    "    \"\"\"\n",
    "    Train a deep neural network and return results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Compiled Keras model to train\n",
    "    X_train, y_train : array-like\n",
    "        Training data and labels\n",
    "    X_val, y_val : array-like\n",
    "        Validation data and labels\n",
    "    class_weights : dict, optional\n",
    "        Class weights for imbalanced data\n",
    "    batch_size : int\n",
    "        Training batch size\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (history, val_score)\n",
    "        - history: Training history object\n",
    "        - val_score: Validation metrics (excluding loss)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    # history, val_score = train_deep_neural_network(\n",
    "    #     model, X_train, y_train, X_val, y_val,\n",
    "    #     class_weights=CLASS_WEIGHTS, batch_size=512, epochs=100\n",
    "    # )\n",
    "    # print(f'Validation Accuracy: {val_score[0]:.2f}')\n",
    "    \"\"\"\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        class_weight=class_weights,\n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    val_score = model.evaluate(X_val, y_val, verbose=0)[1:]\n",
    "    \n",
    "    return history, val_score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}