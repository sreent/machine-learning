{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Twitter%20US%20Airline%20Sentiment/Twitter%20US%20Airline%20Sentiment%20-%20NLP%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Twitter US Airline Sentiment - NLP Example\n\nThis notebook demonstrates the **Universal ML Workflow** applied to a multi-class NLP classification problem using Twitter airline sentiment data.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply the Universal ML Workflow to an NLP text classification problem\n- Convert text data to numerical features using **TF-IDF (Term Frequency-Inverse Document Frequency)** vectorization\n- Handle **imbalanced classes** using class weights during training\n- Build and train deep neural networks for **multi-class classification**\n- Use **Hyperband** for efficient hyperparameter tuning\n- Apply **Dropout + L2 regularisation** to prevent overfitting\n- Evaluate model performance using appropriate metrics for imbalanced data (**F1-Score**, Accuracy, Precision, Recall)\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [Kaggle Twitter US Airline Sentiment](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment) |\n| **Problem Type** | Multi-Class Classification (3 classes) |\n| **Data Balance** | Imbalanced (Negative: ~63%, Neutral: ~21%, Positive: ~16%) |\n| **Data Type** | Unstructured Text (Tweets) |\n| **Input Features** | TF-IDF Vectors (5000 features, bigrams) |\n| **Output** | Sentiment: Negative, Neutral, or Positive |\n| **Imbalance Handling** | Class Weights during Training |\n\n---\n\n## Technique Scope\n\nThis notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n\n| Technique | Status | Rationale |\n|-----------|--------|-----------|\n| **Dense layers (MLP/DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n\nWe demonstrate that **Dropout + L2 regularisation** alone can effectively prevent overfitting without requiring early stopping.\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\nThe first step in any machine learning project is to clearly define the problem and understand the data.\n\n**Problem Statement:** Given a tweet about a US airline, predict the sentiment (Negative, Neutral, or Positive).\n\n**Why this matters:** Airlines can use sentiment analysis to:\n- Identify unhappy customers quickly and respond to complaints\n- Track brand perception over time\n- Discover common pain points in customer experience\n\n**Data Source:** This dataset contains tweets about major US airlines, collected via Twitter's API and labelled by human annotators.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\n### Metric Selection Based on Class Imbalance\n\nThe choice of evaluation metric depends on **class imbalance**. We use practical guidelines derived from the literature:\n\n| Imbalance Ratio | Classification | Primary Metric | Rationale |\n|-----------------|----------------|----------------|-----------|\n| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n\n**Why these thresholds?**\n- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n- **F1-Score**: Harmonic mean of precision and recall, effective for imbalanced data (He and Garcia, 2009)\n\n### References\n\n- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modelling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n\n- Brownlee, J. (2020) *A gentle introduction to imbalanced classification*. Available at: https://machinelearningmastery.com/what-is-imbalanced-classification/ (Accessed: 20 January 2025).\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Luque, A., Carrasco, A., Martín, A. and de las Heras, A. (2019) 'The impact of class imbalance in classification performance metrics based on the binary confusion matrix', *Pattern Recognition*, 91, pp. 216–231.\n\n*Note: The 3:1 threshold is a practical guideline, not a strict academic standard. The literature suggests metric choice depends on domain-specific costs of errors.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\n### Hold-Out vs K-Fold Cross-Validation\n\nThe choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n\n| Dataset Size | Recommended Method | Rationale |\n|--------------|-------------------|-----------|\n| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n| Deep Learning | Hold-Out (preferred) | Training cost prohibitive for K iterations |\n\n**Why 10,000 as a practical threshold?**\n- Below 10,000 samples, hold-out validation has higher variance (Kohavi, 1995)\n- Above 10,000, statistical estimates from hold-out are reliable\n- Deep learning models are expensive to train; K-fold multiplies cost by K (Chollet, 2021)\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n\n- Pedregosa, F. et al. (2011) 'Scikit-learn: machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830. Available at: https://scikit-learn.org/stable/modules/cross_validation.html (Accessed: 20 January 2025).\n\n*Note: The 10,000 threshold is a practical guideline. For computationally cheap models, K-fold is preferred regardless of size.*\n\n### Data Split Strategy (This Notebook)\n\n```\nOriginal Data (14,640 samples) → Hold-Out Selected\n├── Test Set (10%) - Final evaluation only\n└── Training Pool (90%)\n    ├── Training Set (81%) - Model training\n    └── Validation Set (9%) - Hyperparameter tuning\n```\n\n**Important:** We use `stratify` parameter to maintain class proportions in all splits."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries and Set Random Seed\n\nWe set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Keras Tuner for hyperparameter search\n%pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Load and Explore the Dataset\n\nLet's download the Twitter airline sentiment data from Google Drive and examine its structure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data directly from Google Drive\nGDRIVE_FILE_ID = '15XHy_PdD6Q2aa6n-pnWmSFGCv1oK9vWA'\nDATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\n\ntweets = pd.read_csv(DATA_URL)\ntweets = tweets[['text', 'airline_sentiment']]\n\ntweets.head()"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Split Data into Train and Test Sets\n\nWe reserve 10% of the data for final testing. The `stratify` parameter ensures that each split maintains the same class proportions as the original dataset - critical for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "(tweets_train, tweets_test, \n",
    " sentiment_train, sentiment_test) = train_test_split(tweets['text'], tweets['airline_sentiment'], \n",
    "                                                     test_size=TEST_SIZE, stratify=tweets['airline_sentiment'],\n",
    "                                                     shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 Text Vectorization with TF-IDF\n\nNeural networks require numerical input, but tweets are text. We use **TF-IDF (Term Frequency-Inverse Document Frequency)** to convert text to numbers.\n\n**How TF-IDF works:**\n- **TF (Term Frequency):** How often a word appears in a document\n- **IDF (Inverse Document Frequency):** Downweights words that appear in many documents (like \"the\", \"is\")\n- **TF-IDF = TF × IDF:** Words that are frequent in a document but rare overall get high scores\n\n**Our settings:**\n- `max_features=5000`: Keep only the 5000 most important terms\n- `ngram_range=(1, 2)`: Include both single words (unigrams) and word pairs (bigrams) like \"great service\"\n\n---\n\n#### Design Decisions\n\n**Why TF-IDF instead of Word Embeddings (Word2Vec, GloVe)?**\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| **TF-IDF** | Simple, interpretable, no pretrained models needed | Loses word order, no semantic similarity |\n| **Word Embeddings** | Captures semantic meaning, dense vectors | Requires pretrained models or training, more complex |\n\nWe use TF-IDF because:\n1. **Technique scope:** Word embeddings are introduced in Chapter 11 of *Deep Learning with Python* (Chollet, 2021), outside our Ch. 1-4 scope\n2. **Simplicity:** TF-IDF works well for sentiment classification where specific words (e.g., \"terrible\", \"amazing\") are strong predictors\n3. **Interpretability:** Feature weights directly correspond to terms\n\n**Why 5000 features?**\n\nThis balances vocabulary coverage against dimensionality:\n- Too few (e.g., 1000): May miss important domain-specific terms\n- Too many (e.g., 20000): Increases noise and computational cost\n- 5000 is a common practical choice that captures most meaningful terms while keeping the model tractable\n\n**Why bigrams `(1, 2)` instead of just unigrams or trigrams?**\n\n- **Unigrams only:** Misses phrases like \"not good\" or \"very bad\" where meaning depends on word pairs\n- **Bigrams:** Captures common sentiment phrases (\"great service\", \"long wait\", \"never again\")\n- **Trigrams:** Rarely add value for short texts like tweets; increases feature space significantly\n\n**Why minimal text preprocessing?**\n\nWe don't apply stemming, lemmatization, or stopword removal because:\n1. **TF-IDF handles common words:** IDF naturally downweights frequent words like \"the\", \"is\"\n2. **Sentiment in stopwords:** Words like \"not\", \"very\", \"but\" carry sentiment information\n3. **Simplicity:** Minimal preprocessing reduces pipeline complexity and potential errors\n4. **Empirical evidence:** For sentiment analysis, heavy preprocessing often doesn't improve results (Haddi et al., 2013)\n\n*Note: For other NLP tasks (e.g., topic modelling), preprocessing may be more beneficial.*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "NGRAMS = 2\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, NGRAMS), max_features=MAX_FEATURES)\n",
    "tfidf.fit(tweets_train)\n",
    "\n",
    "X_train, X_test = tfidf.transform(tweets_train).toarray(), tfidf.transform(tweets_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 4.5 Encode Labels as One-Hot Vectors\n\nFor multi-class classification with softmax output, we need to convert categorical labels to one-hot encoded vectors:\n- Negative → [1, 0, 0]\n- Neutral → [0, 1, 0]\n- Positive → [0, 0, 1]",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "label_encoder = LabelEncoder()\nlabel_encoder.fit(tweets['airline_sentiment'])\n\ny_train = to_categorical(label_encoder.transform(sentiment_train))\ny_test = to_categorical(label_encoder.transform(sentiment_test))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\nBefore building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n\n### 5.1 Examine Class Distribution\n\nLet's look at how the sentiment classes are distributed:"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>3099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  text\n",
       "0          negative  9178\n",
       "1           neutral  3099\n",
       "2          positive  2363"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = tweets.groupby(['airline_sentiment']).count()\n",
    "counts.reset_index(inplace=True)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n# =============================================================================\n\n# Dataset size analysis (for hold-out vs K-fold decision)\nn_samples = len(tweets)\nHOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000 (Kohavi, 1995; Chollet, 2021)\n\n# Imbalance analysis (for metric selection)\nmajority_class = counts['text'].max()\nminority_class = counts['text'].min()\nimbalance_ratio = majority_class / minority_class\nIMBALANCE_THRESHOLD = 3.0  # Use F1-Score if ratio > 3.0 (He & Garcia, 2009)\n\n# Determine evaluation strategy and metric\nuse_holdout = n_samples > HOLDOUT_THRESHOLD\nuse_f1 = imbalance_ratio > IMBALANCE_THRESHOLD\n\nprint(\"=\" * 60)\nprint(\"DATA-DRIVEN CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\nprint(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples (Kohavi, 1995)\")\nprint(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n\nprint(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\nprint(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1 (He & Garcia, 2009)\")\nprint(f\"   Decision: {'F1-Score (imbalanced)' if use_f1 else 'Accuracy (balanced)'}\")\n\nprint(\"\\n\" + \"=\" * 60)\nPRIMARY_METRIC = 'f1' if use_f1 else 'accuracy'\nprint(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 Calculate Baseline Metrics\n\n**Naive Baseline (Majority Class):** If we always predict \"negative\", we get ~63% accuracy. This is our accuracy baseline.\n\n**Balanced Accuracy Baseline:** A random classifier would achieve 33.3% balanced accuracy (1/3 for each class). This is more meaningful for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6269125683060109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = counts[counts['airline_sentiment']=='negative']['text'].values[0] / counts['text'].sum()\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Balanced accuracy baseline (random classifier)\nbalanced_accuracy_baseline = balanced_accuracy_score(y_train.argmax(axis=1), np.zeros(len(y_train)))\n\nprint(f\"Baseline accuracy (majority class): {baseline:.2f}\")\nprint(f\"Balanced accuracy baseline (random): {balanced_accuracy_baseline:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3 Create Validation Set\n\nWe split off a portion of the training data for validation. This will be used to:\n- Evaluate model performance during hyperparameter tuning\n- Compare models without touching the test set",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=VALIDATION_SIZE, stratify=y_train,\n",
    "                                                  shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 Configure Training Parameters\n\n**Key training settings:**\n- **Optimiser:** Adam - adaptive learning rate optimiser with momentum, widely used for deep learning\n- **Loss:** Categorical cross-entropy - standard loss for multi-class classification\n- **Training Metrics:** Accuracy, Precision, Recall, AUC (tracked by Keras during training)\n- **Primary Metric:** F1-Score (macro-averaged) - computed separately after training using sklearn, since Keras doesn't provide a built-in macro-F1 metric",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "INPUT_DIMENSION = X_train.shape[1]\nOUTPUT_CLASSES = y_train.shape[1]\n\nOPTIMIZER = 'adam'\nLOSS_FUNC = 'categorical_crossentropy'\n\n# Training metrics (tracked by Keras during training)\n# Note: F1-Score (our primary metric) is computed separately using sklearn\n# because Keras doesn't provide a built-in macro-averaged F1 metric\nMETRICS = ['categorical_accuracy', \n           tf.keras.metrics.Precision(name='precision'), \n           tf.keras.metrics.Recall(name='recall'),\n           # multi_label=True is about LABEL FORMAT, not task type:\n           # - Our task: multi-CLASS (3 mutually exclusive classes)\n           # - Our labels: one-hot encoded [[1,0,0], [0,1,0], [0,0,1]]\n           # - multi_label=True tells Keras to expect one-hot format\n           # - multi_label=False expects integer indices [0, 1, 2]\n           tf.keras.metrics.AUC(name='auc', multi_label=True)]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Single-Layer Perceptron (no hidden layers)\nslp_model = Sequential(name='Single_Layer_Perceptron')\nslp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\nslp_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\nslp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAINING CONFIGURATION\n# =============================================================================\n\n# Batch Size Selection:\n# - Large datasets (>10,000 samples): Use 512 for efficient GPU utilisation\n# - Small datasets (<10,000 samples): Use 32-64 for better gradient estimates\n# Twitter US Airline has ~14,640 samples → Use batch size 512\nBATCH_SIZE = 512\n\n# We use DIFFERENT epoch counts for different training phases:\n#\n# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n#   - SLP converges quickly (simple model)\n#   - Unregularised DNN: 100 epochs clearly shows overfitting (val_loss increasing)\n#\n# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2\n#   - WHY train longer? Regularisation SLOWS DOWN learning:\n#     * Dropout randomly masks neurons, so each update uses partial information\n#     * L2 penalty constrains weight updates, preventing large steps\n#     * The model needs MORE iterations to reach the same level of convergence\n#   - Without extra epochs, we'd stop before the model reaches its full potential\n#   - With regularisation, longer training is SAFE (no overfitting risk)\n#\n# The trade-off: Regularisation exchanges faster convergence for overfitting protection.\n# We compensate by allowing more training time.\n\nEPOCHS_BASELINE = 100      # SLP and DNN (no regularisation)\nEPOCHS_REGULARIZED = 150   # DNN with Dropout + L2 (needs more time to converge)"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.5 Handle Class Imbalance with Class Weights\n\nTo handle imbalanced classes, we compute **class weights** that give more importance to minority classes during training:\n- **Negative (majority):** Lower weight (~0.53)\n- **Neutral:** Medium weight (~1.57)\n- **Positive (minority):** Higher weight (~2.06)\n\nThis makes errors on minority classes \"cost more\", encouraging the model to learn them better.\n\n---\n\n#### Why Class Weights Instead of Resampling?\n\n| Technique | How it works | Pros | Cons |\n|-----------|--------------|------|------|\n| **Class Weights** | Adjusts loss function to penalize minority errors more | Simple, no data modification, works with any batch size | Doesn't add information |\n| **Oversampling (SMOTE)** | Creates synthetic minority samples | Adds training data, can improve decision boundaries | Risk of overfitting to synthetic data, increases training time |\n| **Undersampling** | Removes majority class samples | Balances dataset, faster training | Loses potentially useful information |\n\nWe use **class weights** because:\n1. **Simplicity:** No need to modify the dataset or import additional libraries\n2. **No synthetic data risk:** SMOTE can create unrealistic samples, especially for text data\n3. **Efficiency:** Training time unchanged; no additional data to process\n4. **Keras integration:** Native support via `class_weight` parameter in `model.fit()`\n\n*Note: For severely imbalanced data (>10:1 ratio), combining class weights with oversampling may yield better results.*\n\n---\n\n#### How Class Weights Fit Our Evaluation Strategy\n\n| Component | Role | Handles Imbalance? |\n|-----------|------|-------------------|\n| **Class weights** | Training technique - modifies loss function | Yes - penalizes minority class errors more |\n| **AUC** (tuning objective) | Evaluation metric during hyperparameter search | Yes - macro-averages across classes |\n| **F1-macro** (primary metric) | Final evaluation metric | Yes - macro-averages across classes |\n\nThese three components work together consistently:\n- **Class weights** help the model *learn* minority classes better (affects training loss)\n- **AUC** and **F1-macro** *measure* performance across all classes equally (evaluation metrics, computed without weights)\n\nAll three treat classes fairly regardless of size, making this a coherent strategy for imbalanced data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5317352220103514, 1: 1.5748285599031868, 2: 2.064516129032258}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.argmax(y_train, axis=1)\n",
    "weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the Single-Layer Perceptron\nhistory_slp = slp_model.fit(X_train, y_train, \n                            class_weight=CLASS_WEIGHTS,\n                            batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n                            validation_data=(X_val, y_val),\n                            verbose=0)\nval_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display SLP validation metrics\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_slp[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_slp[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_slp[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_slp[3]))\n\npreds_slp_val = slp_model.predict(X_val, verbose=0).argmax(axis=1)\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_slp_val), balanced_accuracy_baseline))\n\n# Calculate F1-Score (primary metric for imbalanced data)\nf1_slp_val = f1_score(y_val.argmax(axis=1), preds_slp_val, average='macro')\nprint(f'F1-Score (Validation): {f1_slp_val:.2f}  ← Primary Metric')"
  },
  {
   "cell_type": "code",
   "source": "def plot_training_history(history, title=None):\n    \"\"\"\n    Plot training and validation metrics over epochs.\n    Plots: (1) Loss, (2) Accuracy\n    \n    Note: We use Accuracy for training visualisation (directly tracked by Keras).\n    F1-Score is computed for final evaluation since it's our primary metric.\n\n    Parameters:\n    -----------\n    history : keras History object\n        Training history from model.fit()\n    title : str, optional\n        Model name to display in plot titles (e.g., 'SLP', 'DNN')\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history.history['loss']) + 1)\n    title_suffix = f' ({title})' if title else ''\n\n    # Plot 1: Loss\n    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n    axs[0].set_title(f'Loss{title_suffix}')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(alpha=0.3)\n\n    # Plot 2: Accuracy (directly tracked by Keras)\n    axs[1].plot(epochs, history.history['categorical_accuracy'], 'b-', label='Training', linewidth=1.5)\n    axs[1].plot(epochs, history.history['val_categorical_accuracy'], 'r-', label='Validation', linewidth=1.5)\n    axs[1].set_title(f'Accuracy{title_suffix}')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "# Plot SLP training history\nplot_training_history(history_slp, title='SLP Baseline')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nThe next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n\n**Strategy:** Add hidden layers and neurons to increase model capacity.\n\n**No regularisation applied:** We intentionally train this model **without any regularisation** (no dropout, no L2, no early stopping) to observe overfitting behaviour. In the training plots, you should see:\n- Training loss continues to decrease\n- Validation loss starts increasing after some epochs (overfitting)\n\nThis demonstrates why regularisation (Section 7) is necessary.\n\n---\n\n#### Architecture Design Decisions\n\n**Why 64 neurons in the hidden layer?**\n\nThis is a practical starting point that balances capacity and efficiency:\n- **Too few (e.g., 16):** May not have enough capacity to learn complex patterns\n- **Too many (e.g., 512):** Increases overfitting risk and training time without proportional benefit\n- **64 neurons:** A common choice for tabular/text data that provides sufficient capacity for most classification tasks\n\n**Why only 1 hidden layer instead of 2-3?**\n\nPer the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n\n1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation, not to add more capacity\n\nIf this 1-layer model *couldn't* overfit (training and validation loss both plateau high), we would then add more layers. But since it does overfit, the architecture is adequate.\n\n*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*\n\n### 6.1 Build a Deep Neural Network (DNN)\n\nLet's add a hidden layer with 64 neurons and ReLU activation:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deep Neural Network (1 hidden layer, no dropout for overfitting demo)\ndnn_model = Sequential(name='Deep_Neural_Network')\ndnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\ndnn_model.add(Dense(64, activation='relu'))\ndnn_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\ndnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n\ndnn_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the Deep Neural Network (without regularisation to demonstrate overfitting)\n# Using EPOCHS_BASELINE: 100 epochs is enough to clearly show overfitting behaviour\nhistory_dnn = dnn_model.fit(X_train, y_train, \n                            class_weight=CLASS_WEIGHTS,\n                            batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n                            validation_data=(X_val, y_val), \n                            verbose=0)\nval_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot DNN training history (expect overfitting: val_loss increasing)\nplot_training_history(history_dnn, title='DNN - No Regularisation')"
  },
  {
   "cell_type": "code",
   "source": "# Display DNN validation metrics\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_dnn[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score_dnn[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score_dnn[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score_dnn[3]))\n\npreds_dnn_val = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n    balanced_accuracy_score(y_val.argmax(axis=1), preds_dnn_val), balanced_accuracy_baseline))\n\n# Calculate F1-Score (primary metric for imbalanced data)\nf1_dnn_val = f1_score(y_val.argmax(axis=1), preds_dnn_val, average='macro')\nprint(f'F1-Score (Validation): {f1_dnn_val:.2f}  ← Primary Metric')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Regularising Your Model and Tuning Hyperparameters\n\nNow we address the overfitting observed in Section 6 by adding **regularisation**. We use two complementary techniques:\n\n| Technique | How it works | Effect |\n|-----------|--------------|--------|\n| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation |\n| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries |\n\n**Same architecture, different regularisation:** We keep the same 1-layer architecture (64 neurons) as Section 6, so the only difference is regularisation. This allows a fair comparison.\n\nUsing **Hyperband** for efficient hyperparameter tuning to find optimal regularisation strengths.\n\n---\n\n#### Why Hyperband?\n\n| Method | How it works | Pros | Cons |\n|--------|--------------|------|------|\n| **Grid Search** | Tries all combinations exhaustively | Thorough, reproducible | Exponentially expensive, wastes time on bad configs |\n| **Random Search** | Samples random combinations | More efficient than grid, good coverage | Still trains all configs to completion |\n| **Bayesian Optimisation** | Models performance to guide search | Sample-efficient, finds good configs fast | Complex to implement, overhead per iteration |\n| **Hyperband** | Early stopping of poor performers | Very efficient for deep learning, parallelisable | May discard slow starters prematurely |\n\nWe use **Hyperband** because:\n1. **Efficiency:** Eliminates poor configurations early, focusing resources on promising ones\n2. **Deep learning fit:** Training epochs are a natural \"resource\" to allocate adaptively\n3. **Keras Tuner integration:** Native support via `kt.Hyperband`\n4. **Scalability:** Handles many hyperparameters efficiently\n\n**How Hyperband works:**\n1. Start training many configurations for a few epochs\n2. Evaluate and keep the top performers\n3. Train survivors for more epochs\n4. Repeat until only the best remain\n\n### 7.1 Hyperband Search"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperband Model Builder for Multi-Class Twitter Airline Classification\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Twitter Airline model with FIXED architecture (1 hidden layer, 64 neurons).\n    Same architecture as Section 6 DNN - only tunes regularisation and learning rate.\n    Uses Adam optimiser (same as Section 6) for fair comparison.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # L2 regularisation strength\n    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n\n    # Fixed architecture: 1 hidden layer with 64 neurons (same as Section 6)\n    model.add(layers.Dense(64, activation='relu', \n                           kernel_regularizer=regularizers.l2(l2_reg)))\n    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(dropout_rate))\n\n    # Output layer for multi-class classification\n    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\n# ===========================================================================\n# TUNING OBJECTIVE: Why AUC instead of F1?\n# ===========================================================================\n# We use F1-Score as our PRIMARY METRIC for final evaluation, but AUC for tuning:\n#\n# 1. Keras limitation: No built-in macro-averaged F1 metric available during training\n#    - F1 requires computing precision/recall across all classes, then averaging\n#    - Keras metrics are computed per-batch, making macro-F1 impractical\n#\n# 2. AUC is a better tuning objective:\n#    - AUC is threshold-independent (evaluates across all decision thresholds)\n#    - F1 is threshold-dependent (requires choosing a cutoff for predictions)\n#    - Both penalize models that ignore minority classes\n#    - Models with high AUC tend to achieve high F1 at optimal thresholds\n#\n# 3. Alternative approaches (not used here):\n#    - Custom Keras metric (complex, slower)\n#    - Validation callback computing F1 each epoch (adds overhead)\n#\n# Final evaluation uses sklearn's f1_score(average='macro') as primary metric.\n# ===========================================================================\n\nTUNING_OBJECTIVE = 'val_categorical_accuracy' if PRIMARY_METRIC == 'accuracy' else 'val_auc'\n\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective=TUNING_OBJECTIVE,\n    max_epochs=20,\n    factor=3,\n    directory='twitter_airline_hyperband',\n    project_name='twitter_airline_tuning',\n    overwrite=True  # Ensure fresh tuning on each run\n)\n\nprint(f\"Tuning objective: {TUNING_OBJECTIVE}\")\nprint(\"(Note: Final evaluation uses F1-Score as primary metric)\")\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=BATCH_SIZE,\n    class_weight=CLASS_WEIGHTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters found by Hyperband:\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\n# =============================================================================\n# CRITICAL: Extract the number of epochs from the best trial\n# =============================================================================\nbest_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\nbest_epochs = best_trial.best_step + 1  # best_step is 0-indexed\n\nprint(f\"\\n>>> Best trial was trained for {best_epochs} epochs <<<\")\nprint(f\"    (This is the epoch count we'll use for retraining)\")\n\n# Build a fresh model with the best hyperparameters\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Retraining with Full Data and Matched Epochs\n\nAfter finding the best hyperparameters, we retrain with two important considerations:\n\n1. **Matched Epochs:** Use the exact number of epochs that produced the best validation score during Hyperband tuning\n2. **Combined Training Data:** Merge training and validation sets to maximise the data available for the final model\n\n---\n\n#### Why Combine Training and Validation Sets?\n\nOnce hyperparameter tuning is complete, the validation set has served its purpose (model selection). For the final model:\n\n| Approach | Training Data | Benefit |\n|----------|--------------|---------|\n| Keep validation separate | ~81% of original | Can monitor overfitting during retrain |\n| **Combine train + validation** | **~90% of original** | **Maximises data for final model** |\n\nWe combine because:\n- **More data = better generalisation:** The model learns from more examples\n- **Validation set's job is done:** It was used for hyperparameter selection, not needed for final training\n- **Standard practice:** This is the recommended approach in production ML pipelines (Chollet, 2021)\n\n---\n\n#### The Epoch Mismatch Problem\n\nHyperband finds hyperparameters optimal at a **specific epoch count**. If we retrain for a different number of epochs, the hyperparameters may no longer be optimal.\n\nWe extract `best_trial.best_step + 1` to match the exact epoch count where Hyperband found the best validation score.\n\n> *\"Combine your data to maximise learning, match your epochs to honour what Hyperband discovered.\"*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RETRAIN WITH FULL DATA AND MATCHED EPOCHS\n# =============================================================================\n\n# Combine training and validation sets for final model\nX_train_full = np.vstack([X_train, X_val])\ny_train_full = np.vstack([y_train, y_val])\n\n# Recompute class weights on combined data\nlabels_full = np.argmax(y_train_full, axis=1)\nweights_full = compute_class_weight('balanced', classes=np.unique(labels_full), y=labels_full)\nCLASS_WEIGHTS_FULL = dict(enumerate(weights_full))\n\nprint(f\"Training data: {len(X_train):,} samples\")\nprint(f\"Validation data: {len(X_val):,} samples\")\nprint(f\"Combined data: {len(X_train_full):,} samples (+{len(X_val)/len(X_train)*100:.1f}%)\")\nprint(f\"\\nRetraining with best hyperparameters for {best_epochs} epochs...\")\n\nhistory_opt = opt_model.fit(\n    X_train_full, y_train_full,\n    epochs=best_epochs,  # CRITICAL: Use matched epochs!\n    batch_size=BATCH_SIZE,\n    class_weight=CLASS_WEIGHTS_FULL,\n    verbose=0\n)\n\nprint(f\"Final model trained on {len(X_train_full):,} samples for {best_epochs} epochs\")"
  },
  {
   "cell_type": "code",
   "source": "# Plot training history for the final model\n# Note: No validation curves since we trained on combined train+val data\ndef plot_training_only(history, title=None):\n    \"\"\"Plot training metrics only (no validation data available).\"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history.history['loss']) + 1)\n    title_suffix = f' ({title})' if title else ''\n\n    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n    axs[0].set_title(f'Loss{title_suffix}')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(alpha=0.3)\n\n    acc_key = 'categorical_accuracy' if 'categorical_accuracy' in history.history else 'accuracy'\n    axs[1].plot(epochs, history.history[acc_key], 'b-', label='Training', linewidth=1.5)\n    axs[1].set_title(f'Accuracy{title_suffix}')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_training_only(history_opt, title=f'Final Model - Full Data ({best_epochs} epochs)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Note: No validation metrics for the final model\n# The validation set has been combined with training data to maximise learning.\n# Model performance is evaluated directly on the test set (Section 7.3).\nprint(\"Validation set merged with training data for final model.\")\nprint(\"Proceeding directly to test set evaluation...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.3 Final Model Evaluation on Test Set\n\nNow we evaluate our best model on the held-out test set that was never used during training or tuning."
  },
  {
   "cell_type": "code",
   "source": "# Final evaluation on test set\ntest_score = opt_model.evaluate(X_test, y_test, verbose=0)[1:]\npreds_test = opt_model.predict(X_test, verbose=0).argmax(axis=1)\n\n# Calculate F1-Score (our primary metric)\ntest_f1 = f1_score(y_test.argmax(axis=1), preds_test, average='macro')\n\nprint('=' * 50)\nprint('FINAL TEST SET RESULTS')\nprint('=' * 50)\nprint(f'F1-Score (Test): {test_f1:.4f}  ← Primary Metric')\nprint(f'Accuracy (Test): {test_score[0]:.4f} (baseline={baseline:.4f})')\nprint(f'Precision (Test): {test_score[1]:.4f}')\nprint(f'Recall (Test): {test_score[2]:.4f}')\nprint(f'AUC (Test): {test_score[3]:.4f}')\nprint(f'Balanced Accuracy (Test): {balanced_accuracy_score(y_test.argmax(axis=1), preds_test):.4f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display confusion matrix for test predictions\nfig, ax = plt.subplots(figsize=(8, 6))\ncm = confusion_matrix(y_test.argmax(axis=1), preds_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Test Set Predictions')\nplt.tight_layout()\nplt.show()\n\n# Print per-class recall (what % of each class was correctly identified)\nprint(\"\\nPer-Class Recall:\")\nfor i, class_name in enumerate(label_encoder.classes_):\n    class_mask = y_test.argmax(axis=1) == i\n    class_recall = (preds_test[class_mask] == i).mean()\n    print(f\"  {class_name.capitalize()}: {class_recall:.2%} ({class_mask.sum()} samples)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Results Summary\n\nThe following dynamically-generated table compares all models trained in this notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# RESULTS SUMMARY\n# =============================================================================\n\n# Create results DataFrame\n# Note: Final model (DNN Dropout + L2) shows only test metrics\n# because validation data was combined with training for final model\nresults = pd.DataFrame({\n    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'DNN (No Regularisation)', 'DNN (Dropout + L2)'],\n    'Accuracy': [baseline, val_score_slp[0], val_score_dnn[0], test_score[0]],\n    'F1-Score': [0.0, f1_slp_val, f1_dnn_val, test_f1],\n    'Dataset': ['N/A', 'Validation', 'Validation', 'Test']\n})\n\nprint(\"=\" * 70)\nprint(\"MODEL COMPARISON - RESULTS SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Primary Metric: F1-SCORE (imbalance ratio: {imbalance_ratio:.2f}:1)\")\nprint(\"=\" * 70)\nprint(results.to_string(index=False, float_format='{:.4f}'.format))\nprint(\"=\" * 70)\nprint(f\"\\nKey Observations:\")\nprint(f\"  - All models outperform naive baseline ({baseline:.2%} accuracy)\")\nprint(f\"  - DNN with Dropout + L2 trained on combined train+val data\")\nprint(f\"  - Final test F1-Score: {test_f1:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Key Takeaways\n\n### Decision Framework Summary\n\n| Decision | Threshold | This Dataset | Choice | Reference |\n|----------|-----------|--------------|--------|-----------|\n| **Hold-Out vs K-Fold** | > 10,000 samples | 14,640 samples | Hold-Out | Kohavi (1995); Chollet (2021) |\n| **Accuracy vs F1-Score** | > 3:1 imbalance | 3.88:1 ratio | F1-Score | He and Garcia (2009) |\n\n### Lessons Learned\n\n1. **Data-Driven Metric Selection:** With imbalance ratio > 3:1, we use F1-Score instead of Accuracy to ensure fair evaluation across all classes.\n\n2. **Data-Driven Evaluation Protocol:** With > 10,000 samples and deep learning, hold-out validation provides reliable estimates while being computationally efficient.\n\n3. **Class Imbalance Handling:** Using class weights during training improves performance on minority classes. This is simpler than resampling techniques (SMOTE, undersampling) and avoids synthetic data risks.\n\n4. **Simple Models Can Work Well:** The SLP achieved competitive F1-Score with good feature engineering (TF-IDF).\n\n5. **Regularisation Prevents Overfitting:** The unregularised DNN showed overfitting; combining **Dropout + L2 regularisation** controls this effectively—no need for deeper architectures once overfitting is demonstrated.\n\n6. **Maximise Data for Final Model:** After hyperparameter tuning, we combine training and validation sets for the final model. The validation set has served its purpose (model selection), and more training data leads to better generalisation.\n\n7. **Technique Scope:** We use only techniques from Chapters 1–4 of *Deep Learning with Python* (Chollet, 2021). Early stopping and word embeddings, while useful, are outside this scope.\n\n8. **Feature Engineering Choices:** TF-IDF with bigrams and minimal preprocessing works well for sentiment classification. Heavy text preprocessing often doesn't improve sentiment analysis results.\n\n### References\n\n- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n\n- Haddi, E., Liu, X. and Shi, Y. (2013) 'The role of text pre-processing in sentiment analysis', *Procedia Computer Science*, 17, pp. 26–32.\n\n- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning*. 2nd edn. New York: Springer.\n\n- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n\n- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Appendix: Modular Helper Functions\n\nFor cleaner code organisation, you can wrap the model building and training patterns into reusable functions. Below are the modular versions of the code used in this notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MODULAR HELPER FUNCTIONS\n# =============================================================================\n\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\n\ndef build_dnn(input_dim, output_dim, hidden_units=None, dropout=0.0, l2_reg=0.0,\n              optimizer='adam', loss='categorical_crossentropy', \n              metrics=['categorical_accuracy'], name=None):\n    \"\"\"\n    Build a Deep Neural Network with configurable architecture and regularisation.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    output_dim : int\n        Number of output classes (1 for binary, >1 for multi-class)\n    hidden_units : list of int, optional\n        Neurons per hidden layer, e.g., [64] or [128, 64, 32]\n        None or [] creates a single-layer perceptron (no hidden layers)\n    dropout : float\n        Dropout rate (0.0 to 0.5) applied after each hidden layer\n    l2_reg : float\n        L2 regularisation strength (0.0 for no regularisation)\n    optimizer : str or keras.optimizers.Optimizer\n        Optimiser name or instance\n    loss : str\n        Loss function name\n    metrics : list\n        Metrics to track during training\n    name : str, optional\n        Model name for identification\n        \n    Returns:\n    --------\n    keras.Sequential : Compiled model ready for training\n    \n    Examples:\n    ---------\n    # Single-layer perceptron (no hidden layers)\n    slp = build_dnn(5000, 3, name='SLP')\n    \n    # DNN with one hidden layer (64 neurons)\n    dnn = build_dnn(5000, 3, hidden_units=[64], name='DNN')\n    \n    # DNN with regularisation\n    dnn_reg = build_dnn(5000, 3, hidden_units=[64], dropout=0.3, l2_reg=0.001)\n    \n    # Deeper network\n    deep = build_dnn(5000, 3, hidden_units=[128, 64, 32], dropout=0.2)\n    \"\"\"\n    model = Sequential(name=name)\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    # Hidden layers\n    hidden_units = hidden_units or []\n    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n    \n    for units in hidden_units:\n        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n        if dropout > 0:\n            model.add(Dropout(dropout))\n    \n    # Output layer\n    output_activation = 'sigmoid' if output_dim == 1 else 'softmax'\n    model.add(Dense(output_dim, activation=output_activation))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model\n\n\ndef train_model(model, X_train, y_train, X_val, y_val,\n                class_weights=None, batch_size=512, epochs=100, verbose=0):\n    \"\"\"\n    Train a model and return training history.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Compiled Keras model\n    X_train, y_train : array-like\n        Training data and labels\n    X_val, y_val : array-like\n        Validation data and labels\n    class_weights : dict, optional\n        Class weights for imbalanced data\n    batch_size : int\n        Training batch size (default: 512)\n    epochs : int\n        Number of training epochs (default: 100)\n    verbose : int\n        Verbosity mode (0=silent, 1=progress bar)\n        \n    Returns:\n    --------\n    keras.callbacks.History : Training history object\n    \"\"\"\n    return model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        class_weight=class_weights,\n        batch_size=batch_size, \n        epochs=epochs,\n        verbose=verbose\n    )\n\n\ndef evaluate_model(model, X, y_true):\n    \"\"\"\n    Evaluate model and return comprehensive metrics.\n    \n    Parameters:\n    -----------\n    model : keras.Model\n        Trained Keras model\n    X : array-like\n        Input features\n    y_true : array-like\n        True labels (one-hot encoded)\n        \n    Returns:\n    --------\n    dict : Dictionary containing all metrics (accuracy, f1_macro, precision, \n           recall, auc, balanced_accuracy)\n    \"\"\"\n    # Get predictions\n    y_pred_proba = model.predict(X, verbose=0)\n    y_pred = y_pred_proba.argmax(axis=1)\n    y_true_labels = y_true.argmax(axis=1)\n    \n    # Calculate metrics (consistent with main notebook)\n    metrics = {\n        'accuracy': accuracy_score(y_true_labels, y_pred),\n        'f1_macro': f1_score(y_true_labels, y_pred, average='macro'),\n        'precision': precision_score(y_true_labels, y_pred, average='macro'),\n        'recall': recall_score(y_true_labels, y_pred, average='macro'),\n        'auc': roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro'),\n        'balanced_accuracy': balanced_accuracy_score(y_true_labels, y_pred),\n    }\n    \n    return metrics\n\n\ndef print_metrics(metrics, dataset_name='Validation', primary_metric='f1_macro'):\n    \"\"\"\n    Print metrics in a formatted way.\n    \n    Parameters:\n    -----------\n    metrics : dict\n        Dictionary of metric names and values\n    dataset_name : str\n        Name of the dataset (e.g., 'Validation', 'Test')\n    primary_metric : str\n        Key of the primary metric to highlight\n    \"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"{dataset_name.upper()} METRICS\")\n    print('='*50)\n    \n    for name, value in metrics.items():\n        suffix = '  ← Primary Metric' if name == primary_metric else ''\n        print(f\"{name.replace('_', ' ').title()}: {value:.4f}{suffix}\")\n\n\ndef plot_history(history, title=None):\n    \"\"\"\n    Plot training and validation metrics over epochs.\n    Plots: (1) Loss, (2) Accuracy\n    \n    Note: We use Accuracy for training visualisation (directly tracked by Keras).\n    F1-Score is computed for final evaluation since it's our primary metric.\n    \n    Parameters:\n    -----------\n    history : keras.callbacks.History\n        Training history from model.fit()\n    title : str, optional\n        Model name to display in plot titles (e.g., 'SLP', 'DNN')\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history.history['loss']) + 1)\n    title_suffix = f' ({title})' if title else ''\n\n    # Plot 1: Loss\n    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n    axs[0].set_title(f'Loss{title_suffix}')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(alpha=0.3)\n\n    # Plot 2: Accuracy (directly tracked by Keras)\n    acc_key = 'categorical_accuracy' if 'categorical_accuracy' in history.history else 'accuracy'\n    axs[1].plot(epochs, history.history[acc_key], 'b-', label='Training', linewidth=1.5)\n    axs[1].plot(epochs, history.history[f'val_{acc_key}'], 'r-', label='Validation', linewidth=1.5)\n    axs[1].set_title(f'Accuracy{title_suffix}')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n\n# =============================================================================\n# USAGE EXAMPLES\n# =============================================================================\n# \n# # Build models\n# slp = build_dnn(INPUT_DIMENSION, OUTPUT_CLASSES, name='SLP')\n# dnn = build_dnn(INPUT_DIMENSION, OUTPUT_CLASSES, hidden_units=[64], name='DNN')\n# dnn_reg = build_dnn(INPUT_DIMENSION, OUTPUT_CLASSES, hidden_units=[64], \n#                     dropout=0.3, l2_reg=0.001, name='DNN_Regularized')\n# \n# # Train\n# history = train_model(dnn, X_train, y_train, X_val, y_val, \n#                       class_weights=CLASS_WEIGHTS)\n# \n# # Evaluate\n# metrics = evaluate_model(dnn, X_val, y_val)\n# print_metrics(metrics, 'Validation')\n# \n# # Plot\n# plot_history(history, 'DNN')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}