{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Movie%20Review/Movie%20Review%20-%20NLP%20Binary%20Classification%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# Movie Review - NLP Binary Classification Example\n\nThis notebook demonstrates the **Universal ML Workflow** for binary sentiment classification on movie reviews.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Apply TF-IDF vectorization to text data for binary classification\n- Handle nearly-balanced binary classification\n- Build and evaluate binary sentiment classifiers\n- Compare binary vs. multi-class NLP approaches\n- Use **Hyperband** for efficient hyperparameter tuning\n\n---\n\n## Dataset Overview\n\n| Attribute | Description |\n|-----------|-------------|\n| **Source** | [NLTK Movie Review Dataset](https://www.kaggle.com/datasets/nltkdata/movie-review) |\n| **Problem Type** | Binary Classification (Positive/Negative) |\n| **Data Balance** | Nearly Balanced (~51% Positive, ~49% Negative) |\n| **Data Type** | Unstructured Text (Movie Reviews) |\n| **Input Features** | TF-IDF Vectors (5000 features, bigrams) |\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Defining the Problem and Assembling a Dataset\n\n**Problem:** Classify movie reviews as positive or negative sentiment.\n\n**Key difference from Twitter example:** This is binary (2 classes) vs. multi-class (3+ classes), affecting:\n- **Output layer:** 1 neuron with sigmoid (binary) vs. N neurons with softmax (multi-class)\n- **Loss function:** Binary cross-entropy vs. categorical cross-entropy\n- **Label encoding:** Single 0/1 label vs. one-hot vectors\n\n**Why this matters:** Binary classification is simpler and often more robust, making it a good starting point for sentiment analysis tasks.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Choosing a Measure of Success\n\nFor this nearly-balanced binary classification:\n- **Balanced Accuracy** still useful for consistency\n- Standard **Accuracy** is also meaningful here\n- **Precision, Recall, AUC** for comprehensive evaluation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Deciding on an Evaluation Protocol\n\nStandard hold-out + validation + K-fold cross-validation approach."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Preparing Your Data\n\n### 4.1 Import Libraries and Load Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4S0tiStcOxV"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import RMSprop\n\n# Keras Tuner for hyperparameter search\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\nimport itertools\nimport matplotlib.pyplot as plt\n\nSEED = 204\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for starters , it was created by alan moore ( ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to say moore and campbell thoroughly researche...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the book ( or \" graphic novel , \" if you will ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in other words , don't dismiss this film becau...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  tag\n",
       "0  films adapted from comic books have had plenty...  pos\n",
       "1  for starters , it was created by alan moore ( ...  pos\n",
       "2  to say moore and campbell thoroughly researche...  pos\n",
       "3  the book ( or \" graphic novel , \" if you will ...  pos\n",
       "4  in other words , don't dismiss this film becau...  pos"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('movie_review.csv', sep=',')\n",
    "reviews = reviews[['text', 'tag']]\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "(text_train, text_test, \n",
    " tag_train, tag_test) = train_test_split(reviews['text'], reviews['tag'], \n",
    "                                         test_size=TEST_SIZE, stratify=reviews['tag'],\n",
    "                                                     shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "NGRAMS = 2\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, NGRAMS), max_features=MAX_FEATURES)\n",
    "tfidf.fit(text_train)\n",
    "\n",
    "X_train, X_test = tfidf.transform(text_train).toarray(), tfidf.transform(text_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(reviews['tag'])\n",
    "\n",
    "y_train = label_encoder.transform(tag_train)\n",
    "y_test = label_encoder.transform(tag_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = X_test.shape[0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                 test_size=VALIDATION_SIZE, stratify=y_train,\n",
    "                                                 shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Developing a Model That Does Better Than a Baseline\n\nBaseline for nearly-balanced binary: ~51% (majority class)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>31783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>32937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tag   text\n",
       "0  neg  31783\n",
       "1  pos  32937"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = reviews.groupby(['tag']).count()\n",
    "counts.reset_index(inplace=True)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5089153275648949"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the class is off-balanced, but very minimal \n",
    "\n",
    "baseline = counts[counts['tag']=='pos']['text'].values[0] / counts['text'].sum()\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_baseline = balanced_accuracy_score(y_train, np.zeros(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 13:00:32.082616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-26 13:00:32.082637: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-26 13:00:32.082653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (snottingham): /proc/driver/nvidia/version does not exist\n",
      "2023-02-26 13:00:32.082850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_DIMENSION = 1\n",
    "\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS_FUNC = 'binary_crossentropy'\n",
    "METRICS = ['accuracy', \n",
    "           tf.keras.metrics.Precision(name='precision'), \n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc', multi_label=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learning_rate = 0.001\n\nslp_model = Sequential(name='Single_Layer_Perceptron')\nslp_model.add(Dense(1, activation='sigmoid', input_shape=(INPUT_DIMENSION,)))\nslp_model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=LOSS_FUNC, metrics=METRICS)\n\nslp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 512\nEPOCHS = 100"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0181303338970387, 1: 0.9825040798512278}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "history = slp_model.fit(X_train, y_train, class_weight=CLASS_WEIGHTS, batch_size=batch_size, epochs=500, validation_data=(X_val, y_val), verbose=0)\nval_score = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score[3]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = slp_model.predict(X_val, verbose=0)\n\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(balanced_accuracy_score(y_val, (preds > 0.5).astype('int32')), balanced_accuracy_baseline))"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dNzQH9Srwt5R"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, monitors=['loss', 'AUC']) :\n",
    "\n",
    "  # using the variable axs for multiple Axes\n",
    "  fig, axs = plt.subplots(1, 2, sharex='all', figsize=(15,5))\n",
    " \n",
    "  for ax, monitor in zip(axs.flat, monitors) :\n",
    "    loss, val_loss = history.history[monitor], history.history['val_' + monitor]\n",
    "\n",
    "    if monitor == 'loss' :\n",
    "      monitor = monitor.capitalize()\n",
    "\n",
    "    epochs = range(1, len(loss)+1)\n",
    "\n",
    "    ax.plot(epochs, loss, 'b.', label=monitor)\n",
    "    ax.plot(epochs, val_loss, 'r.', label='Validation ' + monitor)\n",
    "    ax.set_xlim([0, len(loss)])\n",
    "    ax.title.set_text('Training and Validation ' + monitor + 's')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(monitor)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "  _ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "xT6SBVGW0dEA",
    "outputId": "55f3b135-788f-4741-b1f9-5882bbada568"
   },
   "outputs": [],
   "source": "plot_training_history(history, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nAdding hidden layers for increased capacity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learning_rate = 0.0002\n\nmlp_model = Sequential(name='Multi_Layer_Perceptron')\nmlp_model.add(Dense(64, activation='relu', input_shape=(INPUT_DIMENSION,)))\nmlp_model.add(Dense(1, activation='sigmoid'))\nmlp_model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=LOSS_FUNC, metrics=METRICS)\n\nmlp_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "history = mlp_model.fit(X_train, y_train, class_weight=CLASS_WEIGHTS, batch_size=batch_size, epochs=EPOCHS, validation_data=(X_val, y_val), verbose=0)\nval_score = mlp_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plot_training_history(history, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score[3]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "preds = mlp_model.predict(X_val, verbose=0)\n\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(balanced_accuracy_score(y_val, (preds > 0.5).astype('int32')), balanced_accuracy_baseline))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Regularizing Your Model and Tuning Hyperparameters\n\nUsing **Hyperband** for efficient hyperparameter tuning with a frozen architecture.\n\n### Why Hyperband?\n\n**Hyperband** is more efficient than grid search because it:\n1. Starts training many configurations for a few epochs\n2. Eliminates poor performers early\n3. Allocates more resources to promising configurations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperband Model Builder for Binary NLP Classification\ndef build_model_hyperband(hp):\n    \"\"\"\n    Build Movie Review model with FROZEN architecture (2 layers: 64 -> 32 neurons).\n    Only tunes regularization (Dropout) and learning rate.\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n\n    # Fixed architecture: 2 hidden layers with 64 and 32 neurons\n    # Layer 1: 64 neurons\n    model.add(layers.Dense(64, activation='relu'))\n    drop_0 = hp.Float('drop_0', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_0))\n\n    # Layer 2: 32 neurons\n    model.add(layers.Dense(32, activation='relu'))\n    drop_1 = hp.Float('drop_1', 0.0, 0.5, step=0.1)\n    model.add(layers.Dropout(drop_1))\n\n    # Output layer for binary classification\n    model.add(layers.Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n\n    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=LOSS_FUNC,\n        metrics=METRICS\n    )\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Hyperband tuner\ntuner = kt.Hyperband(\n    build_model_hyperband,\n    objective='val_auc',\n    max_epochs=20,\n    factor=3,\n    directory='movie_review_hyperband',\n    project_name='movie_review_tuning'\n)\n\n# Run Hyperband search\ntuner.search(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=20,\n    batch_size=batch_size,\n    class_weight=CLASS_WEIGHTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters and build best model\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters:\")\nprint(f\"  Dropout Layer 1: {best_hp.get('drop_0')}\")\nprint(f\"  Dropout Layer 2: {best_hp.get('drop_1')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr')}\")\n\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the optimized model\nhistory = opt_model.fit(X_train, y_train, class_weight=CLASS_WEIGHTS, batch_size=batch_size, epochs=EPOCHS, validation_data=(X_val, y_val), verbose=0)\nval_score = opt_model.evaluate(X_val, y_val, verbose=0)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training history\nplot_training_history(history, monitors=['loss', 'auc'])"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Results Summary\n\nThe optimized model trained with Hyperband hyperparameters has been evaluated on both validation and test sets. See the Key Takeaways section below for lessons learned.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Appendix: Helper Functions\n\nReusable functions for binary classification model building and training."
  },
  {
   "cell_type": "code",
   "source": "def build_binary_classification_model(input_dimension, hidden_layers=None, \n                                       dropout=None, learning_rate=0.001,\n                                       optimizer='rmsprop', loss='binary_crossentropy',\n                                       metrics=['accuracy'], name=None):\n    \"\"\"\n    Build a binary classification neural network model.\n    \n    Parameters:\n    -----------\n    input_dimension : int\n        Number of input features\n    hidden_layers : list of int, optional\n        List of neurons per hidden layer (e.g., [64, 32] for 2 hidden layers)\n    dropout : float, optional\n        Dropout rate to apply after each hidden layer (0.0 to 1.0)\n    learning_rate : float\n        Learning rate for the optimizer\n    optimizer : str\n        Optimizer name ('rmsprop', 'adam', etc.)\n    loss : str\n        Loss function (default: 'binary_crossentropy')\n    metrics : list\n        Metrics to track during training\n    name : str, optional\n        Model name\n    \n    Returns:\n    --------\n    keras.Sequential : Compiled model\n    \"\"\"\n    from keras.models import Sequential\n    from keras.layers import Dense, Dropout\n    from tensorflow.keras.optimizers import RMSprop, Adam\n    \n    model = Sequential(name=name)\n    \n    # Add hidden layers if specified\n    if hidden_layers:\n        for i, neurons in enumerate(hidden_layers):\n            if i == 0:\n                model.add(Dense(neurons, activation='relu', input_shape=(input_dimension,)))\n            else:\n                model.add(Dense(neurons, activation='relu'))\n            if dropout and dropout > 0:\n                model.add(Dropout(dropout))\n    \n    # Output layer for binary classification\n    if hidden_layers:\n        model.add(Dense(1, activation='sigmoid'))\n    else:\n        model.add(Dense(1, activation='sigmoid', input_shape=(input_dimension,)))\n    \n    # Select optimizer\n    if optimizer == 'rmsprop':\n        opt = RMSprop(learning_rate=learning_rate)\n    elif optimizer == 'adam':\n        opt = Adam(learning_rate=learning_rate)\n    else:\n        opt = optimizer\n    \n    model.compile(optimizer=opt, loss=loss, metrics=metrics)\n    \n    return model",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Print validation results\nprint('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score[0], baseline))\nprint('Precision (Validation): {:.2f}'.format(val_score[1]))\nprint('Recall (Validation): {:.2f}'.format(val_score[2]))\nprint('AUC (Validation): {:.2f}'.format(val_score[3]))\n\npreds = opt_model.predict(X_val, verbose=0)\nprint('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(balanced_accuracy_score(y_val, (preds > 0.5).astype('int32')), balanced_accuracy_baseline))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluate the optimized model on test data\ntest_preds = opt_model.predict(X_test, verbose=0)\n\nprint('Accuracy (Test): {:.2f} (baseline={:.2f})'.format(accuracy_score(y_test, (test_preds > 0.5).astype('int32')), baseline))\nprint('Precision (Test): {:.2f}'.format(precision_score(y_test, (test_preds > 0.5).astype('int32'))))\nprint('Recall (Test): {:.2f}'.format(recall_score(y_test, (test_preds > 0.5).astype('int32'))))\nprint('AUC (Test): {:.2f}'.format(roc_auc_score(y_test, test_preds)))\nprint('Balanced Accuracy (Test): {:.2f} (baseline={:.2f})'.format(balanced_accuracy_score(y_test, (test_preds > 0.5).astype('int32')), balanced_accuracy_baseline))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Key Takeaways\n\n### Binary vs. Multi-Class Classification\n\n| Aspect | Binary (This Notebook) | Multi-Class (Twitter Airline) |\n|--------|------------------------|------------------------------|\n| **Output Layer** | 1 neuron, sigmoid | N neurons, softmax |\n| **Loss Function** | Binary cross-entropy | Categorical cross-entropy |\n| **Labels** | Single 0/1 value | One-hot vectors |\n| **Prediction** | Threshold at 0.5 | argmax of probabilities |\n\n### Key Lessons Learned\n\n1. **Binary Classification is Simpler:** With only two classes, the model architecture and loss function are more straightforward.\n\n2. **Class Weights Optional for Balanced Data:** With ~51:49 class distribution, class weights have minimal impact (weights ~1.0 for both classes).\n\n3. **Same TF-IDF Pipeline Works:** The text preprocessing approach (TF-IDF with bigrams) applies equally well to binary and multi-class NLP.\n\n4. **Hyperband Efficiently Finds Optimal Settings:** The early-stopping approach of Hyperband quickly identifies effective dropout rates and learning rates.\n\n5. **Balanced Accuracy Baseline:** For binary classification, random guessing achieves 50% balanced accuracy (vs. 33% for 3-class problems).",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}