{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Movie%20Review/Movie%20Review%20-%20NLP%20Binary%20Classification%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Movie Review - NLP Binary Classification Example\n",
    "\n",
    "This notebook demonstrates the **Universal ML Workflow** applied to a binary NLP classification problem using movie review sentiment data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Apply the Universal ML Workflow to a **binary** text classification problem\n",
    "- Understand key differences between **binary** and **multi-class** classification\n",
    "- Convert text data to numerical features using **TF-IDF vectorisation**\n",
    "- Build and train deep neural networks for **binary classification**\n",
    "- Use **Hyperband** for efficient hyperparameter tuning\n",
    "- Apply **Dropout + L2 regularisation** to prevent overfitting\n",
    "- Evaluate model performance using appropriate metrics (Accuracy, Precision, Recall, AUC)\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| **Source** | [NLTK Movie Review Dataset](https://www.kaggle.com/datasets/nltkdata/movie-review) |\n",
    "| **Problem Type** | Binary Classification (2 classes) |\n",
    "| **Classes** | Positive, Negative |\n",
    "| **Data Balance** | Nearly Balanced (~51% Positive, ~49% Negative) |\n",
    "| **Data Type** | Unstructured Text (Movie Reviews) |\n",
    "| **Input Features** | TF-IDF Vectors (5000 features, bigrams) |\n",
    "| **Output** | Sentiment: Positive (1) or Negative (0) |\n",
    "\n",
    "---\n",
    "\n",
    "## Technique Scope\n",
    "\n",
    "This notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n",
    "\n",
    "| Technique | Status | Rationale |\n",
    "|-----------|--------|----------|\n",
    "| **Dense layers (MLP/DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n",
    "| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n",
    "| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n",
    "| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n",
    "| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n",
    "| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n",
    "\n",
    "We demonstrate that **Dropout + L2 regularisation** alone can effectively prevent overfitting without requiring early stopping.\n",
    "\n",
    "---\n",
    "\n",
    "## Binary vs. Multi-Class Classification\n",
    "\n",
    "| Aspect | Binary (This Notebook) | Multi-Class (Twitter Examples) |\n",
    "|--------|------------------------|-------------------------------|\n",
    "| **Output neurons** | 1 neuron | N neurons (one per class) |\n",
    "| **Output activation** | Sigmoid (outputs 0-1 probability) | Softmax (outputs N probabilities summing to 1) |\n",
    "| **Loss function** | Binary cross-entropy | Categorical cross-entropy |\n",
    "| **Label format** | Single value: 0 or 1 | One-hot vector: [1,0,0], [0,1,0], etc. |\n",
    "| **Prediction** | Threshold at 0.5 | argmax of probabilities |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Problem and Assembling a Dataset\n",
    "\n",
    "The first step in any machine learning project is to clearly define the problem and understand the data.\n",
    "\n",
    "**Problem Statement:** Given a movie review, predict whether the sentiment is positive or negative.\n",
    "\n",
    "**Why this matters:** Sentiment analysis of movie reviews helps:\n",
    "- Studios understand audience reception\n",
    "- Streaming platforms recommend content\n",
    "- Critics aggregate opinions at scale\n",
    "\n",
    "**Key difference from multi-class sentiment:** This is a simpler problem with only two outcomes. Binary classification is often more robust and easier to interpret than multi-class alternatives.\n",
    "\n",
    "**Data Source:** This dataset contains movie reviews labelled as positive or negative from the NLTK corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing a Measure of Success\n",
    "\n",
    "### Metric Selection Based on Class Imbalance\n",
    "\n",
    "The choice of evaluation metric depends on **class imbalance**. We use practical guidelines derived from the literature:\n",
    "\n",
    "| Imbalance Ratio | Classification | Primary Metric | Rationale |\n",
    "|-----------------|----------------|----------------|----------|\n",
    "| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n",
    "| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n",
    "| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n",
    "\n",
    "**For this dataset:** With ~51:49 class distribution, the imbalance ratio is ~1.04:1 (essentially balanced). We use **Accuracy** as the primary metric.\n",
    "\n",
    "### References\n",
    "\n",
    "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
    "\n",
    "*Note: Even with balanced data, we still track Precision, Recall, and AUC for a complete picture.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deciding on an Evaluation Protocol\n",
    "\n",
    "### Hold-Out vs K-Fold Cross-Validation\n",
    "\n",
    "The choice between hold-out and K-fold depends on **dataset size** and **computational cost**:\n",
    "\n",
    "| Dataset Size | Recommended Method | Rationale |\n",
    "|--------------|-------------------|----------|\n",
    "| < 1,000 | K-Fold (K=5 or 10) | High variance with small hold-out sets |\n",
    "| 1,000 – 10,000 | K-Fold or Hold-Out | Either works; K-fold more robust |\n",
    "| > 10,000 | Hold-Out | Sufficient data; K-fold computationally expensive |\n",
    "| Deep Learning | Hold-Out (preferred) | Training cost prohibitive for K iterations |\n",
    "\n",
    "### References\n",
    "\n",
    "- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n",
    "\n",
    "- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n",
    "\n",
    "### Data Split Strategy (This Notebook)\n",
    "\n",
    "```\n",
    "Original Data (~65,000 samples) → Hold-Out Selected\n",
    "├── Test Set (10%) - Final evaluation only\n",
    "└── Training Pool (90%)\n",
    "    ├── Training Set (81%) - Model training\n",
    "    └── Validation Set (9%) - Hyperparameter tuning\n",
    "```\n",
    "\n",
    "**Important:** We use `stratify` parameter to maintain class proportions in all splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Your Data\n",
    "\n",
    "### 4.1 Import Libraries and Set Random Seed\n",
    "\n",
    "We set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Keras Tuner for hyperparameter search\n",
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 204\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load and Explore the Dataset\n",
    "\n",
    "Let's download the movie review data from Google Drive and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data directly from Google Drive\n",
    "GDRIVE_FILE_ID = '17zquac-Q4viIEs1hSwHmlBIFYXncn9IB'\n",
    "DATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\n",
    "\n",
    "reviews = pd.read_csv(DATA_URL)\n",
    "reviews = reviews[['text', 'tag']]\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Split Data into Train and Test Sets\n",
    "\n",
    "We reserve 10% of the data for final testing. The `stratify` parameter ensures that each split maintains the same class proportions as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "\n",
    "(text_train, text_test, \n",
    " tag_train, tag_test) = train_test_split(reviews['text'], reviews['tag'], \n",
    "                                         test_size=TEST_SIZE, stratify=reviews['tag'],\n",
    "                                         shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Text Vectorisation with TF-IDF\n",
    "\n",
    "Neural networks require numerical input, but reviews are text. We use **TF-IDF (Term Frequency-Inverse Document Frequency)** to convert text to numbers.\n",
    "\n",
    "**Our settings:**\n",
    "- `max_features=5000`: Keep only the 5000 most important terms\n",
    "- `ngram_range=(1, 2)`: Include both single words and word pairs (bigrams)\n",
    "\n",
    "These settings are consistent with our other NLP notebooks, demonstrating that the same preprocessing pipeline works across different text classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "NGRAMS = 2\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, NGRAMS), max_features=MAX_FEATURES)\n",
    "tfidf.fit(text_train)\n",
    "\n",
    "X_train, X_test = tfidf.transform(text_train).toarray(), tfidf.transform(text_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Encode Labels\n",
    "\n",
    "For **binary classification**, we encode labels as single values (0 or 1), not one-hot vectors:\n",
    "- Negative → 0\n",
    "- Positive → 1\n",
    "\n",
    "This is simpler than multi-class encoding and works with sigmoid output activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(reviews['tag'])\n",
    "\n",
    "# For binary classification: single 0/1 labels (not one-hot)\n",
    "y_train = label_encoder.transform(tag_train)\n",
    "y_test = label_encoder.transform(tag_test)\n",
    "\n",
    "print(f\"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Developing a Model That Does Better Than a Baseline\n",
    "\n",
    "Before building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n",
    "\n",
    "### 5.1 Examine Class Distribution\n",
    "\n",
    "Let's look at how the sentiment classes are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = reviews.groupby(['tag']).count()\n",
    "counts.reset_index(inplace=True)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset size analysis (for hold-out vs K-fold decision)\n",
    "n_samples = len(reviews)\n",
    "HOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000 (Kohavi, 1995; Chollet, 2021)\n",
    "\n",
    "# Imbalance analysis (for metric selection)\n",
    "majority_class = counts['text'].max()\n",
    "minority_class = counts['text'].min()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "IMBALANCE_THRESHOLD = 3.0  # Use F1-Score if ratio > 3.0 (He & Garcia, 2009)\n",
    "\n",
    "# Determine evaluation strategy and metric\n",
    "use_holdout = n_samples > HOLDOUT_THRESHOLD\n",
    "use_f1 = imbalance_ratio > IMBALANCE_THRESHOLD\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA-DRIVEN CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\n",
    "print(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples (Kohavi, 1995)\")\n",
    "print(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n",
    "\n",
    "print(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\n",
    "print(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1 (He & Garcia, 2009)\")\n",
    "print(f\"   Decision: {'F1-Score (imbalanced)' if use_f1 else 'Accuracy (balanced)'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "PRIMARY_METRIC = 'f1' if use_f1 else 'accuracy'\n",
    "print(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate Baseline Metrics\n",
    "\n",
    "**Naive Baseline (Majority Class):** If we always predict the most common class (positive), we get ~51% accuracy. This is our accuracy baseline.\n",
    "\n",
    "**Balanced Accuracy Baseline:** For binary classification, a random classifier achieves 50% balanced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the majority class\n",
    "majority_class_name = counts.loc[counts['text'].idxmax(), 'tag']\n",
    "baseline = counts['text'].max() / counts['text'].sum()\n",
    "\n",
    "# Balanced accuracy baseline (random classifier = 50% for binary)\n",
    "balanced_accuracy_baseline = 0.5\n",
    "\n",
    "print(f\"Majority class: {majority_class_name}\")\n",
    "print(f\"Baseline accuracy (majority class): {baseline:.2f}\")\n",
    "print(f\"Balanced accuracy baseline (random): {balanced_accuracy_baseline:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create Validation Set\n",
    "\n",
    "We split off a portion of the training data for validation. This will be used to:\n",
    "- Evaluate model performance during hyperparameter tuning\n",
    "- Compare models without touching the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=VALIDATION_SIZE, stratify=y_train,\n",
    "                                                  shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Configure Training Parameters\n",
    "\n",
    "**Key training settings for binary classification:**\n",
    "- **Optimiser:** Adam - adaptive learning rate optimiser\n",
    "- **Loss:** Binary cross-entropy - standard loss for binary classification\n",
    "- **Output:** 1 neuron with sigmoid activation (outputs probability 0-1)\n",
    "- **Prediction:** Apply threshold at 0.5 to convert probability to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = X_train.shape[1]\n",
    "OUTPUT_DIMENSION = 1  # Binary classification: single output neuron\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS_FUNC = 'binary_crossentropy'  # Binary classification loss\n",
    "\n",
    "# Training metrics\n",
    "METRICS = ['accuracy', \n",
    "           tf.keras.metrics.Precision(name='precision'), \n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Layer Perceptron (no hidden layers)\n",
    "# For binary: 1 output neuron with sigmoid activation\n",
    "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
    "slp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "slp_model.add(Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n",
    "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "slp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# We use DIFFERENT epoch counts for different training phases:\n",
    "#\n",
    "# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n",
    "# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2\n",
    "#\n",
    "# Regularisation slows learning, so regularised models need more epochs.\n",
    "\n",
    "EPOCHS_BASELINE = 100      # SLP and DNN (no regularisation)\n",
    "EPOCHS_REGULARIZED = 150   # DNN with Dropout + L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Handle Class Imbalance with Class Weights\n",
    "\n",
    "Even though the data is nearly balanced, we still compute class weights for consistency with our other notebooks. With ~51:49 distribution, the weights will be close to 1.0 for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "CLASS_WEIGHTS = dict(enumerate(weights))\n",
    "\n",
    "print(f\"Class weights: {CLASS_WEIGHTS}\")\n",
    "print(\"(Values close to 1.0 indicate balanced classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Single-Layer Perceptron\n",
    "history_slp = slp_model.fit(X_train, y_train, \n",
    "                            class_weight=CLASS_WEIGHTS,\n",
    "                            batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "                            validation_data=(X_val, y_val),\n",
    "                            verbose=0)\n",
    "val_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SLP validation metrics\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_slp[0], baseline))\n",
    "print('Precision (Validation): {:.2f}'.format(val_score_slp[1]))\n",
    "print('Recall (Validation): {:.2f}'.format(val_score_slp[2]))\n",
    "print('AUC (Validation): {:.2f}'.format(val_score_slp[3]))\n",
    "\n",
    "# For binary: threshold predictions at 0.5\n",
    "preds_slp_val = (slp_model.predict(X_val, verbose=0) > 0.5).astype('int32').flatten()\n",
    "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
    "    balanced_accuracy_score(y_val, preds_slp_val), balanced_accuracy_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics over epochs.\n",
    "    Plots: (1) Loss, (2) Accuracy\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    title_suffix = f' ({title})' if title else ''\n",
    "\n",
    "    # Plot 1: Loss\n",
    "    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[0].set_title(f'Loss{title_suffix}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Accuracy\n",
    "    axs[1].plot(epochs, history.history['accuracy'], 'b-', label='Training', linewidth=1.5)\n",
    "    axs[1].plot(epochs, history.history['val_accuracy'], 'r-', label='Validation', linewidth=1.5)\n",
    "    axs[1].set_title(f'Accuracy{title_suffix}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SLP training history\n",
    "plot_training_history(history_slp, title='SLP Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Scaling Up: Developing a Model That Overfits\n\nThe next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n\n**Strategy:** Add a hidden layer with 64 neurons to increase model capacity.\n\n**No regularisation applied:** We intentionally train without regularisation to observe overfitting behaviour.\n\n---\n\n### Architecture Design Decisions\n\n**Why 64 neurons in the hidden layer?**\n\nThis is a practical starting point that balances capacity and efficiency:\n- **Too few (e.g., 16):** May not have enough capacity to learn complex sentiment patterns\n- **Too many (e.g., 512):** Increases overfitting risk and training time without proportional benefit\n- **64 neurons:** A common choice that provides sufficient capacity for most text classification tasks\n\n**Why only 1 hidden layer instead of 2-3?**\n\nPer the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n\n1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation\n\n*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*\n\n### 6.1 Build a Deep Neural Network (DNN)\n\nLet's add a hidden layer with 64 neurons and ReLU activation:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network (1 hidden layer, no regularisation for overfitting demo)\n",
    "# For binary: 1 output neuron with sigmoid\n",
    "dnn_model = Sequential(name='Deep_Neural_Network')\n",
    "dnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n",
    "dnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Deep Neural Network (without regularisation)\n",
    "history_dnn = dnn_model.fit(X_train, y_train, \n",
    "                            class_weight=CLASS_WEIGHTS,\n",
    "                            batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE, \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            verbose=0)\n",
    "val_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DNN training history (expect overfitting: val_loss increasing)\n",
    "plot_training_history(history_dnn, title='DNN - No Regularisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DNN validation metrics\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_dnn[0], baseline))\n",
    "print('Precision (Validation): {:.2f}'.format(val_score_dnn[1]))\n",
    "print('Recall (Validation): {:.2f}'.format(val_score_dnn[2]))\n",
    "print('AUC (Validation): {:.2f}'.format(val_score_dnn[3]))\n",
    "\n",
    "preds_dnn_val = (dnn_model.predict(X_val, verbose=0) > 0.5).astype('int32').flatten()\n",
    "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
    "    balanced_accuracy_score(y_val, preds_dnn_val), balanced_accuracy_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularising Your Model and Tuning Hyperparameters\n",
    "\n",
    "Now we address the overfitting observed in Section 6 by adding **regularisation**. We use two complementary techniques:\n",
    "\n",
    "| Technique | How it works | Effect |\n",
    "|-----------|--------------|--------|\n",
    "| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation |\n",
    "| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries |\n",
    "\n",
    "Using **Hyperband** for efficient hyperparameter tuning.\n",
    "\n",
    "### 7.1 Hyperband Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperband Model Builder for Binary Classification\n",
    "def build_model_hyperband(hp):\n",
    "    \"\"\"\n",
    "    Build Movie Review model with FIXED architecture (1 hidden layer, 64 neurons).\n",
    "    Same architecture as Section 6 DNN - only tunes regularisation and learning rate.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
    "\n",
    "    # L2 regularisation strength\n",
    "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
    "\n",
    "    # Fixed architecture: 1 hidden layer with 64 neurons\n",
    "    model.add(layers.Dense(64, activation='relu', \n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer for binary classification\n",
    "    model.add(layers.Dense(OUTPUT_DIMENSION, activation='sigmoid'))\n",
    "\n",
    "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hyperband tuner\n",
    "# For balanced binary classification, we can use val_accuracy or val_auc\n",
    "TUNING_OBJECTIVE = 'val_accuracy' if PRIMARY_METRIC == 'accuracy' else 'val_auc'\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_hyperband,\n",
    "    objective=TUNING_OBJECTIVE,\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='movie_review_hyperband',\n",
    "    project_name='movie_review_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"Tuning objective: {TUNING_OBJECTIVE}\")\n",
    "\n",
    "# Run Hyperband search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=CLASS_WEIGHTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best hyperparameters\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best hyperparameters found by Hyperband:\")\nprint(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\nprint(f\"  Dropout Rate: {best_hp.get('dropout')}\")\nprint(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")\n\n# =============================================================================\n# CRITICAL: Extract the number of epochs from the best trial\n# =============================================================================\nbest_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\nbest_epochs = best_trial.best_step + 1  # best_step is 0-indexed\n\nprint(f\"\\n>>> Best trial was trained for {best_epochs} epochs <<<\")\nprint(f\"    (This is the epoch count we'll use for retraining)\")\n\n# Build a fresh model with the best hyperparameters\nopt_model = tuner.hypermodel.build(best_hp)\nopt_model.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.2 Retraining with Matched Epochs\n\nAfter extracting the best hyperparameters, we retrain the model using the **exact same number of epochs** that Hyperband used for the best trial.\n\n---\n\n#### The Epoch Mismatch Problem\n\nHyperband uses **successive halving** - most configurations train for few epochs, only top performers get more:\n\n```\nHyperband with max_epochs=20, factor=3:\nRound 1: 81 configs × ~1 epoch  → Keep top 27\nRound 2: 27 configs × ~2 epochs → Keep top 9\nRound 3:  9 configs × ~7 epochs → Keep top 3\nRound 4:  3 configs × ~20 epochs → Select best\n```\n\nThe best hyperparameters were found optimal at a **specific epoch count** (e.g., 7 epochs). If we rebuild and retrain for a different number of epochs (e.g., 150), the hyperparameters may no longer be optimal.\n\n---\n\n#### Solution: Match the Epoch Count\n\nWe extract `best_step` from the best trial (0-indexed epoch where best validation score occurred) and retrain for exactly `best_step + 1` epochs:\n\n| Approach | Epochs Match? | Issue |\n|----------|---------------|-------|\n| ~~Rebuild + retrain for 150 epochs~~ | No | Hyperparameters may be suboptimal at 150 epochs |\n| ~~Use get_best_models() directly~~ | Yes | No training history available for plotting |\n| **Rebuild + retrain for best_epochs** | Yes | Best of both: matched epochs + training history |\n\n> *\"Use the epoch count that Hyperband determined was optimal for these hyperparameters.\"*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RETRAIN WITH MATCHED EPOCHS\n# =============================================================================\nprint(f\"Retraining with best hyperparameters for {best_epochs} epochs...\")\nprint(f\"(Matching the epoch count from Hyperband's best trial)\")\n\nhistory_opt = opt_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=best_epochs,  # CRITICAL: Use matched epochs!\n    batch_size=BATCH_SIZE,\n    verbose=0\n)\n\nval_score_opt = opt_model.evaluate(X_val, y_val, verbose=0)[1:]\nprint(f\"\\nValidation Accuracy: {val_score_opt[0]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training history for the optimised model\n# Now available because we retrained with matched epochs!\nplot_training_history(history_opt, title=f'DNN - Dropout + L2 ({best_epochs} epochs)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_opt_val = (opt_model.predict(X_val, verbose=0) > 0.5).astype('int32').flatten()\n",
    "\n",
    "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_opt[0], baseline))\n",
    "print('Precision (Validation): {:.2f}'.format(val_score_opt[1]))\n",
    "print('Recall (Validation): {:.2f}'.format(val_score_opt[2]))\n",
    "print('AUC (Validation): {:.2f}'.format(val_score_opt[3]))\n",
    "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
    "    balanced_accuracy_score(y_val, preds_opt_val), balanced_accuracy_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Final Model Evaluation on Test Set\n",
    "\n",
    "Now we evaluate our best model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "test_score = opt_model.evaluate(X_test, y_test, verbose=0)[1:]\n",
    "preds_test = (opt_model.predict(X_test, verbose=0) > 0.5).astype('int32').flatten()\n",
    "\n",
    "print('=' * 50)\n",
    "print('FINAL TEST SET RESULTS')\n",
    "print('=' * 50)\n",
    "print(f'Accuracy (Test): {test_score[0]:.4f} (baseline={baseline:.4f})  ← Primary Metric')\n",
    "print(f'Precision (Test): {test_score[1]:.4f}')\n",
    "print(f'Recall (Test): {test_score[2]:.4f}')\n",
    "print(f'AUC (Test): {test_score[3]:.4f}')\n",
    "print(f'Balanced Accuracy (Test): {balanced_accuracy_score(y_test, preds_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for test predictions\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, preds_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Test Set Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print per-class metrics\n",
    "print(\"\\nPer-Class Recall:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_mask = y_test == i\n",
    "    class_recall = (preds_test[class_mask] == i).mean()\n",
    "    print(f\"  {class_name}: {class_recall:.2%} ({class_mask.sum()} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Results Summary\n",
    "\n",
    "The following table compares all models trained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'DNN (No Regularisation)', 'DNN (Dropout + L2)', 'DNN (Dropout + L2) - Test'],\n",
    "    'Accuracy': [baseline, val_score_slp[0], val_score_dnn[0], val_score_opt[0], test_score[0]],\n",
    "    'AUC': [0.5, val_score_slp[3], val_score_dnn[3], val_score_opt[3], test_score[3]],\n",
    "    'Dataset': ['N/A', 'Validation', 'Validation', 'Validation', 'Test']\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON - RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Primary Metric: ACCURACY (imbalance ratio: {imbalance_ratio:.2f}:1 - balanced)\")\n",
    "print(\"=\" * 70)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"  - All models significantly outperform naive baseline ({baseline:.2%})\")\n",
    "print(f\"  - Final test accuracy: {test_score[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### Decision Framework Summary\n",
    "\n",
    "| Decision | Threshold | This Dataset | Choice | Reference |\n",
    "|----------|-----------|--------------|--------|----------|\n",
    "| **Hold-Out vs K-Fold** | > 10,000 samples | ~65,000 samples | Hold-Out | Kohavi (1995); Chollet (2021) |\n",
    "| **Accuracy vs F1-Score** | > 3:1 imbalance | ~1.04:1 ratio | Accuracy | He and Garcia (2009) |\n",
    "\n",
    "### Binary vs. Multi-Class Classification\n",
    "\n",
    "| Aspect | Binary (This Notebook) | Multi-Class (Twitter Examples) |\n",
    "|--------|------------------------|-------------------------------|\n",
    "| **Output neurons** | 1 neuron | N neurons |\n",
    "| **Output activation** | Sigmoid | Softmax |\n",
    "| **Loss function** | Binary cross-entropy | Categorical cross-entropy |\n",
    "| **Label format** | Single 0/1 value | One-hot vector |\n",
    "| **Prediction** | Threshold at 0.5 | argmax |\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "1. **Binary Classification is Simpler:** With only two classes, the model architecture and loss function are more straightforward than multi-class.\n",
    "\n",
    "2. **Class Weights Optional for Balanced Data:** With ~51:49 class distribution, class weights have minimal impact (weights ≈ 1.0 for both classes).\n",
    "\n",
    "3. **Same TF-IDF Pipeline Works:** The text preprocessing approach (TF-IDF with bigrams) applies equally well to binary and multi-class NLP problems.\n",
    "\n",
    "4. **Accuracy is Appropriate for Balanced Data:** With imbalance ratio < 3:1, accuracy is a meaningful primary metric.\n",
    "\n",
    "5. **Regularisation Prevents Overfitting:** Combining Dropout + L2 regularisation controls overfitting effectively.\n",
    "\n",
    "6. **Code Reusability:** The core workflow and code patterns are nearly identical to the multi-class Twitter notebooks—only the output layer, loss function, and label encoding differ.\n",
    "\n",
    "### References\n",
    "\n",
    "- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n",
    "\n",
    "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
    "\n",
    "- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Appendix: Modular Helper Functions\n\nFor cleaner code organisation, you can wrap the model building and training patterns into reusable functions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# MODULAR HELPER FUNCTIONS\n# =============================================================================\n\ndef build_binary_nlp_classifier(input_dim, hidden_units=None, dropout=0.0, l2_reg=0.0,\n                                 optimizer='adam', learning_rate=None, name=None):\n    \"\"\"\n    Build a binary NLP classification neural network.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features (TF-IDF vector dimension)\n    hidden_units : list of int, optional\n        Neurons per hidden layer, e.g., [64] or [128, 64]\n    dropout : float\n        Dropout rate (0.0 to 0.5)\n    l2_reg : float\n        L2 regularisation strength\n    learning_rate : float, optional\n        Custom learning rate\n    name : str, optional\n        Model name\n        \n    Returns:\n    --------\n    keras.Sequential : Compiled model ready for training\n    \"\"\"\n    model = Sequential(name=name)\n    model.add(layers.Input(shape=(input_dim,)))\n    \n    hidden_units = hidden_units or []\n    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n    \n    for units in hidden_units:\n        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n        if dropout > 0:\n            model.add(Dropout(dropout))\n    \n    # Binary output\n    model.add(Dense(1, activation='sigmoid'))\n    \n    if learning_rate is not None:\n        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n    else:\n        opt = optimizer\n    \n    metrics = ['accuracy', \n               tf.keras.metrics.Precision(name='precision'),\n               tf.keras.metrics.Recall(name='recall'),\n               tf.keras.metrics.AUC(name='auc')]\n    \n    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=metrics)\n    return model\n\n\ndef train_with_class_weights(model, X_train, y_train, X_val, y_val,\n                              batch_size=512, epochs=100, verbose=0):\n    \"\"\"Train model with automatic class weight computation.\"\"\"\n    weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n    class_weights = dict(enumerate(weights))\n    \n    return model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        batch_size=batch_size, \n        epochs=epochs,\n        class_weight=class_weights,\n        verbose=verbose\n    )\n\n\ndef evaluate_binary_nlp(model, X, y_true, threshold=0.5):\n    \"\"\"\n    Evaluate binary NLP classification model.\n    \n    Returns:\n    --------\n    dict : Dictionary containing all binary classification metrics\n    \"\"\"\n    y_pred_proba = model.predict(X, verbose=0).flatten()\n    y_pred = (y_pred_proba > threshold).astype('int32')\n    \n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred),\n        'recall': recall_score(y_true, y_pred),\n        'f1': f1_score(y_true, y_pred),\n        'auc': roc_auc_score(y_true, y_pred_proba),\n        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n    }\n    \n    return metrics\n\n\n# =============================================================================\n# USAGE EXAMPLES\n# =============================================================================\n# \n# # Build models\n# slp = build_binary_nlp_classifier(INPUT_DIMENSION, name='SLP')\n# mlp = build_binary_nlp_classifier(INPUT_DIMENSION, hidden_units=[64], name='MLP')\n# mlp_reg = build_binary_nlp_classifier(INPUT_DIMENSION, hidden_units=[64], \n#                                       dropout=0.3, l2_reg=0.001, learning_rate=0.001)\n# \n# # Train with class weights\n# history = train_with_class_weights(mlp, X_train, y_train, X_val, y_val)\n# \n# # Evaluate\n# metrics = evaluate_binary_nlp(mlp, X_val, y_val)\n# print(f\"Accuracy: {metrics['accuracy']:.4f}, AUC: {metrics['auc']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}