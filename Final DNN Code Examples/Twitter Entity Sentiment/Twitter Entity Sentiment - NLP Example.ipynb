{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfjlMRcvqjsY"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Final%20DNN%20Code%20Examples/Twitter%20Entity%20Sentiment/Twitter%20Entity%20Sentiment%20-%20NLP%20Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Twitter Entity Sentiment - NLP Example\n",
        "\n",
        "This notebook demonstrates the **Universal ML Workflow** applied to a multi-class NLP classification problem using Twitter entity sentiment data.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "- Apply the Universal ML Workflow to an NLP text classification problem\n",
        "- Convert text data to numerical features using **TF-IDF (Term Frequency-Inverse Document Frequency)** vectorisation\n",
        "- Handle **imbalanced classes** using class weights during training\n",
        "- Build and train deep neural networks for **multi-class classification** (4 classes)\n",
        "- Use **Hyperband** for efficient hyperparameter tuning\n",
        "- Apply **Dropout + L2 regularisation** to prevent overfitting\n",
        "- Evaluate model performance using appropriate metrics for imbalanced data (**F1-Score**, Accuracy, Precision, Recall)\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "| Attribute | Description |\n",
        "|-----------|-------------|\n",
        "| **Source** | [Kaggle Twitter Entity Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis) |\n",
        "| **Problem Type** | Multi-Class Classification (4 classes) |\n",
        "| **Classes** | Irrelevant, Negative, Neutral, Positive |\n",
        "| **Data Balance** | Imbalanced |\n",
        "| **Data Type** | Unstructured Text (Tweets) |\n",
        "| **Input Features** | TF-IDF Vectors (5000 features, bigrams) |\n",
        "| **Output** | Sentiment: Irrelevant, Negative, Neutral, or Positive |\n",
        "| **Imbalance Handling** | Class Weights during Training |\n",
        "\n",
        "---\n",
        "\n",
        "## Technique Scope\n",
        "\n",
        "This notebook uses only techniques from **Chapters 1–4** of *Deep Learning with Python* (Chollet, 2021). This means:\n",
        "\n",
        "| Technique | Status | Rationale |\n",
        "|-----------|--------|----------|\n",
        "| **Dense layers (MLP/DNN)** | ✓ Used | Core building block (Ch. 3-4) |\n",
        "| **Dropout** | ✓ Used | Regularisation technique (Ch. 4) |\n",
        "| **L2 regularisation** | ✓ Used | Weight penalty (Ch. 4) |\n",
        "| **Early stopping** | ✗ Not used | Introduced in Ch. 7 |\n",
        "| **CNN** | ✗ Not used | Introduced in Ch. 8 |\n",
        "| **RNN/LSTM** | ✗ Not used | Introduced in Ch. 10 |\n",
        "\n",
        "We demonstrate that **Dropout + L2 regularisation** alone can effectively prevent overfitting without requiring early stopping.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpDQ1hdDqjsZ"
      },
      "source": [
        "## 1. Defining the Problem and Assembling a Dataset\n",
        "\n",
        "The first step in any machine learning project is to clearly define the problem and understand the data.\n",
        "\n",
        "**Problem Statement:** Given a tweet mentioning an entity (brand, product, person), predict the sentiment expressed (Irrelevant, Negative, Neutral, or Positive).\n",
        "\n",
        "**Why this matters:** Entity-level sentiment analysis helps:\n",
        "- Brands monitor public perception of their products\n",
        "- Marketers understand customer reactions to campaigns\n",
        "- Analysts track sentiment trends over time\n",
        "\n",
        "**Key difference from 3-class sentiment:** This dataset includes an **Irrelevant** class for tweets that mention the entity but don't express sentiment about it. This is common in real-world applications where not all mentions are relevant.\n",
        "\n",
        "**Data Source:** This dataset contains tweets with entity-level sentiment annotations from Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEE74l7IqjsZ"
      },
      "source": [
        "## 2. Choosing a Measure of Success\n",
        "\n",
        "### Metric Selection Based on Class Imbalance\n",
        "\n",
        "The choice of evaluation metric depends on **class imbalance**. We use practical guidelines derived from the literature:\n",
        "\n",
        "| Imbalance Ratio | Classification | Primary Metric | Rationale |\n",
        "|-----------------|----------------|----------------|----------|\n",
        "| ≤ 1.5:1 | Balanced | **Accuracy** | Classes roughly equal |\n",
        "| 1.5:1 – 3:1 | Mild Imbalance | **Accuracy** | Majority class < 75% |\n",
        "| > 3:1 | Moderate/Severe | **F1-Score** | Accuracy becomes misleading |\n",
        "\n",
        "**Why these thresholds?**\n",
        "- **3:1 ratio**: When majority class exceeds 75%, a naive classifier achieves high accuracy while ignoring minority classes\n",
        "- **F1-Score**: Harmonic mean of precision and recall, effective for imbalanced data (He and Garcia, 2009)\n",
        "\n",
        "### References\n",
        "\n",
        "- Branco, P., Torgo, L. and Ribeiro, R.P. (2016) 'A survey of predictive modelling on imbalanced domains', *ACM Computing Surveys*, 49(2), pp. 1–50.\n",
        "\n",
        "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
        "\n",
        "*Note: The 3:1 threshold is a practical guideline, not a strict academic standard. The literature suggests metric choice depends on domain-specific costs of errors.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scz_X_gEqjsZ"
      },
      "source": [
        "## 3. Deciding on an Evaluation Protocol\n",
        "\n",
        "### Hold-Out vs K-Fold Cross-Validation\n",
        "\n",
        "The choice between hold-out validation and k-fold cross-validation is a trade-off between estimate stability and computational cost. For model selection, k-fold cross-validation is widely used because it averages performance over multiple splits; Kohavi (1995) reports stratified 10-fold cross-validation as a strong general default for model selection. However, k-fold cross-validation requires training the model k times, which can be computationally expensive, especially for larger datasets and heavier models.\n",
        "\n",
        "| Situation        | Recommended method                                   | Rationale |\n",
        "|-----------------|--------------------------------------------------------|-----------|\n",
        "| Smaller datasets | Stratified k-fold cross-validation (commonly 5 or 10 folds) | A single hold-out split may be unstable when the validation set is small; averaging across folds typically provides a more reliable model-selection signal. |\n",
        "| Larger datasets  | Hold-out validation + separate test set                | With sufficient data, a single validation split is often adequate while avoiding the k× training cost of k-fold; a held-out test set supports final unbiased reporting. |\n",
        "\n",
        "*Note:* This table summarises a rule-of-thumb stability–cost trade-off rather than fixed numeric cut-offs.\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n",
        "\n",
        "- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n",
        "\n",
        "- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *Proceedings of the 14th International Joint Conference on Artificial Intelligence*, 2, pp. 1137–1145.\n",
        "\n",
        "- Pedregosa, F. et al. (2011) 'Scikit-learn: machine learning in Python', *Journal of Machine Learning Research*, 12, pp. 2825–2830. Available at: https://scikit-learn.org/stable/modules/cross_validation.html (Accessed: 20 January 2025).\n",
        "\n",
        "*Note: The 10,000 threshold is a practical guideline. For computationally cheap models, K-fold is preferred regardless of size.*\n",
        "\n",
        "### Data Split Strategy (This Notebook)\n",
        "\n",
        "```\n",
        "Original Data → Hold-Out Selected\n",
        "├── Test Set (10%) - Final evaluation only\n",
        "└── Training Pool (90%)\n",
        "    ├── Training Set (81%) - Model training\n",
        "    └── Validation Set (9%) - Hyperparameter tuning\n",
        "```\n",
        "\n",
        "**Important:** We use `stratify` parameter to maintain class proportions in all splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvBLt-xLqjsZ"
      },
      "source": [
        "## 4. Preparing Your Data\n",
        "\n",
        "### 4.1 Import Libraries and Set Random Seed\n",
        "\n",
        "We set random seeds for reproducibility - this ensures that running the notebook multiple times produces the same results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFw0WaZwqjsZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Keras Tuner for hyperparameter search\n",
        "%pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 204\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1vXgRJXqjsZ"
      },
      "source": [
        "### 4.2 Load and Explore the Dataset\n",
        "\n",
        "Let's download the Twitter entity sentiment data from Google Drive and examine its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SW3iDa3qjsZ"
      },
      "outputs": [],
      "source": [
        "# Load data directly from Google Drive\n",
        "GDRIVE_FILE_ID = '14RgPMuGdicl0lg10L7T8tFEnYs9PXudK'\n",
        "DATA_URL = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}&export=download'\n",
        "\n",
        "# This dataset has no header row\n",
        "tweets = pd.read_csv(DATA_URL, header=None)\n",
        "\n",
        "# Columns: 0=tweet_id, 1=entity, 2=sentiment, 3=text\n",
        "# We only need sentiment and text\n",
        "tweets = tweets[[2, 3]]\n",
        "tweets.columns = ['sentiment', 'text']\n",
        "\n",
        "# Remove any rows with missing values\n",
        "tweets.dropna(inplace=True)\n",
        "\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1r2YoLbqjsZ"
      },
      "source": [
        "### 4.3 Split Data into Train and Test Sets\n",
        "\n",
        "We reserve 10% of the data for final testing. The `stratify` parameter ensures that each split maintains the same class proportions as the original dataset - critical for imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T6VTUTIqjsa"
      },
      "outputs": [],
      "source": [
        "TEST_SIZE = 0.1\n",
        "\n",
        "(tweets_train, tweets_test,\n",
        " sentiment_train, sentiment_test) = train_test_split(tweets['text'], tweets['sentiment'],\n",
        "                                                     test_size=TEST_SIZE, stratify=tweets['sentiment'],\n",
        "                                                     shuffle=True, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB7sJt-Vqjsa"
      },
      "source": [
        "### 4.4 Text Vectorisation with TF-IDF\n",
        "\n",
        "Neural networks require numerical input, but tweets are text. We use **TF-IDF (Term Frequency-Inverse Document Frequency)** to convert text to numbers.\n",
        "\n",
        "**How TF-IDF works:**\n",
        "- **TF (Term Frequency):** How often a word appears in a document\n",
        "- **IDF (Inverse Document Frequency):** Downweights words that appear in many documents (like \"the\", \"is\")\n",
        "- **TF-IDF = TF × IDF:** Words that are frequent in a document but rare overall get high scores\n",
        "\n",
        "**Our settings:**\n",
        "- `max_features=5000`: Keep only the 5000 most important terms\n",
        "- `ngram_range=(1, 2)`: Include both single words (unigrams) and word pairs (bigrams) like \"great service\"\n",
        "\n",
        "---\n",
        "\n",
        "#### Design Decisions\n",
        "\n",
        "**Why TF-IDF instead of Word Embeddings (Word2Vec, GloVe)?**\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| **TF-IDF** | Simple, interpretable, no pretrained models needed | Loses word order, no semantic similarity |\n",
        "| **Word Embeddings** | Captures semantic meaning, dense vectors | Requires pretrained models or training, more complex |\n",
        "\n",
        "We use TF-IDF because:\n",
        "1. **Technique scope:** Word embeddings are introduced in Chapter 11 of *Deep Learning with Python* (Chollet, 2021), outside our Ch. 1-4 scope\n",
        "2. **Simplicity:** TF-IDF works well for sentiment classification where specific words (e.g., \"terrible\", \"amazing\") are strong predictors\n",
        "3. **Interpretability:** Feature weights directly correspond to terms\n",
        "\n",
        "**Why 5000 features?**\n",
        "\n",
        "This balances vocabulary coverage against dimensionality:\n",
        "- Too few (e.g., 1000): May miss important domain-specific terms\n",
        "- Too many (e.g., 20000): Increases noise and computational cost\n",
        "- 5000 is a common practical choice that captures most meaningful terms while keeping the model tractable\n",
        "\n",
        "**Why bigrams `(1, 2)` instead of just unigrams or trigrams?**\n",
        "\n",
        "- **Unigrams only:** Misses phrases like \"not good\" or \"very bad\" where meaning depends on word pairs\n",
        "- **Bigrams:** Captures common sentiment phrases (\"great service\", \"long wait\", \"never again\")\n",
        "- **Trigrams:** Rarely add value for short texts like tweets; increases feature space significantly\n",
        "\n",
        "**Why minimal text preprocessing?**\n",
        "\n",
        "We don't apply stemming, lemmatization, or stopword removal because:\n",
        "1. **TF-IDF handles common words:** IDF naturally downweights frequent words like \"the\", \"is\"\n",
        "2. **Sentiment in stopwords:** Words like \"not\", \"very\", \"but\" carry sentiment information\n",
        "3. **Simplicity:** Minimal preprocessing reduces pipeline complexity and potential errors\n",
        "4. **Empirical evidence:** For sentiment analysis, heavy preprocessing often doesn't improve results (Haddi et al., 2013)\n",
        "\n",
        "*Note: For other NLP tasks (e.g., topic modelling), preprocessing may be more beneficial.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAM0Jkztqjsa"
      },
      "outputs": [],
      "source": [
        "MAX_FEATURES = 5000\n",
        "NGRAMS = 2\n",
        "\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, NGRAMS), max_features=MAX_FEATURES)\n",
        "tfidf.fit(tweets_train)\n",
        "\n",
        "X_train, X_test = tfidf.transform(tweets_train).toarray(), tfidf.transform(tweets_test).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jOzy35Sqjsa"
      },
      "source": [
        "### 4.5 Encode Labels as One-Hot Vectors\n",
        "\n",
        "For multi-class classification with softmax output, we need to convert categorical labels to one-hot encoded vectors:\n",
        "- Irrelevant → [1, 0, 0, 0]\n",
        "- Negative → [0, 1, 0, 0]\n",
        "- Neutral → [0, 0, 1, 0]\n",
        "- Positive → [0, 0, 0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "011gabFmqjsa"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(tweets['sentiment'])\n",
        "\n",
        "y_train = to_categorical(label_encoder.transform(sentiment_train))\n",
        "y_test = to_categorical(label_encoder.transform(sentiment_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX5m2VLQqjsa"
      },
      "source": [
        "## 5. Developing a Model That Does Better Than a Baseline\n",
        "\n",
        "Before building complex models, we need to establish **baseline performance**. This gives us a reference point to know if our model is actually learning something useful.\n",
        "\n",
        "### 5.1 Examine Class Distribution\n",
        "\n",
        "Let's look at how the sentiment classes are distributed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8lAjJ9Uqjsa"
      },
      "outputs": [],
      "source": [
        "counts = tweets.groupby(['sentiment']).count()\n",
        "counts.reset_index(inplace=True)\n",
        "\n",
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzXzRxCsqjsa"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA-DRIVEN ANALYSIS: Dataset Size & Imbalance\n",
        "# =============================================================================\n",
        "\n",
        "# Dataset size analysis (for hold-out vs K-fold decision)\n",
        "n_samples = len(tweets)\n",
        "HOLDOUT_THRESHOLD = 10000  # Use hold-out if samples > 10,000 (Kohavi, 1995; Chollet, 2021)\n",
        "\n",
        "# Imbalance analysis (for metric selection)\n",
        "majority_class = counts['text'].max()\n",
        "minority_class = counts['text'].min()\n",
        "imbalance_ratio = majority_class / minority_class\n",
        "IMBALANCE_THRESHOLD = 3.0  # Use F1-Score if ratio > 3.0 (He & Garcia, 2009)\n",
        "\n",
        "# Determine evaluation strategy and metric\n",
        "use_holdout = n_samples > HOLDOUT_THRESHOLD\n",
        "use_f1 = imbalance_ratio > IMBALANCE_THRESHOLD\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA-DRIVEN CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n1. DATASET SIZE: {n_samples:,} samples\")\n",
        "print(f\"   Threshold: {HOLDOUT_THRESHOLD:,} samples (Kohavi, 1995)\")\n",
        "print(f\"   Decision: {'Hold-Out' if use_holdout else 'K-Fold Cross-Validation'}\")\n",
        "\n",
        "print(f\"\\n2. CLASS IMBALANCE: {imbalance_ratio:.2f}:1 ratio\")\n",
        "print(f\"   Threshold: {IMBALANCE_THRESHOLD:.1f}:1 (He & Garcia, 2009)\")\n",
        "print(f\"   Decision: {'F1-Score (imbalanced)' if use_f1 else 'Accuracy (balanced)'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "PRIMARY_METRIC = 'f1' if use_f1 else 'accuracy'\n",
        "print(f\"PRIMARY METRIC: {PRIMARY_METRIC.upper()}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfZrYVTBqjsa"
      },
      "source": [
        "### 5.2 Calculate Baseline Metrics\n",
        "\n",
        "**Naive Baseline (Majority Class):** If we always predict the most common class, we get the majority class accuracy. This is our accuracy baseline.\n",
        "\n",
        "**Balanced Accuracy Baseline:** For K classes, a random classifier would achieve 1/K balanced accuracy. This is more meaningful for imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAEI_Jxqqjsa"
      },
      "outputs": [],
      "source": [
        "# Find the majority class\n",
        "majority_class_name = counts.loc[counts['text'].idxmax(), 'sentiment']\n",
        "baseline = counts['text'].max() / counts['text'].sum()\n",
        "\n",
        "print(f\"Majority class: {majority_class_name}\")\n",
        "print(f\"Baseline accuracy (majority class): {baseline:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvN0bqsTqjsa"
      },
      "outputs": [],
      "source": [
        "# Balanced accuracy baseline (random classifier)\n",
        "n_classes = len(counts)\n",
        "balanced_accuracy_baseline = 1.0 / n_classes\n",
        "\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"Balanced accuracy baseline (random): {balanced_accuracy_baseline:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPS33HXqjsa"
      },
      "source": [
        "### 5.3 Create Validation Set\n",
        "\n",
        "We split off a portion of the training data for validation. This will be used to:\n",
        "- Evaluate model performance during hyperparameter tuning\n",
        "- Compare models without touching the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGsFwDW2qjsa"
      },
      "outputs": [],
      "source": [
        "VALIDATION_SIZE = 0.1\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                  test_size=VALIDATION_SIZE, stratify=y_train,\n",
        "                                                  shuffle=True, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N6la0gwqjsa"
      },
      "source": [
        "### 5.4 Configure Training Parameters\n",
        "\n",
        "**Key training settings:**\n",
        "- **Optimiser:** Adam - adaptive learning rate optimiser with momentum, widely used for deep learning\n",
        "- **Loss:** Categorical cross-entropy - standard loss for multi-class classification\n",
        "- **Training Metrics:** Accuracy, Precision, Recall, AUC (tracked by Keras during training)\n",
        "- **Primary Metric:** F1-Score (macro-averaged) - computed separately after training using sklearn, since Keras doesn't provide a built-in macro-F1 metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6v0ROu8qjsa"
      },
      "outputs": [],
      "source": [
        "INPUT_DIMENSION = X_train.shape[1]\n",
        "OUTPUT_CLASSES = y_train.shape[1]\n",
        "\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS_FUNC = 'categorical_crossentropy'\n",
        "\n",
        "# Training metrics (tracked by Keras during training)\n",
        "# Note: F1-Score (our primary metric) is computed separately using sklearn\n",
        "# because Keras doesn't provide a built-in macro-averaged F1 metric\n",
        "METRICS = ['categorical_accuracy',\n",
        "           tf.keras.metrics.Precision(name='precision'),\n",
        "           tf.keras.metrics.Recall(name='recall'),\n",
        "           # multi_label=True is about LABEL FORMAT, not task type:\n",
        "           # - Our task: multi-CLASS (4 mutually exclusive classes)\n",
        "           # - Our labels: one-hot encoded [[1,0,0,0], [0,1,0,0], ...]\n",
        "           # - multi_label=True tells Keras to expect one-hot format\n",
        "           # - multi_label=False expects integer indices [0, 1, 2, 3]\n",
        "           tf.keras.metrics.AUC(name='auc', multi_label=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcJtKgW7qjsb"
      },
      "outputs": [],
      "source": [
        "# Single-Layer Perceptron (no hidden layers)\n",
        "slp_model = Sequential(name='Single_Layer_Perceptron')\n",
        "slp_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
        "slp_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
        "slp_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
        "\n",
        "slp_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycx7myJIqjsb"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Batch Size Selection:\n",
        "# - Large datasets (>10,000 samples): Use 512 for efficient GPU utilisation\n",
        "# - Small datasets (<10,000 samples): Use 32-64 for better gradient estimates\n",
        "# Twitter Entity Sentiment has ~74,000 samples → Use batch size 512\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# We use DIFFERENT epoch counts for different training phases:\n",
        "#\n",
        "# EPOCHS_BASELINE (100): For SLP and unregularised DNN\n",
        "#   - SLP converges quickly (simple model)\n",
        "#   - Unregularised DNN: 100 epochs clearly shows overfitting (val_loss increasing)\n",
        "#\n",
        "# EPOCHS_REGULARIZED (150): For DNN with Dropout + L2\n",
        "#   - WHY train longer? Regularisation SLOWS DOWN learning:\n",
        "#     * Dropout randomly masks neurons, so each update uses partial information\n",
        "#     * L2 penalty constrains weight updates, preventing large steps\n",
        "#     * The model needs MORE iterations to reach the same level of convergence\n",
        "#   - Without extra epochs, we'd stop before the model reaches its full potential\n",
        "#   - With regularisation, longer training is SAFE (no overfitting risk)\n",
        "#\n",
        "# The trade-off: Regularisation exchanges faster convergence for overfitting protection.\n",
        "# We compensate by allowing more training time.\n",
        "\n",
        "EPOCHS_BASELINE = 100      # SLP and DNN (no regularisation)\n",
        "EPOCHS_REGULARIZED = 150   # DNN with Dropout + L2 (needs more time to converge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpLOk8tMqjsb"
      },
      "source": [
        "### 5.5 Handle Class Imbalance with Class Weights\n",
        "\n",
        "To handle imbalanced classes, we compute **class weights** that give more importance to minority classes during training.\n",
        "\n",
        "This makes errors on minority classes \"cost more\", encouraging the model to learn them better.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Class Weights Instead of Resampling?\n",
        "\n",
        "| Technique | How it works | Pros | Cons |\n",
        "|-----------|--------------|------|------|\n",
        "| **Class Weights** | Adjusts loss function to penalise minority errors more | Simple, no data modification, works with any batch size | Doesn't add information |\n",
        "| **Oversampling (SMOTE)** | Creates synthetic minority samples | Adds training data, can improve decision boundaries | Risk of overfitting to synthetic data, increases training time |\n",
        "| **Undersampling** | Removes majority class samples | Balances dataset, faster training | Loses potentially useful information |\n",
        "\n",
        "We use **class weights** because:\n",
        "1. **Simplicity:** No need to modify the dataset or import additional libraries\n",
        "2. **No synthetic data risk:** SMOTE can create unrealistic samples, especially for text data\n",
        "3. **Efficiency:** Training time unchanged; no additional data to process\n",
        "4. **Keras integration:** Native support via `class_weight` parameter in `model.fit()`\n",
        "\n",
        "*Note: For severely imbalanced data (>10:1 ratio), combining class weights with oversampling may yield better results.*\n",
        "\n",
        "---\n",
        "\n",
        "#### How Class Weights Fit Our Evaluation Strategy\n",
        "\n",
        "| Component | Role | Handles Imbalance? |\n",
        "|-----------|------|-------------------|\n",
        "| **Class weights** | Training technique - modifies loss function | Yes - penalises minority class errors more |\n",
        "| **AUC** (tuning objective) | Evaluation metric during hyperparameter search | Yes - macro-averages across classes |\n",
        "| **F1-macro** (primary metric) | Final evaluation metric | Yes - macro-averages across classes |\n",
        "\n",
        "These three components work together consistently:\n",
        "- **Class weights** help the model *learn* minority classes better (affects training loss)\n",
        "- **AUC** and **F1-macro** *measure* performance across all classes equally (evaluation metrics, computed without weights)\n",
        "\n",
        "All three treat classes fairly regardless of size, making this a coherent strategy for imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Js6HQWkqjsb"
      },
      "outputs": [],
      "source": [
        "labels = np.argmax(y_train, axis=1)\n",
        "weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "CLASS_WEIGHTS = dict(enumerate(weights))\n",
        "\n",
        "CLASS_WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAa07XPbqjsb"
      },
      "outputs": [],
      "source": [
        "# Train the Single-Layer Perceptron\n",
        "history_slp = slp_model.fit(X_train, y_train,\n",
        "                            class_weight=CLASS_WEIGHTS,\n",
        "                            batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            verbose=0)\n",
        "val_score_slp = slp_model.evaluate(X_val, y_val, verbose=0)[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vJKYv05qjsb"
      },
      "outputs": [],
      "source": [
        "# Display SLP validation metrics\n",
        "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_slp[0], baseline))\n",
        "print('Precision (Validation): {:.2f}'.format(val_score_slp[1]))\n",
        "print('Recall (Validation): {:.2f}'.format(val_score_slp[2]))\n",
        "print('AUC (Validation): {:.2f}'.format(val_score_slp[3]))\n",
        "\n",
        "preds_slp_val = slp_model.predict(X_val, verbose=0).argmax(axis=1)\n",
        "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
        "    balanced_accuracy_score(y_val.argmax(axis=1), preds_slp_val), balanced_accuracy_baseline))\n",
        "\n",
        "# Calculate F1-Score (primary metric for imbalanced data)\n",
        "f1_slp_val = f1_score(y_val.argmax(axis=1), preds_slp_val, average='macro')\n",
        "print(f'F1-Score (Validation): {f1_slp_val:.2f}  ← Primary Metric')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YiHfDpBqjsb"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, title=None):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics over epochs.\n",
        "    Plots: (1) Loss, (2) Accuracy\n",
        "\n",
        "    Note: We use Accuracy for training visualisation (directly tracked by Keras).\n",
        "    F1-Score is computed for final evaluation since it's our primary metric.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    history : keras History object\n",
        "        Training history from model.fit()\n",
        "    title : str, optional\n",
        "        Model name to display in plot titles (e.g., 'SLP', 'DNN')\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    epochs = range(1, len(history.history['loss']) + 1)\n",
        "    title_suffix = f' ({title})' if title else ''\n",
        "\n",
        "    # Plot 1: Loss\n",
        "    axs[0].plot(epochs, history.history['loss'], 'b-', label='Training', linewidth=1.5)\n",
        "    axs[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation', linewidth=1.5)\n",
        "    axs[0].set_title(f'Loss{title_suffix}')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(alpha=0.3)\n",
        "\n",
        "    # Plot 2: Accuracy (directly tracked by Keras)\n",
        "    axs[1].plot(epochs, history.history['categorical_accuracy'], 'b-', label='Training', linewidth=1.5)\n",
        "    axs[1].plot(epochs, history.history['val_categorical_accuracy'], 'r-', label='Validation', linewidth=1.5)\n",
        "    axs[1].set_title(f'Accuracy{title_suffix}')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOwjY3bRqjsb"
      },
      "outputs": [],
      "source": [
        "# Plot SLP training history\n",
        "plot_training_history(history_slp, title='SLP Baseline')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5X_iKqBqjsb"
      },
      "source": [
        "## 6. Scaling Up: Developing a Model That Overfits\n",
        "\n",
        "The next step in the Universal ML Workflow is to build a model with **enough capacity to overfit**. If a model can't overfit, it may be too simple to learn the patterns in the data.\n",
        "\n",
        "**Strategy:** Add hidden layers and neurons to increase model capacity.\n",
        "\n",
        "**No regularisation applied:** We intentionally train this model **without any regularisation** (no dropout, no L2, no early stopping) to observe overfitting behaviour. In the training plots, you should see:\n",
        "- Training loss continues to decrease\n",
        "- Validation loss starts increasing after some epochs (overfitting)\n",
        "\n",
        "This demonstrates why regularisation (Section 7) is necessary.\n",
        "\n",
        "---\n",
        "\n",
        "#### Architecture Design Decisions\n",
        "\n",
        "**Why 64 neurons in the hidden layer?**\n",
        "\n",
        "This is a practical starting point that balances capacity and efficiency:\n",
        "- **Too few (e.g., 16):** May not have enough capacity to learn complex patterns\n",
        "- **Too many (e.g., 512):** Increases overfitting risk and training time without proportional benefit\n",
        "- **64 neurons:** A common choice for tabular/text data that provides sufficient capacity for most classification tasks\n",
        "\n",
        "**Why only 1 hidden layer instead of 2-3?**\n",
        "\n",
        "Per the **Universal ML Workflow**, the goal of this step is to demonstrate that the model *can* overfit—proving it has sufficient capacity to capture the underlying patterns. Once overfitting is observed:\n",
        "\n",
        "1. **Capacity is proven sufficient:** If the model overfits, it can learn the training data's complexity\n",
        "2. **No need for more depth:** Adding layers would increase overfitting further without benefit\n",
        "3. **Regularise, don't expand:** The next step (Section 7) is to *reduce* overfitting through regularisation, not to add more capacity\n",
        "\n",
        "If this 1-layer model *couldn't* overfit (training and validation loss both plateau high), we would then add more layers. But since it does overfit, the architecture is adequate.\n",
        "\n",
        "*\"The right question is not 'How many layers?' but 'Can it overfit?' If yes, regularise. If no, add capacity.\"*\n",
        "\n",
        "### 6.1 Build a Deep Neural Network (DNN)\n",
        "\n",
        "Let's add a hidden layer with 64 neurons and ReLU activation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTEGXCNHqjsb"
      },
      "outputs": [],
      "source": [
        "# Deep Neural Network (1 hidden layer, no dropout for overfitting demo)\n",
        "dnn_model = Sequential(name='Deep_Neural_Network')\n",
        "dnn_model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
        "dnn_model.add(Dense(64, activation='relu'))\n",
        "dnn_model.add(Dense(OUTPUT_CLASSES, activation='softmax'))\n",
        "dnn_model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNC, metrics=METRICS)\n",
        "\n",
        "dnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRZ_K-zPqjsb"
      },
      "outputs": [],
      "source": [
        "# Train the Deep Neural Network (without regularisation to demonstrate overfitting)\n",
        "# Using EPOCHS_BASELINE: 100 epochs is enough to clearly show overfitting behaviour\n",
        "history_dnn = dnn_model.fit(X_train, y_train,\n",
        "                            class_weight=CLASS_WEIGHTS,\n",
        "                            batch_size=BATCH_SIZE, epochs=EPOCHS_BASELINE,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            verbose=0)\n",
        "val_score_dnn = dnn_model.evaluate(X_val, y_val, verbose=0)[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANDHRa5Vqjsb"
      },
      "outputs": [],
      "source": [
        "# Plot DNN training history (expect overfitting: val_loss increasing)\n",
        "plot_training_history(history_dnn, title='DNN - No Regularisation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDTodhD0qjsb"
      },
      "outputs": [],
      "source": [
        "# Display DNN validation metrics\n",
        "print('Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(val_score_dnn[0], baseline))\n",
        "print('Precision (Validation): {:.2f}'.format(val_score_dnn[1]))\n",
        "print('Recall (Validation): {:.2f}'.format(val_score_dnn[2]))\n",
        "print('AUC (Validation): {:.2f}'.format(val_score_dnn[3]))\n",
        "\n",
        "preds_dnn_val = dnn_model.predict(X_val, verbose=0).argmax(axis=1)\n",
        "print('Balanced Accuracy (Validation): {:.2f} (baseline={:.2f})'.format(\n",
        "    balanced_accuracy_score(y_val.argmax(axis=1), preds_dnn_val), balanced_accuracy_baseline))\n",
        "\n",
        "# Calculate F1-Score (primary metric for imbalanced data)\n",
        "f1_dnn_val = f1_score(y_val.argmax(axis=1), preds_dnn_val, average='macro')\n",
        "print(f'F1-Score (Validation): {f1_dnn_val:.2f}  ← Primary Metric')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVOn8WuGqjsb"
      },
      "source": [
        "## 7. Regularising Your Model and Tuning Hyperparameters\n",
        "\n",
        "Now we address the overfitting observed in Section 6 by adding **regularisation**. We use two complementary techniques:\n",
        "\n",
        "| Technique | How it works | Effect |\n",
        "|-----------|--------------|--------|\n",
        "| **Dropout** | Randomly drops neurons during training | Acts like ensemble averaging, reduces co-adaptation |\n",
        "| **L2 (Weight Decay)** | Adds penalty for large weights to loss | Keeps weights small, smoother decision boundaries |\n",
        "\n",
        "**Same architecture, different regularisation:** We keep the same 1-layer architecture (64 neurons) as Section 6, so the only difference is regularisation. This allows a fair comparison.\n",
        "\n",
        "Using **Hyperband** for efficient hyperparameter tuning to find optimal regularisation strengths.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Hyperband?\n",
        "\n",
        "| Method | How it works | Pros | Cons |\n",
        "|--------|--------------|------|------|\n",
        "| **Grid Search** | Tries all combinations exhaustively | Thorough, reproducible | Exponentially expensive, wastes time on bad configs |\n",
        "| **Random Search** | Samples random combinations | More efficient than grid, good coverage | Still trains all configs to completion |\n",
        "| **Bayesian Optimisation** | Models performance to guide search | Sample-efficient, finds good configs fast | Complex to implement, overhead per iteration |\n",
        "| **Hyperband** | Early stopping of poor performers | Very efficient for deep learning, parallelisable | May discard slow starters prematurely |\n",
        "\n",
        "We use **Hyperband** because:\n",
        "1. **Efficiency:** Eliminates poor configurations early, focusing resources on promising ones\n",
        "2. **Deep learning fit:** Training epochs are a natural \"resource\" to allocate adaptively\n",
        "3. **Keras Tuner integration:** Native support via `kt.Hyperband`\n",
        "4. **Scalability:** Handles many hyperparameters efficiently\n",
        "\n",
        "**How Hyperband works:**\n",
        "1. Start training many configurations for a few epochs\n",
        "2. Evaluate and keep the top performers\n",
        "3. Train survivors for more epochs\n",
        "4. Repeat until only the best remain\n",
        "\n",
        "### 7.1 Hyperband Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJqpZETwqjsb"
      },
      "outputs": [],
      "source": [
        "# Hyperband Model Builder for Multi-Class Twitter Entity Sentiment Classification\n",
        "def build_model_hyperband(hp):\n",
        "    \"\"\"\n",
        "    Build Twitter Entity Sentiment model with FIXED architecture (1 hidden layer, 64 neurons).\n",
        "    Same architecture as Section 6 DNN - only tunes regularisation and learning rate.\n",
        "    Uses Adam optimiser (same as Section 6) for fair comparison.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(INPUT_DIMENSION,)))\n",
        "\n",
        "    # L2 regularisation strength\n",
        "    l2_reg = hp.Float('l2_reg', 1e-5, 1e-2, sampling='log')\n",
        "\n",
        "    # Fixed architecture: 1 hidden layer with 64 neurons (same as Section 6)\n",
        "    model.add(layers.Dense(64, activation='relu',\n",
        "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    dropout_rate = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer for multi-class classification\n",
        "    model.add(layers.Dense(OUTPUT_CLASSES, activation='softmax'))\n",
        "\n",
        "    lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=LOSS_FUNC,\n",
        "        metrics=METRICS\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wWDdewIqjsc"
      },
      "outputs": [],
      "source": [
        "# Configure Hyperband tuner\n",
        "# ===========================================================================\n",
        "# TUNING OBJECTIVE: Why AUC instead of F1?\n",
        "# ===========================================================================\n",
        "# We use F1-Score as our PRIMARY METRIC for final evaluation, but AUC for tuning:\n",
        "#\n",
        "# 1. Keras limitation: No built-in macro-averaged F1 metric available during training\n",
        "#    - F1 requires computing precision/recall across all classes, then averaging\n",
        "#    - Keras metrics are computed per-batch, making macro-F1 impractical\n",
        "#\n",
        "# 2. AUC is a better tuning objective:\n",
        "#    - AUC is threshold-independent (evaluates across all decision thresholds)\n",
        "#    - F1 is threshold-dependent (requires choosing a cutoff for predictions)\n",
        "#    - Both penalise models that ignore minority classes\n",
        "#    - Models with high AUC tend to achieve high F1 at optimal thresholds\n",
        "#\n",
        "# 3. Alternative approaches (not used here):\n",
        "#    - Custom Keras metric (complex, slower)\n",
        "#    - Validation callback computing F1 each epoch (adds overhead)\n",
        "#\n",
        "# Final evaluation uses sklearn's f1_score(average='macro') as primary metric.\n",
        "# ===========================================================================\n",
        "\n",
        "TUNING_OBJECTIVE = 'val_categorical_accuracy' if PRIMARY_METRIC == 'accuracy' else 'val_auc'\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    build_model_hyperband,\n",
        "    objective=TUNING_OBJECTIVE,\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory='twitter_entity_hyperband',\n",
        "    project_name='twitter_entity_tuning',\n",
        "    overwrite=True  # Ensure fresh tuning on each run\n",
        ")\n",
        "\n",
        "print(f\"Tuning objective: {TUNING_OBJECTIVE}\")\n",
        "print(\"(Note: Final evaluation uses F1-Score as primary metric)\")\n",
        "\n",
        "# Run Hyperband search\n",
        "tuner.search(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=CLASS_WEIGHTS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4o_5x2aqjsc"
      },
      "outputs": [],
      "source": [
        "# Get best hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found by Hyperband:\")\n",
        "print(f\"  L2 Regularisation: {best_hp.get('l2_reg'):.6f}\")\n",
        "print(f\"  Dropout Rate: {best_hp.get('dropout')}\")\n",
        "print(f\"  Learning Rate: {best_hp.get('lr'):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlhIk0FTqjsc"
      },
      "source": [
        "### 7.2 Sanity Check and Final Retraining\n",
        "\n",
        "After finding the best hyperparameters, we follow a two-step process:\n",
        "\n",
        "1. **Sanity Check:** Retrain with the best hyperparameters using training and validation data to visually confirm the model is not overfitting. This validates that Hyperband found hyperparameters that generalise well.\n",
        "\n",
        "2. **Final Refit:** Combine training and validation sets and retrain without validation. Since the hyperparameters have been validated, we maximise the data available for the final model.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why This Two-Step Approach?\n",
        "\n",
        "| Step | Purpose | Validation Data |\n",
        "|------|---------|-----------------|\n",
        "| **Sanity Check** | Confirm hyperparameters prevent overfitting | ✓ Used for monitoring |\n",
        "| **Final Refit** | Maximise training data for production model | ✗ Merged into training |\n",
        "\n",
        "Once the sanity check confirms no overfitting, we can confidently combine all available data for the final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C85mJ-kKqjsc"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 1: SANITY CHECK - Retrain with validation to confirm no overfitting\n",
        "# =============================================================================\n",
        "\n",
        "# Extract the number of epochs from the best trial\n",
        "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
        "best_epochs = best_trial.best_step + 1  # best_step is 0-indexed\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SANITY CHECK: Retraining with Validation\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training for {best_epochs} epochs (matched from Hyperband's best trial)\")\n",
        "print(\"Purpose: Visually confirm the hyperparameters prevent overfitting\\n\")\n",
        "\n",
        "# Build a fresh model with the best hyperparameters\n",
        "sanity_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "history_sanity = sanity_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=best_epochs,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        "    validation_data=(X_val, y_val),  # Include validation for monitoring\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Sanity check training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KChfajKCqjsc"
      },
      "outputs": [],
      "source": [
        "# Plot sanity check training history (with validation curves)\n",
        "plot_training_history(history_sanity, title=f'Sanity Check - Best Hyperparameters ({best_epochs} epochs)')\n",
        "\n",
        "# Verify no overfitting: validation loss should not increase significantly\n",
        "val_losses = history_sanity.history['val_loss']\n",
        "min_val_loss_epoch = val_losses.index(min(val_losses)) + 1\n",
        "final_val_loss = val_losses[-1]\n",
        "min_val_loss = min(val_losses)\n",
        "\n",
        "print(f\"\\nSanity Check Results:\")\n",
        "print(f\"  Minimum validation loss: {min_val_loss:.4f} at epoch {min_val_loss_epoch}\")\n",
        "print(f\"  Final validation loss: {final_val_loss:.4f}\")\n",
        "if final_val_loss <= min_val_loss * 1.1:  # Within 10% of minimum\n",
        "    print(\"  ✓ No significant overfitting detected - hyperparameters are validated\")\n",
        "else:\n",
        "    print(\"  ⚠ Some overfitting detected - consider adjusting epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPtLtVnXqjsc"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 2: FINAL REFIT - Combine data and retrain for production\n",
        "# =============================================================================\n",
        "\n",
        "# Combine training and validation sets for final model\n",
        "X_train_full = np.vstack([X_train, X_val])\n",
        "y_train_full = np.vstack([y_train, y_val])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL REFIT: Training on Combined Data\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training data: {X_train.shape[0]:,} samples\")\n",
        "print(f\"Validation data: {X_val.shape[0]:,} samples\")\n",
        "print(f\"Combined data: {X_train_full.shape[0]:,} samples\")\n",
        "print(f\"  → {(X_train_full.shape[0] / X_train.shape[0] - 1) * 100:.1f}% more training data\")\n",
        "\n",
        "# Recompute class weights on combined data\n",
        "labels_full = np.argmax(y_train_full, axis=1)\n",
        "weights_full = compute_class_weight('balanced', classes=np.unique(labels_full), y=labels_full)\n",
        "CLASS_WEIGHTS_FULL = dict(enumerate(weights_full))\n",
        "print(f\"\\nClass weights (combined): {CLASS_WEIGHTS_FULL}\")\n",
        "\n",
        "# Build and train final model on combined data\n",
        "print(f\"\\nRetraining for {best_epochs} epochs on combined data...\")\n",
        "\n",
        "opt_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "opt_model.fit(\n",
        "    X_train_full, y_train_full,\n",
        "    epochs=best_epochs,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=CLASS_WEIGHTS_FULL,\n",
        "    verbose=0\n",
        "    # No validation_data - merged into training\n",
        "    # No plotting needed - sanity check already validated the hyperparameters\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Final model training complete on combined dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YIaV6Cwqjsc"
      },
      "source": [
        "### 7.3 Final Model Evaluation on Test Set\n",
        "\n",
        "Now we evaluate our best model on the held-out test set that was never used during training or tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZsYuX9xqjsc"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "test_score = opt_model.evaluate(X_test, y_test, verbose=0)[1:]\n",
        "preds_test = opt_model.predict(X_test, verbose=0).argmax(axis=1)\n",
        "\n",
        "# Calculate F1-Score (our primary metric)\n",
        "test_f1 = f1_score(y_test.argmax(axis=1), preds_test, average='macro')\n",
        "\n",
        "print('=' * 50)\n",
        "print('FINAL TEST SET RESULTS')\n",
        "print('=' * 50)\n",
        "print(f'F1-Score (Test): {test_f1:.4f}  ← Primary Metric')\n",
        "print(f'Accuracy (Test): {test_score[0]:.4f} (baseline={baseline:.4f})')\n",
        "print(f'Precision (Test): {test_score[1]:.4f}')\n",
        "print(f'Recall (Test): {test_score[2]:.4f}')\n",
        "print(f'AUC (Test): {test_score[3]:.4f}')\n",
        "print(f'Balanced Accuracy (Test): {balanced_accuracy_score(y_test.argmax(axis=1), preds_test):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWZfuKcWqjsc"
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix for test predictions\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test.argmax(axis=1), preds_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix - Test Set Predictions')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print per-class recall (what % of each class was correctly identified)\n",
        "print(\"\\nPer-Class Recall:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    class_mask = y_test.argmax(axis=1) == i\n",
        "    class_recall = (preds_test[class_mask] == i).mean()\n",
        "    print(f\"  {class_name}: {class_recall:.2%} ({class_mask.sum()} samples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot7uOIh7qjsc"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Results Summary\n",
        "\n",
        "The following dynamically-generated table compares all models trained in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXFt37r7qjsc"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RESULTS SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "# Create results DataFrame\n",
        "# Note: Final model (DNN Dropout + L2) shows only test metrics\n",
        "# because validation data was combined with training for final model\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Naive Baseline', 'SLP (No Hidden)', 'DNN (No Regularisation)', 'DNN (Dropout + L2)'],\n",
        "    'Accuracy': [baseline, val_score_slp[0], val_score_dnn[0], test_score[0]],\n",
        "    'F1-Score': [0.0, f1_slp_val, f1_dnn_val, test_f1],\n",
        "    'Dataset': ['N/A', 'Validation', 'Validation', 'Test']\n",
        "})\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL COMPARISON - RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Primary Metric: F1-SCORE (imbalance ratio: {imbalance_ratio:.2f}:1)\")\n",
        "print(\"=\" * 70)\n",
        "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nKey Observations:\")\n",
        "print(f\"  - All models outperform naive baseline ({baseline:.2%} accuracy)\")\n",
        "print(f\"  - DNN with Dropout + L2 trained on combined train+val data\")\n",
        "print(f\"  - Final test F1-Score: {test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pmvNH8tqjsc"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Key Takeaways\n",
        "\n",
        "### Decision Framework Summary\n",
        "\n",
        "| Decision | Threshold | This Dataset | Choice | Reference |\n",
        "|----------|-----------|--------------|--------|----------|\n",
        "| **Hold-Out vs K-Fold** | > 10,000 samples | ~74,000 samples | Hold-Out | Kohavi (1995); Chollet (2021) |\n",
        "| **Accuracy vs F1-Score** | > 3:1 imbalance | ~1.7:1 ratio | F1-Score* | He and Garcia (2009) |\n",
        "\n",
        "*Note: The imbalance ratio for this dataset is mild (~1.7:1), but we still use F1-Score for consistency with the workflow and to demonstrate the data-driven decision process. The code dynamically selects the metric based on the computed ratio.*\n",
        "\n",
        "### Lessons Learned\n",
        "\n",
        "1. **Data-Driven Metric Selection:** We compute the imbalance ratio from data and use F1-Score if > 3:1 to ensure fair evaluation across all classes.\n",
        "\n",
        "2. **Data-Driven Evaluation Protocol:** With > 10,000 samples and deep learning, hold-out validation provides reliable estimates while being computationally efficient.\n",
        "\n",
        "3. **Class Imbalance Handling:** Using class weights during training improves performance on minority classes. This is simpler than resampling techniques (SMOTE, undersampling) and avoids synthetic data risks.\n",
        "\n",
        "4. **Simple Models Can Work Well:** The SLP achieved competitive F1-Score with good feature engineering (TF-IDF).\n",
        "\n",
        "5. **Regularisation Prevents Overfitting:** The unregularised DNN showed overfitting; combining **Dropout + L2 regularisation** controls this effectively—no need for deeper architectures once overfitting is demonstrated.\n",
        "\n",
        "6. **Maximise Data for Final Model:** After hyperparameter tuning, we combine training and validation sets for the final model. The validation set has served its purpose (model selection), and more training data leads to better generalisation.\n",
        "\n",
        "7. **Technique Scope:** We use only techniques from Chapters 1–4 of *Deep Learning with Python* (Chollet, 2021). Early stopping and word embeddings, while useful, are outside this scope.\n",
        "\n",
        "8. **4-Class vs 3-Class Sentiment:** This dataset includes an \"Irrelevant\" class, which is common in real-world entity sentiment analysis where not all mentions express sentiment.\n",
        "\n",
        "9. **Code Reusability:** The core workflow and code patterns are identical to the Twitter US Airline Sentiment notebook—only the data loading and class-specific details change. This demonstrates that the Universal ML Workflow is truly universal.\n",
        "\n",
        "### References\n",
        "\n",
        "- Chollet, F. (2021) *Deep learning with Python*. 2nd edn. Shelter Island, NY: Manning Publications.\n",
        "\n",
        "- Haddi, E., Liu, X. and Shi, Y. (2013) 'The role of text pre-processing in sentiment analysis', *Procedia Computer Science*, 17, pp. 26–32.\n",
        "\n",
        "- Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The elements of statistical learning: data mining, inference, and prediction*. 2nd edn. New York: Springer.\n",
        "\n",
        "- He, H. and Garcia, E.A. (2009) 'Learning from imbalanced data', *IEEE Transactions on Knowledge and Data Engineering*, 21(9), pp. 1263–1284.\n",
        "\n",
        "- Kohavi, R. (1995) 'A study of cross-validation and bootstrap for accuracy estimation and model selection', *IJCAI*, 2, pp. 1137–1145."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Appendix: Modular Helper Functions\n",
        "\n",
        "For cleaner code organisation, you can wrap the model building and training patterns into reusable functions."
      ],
      "metadata": {
        "id": "P1_NRx0Vqjsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MODULAR HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def build_multiclass_nlp_classifier(input_dim, num_classes, hidden_units=None,\n",
        "                                     dropout=0.0, l2_reg=0.0,\n",
        "                                     optimizer='adam', learning_rate=None, name=None):\n",
        "    \"\"\"\n",
        "    Build a multi-class NLP classification neural network.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_dim : int\n",
        "        Number of input features (TF-IDF vector dimension)\n",
        "    num_classes : int\n",
        "        Number of output classes\n",
        "    hidden_units : list of int, optional\n",
        "        Neurons per hidden layer, e.g., [64] or [128, 64]\n",
        "    dropout : float\n",
        "        Dropout rate (0.0 to 0.5)\n",
        "    l2_reg : float\n",
        "        L2 regularisation strength\n",
        "    learning_rate : float, optional\n",
        "        Custom learning rate\n",
        "    name : str, optional\n",
        "        Model name\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    keras.Sequential : Compiled model ready for training\n",
        "    \"\"\"\n",
        "    model = Sequential(name=name)\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "\n",
        "    hidden_units = hidden_units or []\n",
        "    kernel_reg = regularizers.l2(l2_reg) if l2_reg > 0 else None\n",
        "\n",
        "    for units in hidden_units:\n",
        "        model.add(Dense(units, activation='relu', kernel_regularizer=kernel_reg))\n",
        "        if dropout > 0:\n",
        "            model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    if learning_rate is not None:\n",
        "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = optimizer\n",
        "\n",
        "    metrics = ['categorical_accuracy',\n",
        "               tf.keras.metrics.Precision(name='precision'),\n",
        "               tf.keras.metrics.Recall(name='recall'),\n",
        "               tf.keras.metrics.AUC(name='auc', multi_label=True)]\n",
        "\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=metrics)\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_with_class_weights(model, X_train, y_train, X_val, y_val,\n",
        "                              batch_size=512, epochs=100, verbose=0):\n",
        "    \"\"\"Train model with automatic class weight computation.\"\"\"\n",
        "    labels = np.argmax(y_train, axis=1)\n",
        "    weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "    class_weights = dict(enumerate(weights))\n",
        "\n",
        "    return model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        class_weight=class_weights,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate_multiclass_nlp(model, X, y_true_onehot, average='macro'):\n",
        "    \"\"\"\n",
        "    Evaluate multi-class NLP classification model.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing all multi-class classification metrics\n",
        "    \"\"\"\n",
        "    y_pred_proba = model.predict(X, verbose=0)\n",
        "    y_pred = y_pred_proba.argmax(axis=1)\n",
        "    y_true = y_true_onehot.argmax(axis=1)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred, average=average),\n",
        "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# USAGE EXAMPLES\n",
        "# =============================================================================\n",
        "#\n",
        "# # Build models\n",
        "# slp = build_multiclass_nlp_classifier(INPUT_DIMENSION, OUTPUT_CLASSES, name='SLP')\n",
        "# mlp = build_multiclass_nlp_classifier(INPUT_DIMENSION, OUTPUT_CLASSES,\n",
        "#                                       hidden_units=[64], name='MLP')\n",
        "# mlp_reg = build_multiclass_nlp_classifier(INPUT_DIMENSION, OUTPUT_CLASSES,\n",
        "#                                           hidden_units=[64], dropout=0.3, l2_reg=0.001,\n",
        "#                                           learning_rate=0.001)\n",
        "#\n",
        "# # Train with class weights\n",
        "# history = train_with_class_weights(mlp, X_train, y_train, X_val, y_val)\n",
        "#\n",
        "# # Evaluate\n",
        "# metrics = evaluate_multiclass_nlp(mlp, X_val, y_val)\n",
        "# print(f\"Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "nrM1-HVGqjsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}