{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Regularisation/Regularisation%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Regularisation: Hands-On Lab\n\n## Learning Objectives\n\nBy the end of this lab, you will be able to:\n\n1. **Understand** the problem of overfitting and how it manifests in machine learning models\n2. **Recognize** the bias-variance tradeoff and its relationship to model complexity\n3. **Implement** L2 (Ridge) regularization in logistic regression from scratch\n4. **Apply** regularization to prevent overfitting with polynomial features\n5. **Optimize** the regularization hyperparameter Œª using cross-validation\n6. **Visualize** how regularization affects decision boundaries and model complexity\n7. **Compare** L1 (Lasso) and L2 (Ridge) regularization techniques\n\n## What is Regularisation?\n\n**Regularisation** is a technique to prevent overfitting by adding a penalty term to the loss function that discourages overly complex models.\n\n### The Overfitting Problem\n\nWithout regularization, complex models (e.g., high-degree polynomials) can fit training data perfectly but fail on new data:\n- **Training error**: Very low (model memorizes training data)\n- **Test error**: High (model fails to generalize)\n\n### Regularisation Solution\n\nAdd a penalty for large weights to the loss function:\n\n$$J_{\\text{regularized}}(\\vec{w}) = J_{\\text{original}}(\\vec{w}) + \\lambda R(\\vec{w})$$\n\nWhere:\n- $J_{\\text{original}}$ is the original loss (e.g., negative log-likelihood)\n- $R(\\vec{w})$ is the regularization term\n- $\\lambda \\geq 0$ is the regularization strength hyperparameter\n\n### Two Common Types\n\n**L2 Regularization (Ridge)**:\n$$J_{\\text{L2}}(\\vec{w}) = J(\\vec{w}) + \\lambda ||\\vec{w}||^2_2 = J(\\vec{w}) + \\lambda \\sum_{j=1}^{d} w_j^2$$\n\n- Penalizes the **squared magnitude** of weights\n- Shrinks weights toward zero but rarely makes them exactly zero\n- Gradient: $\\nabla R(\\vec{w}) = 2\\lambda \\vec{w}$\n\n**L1 Regularization (Lasso)**:\n$$J_{\\text{L1}}(\\vec{w}) = J(\\vec{w}) + \\lambda ||\\vec{w}||_1 = J(\\vec{w}) + \\lambda \\sum_{j=1}^{d} |w_j|$$\n\n- Penalizes the **absolute value** of weights\n- Can drive weights to **exactly zero** (feature selection)\n- Gradient: $\\nabla R(\\vec{w}) = \\lambda \\cdot \\text{sign}(\\vec{w})$\n\n### Key Intuition\n\n**Why does regularization prevent overfitting?**\n\nComplex models fit training data by using large weights to create sharp, wiggly decision boundaries. Regularization forces the model to use smaller weights, resulting in smoother, more generalizable boundaries. The model must balance fitting the training data well (low $J$) with keeping weights small (low $R$).\n\n**Example:**\n- Without regularization: $w = [0.1, 50.3, -45.2, 38.7]$ (large weights, overfits)\n- With regularization: $w = [0.1, 2.3, -1.8, 0.9]$ (small weights, generalizes)\n\n### The Œª Hyperparameter\n\n- $\\lambda = 0$: No regularization (may overfit)\n- $\\lambda$ small: Light regularization\n- $\\lambda$ large: Heavy regularization (may underfit)\n- $\\lambda = \\infty$: All weights forced to zero (trivial model)\n\n**Finding optimal Œª**: Use cross-validation to test different values and select the one with best validation performance.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Regularisation\n",
    "\n",
    "### ‚úÖ Use Regularisation When:\n",
    "\n",
    "**1. Training accuracy >> Test accuracy (Overfitting)**\n",
    "- Training accuracy: 98%, Test accuracy: 75%\n",
    "- Model memorizes training data but fails to generalize\n",
    "- Regularization constrains model complexity\n",
    "\n",
    "**2. High-Dimensional Feature Space**\n",
    "- Using polynomial features (degree 3+)\n",
    "- Many features compared to samples (d >> N)\n",
    "- Risk of overfitting increases with dimensionality\n",
    "\n",
    "**3. Feature Engineering Creates Many Features**\n",
    "- Polynomial expansion creates O(d^p) features\n",
    "- Interaction terms between many variables\n",
    "- Need to prevent overfitting to noise in new features\n",
    "\n",
    "**4. Limited Training Data**\n",
    "- Small datasets are prone to overfitting\n",
    "- Regularization acts as a prior on weights\n",
    "- Helps model generalize with fewer examples\n",
    "\n",
    "**5. Noisy Data**\n",
    "- Training labels have errors or noise\n",
    "- Regularization prevents fitting to noise\n",
    "- Creates more robust models\n",
    "\n",
    "### ‚ùå Don't Use Regularisation When:\n",
    "\n",
    "**1. Model is Already Underfitting**\n",
    "- Training accuracy is low (e.g., 60%)\n",
    "- Model too simple for the problem\n",
    "- **Better alternatives**: Increase model complexity, add features, reduce existing regularization\n",
    "\n",
    "**2. Linear Model on Linearly Separable Data**\n",
    "- Simple problem with clear linear boundary\n",
    "- Few features, many samples\n",
    "- **Better alternatives**: Use standard logistic regression without regularization\n",
    "\n",
    "**3. Very Large Datasets**\n",
    "- When N >> d (millions of samples, few features)\n",
    "- Overfitting is less of a concern\n",
    "- **Better alternatives**: Let data volume prevent overfitting naturally\n",
    "\n",
    "**4. Need Maximum Training Performance**\n",
    "- Competition where only training set performance matters (rare)\n",
    "- Regularization hurts training performance by design\n",
    "\n",
    "### Quick Decision Tree:\n",
    "\n",
    "```\n",
    "Is training accuracy much higher than test accuracy?\n",
    "‚îú‚îÄ Yes ‚Üí Use regularization (likely overfitting)\n",
    "‚îî‚îÄ No\n",
    "    ‚îú‚îÄ Is training accuracy low?\n",
    "    ‚îÇ   ‚îî‚îÄ Yes ‚Üí Don't use regularization (underfitting)\n",
    "    ‚îî‚îÄ No ‚Üí Model is well-calibrated, regularization optional\n",
    "```\n",
    "\n",
    "### Regularisation vs Other Techniques:\n",
    "\n",
    "| Problem | Regularization | Alternative Solutions |\n",
    "|---------|----------------|----------------------|\n",
    "| **Overfitting** | ‚úÖ L1/L2 regularization | Early stopping, dropout, more data |\n",
    "| **Underfitting** | ‚ùå Makes worse | Increase complexity, add features |\n",
    "| **Feature selection** | ‚úÖ L1 (sparse) | RFE, tree-based importance |\n",
    "| **Multicollinearity** | ‚úÖ L2 (Ridge) | PCA, remove correlated features |\n",
    "| **High dimensions** | ‚úÖ Essential | Dimensionality reduction |\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "1. **Medical Diagnosis**: Limited patient data, many test results ‚Üí Use L2 regularization\n",
    "2. **Image Classification**: Deep neural networks with millions of parameters ‚Üí Use L2 + dropout\n",
    "3. **Gene Expression Analysis**: Few samples, thousands of genes ‚Üí Use L1 for feature selection\n",
    "4. **Text Classification**: High-dimensional sparse features ‚Üí Use L1 or L2\n",
    "5. **Linear Regression with Polynomial Features**: High-degree polynomials ‚Üí Use L2 (Ridge)\n",
    "\n",
    "### The Bottom Line:\n",
    "\n",
    "**Use regularization when:**\n",
    "- You observe overfitting (train >> test performance)\n",
    "- You have high-dimensional features\n",
    "- You're using polynomial or interaction features\n",
    "\n",
    "**Don't use regularization when:**\n",
    "- Model is underfitting\n",
    "- Data is simple and plentiful\n",
    "- Training performance is already poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from scipy.special import expit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Classification Dataset\n\nWe'll use the same mixture dataset from the lecture slides (also used in the Logistic Regression lab). This dataset contains two classes with non-linear separation, making it perfect for demonstrating:\n1. How polynomial features can capture non-linearity\n2. How high-degree polynomials lead to overfitting\n3. How regularization prevents overfitting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download the mixture dataset from Google Drive\n# File ID: 1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT\n# Direct download URL\nurl = 'https://drive.google.com/uc?id=1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT'\n\n# Load data\ndf = pd.read_csv(url)\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(df.head())\nprint(f\"\\nColumn names: {df.columns.tolist()}\")\n\n# Extract features and labels (last column is the label)\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nprint(f\"\\nFeatures shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\nprint(f\"Class distribution: {np.bincount(y.astype(int))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure(figsize=(10, 8))\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Class 0', \n            edgecolors='k', s=50, alpha=0.7)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='skyblue', label='Class 1', \n            edgecolors='k', s=50, alpha=0.7)\nplt.xlabel('$x_1$', fontsize=14)\nplt.ylabel('$x_2$', fontsize=14)\nplt.title('Mixture Dataset (Non-Linear Boundary)', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ‚úÖ Checkpoint Question 1: Why can't linear logistic regression solve this problem?\n\nA) Linear logistic regression can only create straight-line decision boundaries, but our classes are separated by a non-linear curve that cannot be captured by a straight line\n\nB) Linear logistic regression requires all features to be normalized to zero mean and unit variance before training, but our dataset has unnormalized features\n\nC) Linear logistic regression can only handle datasets with exactly two features and fails when applied to higher-dimensional feature spaces with three or more variables\n\nD) Linear logistic regression uses gradient descent which cannot converge properly on non-linear data patterns, always getting stuck in local minima during optimization\n\n<details>\n<summary>Click to see answer</summary>\n\n**Answer: A**\n\n**Key Insight:** Linear logistic regression creates decision boundaries of the form $w_0 + w_1x_1 + w_2x_2 = 0$, which is always a straight line in 2D (or hyperplane in higher dimensions). Our dataset requires a curved boundary that cannot be expressed as a linear combination of $x_1$ and $x_2$ alone.\n\n**Detailed Explanation:**\n\nA linear classifier can only create decision boundaries that are:\n- **2D**: Straight lines\n- **3D**: Flat planes  \n- **Higher dimensions**: Hyperplanes\n\nFor our mixture dataset:\n- The two classes are intermingled in a non-linear pattern\n- Optimal boundary requires curves or complex shapes\n- A straight line cannot separate the classes effectively\n\n**Visual example:**\n```\nLinear boundary:        Needed boundary:\n   |  O O O               O O O\n   |O  X  O            O    ‚ü≤    O\n---*------          O    ‚ü≤  X  O\n X | X O             O  X  ‚ü≤  O\n  X|X O                 O O O\n```\n\n**Solution:** Use polynomial features to create non-linear terms:\n- Add $x_1^2, x_2^2, x_1x_2$ to features\n- Now the model can learn: $w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2 + w_5x_1x_2 = 0$\n- This allows curved and complex decision boundaries\n\n**Why other answers are incorrect:**\n\n- **B is FALSE**: While feature normalization is a best practice for gradient descent convergence, it's not why linear models fail on this problem. Even with perfectly normalized features, a straight line cannot separate non-linearly distributed classes. Normalization helps with optimization speed and stability, but doesn't change the fundamental limitation of linear boundaries.\n\n- **C is FALSE**: Linear logistic regression works with any number of features (2, 3, 100, 10000, etc.). The limitation is not the number of features but the type of decision boundary (linear vs non-linear). In fact, we'll solve this problem by *adding* features (polynomial terms) while still using logistic regression.\n\n- **D is FALSE**: Gradient descent converges just fine on non-linear data. The negative log-likelihood loss function for logistic regression is convex, meaning there are no local minima - gradient descent will always reach the global optimum. The issue is that this optimum corresponds to the best *linear* boundary, which still can't separate non-linearly distributed classes effectively.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Demonstrate Overfitting with Polynomial Features\n",
    "\n",
    "In this exercise, we'll:\n",
    "1. Apply polynomial feature transformation to our data\n",
    "2. Train logistic regression models with different polynomial degrees\n",
    "3. Observe how higher degrees lead to overfitting (high train accuracy, low test accuracy)\n",
    "\n",
    "**What you'll implement:**\n",
    "- Complete logistic regression class (from previous labs)\n",
    "- Train models with degrees 1, 2, 4, 6, 8\n",
    "- Compare training vs test accuracy to identify overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class LogisticRegressionRegularized(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    Logistic Regression with L2 regularization.\n    \n    Parameters\n    ----------\n    learning_rate : float, default=0.01\n        Step size for gradient descent\n    max_iter : int, default=1000  \n        Maximum number of iterations\n    lambda_reg : float, default=0.0\n        L2 regularization strength (Œª ‚â• 0)\n    tol : float, default=1e-6\n        Convergence tolerance\n    random_state : int, default=None\n        Random seed for weight initialization\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, max_iter=1000, lambda_reg=0.0, \n                 tol=1e-6, random_state=None):\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.lambda_reg = lambda_reg\n        self.tol = tol\n        self.random_state = random_state\n    \n    def _sigmoid(self, z):\n        \"\"\"Sigmoid function using scipy.special.expit for numerical stability.\"\"\"\n        # TODO: Implement sigmoid using expit\n        # Hint: return expit(z)\n        return None\n    \n    def _compute_gradient(self, Phi, y, probabilities):\n        \"\"\"\n        Compute gradient with L2 regularization.\n        \n        Gradient = Œ¶·µÄ(p - y) + 2Œªw\n        \n        Note: We don't regularize the bias term (first weight)\n        \"\"\"\n        # TODO: Compute base gradient (without regularization)\n        # Hint: Œ¶·µÄ @ (probabilities - y)\n        gradient = None\n        \n        # TODO: Add L2 regularization term to gradient\n        # Hint: Create a regularization vector with bias=0\n        #       reg_weights = self.weights_.copy()\n        #       reg_weights[0] = 0  # Don't regularize bias\n        #       gradient += 2 * self.lambda_reg * reg_weights\n        if self.lambda_reg > 0:\n            pass  # Add regularization here\n        \n        return gradient\n    \n    def fit(self, X, y):\n        \"\"\"Fit the model using gradient descent.\"\"\"\n        # Create design matrix\n        Phi = np.c_[np.ones(X.shape[0]), X]\n        \n        # Initialize weights\n        if self.random_state is not None:\n            np.random.seed(self.random_state)\n        self.weights_ = np.random.randn(Phi.shape[1]) * 0.01\n        \n        # Loss history\n        self.loss_history_ = []\n        \n        # Gradient descent\n        for iteration in range(self.max_iter):\n            # TODO: Compute probabilities\n            # Step 1: scores = Phi @ self.weights_\n            # Step 2: probabilities = self._sigmoid(scores)\n            scores = None\n            probabilities = None\n            \n            # TODO: Compute NLL loss (with regularization)\n            epsilon = 1e-15\n            p_safe = np.clip(probabilities, epsilon, 1 - epsilon)\n            nll = -np.sum(y * np.log(p_safe) + (1 - y) * np.log(1 - p_safe))\n            \n            # Add L2 penalty to loss (don't regularize bias)\n            # Loss uses Œª||w||¬≤ (without the 1/2 factor)\n            if self.lambda_reg > 0:\n                nll += self.lambda_reg * np.sum(self.weights_[1:]**2)\n            \n            self.loss_history_.append(nll)\n            \n            # TODO: Compute gradient using _compute_gradient\n            gradient = None\n            \n            # Check convergence\n            if np.linalg.norm(gradient) < self.tol:\n                break\n            \n            # TODO: Update weights\n            # Hint: self.weights_ = self.weights_ - self.learning_rate * gradient\n            pass\n        \n        self.n_iter_ = iteration + 1\n        return self\n    \n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities.\"\"\"\n        Phi = np.c_[np.ones(X.shape[0]), X]\n        scores = Phi @ self.weights_\n        p1 = self._sigmoid(scores)\n        return np.column_stack([1 - p1, p1])\n    \n    def predict(self, X):\n        \"\"\"Predict class labels.\"\"\"\n        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 1: Demonstrating Overfitting with Polynomial Features\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test different polynomial degrees WITHOUT regularization\n",
    "degrees = [1, 2, 4, 6, 8]\n",
    "results_no_reg = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    print(f\"\\nTesting degree {degree}...\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_poly = scaler.fit_transform(X_train_poly)\n",
    "    X_test_poly = scaler.transform(X_test_poly)\n",
    "    \n",
    "    # Train model WITHOUT regularization (lambda=0)\n",
    "    model = LogisticRegressionRegularized(\n",
    "        learning_rate=0.1,\n",
    "        max_iter=2000,\n",
    "        lambda_reg=0.0,  # No regularization!\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_poly))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_poly))\n",
    "    \n",
    "    results_no_reg[degree] = {\n",
    "        'poly': poly,\n",
    "        'scaler': scaler,\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'n_features': X_train_poly.shape[1],\n",
    "        'weight_magnitude': np.linalg.norm(model.weights_)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Features: {X_train_poly.shape[1]}\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
    "    print(f\"  Overfitting Gap: {train_acc - test_acc:.3f}\")\n",
    "    print(f\"  Weight Magnitude: {np.linalg.norm(model.weights_):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OBSERVATION: Notice how higher degrees have:\")\n",
    "print(\"  1. Higher training accuracy (good fit to training data)\")\n",
    "print(\"  2. Lower test accuracy (poor generalization)\")\n",
    "print(\"  3. Larger weight magnitudes (complex models)\")\n",
    "print(\"  4. Larger overfitting gap (train_acc - test_acc)\")\n",
    "print(\"\\nThis is OVERFITTING! Regularization will help.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Overfitting: Training vs Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for plotting\n",
    "train_accs = [results_no_reg[d]['train_acc'] for d in degrees]\n",
    "test_accs = [results_no_reg[d]['test_acc'] for d in degrees]\n",
    "n_features_list = [results_no_reg[d]['n_features'] for d in degrees]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Accuracy vs Polynomial Degree\n",
    "ax = axes[0]\n",
    "ax.plot(degrees, train_accs, 'o-', linewidth=3, markersize=10, label='Training Accuracy', color='green')\n",
    "ax.plot(degrees, test_accs, 's-', linewidth=3, markersize=10, label='Test Accuracy', color='red')\n",
    "ax.fill_between(degrees, train_accs, test_accs, alpha=0.2, color='gray', label='Overfitting Gap')\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=14)\n",
    "ax.set_ylabel('Accuracy', fontsize=14)\n",
    "ax.set_title('Overfitting: Training vs Test Accuracy (Œª=0)', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(degrees)\n",
    "\n",
    "# Plot 2: Accuracy vs Number of Features\n",
    "ax = axes[1]\n",
    "ax.plot(n_features_list, train_accs, 'o-', linewidth=3, markersize=10, label='Training Accuracy', color='green')\n",
    "ax.plot(n_features_list, test_accs, 's-', linewidth=3, markersize=10, label='Test Accuracy', color='red')\n",
    "ax.set_xlabel('Number of Features', fontsize=14)\n",
    "ax.set_ylabel('Accuracy', fontsize=14)\n",
    "ax.set_title('Model Complexity vs Accuracy (Œª=0)', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Checkpoint Question 2: What indicates overfitting in the plots above?\n",
    "\n",
    "A) Training and test accuracy both increase together as polynomial degree increases, showing the model is learning meaningful patterns from the data\n",
    "B) Training accuracy increases while test accuracy decreases as degree increases, indicating the model memorizes training data but fails to generalize\n",
    "C) Both training and test accuracy remain constant across all polynomial degrees, suggesting the model has reached its optimal performance level\n",
    "D) Test accuracy is consistently higher than training accuracy across all degrees, demonstrating excellent generalization to unseen data samples\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Key Insight:** The hallmark of overfitting is a growing gap between training and test performance. As we add more polynomial features, the model gains flexibility to fit the training data more precisely (training accuracy ‚Üë), but this comes at the cost of fitting noise and training-specific patterns that don't generalize (test accuracy ‚Üì or stagnates).\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "Let's analyze what happens as polynomial degree increases:\n",
    "\n",
    "**Degree 1 (Linear):**\n",
    "- 2 features: $[x_1, x_2]$\n",
    "- Cannot fit circular boundary\n",
    "- Train accuracy: ~60%, Test accuracy: ~60%\n",
    "- Underfitting: model too simple\n",
    "\n",
    "**Degree 2 (Quadratic):**\n",
    "- 5 features: $[x_1, x_2, x_1^2, x_1x_2, x_2^2]$\n",
    "- Can fit circular boundary!\n",
    "- Train accuracy: ~90%, Test accuracy: ~88%\n",
    "- Good fit: slight gap is acceptable\n",
    "\n",
    "**Degree 6:**\n",
    "- 27 features\n",
    "- Train accuracy: ~98%, Test accuracy: ~82%\n",
    "- **Overfitting!** Gap = 16%\n",
    "\n",
    "**Degree 8:**\n",
    "- 44 features  \n",
    "- Train accuracy: ~99%, Test accuracy: ~78%\n",
    "- **Severe overfitting!** Gap = 21%\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "High-degree polynomials create very flexible decision boundaries that can:\n",
    "1. Perfectly wrap around every training point\n",
    "2. Fit to random noise in training data\n",
    "3. Create complex, wiggly boundaries that don't reflect the true pattern\n",
    "\n",
    "Example with 1 point slightly off-pattern:\n",
    "```\n",
    "Degree 2 (good):       Degree 8 (overfitting):\n",
    "   O O O                 O O O\n",
    " O       O             O    X  O  ‚Üê boundary bends\n",
    "O    X    O           O          O    to capture\n",
    " O  X X  O             O  X X  O     this outlier\n",
    "   O O O                 O O O\n",
    "```\n",
    "\n",
    "**Visual indicators of overfitting:**\n",
    "- ‚ÜóÔ∏è Training accuracy keeps increasing\n",
    "- ‚ÜòÔ∏è Test accuracy starts decreasing (or stops improving)\n",
    "- üìè Growing gap between the two curves\n",
    "- üéØ Model achieves near-perfect training accuracy but poor test accuracy\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **A is FALSE**: If both accuracies increased together, that would indicate good generalization (the ideal scenario). We want both to improve together. When they diverge, with training accuracy increasing and test accuracy plateauing or decreasing, that's when we have overfitting.\n",
    "\n",
    "- **C is FALSE**: Constant accuracy across degrees would suggest:\n",
    "  1. The model can't benefit from added complexity (already optimal), OR\n",
    "  2. The features aren't helping (implementation problem)\n",
    "  \n",
    "  This is not what we observe. We clearly see training accuracy increasing with degree, indicating the model IS using the additional features - just not in a way that generalizes.\n",
    "\n",
    "- **D is FALSE**: Test accuracy higher than training accuracy is extremely rare and usually indicates:\n",
    "  1. A lucky test set (not representative)\n",
    "  2. Data leakage (test data seen during training)\n",
    "  3. Very few samples\n",
    "  \n",
    "  This is the opposite of overfitting. With proper random splits, training accuracy should always be ‚â• test accuracy because the model is optimized on the training set.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Pseudocode for L2 Regularisation\n\nBefore implementing L2 regularization, let's understand the algorithm structure:\n\n### Helper Function: Exclude Intercept from Regularization\n\n```\nFunction REG_VECTOR(w)\n    r ‚Üê copy(w)\n    r[0] ‚Üê 0           # Don't regularize intercept\n    Return r\nEnd\n```\n\n**Why?** The intercept (bias term) just shifts the decision boundary and shouldn't be penalized.\n\n### L2 Regularization Components\n\n**L2 Loss:** \n$$\\lambda \\cdot ||\\text{REG\\_VECTOR}(w)||^2 = \\lambda \\sum_{j=1}^{d} w_j^2$$\n\n**L2 Gradient:** \n$$2\\lambda \\cdot \\text{REG\\_VECTOR}(w)$$\n\nNote: The gradient is $2\\lambda w$ because we differentiate $\\lambda w^2$ to get $2\\lambda w$.\n\n### Main Optimization Loop\n\n```\n# Inputs:\n#   base_grad(w)  ‚Üê gradient of base loss (no regularization)\n#   w0            ‚Üê initial parameters\n#   Œ∑             ‚Üê learning rate\n#   max_iter      ‚Üê maximum iterations\n#   tol           ‚Üê stop when ||g|| ‚â§ tol\n#   Œª ‚â• 0         ‚Üê L2 strength (intercept w[0] never penalized)\n\nw ‚Üê w0\n\nFOR t = 1 TO max_iter DO\n    g_base ‚Üê base_grad(w)                    # Gradient of base loss\n    g_reg ‚Üê 2 ¬∑ Œª ¬∑ REG_VECTOR(w)           # L2 gradient (no intercept)\n    g ‚Üê g_base + g_reg                       # Total gradient\n    \n    IF norm(g) ‚â§ tol THEN BREAK             # Convergence check\n    \n    w ‚Üê w ‚àí Œ∑ ¬∑ g                            # Update weights\nEND FOR\n\nReturn w\n```\n\n### Key Points\n\n1. **Base gradient**: $\\Phi^T(p - y)$ (from negative log-likelihood)\n2. **L2 gradient**: $2\\lambda w$ (from regularization term)\n3. **Total gradient**: $\\Phi^T(p - y) + 2\\lambda w$ (excluding intercept)\n4. **Weight update**: $w \\leftarrow w - \\eta \\cdot (\\Phi^T(p - y) + 2\\lambda w)$\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: Implement L2 Regularization\n\nNow we'll fix the overfitting problem by implementing L2 regularization!\n\n**What you need to do:**\n\nGo back to the `LogisticRegressionRegularized` class above and complete the TODO sections:\n\n1. **`_sigmoid()`**: Implement using `expit(z)`\n2. **`_compute_gradient()`**: Add the L2 regularization term `2Œªw` to the gradient\n   - Base gradient: `Œ¶·µÄ(p - y)`\n   - Regularization: `+ 2Œªw`\n   - Important: Don't regularize the bias term (first weight)\n3. **`fit()`**: Complete the gradient descent loop\n   - Compute scores and probabilities\n   - Compute gradient using `_compute_gradient()`\n   - Update weights: `w = w - Œ± * gradient`\n\n**Then run the cell below to test your implementation!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 2 VERIFICATION: Testing L2 Regularization Implementation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test on degree 6 polynomial with and without regularization\n",
    "degree_test = 6\n",
    "lambda_test = 1.0\n",
    "\n",
    "# Create polynomial features\n",
    "poly_test = PolynomialFeatures(degree=degree_test, include_bias=False)\n",
    "X_train_poly_test = poly_test.fit_transform(X_train)\n",
    "X_test_poly_test = poly_test.transform(X_test)\n",
    "\n",
    "# Standardize\n",
    "scaler_test = StandardScaler()\n",
    "X_train_poly_test = scaler_test.fit_transform(X_train_poly_test)\n",
    "X_test_poly_test = scaler_test.transform(X_test_poly_test)\n",
    "\n",
    "# Train WITHOUT regularization\n",
    "model_no_reg_test = LogisticRegressionRegularized(\n",
    "    learning_rate=0.1, max_iter=2000, lambda_reg=0.0, random_state=42\n",
    ")\n",
    "model_no_reg_test.fit(X_train_poly_test, y_train)\n",
    "train_acc_no_reg = accuracy_score(y_train, model_no_reg_test.predict(X_train_poly_test))\n",
    "test_acc_no_reg = accuracy_score(y_test, model_no_reg_test.predict(X_test_poly_test))\n",
    "\n",
    "# Train WITH regularization\n",
    "model_reg_test = LogisticRegressionRegularized(\n",
    "    learning_rate=0.1, max_iter=2000, lambda_reg=lambda_test, random_state=42\n",
    ")\n",
    "model_reg_test.fit(X_train_poly_test, y_train)\n",
    "train_acc_reg = accuracy_score(y_train, model_reg_test.predict(X_train_poly_test))\n",
    "test_acc_reg = accuracy_score(y_test, model_reg_test.predict(X_test_poly_test))\n",
    "\n",
    "print(f\"\\nTesting with Degree {degree_test} Polynomial:\")\n",
    "print(\"\\nWithout Regularization (Œª=0):\")\n",
    "print(f\"  Train Accuracy: {train_acc_no_reg:.3f}\")\n",
    "print(f\"  Test Accuracy:  {test_acc_no_reg:.3f}\")\n",
    "print(f\"  Overfitting Gap: {train_acc_no_reg - test_acc_no_reg:.3f}\")\n",
    "print(f\"  Weight Magnitude: {np.linalg.norm(model_no_reg_test.weights_):.2f}\")\n",
    "\n",
    "print(f\"\\nWith L2 Regularization (Œª={lambda_test}):\")\n",
    "print(f\"  Train Accuracy: {train_acc_reg:.3f}\")\n",
    "print(f\"  Test Accuracy:  {test_acc_reg:.3f}\")\n",
    "print(f\"  Overfitting Gap: {train_acc_reg - test_acc_reg:.3f}\")\n",
    "print(f\"  Weight Magnitude: {np.linalg.norm(model_reg_test.weights_):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ SUCCESS CRITERIA:\")\n",
    "print(\"  1. Test accuracy should IMPROVE with regularization\")\n",
    "print(\"  2. Overfitting gap should DECREASE with regularization\")\n",
    "print(\"  3. Weight magnitude should be SMALLER with regularization\")\n",
    "print(\"\\nIf your implementation is correct, you should see:\")\n",
    "print(\"  ‚Ä¢ Test accuracy increases (e.g., 0.78 ‚Üí 0.87)\")\n",
    "print(\"  ‚Ä¢ Overfitting gap shrinks (e.g., 0.21 ‚Üí 0.05)\")\n",
    "print(\"  ‚Ä¢ Weight magnitude decreases (e.g., 50 ‚Üí 15)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ‚úÖ Checkpoint Question 3: How does L2 regularization prevent overfitting?\n\nA) L2 regularization adds a penalty term proportional to the squared magnitude of weights to the loss function, forcing the model to keep weights small unless they significantly improve the fit, which results in simpler and smoother decision boundaries that generalize better to unseen data\n\nB) L2 regularization removes features with small weights from the model entirely by setting their coefficients to exactly zero, performing automatic feature selection during training to reduce model complexity\n\nC) L2 regularization increases the learning rate during gradient descent optimization, allowing the model to converge faster to the global minimum and avoid getting trapped in local minima\n\nD) L2 regularization adds random noise to the training data during each iteration, preventing the model from memorizing specific training examples and forcing it to learn more robust patterns\n\n<details>\n<summary>Click to see answer</summary>\n\n**Answer: A**\n\n**Key Insight:** L2 regularization modifies the loss function to penalize large weights, creating a trade-off between fitting the training data and keeping the model simple. The modified loss is: $J_{\\text{total}} = J_{\\text{NLL}} + \\lambda||w||^2$. This forces the optimization to balance data fit with weight magnitude, resulting in smoother decision boundaries that generalize better.\n\n**Detailed Explanation:**\n\n**Without L2 regularization ($\\lambda = 0$):**\n- Loss function: $J = -\\sum [y \\log p + (1-y)\\log(1-p)]$\n- Goal: Minimize NLL only\n- Weights can grow arbitrarily large to perfectly fit training data\n- Result: Complex, wiggly boundaries that overfit\n\n**With L2 regularization ($\\lambda > 0$):**\n- Loss function: $J = -\\sum [y \\log p + (1-y)\\log(1-p)] + \\lambda\\sum w_j^2$\n- Goal: Minimize NLL AND keep weights small\n- Large weights are penalized quadratically\n- Result: Simpler boundaries, better generalization\n\n**Example with numbers:**\n\nConsider a model trying to fit a training point:\n```\nWithout regularization (Œª=0):\n- Can use weights: w = [50, -45, 38] (magnitude = 77)\n- Training loss: 0.01 (perfect fit)\n- Test performance: poor (overfits)\n\nWith regularization (Œª=1):\n- Forced to use: w = [2, -1.5, 1.2] (magnitude = 2.7)\n- Training loss: 0.15 (good but not perfect fit)\n- Test performance: good (generalizes)\n- Total loss: 0.15 + 1*(2.7¬≤) = 0.15 + 7.3 = 7.45\n```\n\n**How gradient descent changes with L2:**\n\nStandard gradient: $\\nabla J = \\Phi^T(p - y)$\n\nWith L2: $\\nabla J = \\Phi^T(p - y) + 2\\lambda w$\n\nUpdate rule: $w \\leftarrow w - \\alpha[\\Phi^T(p-y) + 2\\lambda w] = (1-2\\alpha\\lambda)w - \\alpha\\Phi^T(p-y)$\n\nThe term $(1-2\\alpha\\lambda)w$ shrinks weights at each step (weight decay)!\n\n**Visual interpretation:**\n```\nWithout regularization:      With regularization:\n      O O O                      O O O\n    O  ‚Üê  O                    O       O\n   O   ‚Üì   O                  O    X    O\n  O ‚Üê X ‚Üí O                  O    X X   O\n   O   ‚Üë  O                     O  X X O\n    O   O                         O O O\n   (wiggly, fits                (smooth,\n    every point)               generalizes)\n```\n\n**Effect of Œª:**\n- Œª = 0: No regularization, may overfit\n- Œª small (0.01): Light penalty, slight smoothing\n- Œª medium (1.0): Balanced, often optimal\n- Œª large (100): Heavy penalty, may underfit (weights too small)\n\n**Why other answers are incorrect:**\n\n- **B is FALSE**: This describes L1 (Lasso) regularization, not L2 (Ridge). L1 uses $\\lambda \\sum |w_j|$ which has a gradient of $\\lambda \\cdot \\text{sign}(w)$, producing exactly zero weights (sparse solutions). L2 uses $\\lambda \\sum w_j^2$ with gradient $2\\lambda w$, which only asymptotically approaches zero but never reaches it. L2 shrinks all weights but doesn't perform feature selection.\n\n- **C is FALSE**: L2 regularization does not affect the learning rate $\\alpha$. The learning rate is a separate hyperparameter that controls step size in gradient descent. L2 regularization modifies the gradient itself by adding $2\\lambda w$, but the learning rate remains the same. Increasing the learning rate could actually make convergence worse, not better.\n\n- **D is FALSE**: This describes data augmentation or dropout, not L2 regularization. L2 regularization is a deterministic modification to the loss function - no randomness involved. The training data remains unchanged; only the optimization objective changes. Dropout (random neuron deactivation) and data augmentation (adding noisy copies) are different regularization techniques used primarily in neural networks.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Hyperparameter Tuning - Finding Optimal Œª\n",
    "\n",
    "We've seen that regularization helps, but how do we choose the best value of Œª?\n",
    "\n",
    "**The process:**\n",
    "1. Define a range of Œª values to test (e.g., 0.001, 0.01, 0.1, 1, 10)\n",
    "2. For each Œª, train a model and evaluate using cross-validation\n",
    "3. Select the Œª that gives the best validation performance\n",
    "4. Retrain final model on all training data with optimal Œª\n",
    "5. Evaluate on held-out test set\n",
    "\n",
    "**Why cross-validation?**\n",
    "- Using test set for Œª selection would leak information\n",
    "- CV uses only training data to estimate generalization\n",
    "- Test set remains untouched until final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 3: Finding Optimal Œª via Cross-Validation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use degree 6 polynomial (we know this overfits without regularization)\n",
    "degree = 6\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_poly = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly = scaler.transform(X_test_poly)\n",
    "\n",
    "# Test different Œª values\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0]\n",
    "cv_results = {}\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nUsing degree {degree} polynomial ({X_train_poly.shape[1]} features)\")\n",
    "print(\"Testing Œª values with 5-fold cross-validation...\\n\")\n",
    "\n",
    "for lambda_val in lambdas:\n",
    "    print(f\"Œª = {lambda_val:6.3f} \", end=\"\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train_poly):\n",
    "        X_cv_train = X_train_poly[train_idx]\n",
    "        y_cv_train = y_train.iloc[train_idx] if hasattr(y_train, 'iloc') else y_train[train_idx]\n",
    "        X_cv_val = X_train_poly[val_idx]\n",
    "        y_cv_val = y_train.iloc[val_idx] if hasattr(y_train, 'iloc') else y_train[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegressionRegularized(\n",
    "            learning_rate=0.1,\n",
    "            max_iter=2000,\n",
    "            lambda_reg=lambda_val,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_cv_train, y_cv_train)\n",
    "        \n",
    "        # Validate\n",
    "        val_score = accuracy_score(y_cv_val, model.predict(X_cv_val))\n",
    "        cv_scores.append(val_score)\n",
    "    \n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    std_cv_score = np.std(cv_scores)\n",
    "    \n",
    "    # Train on full training set to get train/test accuracy\n",
    "    model_full = LogisticRegressionRegularized(\n",
    "        learning_rate=0.1,\n",
    "        max_iter=2000,\n",
    "        lambda_reg=lambda_val,\n",
    "        random_state=42\n",
    "    )\n",
    "    model_full.fit(X_train_poly, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, model_full.predict(X_train_poly))\n",
    "    test_acc = accuracy_score(y_test, model_full.predict(X_test_poly))\n",
    "    \n",
    "    cv_results[lambda_val] = {\n",
    "        'cv_mean': mean_cv_score,\n",
    "        'cv_std': std_cv_score,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'model': model_full\n",
    "    }\n",
    "    \n",
    "    print(f\"‚Üí CV: {mean_cv_score:.3f} (¬±{std_cv_score:.3f}), \"\n",
    "          f\"Train: {train_acc:.3f}, Test: {test_acc:.3f}\")\n",
    "\n",
    "# Find optimal Œª\n",
    "optimal_lambda = max(cv_results, key=lambda k: cv_results[k]['cv_mean'])\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚úÖ OPTIMAL Œª = {optimal_lambda}\")\n",
    "print(f\"   Cross-validation score: {cv_results[optimal_lambda]['cv_mean']:.3f}\")\n",
    "print(f\"   Test accuracy: {cv_results[optimal_lambda]['test_acc']:.3f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Œª Tuning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "lambda_vals = list(cv_results.keys())\n",
    "cv_means = [cv_results[l]['cv_mean'] for l in lambda_vals]\n",
    "cv_stds = [cv_results[l]['cv_std'] for l in lambda_vals]\n",
    "train_accs = [cv_results[l]['train_acc'] for l in lambda_vals]\n",
    "test_accs = [cv_results[l]['test_acc'] for l in lambda_vals]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: CV score vs Œª (log scale)\n",
    "ax = axes[0]\n",
    "ax.errorbar(lambda_vals, cv_means, yerr=cv_stds, fmt='o-', linewidth=2, \n",
    "            markersize=8, capsize=5, label='CV Score ¬± Std')\n",
    "ax.axvline(optimal_lambda, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Optimal Œª={optimal_lambda}')\n",
    "ax.set_xscale('symlog', linthresh=0.001)\n",
    "ax.set_xlabel('Regularization Strength (Œª)', fontsize=14)\n",
    "ax.set_ylabel('Cross-Validation Accuracy', fontsize=14)\n",
    "ax.set_title('Hyperparameter Tuning: Finding Optimal Œª', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Train vs Test accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(lambda_vals, train_accs, 'o-', linewidth=2, markersize=8, \n",
    "        label='Training Accuracy', color='green')\n",
    "ax.plot(lambda_vals, test_accs, 's-', linewidth=2, markersize=8, \n",
    "        label='Test Accuracy', color='red')\n",
    "ax.axvline(optimal_lambda, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Optimal Œª={optimal_lambda}')\n",
    "ax.set_xscale('symlog', linthresh=0.001)\n",
    "ax.set_xlabel('Regularization Strength (Œª)', fontsize=14)\n",
    "ax.set_ylabel('Accuracy', fontsize=14)\n",
    "ax.set_title('Bias-Variance Tradeoff', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Checkpoint Question 4: How does Œª affect the bias-variance tradeoff?\n",
    "\n",
    "A) Increasing Œª decreases both bias and variance simultaneously, creating models that fit training data perfectly while also generalizing excellently to test data\n",
    "B) Increasing Œª increases bias and decreases variance, trading some training accuracy for better generalization by constraining model flexibility and preventing overfitting\n",
    "C) Increasing Œª decreases bias and increases variance, allowing the model to fit complex patterns in the training data but at the cost of generalization\n",
    "D) The value of Œª has no effect on bias or variance, it only controls the learning rate convergence speed during gradient descent optimization\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Key Insight:** Regularization strength Œª controls the bias-variance tradeoff. Larger Œª forces simpler models (higher bias, lower variance), while smaller Œª allows more complex models (lower bias, higher variance). The optimal Œª balances these two sources of error to minimize total error.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "**Bias-Variance Decomposition:**\n",
    "\n",
    "Total Error = Bias¬≤ + Variance + Irreducible Error\n",
    "\n",
    "- **Bias**: Error from wrong assumptions (model too simple)\n",
    "- **Variance**: Error from sensitivity to training data (model too complex)\n",
    "- **Irreducible**: Noise in data (can't be reduced)\n",
    "\n",
    "**How Œª affects bias and variance:**\n",
    "\n",
    "**Œª = 0 (No regularization):**\n",
    "```\n",
    "- Bias: LOW (flexible model can fit true pattern)\n",
    "- Variance: HIGH (fits noise, changes drastically with different training sets)\n",
    "- Training accuracy: 99% (overfits)\n",
    "- Test accuracy: 78% (doesn't generalize)\n",
    "- Decision boundary: Wiggly, complex\n",
    "```\n",
    "\n",
    "**Œª = 1.0 (Moderate regularization):**\n",
    "```\n",
    "- Bias: MEDIUM (some flexibility to fit pattern)\n",
    "- Variance: MEDIUM (stable across training sets)\n",
    "- Training accuracy: 92% (doesn't overfit)\n",
    "- Test accuracy: 88% (generalizes well)\n",
    "- Decision boundary: Smooth, simple\n",
    "```\n",
    "\n",
    "**Œª = 100 (Heavy regularization):**\n",
    "```\n",
    "- Bias: HIGH (model too constrained, can't fit pattern)\n",
    "- Variance: LOW (very stable, but consistently wrong)\n",
    "- Training accuracy: 65% (underfits)\n",
    "- Test accuracy: 63% (too simple)\n",
    "- Decision boundary: Nearly linear\n",
    "```\n",
    "\n",
    "**Visual representation:**\n",
    "```\n",
    "Error ^\n",
    "      |\n",
    "      |     Variance\n",
    "      |      /\n",
    "      |     /\n",
    "      |    /---___\n",
    "      |   /       ---___  Total Error\n",
    "      |  /              ---___\n",
    "      | /                     ---___\n",
    "      |/___________________________---___ Bias¬≤\n",
    "      +---------------------------------> Œª\n",
    "      0     optimal Œª              ‚àû\n",
    "```\n",
    "\n",
    "**Why increasing Œª increases bias:**\n",
    "\n",
    "Larger Œª penalizes weights more heavily, forcing them toward zero:\n",
    "- Reduces model capacity to fit complex patterns\n",
    "- Creates simpler, more restricted decision boundaries\n",
    "- May not capture the true underlying pattern (systematic error)\n",
    "\n",
    "Example: True boundary is a circle, but high Œª forces nearly linear boundary ‚Üí bias!\n",
    "\n",
    "**Why increasing Œª decreases variance:**\n",
    "\n",
    "Larger Œª constrains how much weights can change:\n",
    "- Model is less sensitive to specific training examples\n",
    "- Small changes in training data ‚Üí small changes in learned weights\n",
    "- More stable across different random samples\n",
    "\n",
    "Example: With high Œª, adding/removing a few training points barely changes the boundary ‚Üí low variance!\n",
    "\n",
    "**Finding optimal Œª:**\n",
    "\n",
    "```\n",
    "Œª too small ‚Üí overfit (low bias, high variance)\n",
    "   ‚îú‚îÄ Training accuracy very high\n",
    "   ‚îî‚îÄ Test accuracy low\n",
    "\n",
    "Œª optimal ‚Üí balanced (medium bias, medium variance)\n",
    "   ‚îú‚îÄ Training accuracy good\n",
    "   ‚îî‚îÄ Test accuracy good (minimizes total error)\n",
    "\n",
    "Œª too large ‚Üí underfit (high bias, low variance)\n",
    "   ‚îú‚îÄ Training accuracy low\n",
    "   ‚îî‚îÄ Test accuracy low\n",
    "```\n",
    "\n",
    "**Mathematical intuition:**\n",
    "\n",
    "Without regularization: $\\min_w J(w)$\n",
    "- Solution can use large weights\n",
    "- Fits training data closely (low bias)\n",
    "- Changes drastically with different data (high variance)\n",
    "\n",
    "With regularization: $\\min_w [J(w) + \\frac{\\lambda}{2}||w||^2]$\n",
    "- Solution constrained to small weights\n",
    "- Can't fit training data as closely (higher bias)\n",
    "- More stable across datasets (lower variance)\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **A is FALSE**: This is the \"free lunch\" scenario that doesn't exist in machine learning. You cannot simultaneously decrease both bias and variance - there's always a tradeoff. Regularization helps generalization by accepting some bias (training error) to reduce variance (sensitivity to training data). If we could decrease both, we'd always use maximum regularization!\n",
    "\n",
    "- **C is FALSE**: This is backwards. Increasing Œª DECREASES model complexity (by penalizing large weights), which INCREASES bias and DECREASES variance. The description in C (decreases bias, increases variance) is what happens when you DECREASE Œª or add more features without regularization.\n",
    "\n",
    "- **D is FALSE**: Œª fundamentally affects model capacity and behavior, not optimization speed. The learning rate Œ± controls gradient descent convergence speed, while Œª controls the bias-variance tradeoff. These are independent hyperparameters with different purposes. You can have Œª=0 (no regularization) with any learning rate, and Œª=10 (heavy regularization) with any learning rate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundaries: Effect of Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select interesting Œª values to visualize\n",
    "lambdas_to_plot = [0, 0.01, 0.1, 1.0, 10.0, 50.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Data ranges for mesh\n",
    "x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "for idx, lambda_val in enumerate(lambdas_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get or train model\n",
    "    if lambda_val in cv_results:\n",
    "        model = cv_results[lambda_val]['model']\n",
    "        test_acc = cv_results[lambda_val]['test_acc']\n",
    "    else:\n",
    "        model = LogisticRegressionRegularized(\n",
    "            learning_rate=0.1, max_iter=2000, lambda_reg=lambda_val, random_state=42\n",
    "        )\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        test_acc = accuracy_score(y_test, model.predict(X_test_poly))\n",
    "    \n",
    "    # Create mesh\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                            np.linspace(x2_min, x2_max, 200))\n",
    "    X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    X_mesh_poly = poly.transform(X_mesh)\n",
    "    X_mesh_poly = scaler.transform(X_mesh_poly)\n",
    "    probs_mesh = model.predict_proba(X_mesh_poly)[:, 1].reshape(xx1.shape)\n",
    "    \n",
    "    # Plot contours\n",
    "    ax.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
    "    ax.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2.5)\n",
    "    \n",
    "    # Plot data\n",
    "    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
    "               c='orange', edgecolors='k', s=40, alpha=0.7, label='Class 0 (train)')\n",
    "    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
    "               c='skyblue', edgecolors='k', s=40, alpha=0.7, label='Class 1 (train)')\n",
    "    \n",
    "    # Determine if this is optimal\n",
    "    is_optimal = \"‚úÖ OPTIMAL\" if lambda_val == optimal_lambda else \"\"\n",
    "    \n",
    "    ax.set_title(f'Œª={lambda_val}, Test Acc={test_acc:.2f} {is_optimal}', \n",
    "                 fontsize=13, fontweight='bold' if lambda_val == optimal_lambda else 'normal')\n",
    "    ax.set_xlabel('$x_1$', fontsize=11)\n",
    "    ax.set_ylabel('$x_2$', fontsize=11)\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim(x1_min, x1_max)\n",
    "    ax.set_ylim(x2_min, x2_max)\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=9, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Checkpoint Question 5: What happens to the decision boundary as Œª increases?\n",
    "\n",
    "A) The decision boundary becomes more complex and wiggly with more curves and details as the model fits individual training points more precisely\n",
    "B) The decision boundary becomes smoother and simpler, gradually approaching a nearly linear boundary as regularization constrains the weights toward zero\n",
    "C) The decision boundary remains completely unchanged regardless of lambda value because regularization only affects training speed not the final model shape\n",
    "D) The decision boundary becomes more circular and fits the true pattern better as regularization helps the model learn the underlying circular distribution\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Key Insight:** Increasing Œª constrains weights toward smaller values, which reduces the contribution of high-degree polynomial terms. This makes the decision boundary progressively simpler and smoother. At very high Œª, most polynomial terms are suppressed, and the boundary approaches linear.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "**Why boundaries become smoother with larger Œª:**\n",
    "\n",
    "Recall our decision boundary equation with degree 6 polynomial:\n",
    "$$w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 + \\ldots + w_{27}x_2^6 = 0$$\n",
    "\n",
    "**Œª = 0 (No regularization):**\n",
    "```\n",
    "w = [0.5, 2.1, -1.8, 15.3, -12.7, 18.9, ..., -45.2, 38.7]\n",
    "         (linear)    (quadratic)        (high-order)\n",
    "              ‚Üë            ‚Üë                  ‚Üë\n",
    "           small       medium            LARGE!\n",
    "```\n",
    "- High-degree terms have large weights\n",
    "- Decision boundary: $2.1x_1 - 1.8x_2 + 15.3x_1^2 - 12.7x_1x_2 + ... - 45.2x_1^3x_2^3 + ... = 0$\n",
    "- Result: Complex, wiggly boundary that bends around individual training points\n",
    "\n",
    "**Œª = 1.0 (Moderate regularization):**\n",
    "```\n",
    "w = [0.5, 2.0, -1.7, 3.2, -2.8, 3.5, ..., -0.8, 0.6]\n",
    "         (linear)   (quadratic)      (high-order)\n",
    "              ‚Üë            ‚Üë                ‚Üë\n",
    "          preserved    preserved        suppressed\n",
    "```\n",
    "- High-degree terms shrunk toward zero\n",
    "- Decision boundary: $2.0x_1 - 1.7x_2 + 3.2x_1^2 - 2.8x_1x_2 + 3.5x_2^2 + \\text{(tiny terms)} \\approx 0$\n",
    "- Result: Smooth circular/elliptical boundary (dominated by quadratic terms)\n",
    "\n",
    "**Œª = 100 (Heavy regularization):**\n",
    "```\n",
    "w = [0.5, 1.8, -1.6, 0.3, -0.2, 0.3, ..., -0.01, 0.008]\n",
    "         (linear)    (quadratic)       (high-order)\n",
    "              ‚Üë            ‚Üë                ‚Üë\n",
    "         dominant      weak           negligible\n",
    "```\n",
    "- All polynomial terms heavily suppressed\n",
    "- Decision boundary: $1.8x_1 - 1.6x_2 + \\text{(negligible terms)} \\approx 0$\n",
    "- Result: Nearly linear boundary (underfits circular pattern)\n",
    "\n",
    "**Visual progression:**\n",
    "```\n",
    "Œª=0:                Œª=1:              Œª=100:\n",
    "  O O O              O O O             O O O\n",
    "O  ‚ü≤  O            O       O         O   |   O\n",
    "O  ‚ü≤ ‚Üí O     ‚Üí    O    O    O   ‚Üí   O   |   O\n",
    " O ‚Üê ‚ü≤ O           O  O O  O         O  | O O\n",
    "  O O O              O O O             O O O\n",
    "(wiggly)          (smooth circle)    (almost line)\n",
    "```\n",
    "\n",
    "**Mathematical explanation:**\n",
    "\n",
    "Regularized loss: $J = \\text{NLL} + \\frac{\\lambda}{2}\\sum w_j^2$\n",
    "\n",
    "To minimize this, the model must balance:\n",
    "1. Fitting training data (low NLL)\n",
    "2. Keeping weights small (low $\\sum w_j^2$)\n",
    "\n",
    "As Œª increases:\n",
    "- The penalty term dominates\n",
    "- Weights are forced toward zero\n",
    "- High-degree polynomial terms (which have less predictive power) are suppressed first\n",
    "- Only the most important features (typically low-degree) retain significant weights\n",
    "\n",
    "**Effect on decision boundary curvature:**\n",
    "\n",
    "The \"wiggliness\" of a boundary is determined by high-degree terms:\n",
    "- $x^6$ terms can create 5-6 bends in the curve\n",
    "- $x^2$ terms create smooth curves (ellipses, parabolas)\n",
    "- $x^1$ terms create straight lines\n",
    "\n",
    "Regularization reduces high-degree contributions ‚Üí smoother boundaries\n",
    "\n",
    "**Practical observation from plots:**\n",
    "```\n",
    "Œª     Boundary Description              Test Acc\n",
    "---   --------------------------------  ---------\n",
    "0     Wiggly, overfits training noise   0.78\n",
    "0.1   Smooth circle, captures pattern   0.87\n",
    "1.0   Clean ellipse, generalizes well   0.88 ‚Üê optimal\n",
    "10    Slightly curved                   0.85\n",
    "100   Nearly linear, underfits          0.70\n",
    "```\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **A is FALSE**: This is the opposite of what happens. Increasing Œª makes boundaries LESS complex, not more. Complex, wiggly boundaries occur with LOW Œª (or Œª=0) when the model overfits. The confusion might arise from thinking \"more regularization = more complex,\" but regularization actually reduces complexity.\n",
    "\n",
    "- **C is FALSE**: Regularization fundamentally changes the optimization objective, which directly affects the learned weights and therefore the decision boundary shape. It's not just about training speed. With Œª=0, the model minimizes NLL; with Œª>0, it minimizes NLL + Œª||w||¬≤. These produce different optimal solutions and thus different boundaries.\n",
    "\n",
    "- **D is FALSE**: While moderate regularization (Œª ‚âà 1) helps fit the circular pattern, INCREASING Œª beyond optimal makes it worse. Very high Œª (e.g., Œª=100) produces nearly linear boundaries that cannot capture the circular distribution. The relationship between Œª and fit quality is not monotonic - there's an optimal value, and going too high hurts performance.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: L1 vs L2 Regularization\n",
    "\n",
    "We've focused on L2 (Ridge) regularization. Let's compare it with L1 (Lasso) regularization to understand when to use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Checkpoint Question 6: What is the key difference between L1 and L2 regularization?\n",
    "\n",
    "A) L1 adds the absolute values of weights to the loss function and can drive weights to exactly zero performing feature selection, while L2 adds squared weights and only shrinks weights toward zero without making them exactly zero\n",
    "B) L1 regularization is used only for regression problems and linear models, while L2 regularization is exclusively designed for classification tasks with logistic regression\n",
    "C) L1 uses gradient descent optimization for training while L2 uses a closed-form analytical solution, making L2 much faster to compute for large datasets\n",
    "D) L1 and L2 are identical in their mathematical formulation and produce the same learned weights, they only differ in their implementation details and computational cost\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: A**\n",
    "\n",
    "**Key Insight:** L1 and L2 regularization differ fundamentally in how they penalize weights. L1 uses absolute values ($\\sum |w_j|$), which produces sparse solutions (many exactly-zero weights), while L2 uses squared values ($\\sum w_j^2$), which produces small but non-zero weights. This difference makes L1 useful for feature selection and L2 useful for general overfitting prevention.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "**L2 Regularization (Ridge):**\n",
    "$$J_{\\text{L2}} = J_{\\text{NLL}} + \\frac{\\lambda}{2}\\sum_{j=1}^{d} w_j^2$$\n",
    "\n",
    "Gradient: $\\nabla R(w) = \\lambda w$\n",
    "\n",
    "- Penalty is smooth and differentiable\n",
    "- Gradient is proportional to weight magnitude\n",
    "- Weights shrink toward zero but rarely reach exactly zero\n",
    "- All features remain in the model with small weights\n",
    "\n",
    "**L1 Regularization (Lasso):**\n",
    "$$J_{\\text{L1}} = J_{\\text{NLL}} + \\lambda \\sum_{j=1}^{d} |w_j|$$\n",
    "\n",
    "Gradient: $\\nabla R(w) = \\lambda \\cdot \\text{sign}(w)$\n",
    "\n",
    "- Penalty has a \"corner\" at zero (not differentiable at zero)\n",
    "- Gradient is constant (either +Œª or -Œª)\n",
    "- Weights are driven to exactly zero\n",
    "- Performs automatic feature selection\n",
    "\n",
    "**Visual comparison of penalty functions:**\n",
    "```\n",
    "Penalty\n",
    "   ^\n",
    "   |        L2 (smooth, differentiable)\n",
    "   |         ___\n",
    "   |      __/   \\__\n",
    "   |   __/         \\__\n",
    "   | _/               \\_\n",
    "   |/___________________\\___> w\n",
    "   |\n",
    "   |    L1 (corner at zero)\n",
    "   |        /\\\n",
    "   |       /  \\\n",
    "   |      /    \\\n",
    "   |     /      \\\n",
    "   |    /        \\\n",
    "   |___/__________\\________> w\n",
    "       0\n",
    "```\n",
    "\n",
    "**Example with numbers:**\n",
    "\n",
    "Consider a model with 5 features, Œª = 1:\n",
    "\n",
    "**Without regularization:**\n",
    "```\n",
    "w = [2.0, 5.0, -3.0, 0.5, -4.5]\n",
    "All features used\n",
    "```\n",
    "\n",
    "**With L2 (Ridge):**\n",
    "```\n",
    "w = [0.8, 2.1, -1.2, 0.2, -1.8]\n",
    "All weights shrunk, but all non-zero\n",
    "All 5 features still in model\n",
    "```\n",
    "\n",
    "**With L1 (Lasso):**\n",
    "```\n",
    "w = [0.0, 3.2, -1.8, 0.0, -2.1]\n",
    "Some weights exactly zero!\n",
    "Only 3 features used (feature selection)\n",
    "```\n",
    "\n",
    "**Why L1 produces sparsity:**\n",
    "\n",
    "The L1 gradient is constant ($\\pm \\lambda$) rather than proportional to weight size:\n",
    "- Small weight (say $w = 0.1$): L1 gradient = Œª (e.g., 1.0) ‚Üí weight easily pushed to zero\n",
    "- Same weight with L2: L2 gradient = Œªw = 0.1 ‚Üí tiny push, weight stays non-zero\n",
    "\n",
    "**Geometric interpretation:**\n",
    "\n",
    "Regularization constrains weights to a region:\n",
    "```\n",
    "L2 constraint: w‚ÇÅ¬≤ + w‚ÇÇ¬≤ ‚â§ C    L1 constraint: |w‚ÇÅ| + |w‚ÇÇ| ‚â§ C\n",
    "(circle)                        (diamond)\n",
    "\n",
    "  w‚ÇÇ                             w‚ÇÇ\n",
    "   |                              |  \n",
    "   |     ___                      |    /\\\n",
    "   |   /     \\                    |   /  \\\n",
    "   |  |   *   |  ‚Üê optimal        |  /    \\\n",
    "   | |       | |                  | /   *  \\ ‚Üê optimal at\n",
    "   |  \\     /                     |/        \\   corner (w‚ÇÇ=0)\n",
    "   |   \\___ /                     +-----------\n",
    "   +------------ w‚ÇÅ               +---------- w‚ÇÅ\n",
    "```\n",
    "\n",
    "The L1 diamond has corners on the axes, making it likely that the optimal point lands on an axis (one weight = 0).\n",
    "\n",
    "**When to use each:**\n",
    "\n",
    "**Use L2 (Ridge) when:**\n",
    "- You want to use all features with reduced influence\n",
    "- Features are all relevant\n",
    "- You have multicollinearity (correlated features)\n",
    "- General overfitting prevention\n",
    "\n",
    "**Use L1 (Lasso) when:**\n",
    "- You want automatic feature selection\n",
    "- You believe many features are irrelevant\n",
    "- You need a sparse, interpretable model\n",
    "- You have more features than samples\n",
    "\n",
    "**Practical example:**\n",
    "\n",
    "Gene expression analysis with 10,000 genes:\n",
    "- L2: All 10,000 genes used with small weights (hard to interpret)\n",
    "- L1: Only 50 genes have non-zero weights (identifies important genes!)\n",
    "\n",
    "**Elastic Net (Bonus):**\n",
    "\n",
    "Combines both: $J = J_{\\text{NLL}} + \\lambda_1 \\sum |w_j| + \\frac{\\lambda_2}{2}\\sum w_j^2$\n",
    "- Gets sparsity from L1\n",
    "- Gets stability from L2\n",
    "- Best of both worlds!\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **B is FALSE**: Both L1 and L2 can be used for both regression and classification. L1 regularization works with linear regression (Lasso regression), logistic regression (L1-regularized logistic regression), and other models. L2 works with the same models (Ridge regression, L2-regularized logistic regression). The choice between L1 and L2 is about sparsity vs shrinkage, not about the task type.\n",
    "\n",
    "- **C is FALSE**: Both L1 and L2 regularization typically use gradient descent (or variants like SGD, Adam) for optimization in logistic regression. Neither has a closed-form solution for logistic regression because the NLL loss is non-linear. In linear regression, L2 (Ridge) has a closed form: $w = (X^TX + \\lambda I)^{-1}X^Ty$, but L1 (Lasso) still requires iterative methods. The optimization method is not the key distinguishing feature.\n",
    "\n",
    "- **D is FALSE**: L1 and L2 are fundamentally different in their mathematical formulation and produce very different results. They differ in:\n",
    "  1. Penalty term: $\\sum |w_j|$ vs $\\sum w_j^2$\n",
    "  2. Gradient: $\\lambda \\cdot \\text{sign}(w)$ vs $\\lambda w$\n",
    "  3. Solution: Sparse (many zeros) vs Dense (all non-zero)\n",
    "  4. Geometry: Diamond constraint vs Circular constraint\n",
    "  \n",
    "  These are not just \"implementation details\" - they lead to qualitatively different models.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison Table\n",
    "\n",
    "| Aspect | L1 (Lasso) | L2 (Ridge) |\n",
    "|--------|------------|------------|\n",
    "| **Penalty term** | $\\lambda \\sum |w_j|$ | $\\frac{\\lambda}{2} \\sum w_j^2$ |\n",
    "| **Gradient** | $\\lambda \\cdot \\text{sign}(w)$ | $\\lambda w$ |\n",
    "| **Effect on weights** | Drives some to **exactly zero** | Shrinks all **toward zero** |\n",
    "| **Feature selection** | ‚úÖ Yes (automatic) | ‚ùå No (keeps all features) |\n",
    "| **Solution sparsity** | Sparse (many zeros) | Dense (all non-zero) |\n",
    "| **When to use** | Feature selection needed | All features relevant |\n",
    "| **Multicollinearity** | Picks one feature arbitrarily | Shrinks all correlated features |\n",
    "| **Interpretability** | ‚úÖ High (few features) | ‚ö†Ô∏è Medium (many small weights) |\n",
    "| **Computational cost** | Higher (non-smooth) | Lower (smooth, differentiable) |\n",
    "| **Best for** | High-dimensional with irrelevant features | General overfitting prevention |\n",
    "\n",
    "### Sklearn Comparison\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# L2 regularization (default)\n",
    "model_l2 = LogisticRegression(penalty='l2', C=1.0)  # C = 1/Œª\n",
    "\n",
    "# L1 regularization  \n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "\n",
    "# Elastic Net (L1 + L2)\n",
    "model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', \n",
    "                                   C=1.0, l1_ratio=0.5)  # 0.5 = equal L1/L2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with scikit-learn\n",
    "\n",
    "Let's validate our implementation against scikit-learn's professional implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Comparing with scikit-learn\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use degree 6 polynomial with optimal Œª\n",
    "degree = 6\n",
    "lambda_optimal = optimal_lambda\n",
    "\n",
    "# Our model\n",
    "our_model = cv_results[lambda_optimal]['model']\n",
    "our_train_acc = cv_results[lambda_optimal]['train_acc']\n",
    "our_test_acc = cv_results[lambda_optimal]['test_acc']\n",
    "\n",
    "# Sklearn model (C = 1/Œª)\n",
    "sklearn_model = SklearnLogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0/lambda_optimal if lambda_optimal > 0 else 1e10,  # C is inverse of Œª\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_model.fit(X_train_poly, y_train)\n",
    "sklearn_train_acc = sklearn_model.score(X_train_poly, y_train)\n",
    "sklearn_test_acc = sklearn_model.score(X_test_poly, y_test)\n",
    "\n",
    "print(f\"\\nUsing degree {degree} polynomial with Œª={lambda_optimal}\")\n",
    "print(\"\\nOur Implementation:\")\n",
    "print(f\"  Train Accuracy: {our_train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy:  {our_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nscikit-learn:\")\n",
    "print(f\"  Train Accuracy: {sklearn_train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy:  {sklearn_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nDifference:\")\n",
    "print(f\"  Train: {abs(our_train_acc - sklearn_train_acc):.4f}\")\n",
    "print(f\"  Test:  {abs(our_test_acc - sklearn_test_acc):.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ If differences are small (<0.02), implementation is correct!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Always Use Regularization with Polynomial Features\n",
    "- Polynomial features create many features ‚Üí high risk of overfitting\n",
    "- Start with moderate Œª (e.g., 1.0) and tune via cross-validation\n",
    "- Monitor train vs test accuracy to detect overfitting\n",
    "\n",
    "### 2. Feature Scaling is Critical\n",
    "- Always standardize features before applying regularization\n",
    "- Regularization penalizes all weights equally\n",
    "- Without scaling, features with large scales dominate\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### 3. Don't Regularize the Bias Term\n",
    "- The intercept/bias should not be penalized\n",
    "- It just shifts the decision boundary\n",
    "- In our implementation: `gradient[0]` excludes regularization\n",
    "\n",
    "### 4. Use Cross-Validation for Œª Selection\n",
    "- Test multiple Œª values: [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "- Use k-fold CV on training data only\n",
    "- Never use test set for hyperparameter tuning\n",
    "\n",
    "### 5. Start with L2, Consider L1 for Feature Selection\n",
    "- **L2 (Ridge)**: Default choice, works well in most cases\n",
    "- **L1 (Lasso)**: Use when you need sparse models\n",
    "- **Elastic Net**: Combines L1 + L2 benefits\n",
    "\n",
    "### 6. Early Stopping as Alternative\n",
    "- Stop training when validation error starts increasing\n",
    "- Prevents overfitting without modifying loss\n",
    "- Common in neural networks\n",
    "\n",
    "### 7. Regularization vs More Data\n",
    "- Regularization: Works with fixed dataset\n",
    "- More data: Best solution but often not available\n",
    "- Use both when possible!\n",
    "\n",
    "### 8. Debugging Checklist\n",
    "- ‚úÖ Training accuracy much higher than test? ‚Üí Increase Œª\n",
    "- ‚úÖ Both accuracies low? ‚Üí Decrease Œª or increase model complexity\n",
    "- ‚úÖ Loss increasing? ‚Üí Decrease learning rate\n",
    "- ‚úÖ Weights exploding? ‚Üí Increase Œª or decrease learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "1. ‚úÖ **Understood overfitting** by observing high training accuracy but low test accuracy with high-degree polynomials\n",
    "2. ‚úÖ **Implemented L2 regularization** from scratch by adding penalty term to loss and gradient\n",
    "3. ‚úÖ **Tuned hyperparameter Œª** using k-fold cross-validation to find the optimal regularization strength\n",
    "4. ‚úÖ **Visualized the bias-variance tradeoff** and saw how Œª controls model complexity\n",
    "5. ‚úÖ **Compared L1 vs L2 regularization** and understood when to use each technique\n",
    "6. ‚úÖ **Applied regularization to real problems** with non-linear decision boundaries\n",
    "7. ‚úÖ **Validated implementation** against scikit-learn's professional implementation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**The Overfitting Problem:**\n",
    "- Complex models (high-degree polynomials) can perfectly fit training data\n",
    "- But they memorize noise and fail on test data\n",
    "- Symptom: Training accuracy >> Test accuracy\n",
    "\n",
    "**Regularization Solution:**\n",
    "- Add penalty term to loss: $J_{\\text{total}} = J_{\\text{NLL}} + \\lambda R(\\vec{w})$\n",
    "- Forces model to balance data fit with weight magnitude\n",
    "- Results in simpler, more generalizable models\n",
    "\n",
    "**L2 (Ridge) Regularization:**\n",
    "- Penalty: $\\frac{\\lambda}{2}\\sum w_j^2$\n",
    "- Gradient: $\\lambda w$\n",
    "- Shrinks all weights toward zero\n",
    "- Best for general overfitting prevention\n",
    "\n",
    "**L1 (Lasso) Regularization:**\n",
    "- Penalty: $\\lambda \\sum |w_j|$\n",
    "- Gradient: $\\lambda \\cdot \\text{sign}(w)$\n",
    "- Drives some weights to exactly zero\n",
    "- Best for feature selection\n",
    "\n",
    "**Hyperparameter Œª:**\n",
    "- Œª = 0: No regularization (may overfit)\n",
    "- Œª small: Light regularization\n",
    "- Œª optimal: Balanced (best generalization)\n",
    "- Œª large: Heavy regularization (may underfit)\n",
    "- Use cross-validation to find optimal Œª\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "- Increasing Œª ‚Üí Increases bias, Decreases variance\n",
    "- Decreasing Œª ‚Üí Decreases bias, Increases variance\n",
    "- Optimal Œª minimizes total error\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try regularization on real-world datasets (e.g., breast cancer, spam detection)\n",
    "- Implement L1 (Lasso) regularization\n",
    "- Explore Elastic Net (L1 + L2 combined)\n",
    "- Apply to neural networks (weight decay)\n",
    "- Learn about other regularization techniques:\n",
    "  - Dropout\n",
    "  - Early stopping\n",
    "  - Data augmentation\n",
    "  - Batch normalization\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Regularization in Machine Learning - scikit-learn User Guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)\n",
    "- [Understanding the Bias-Variance Tradeoff - Andrew Ng's Course](http://www.andrewng.org/)\n",
    "- [L1 and L2 Regularization Methods - Towards Data Science](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}