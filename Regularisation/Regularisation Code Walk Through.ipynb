{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Regularisation/Regularisation%20Code%20Walk%20Through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koxRAqDceAy2"
      },
      "source": [
        "# Regularisation: Code Walk Through\n",
        "\n",
        "This notebook provides a step-by-step computational walkthrough of **L2 Regularisation (Ridge Regression)** applied to logistic regression.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How **overfitting** manifests with large weights in high-degree polynomial features\n",
        "- How to add the **L2 penalty term** ($\\lambda ||\\vec{w}||^2$) to the loss function\n",
        "- How to compute the **regularised gradient**: $\\nabla J = \\Phi^T(p - y) + 2\\lambda\\vec{w}$\n",
        "- How regularisation **shrinks weights** to prevent overfitting\n",
        "- How the **regularisation parameter λ** controls the bias-variance tradeoff\n",
        "- How to visualise decision boundaries with different λ values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K57YH5sleAy2"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjdKdsWXeAy2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit  # Numerically stable sigmoid\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEofWaTieAy3"
      },
      "source": [
        "## 2. Generate Synthetic Binary Classification Data\n",
        "\n",
        "We'll use the same mixture dataset pattern from the lecture slides - two classes with non-linear separation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQJ4pnQ7eAy3"
      },
      "outputs": [],
      "source": [
        "# Generate two-class data (matching lecture slides pattern)\n",
        "m = 25  # samples per class\n",
        "n = 2   # features\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Class 0: centered around (1.5, -1.5)\n",
        "class_0 = np.hstack((\n",
        "    1.5 + np.random.randn(m, 1),\n",
        "    -1.5 + np.random.randn(m, 1)\n",
        "))\n",
        "\n",
        "# Class 1: centered around (-1.5, 1.5)\n",
        "class_1 = np.hstack((\n",
        "    -1.5 + np.random.randn(m, 1),\n",
        "    1.5 + np.random.randn(m, 1)\n",
        "))\n",
        "\n",
        "# Combine into training set\n",
        "X = np.vstack((class_0, class_1))  # shape (2m, 2)\n",
        "y = np.concatenate([np.zeros(m), np.ones(m)])  # shape (2m,)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvujceqjeAy3"
      },
      "source": [
        "## 3. Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMRMq5PkeAy3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1],\n",
        "            c='orange', label='Class 0', edgecolors='k', s=50)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1],\n",
        "            c='skyblue', label='Class 1', edgecolors='k', s=50)\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Binary Classification Data', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYWXDpe1eAy3"
      },
      "source": [
        "## 4. The Overfitting Problem\n",
        "\n",
        "Let's first demonstrate **why we need regularisation** by training a logistic regression model with high-degree polynomial features **without regularisation**.\n",
        "\n",
        "### Create Polynomial Features (Degree 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8JtvP5teAy3"
      },
      "outputs": [],
      "source": [
        "# Create 8th degree polynomial features (like in lecture slides)\n",
        "degree = 8\n",
        "poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "print(f\"Original features: {X.shape[1]}\")\n",
        "print(f\"Polynomial features (degree {degree}): {X_poly.shape[1]}\")\n",
        "print(f\"\\nFeature names: {poly.get_feature_names_out(['x1', 'x2'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyV6uhSeAy3"
      },
      "source": [
        "### Prepare Design Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lx6ecByeAy3"
      },
      "outputs": [],
      "source": [
        "# Add bias column: Φ = [1 | X]\n",
        "Phi = np.c_[np.ones(len(X_poly)), X_poly]\n",
        "print(f\"Design matrix Φ shape: {Phi.shape}\")\n",
        "print(f\"First row (bias + features): {Phi[0][:5]}... (showing first 5 values)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNARpurGeAy4"
      },
      "source": [
        "### Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hegeImmweAy4"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "weights = np.zeros(Phi.shape[1])  # Initialize to zeros\n",
        "print(f\"Initial weights shape: {weights.shape}\")\n",
        "print(f\"Initial weights: {weights[:5]}... (all zeros)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6_nbV_CeAy4"
      },
      "source": [
        "## 5. Step-by-Step: One Gradient Descent Iteration WITHOUT Regularisation\n",
        "\n",
        "Let's walk through **one iteration** to understand the standard logistic regression update (without regularisation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZDCqa0weAy4"
      },
      "source": [
        "### Step 1: Compute Scores (Logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXF5ZX0reAy4"
      },
      "outputs": [],
      "source": [
        "scores = Phi @ weights\n",
        "print(f\"Scores shape: {scores.shape}\")\n",
        "print(f\"First 5 scores: {scores[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uGy-YZseAy4"
      },
      "source": [
        "### Step 2: Apply Sigmoid to Get Probabilities\n",
        "\n",
        "$$P(y=1|\\vec{x}, \\vec{w}) = \\sigma(\\vec{x}^T \\times \\vec{w}) = \\frac{1}{1 + e^{-\\vec{x}^T \\times \\vec{w}}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFWHDRyDeAy4"
      },
      "outputs": [],
      "source": [
        "probabilities = expit(scores)\n",
        "print(f\"Probabilities shape: {probabilities.shape}\")\n",
        "print(f\"First 5 probabilities: {probabilities[:5]}\")\n",
        "print(f\"All probabilities in [0,1]: {np.all((probabilities >= 0) & (probabilities <= 1))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5BndQPveAy4"
      },
      "source": [
        "### Step 3: Compute Gradient (WITHOUT Regularisation)\n",
        "\n",
        "**Standard gradient (no regularisation):**\n",
        "\n",
        "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W15weLfjeAy4"
      },
      "outputs": [],
      "source": [
        "errors = probabilities - y\n",
        "gradient_no_reg = Phi.T @ errors\n",
        "\n",
        "print(f\"Gradient shape: {gradient_no_reg.shape}\")\n",
        "print(f\"Gradient (no regularisation): {gradient_no_reg[:5]}\")\n",
        "print(f\"Gradient magnitude: {np.linalg.norm(gradient_no_reg):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-CFHGV_eAy4"
      },
      "source": [
        "## 6. Now Add L2 Regularisation!\n",
        "\n",
        "### The Key Addition: Regularisation Gradient Term\n",
        "\n",
        "**Total Cost with L2:**\n",
        "\n",
        "$$J(\\vec{w}) = -\\sum [y \\log p + (1-y)\\log(1-p)] + \\lambda ||\\vec{w}||^2$$\n",
        "\n",
        "**Regularised Gradient:**\n",
        "\n",
        "$$\\nabla_{\\vec{w}} J = \\Phi^T (\\vec{p} - \\vec{y}) + 2\\lambda\\vec{w}$$\n",
        "\n",
        "**Important:** We don't regularise the bias term (intercept)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q05hCJXSeAy4"
      },
      "source": [
        "### Step 4: Compute L2 Regularisation Term\n",
        "\n",
        "The regularisation adds $2\\lambda\\vec{w}$ to the gradient (excluding the bias term)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsdDqRlOeAy4"
      },
      "outputs": [],
      "source": [
        "# Set regularisation strength\n",
        "lambda_reg = 1.0\n",
        "\n",
        "# Create regularisation vector (exclude bias w[0])\n",
        "reg_vector = weights.copy()\n",
        "reg_vector[0] = 0.0  # Don't regularise the intercept\n",
        "\n",
        "# Compute regularisation gradient term: 2λw\n",
        "gradient_reg_term = 2 * lambda_reg * reg_vector\n",
        "\n",
        "print(f\"Regularisation term (2λw): {gradient_reg_term[:5]}\")\n",
        "print(f\"Note: First element (bias) is {gradient_reg_term[0]} (not regularised)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OplZJc8veAy4"
      },
      "source": [
        "### Step 5: Compute Total Gradient (WITH Regularisation)\n",
        "\n",
        "$$\\nabla_{\\vec{w}} J = \\underbrace{\\Phi^T (\\vec{p} - \\vec{y})}_{\\text{base gradient}} + \\underbrace{2\\lambda\\vec{w}}_{\\text{L2 penalty}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcEkLakqeAy4"
      },
      "outputs": [],
      "source": [
        "# Total gradient = base gradient + regularisation term\n",
        "gradient_with_reg = gradient_no_reg + gradient_reg_term\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: Gradient WITHOUT vs WITH Regularisation\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nGradient (no reg):   {gradient_no_reg[:3]}\")\n",
        "print(f\"Reg term (2λw):      {gradient_reg_term[:3]}\")\n",
        "print(f\"Gradient (with reg): {gradient_with_reg[:3]}\")\n",
        "print(f\"\\nMagnitude without reg: {np.linalg.norm(gradient_no_reg):.6f}\")\n",
        "print(f\"Magnitude with reg:    {np.linalg.norm(gradient_with_reg):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5p8ltDeAy4"
      },
      "source": [
        "### Step 6: Update Weights\n",
        "\n",
        "$$\\vec{w}_{\\text{new}} = \\vec{w}_{\\text{old}} - \\alpha \\nabla_{\\vec{w}} J$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1yID94seAy4"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "weights_new = weights - learning_rate * gradient_with_reg\n",
        "\n",
        "print(f\"Old weights: {weights[:3]}\")\n",
        "print(f\"New weights: {weights_new[:3]}\")\n",
        "print(f\"Weight change: {weights_new[:3] - weights[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gov5VCMVeAy4"
      },
      "source": [
        "## 7. Full Gradient Descent Function with L2 Regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWKu7hWreAy4"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_l2(Phi, y, lambda_reg=0.0, learning_rate=0.1,\n",
        "                        max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Gradient descent with L2 regularisation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    Phi : design matrix with bias column\n",
        "    y : target labels\n",
        "    lambda_reg : L2 regularisation strength (λ)\n",
        "    learning_rate : step size (α)\n",
        "    max_iter : maximum iterations\n",
        "    tol : convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    weights : learned weights\n",
        "    loss_history : loss at each iteration\n",
        "    weight_history : weights at each iteration\n",
        "    \"\"\"\n",
        "    # Initialize weights\n",
        "    weights = np.zeros(Phi.shape[1])\n",
        "    loss_history = []\n",
        "    weight_history = [weights.copy()]\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        # Forward pass\n",
        "        scores = Phi @ weights\n",
        "        probabilities = expit(scores)\n",
        "\n",
        "        # Compute loss (NLL + L2 penalty)\n",
        "        epsilon = 1e-15\n",
        "        p_safe = np.clip(probabilities, epsilon, 1 - epsilon)\n",
        "        nll = -np.sum(y * np.log(p_safe) + (1 - y) * np.log(1 - p_safe))\n",
        "\n",
        "        # Add L2 penalty (don't regularise bias)\n",
        "        l2_penalty = lambda_reg * np.sum(weights[1:]**2)\n",
        "        total_loss = nll + l2_penalty\n",
        "        loss_history.append(total_loss)\n",
        "\n",
        "        # Compute gradient\n",
        "        errors = probabilities - y\n",
        "        gradient_base = Phi.T @ errors\n",
        "\n",
        "        # Add L2 regularisation term (exclude bias)\n",
        "        reg_vector = weights.copy()\n",
        "        reg_vector[0] = 0.0  # Don't regularise intercept\n",
        "        gradient_reg = 2 * lambda_reg * reg_vector\n",
        "\n",
        "        gradient = gradient_base + gradient_reg\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm(gradient) < tol:\n",
        "            break\n",
        "\n",
        "        # Update weights\n",
        "        weights = weights - learning_rate * gradient\n",
        "        weight_history.append(weights.copy())\n",
        "\n",
        "    return weights, loss_history, weight_history, iteration + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnjr9bBweAy5"
      },
      "source": [
        "## 8. Train Models with Different λ Values\n",
        "\n",
        "Let's train three models to see the effect of regularisation:\n",
        "1. **λ = 0** (No regularisation) → Overfitting\n",
        "2. **λ = 1** (Moderate regularisation) → Balanced\n",
        "3. **λ = 100** (Heavy regularisation) → Underfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSV7ECqceAy5"
      },
      "outputs": [],
      "source": [
        "# Train models with different λ values\n",
        "lambdas = [0, 1.0, 100]\n",
        "results = {}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Training Models with Different λ Values\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for lam in lambdas:\n",
        "    print(f\"\\nTraining with λ = {lam}...\")\n",
        "    weights, loss_hist, weight_hist, n_iter = gradient_descent_l2(\n",
        "        Phi, y, lambda_reg=lam, learning_rate=0.1, max_iter=2000\n",
        "    )\n",
        "\n",
        "    results[lam] = {\n",
        "        'weights': weights,\n",
        "        'loss_history': loss_hist,\n",
        "        'weight_history': weight_hist,\n",
        "        'n_iter': n_iter\n",
        "    }\n",
        "\n",
        "    # Compute predictions\n",
        "    probs = expit(Phi @ weights)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    accuracy = np.mean(preds == y)\n",
        "\n",
        "    # Weight magnitude\n",
        "    weight_magnitude = np.linalg.norm(weights)\n",
        "\n",
        "    print(f\"  Converged in {n_iter} iterations\")\n",
        "    print(f\"  Final loss: {loss_hist[-1]:.4f}\")\n",
        "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"  Weight magnitude: {weight_magnitude:.2f}\")\n",
        "    print(f\"  Max |weight|: {np.max(np.abs(weights[1:])):.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OBSERVATION: As λ increases, weight magnitudes decrease!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhfB-eQpeAy5"
      },
      "source": [
        "## 9. Visualize Loss Convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08NpumL7eAy5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "for lam in lambdas:\n",
        "    loss_hist = results[lam]['loss_history']\n",
        "    plt.plot(loss_hist, linewidth=2, label=f'λ = {lam}')\n",
        "\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Total Loss (NLL + λ||w||²)', fontsize=12)\n",
        "plt.title('Loss Convergence with Different λ Values', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICrBC2lReAy5"
      },
      "source": [
        "## 10. Compare Weight Magnitudes\n",
        "\n",
        "Let's see how regularisation **shrinks weights** to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUXafNlFeAy5"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "for idx, lam in enumerate(lambdas):\n",
        "    ax = axes[idx]\n",
        "    weights = results[lam]['weights']\n",
        "\n",
        "    # Plot weights (excluding bias)\n",
        "    ax.bar(range(1, len(weights)), weights[1:], alpha=0.7)\n",
        "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "    ax.set_xlabel('Weight Index', fontsize=11)\n",
        "    ax.set_ylabel('Weight Value', fontsize=11)\n",
        "    ax.set_title(f'λ = {lam}\\n||w|| = {np.linalg.norm(weights):.2f}',\n",
        "                 fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"OBSERVATION:\")\n",
        "print(\"• λ = 0: Large weights → complex boundary → overfitting\")\n",
        "print(\"• λ = 1: Moderate weights → smooth boundary → good fit\")\n",
        "print(\"• λ = 100: Tiny weights → simple boundary → underfitting\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSample Weights Comparison:\")\n",
        "print(f\"Feature x1²: λ=0: {results[0]['weights'][3]:.2f}, \"\n",
        "      f\"λ=1: {results[1.0]['weights'][3]:.2f}, \"\n",
        "      f\"λ=100: {results[100]['weights'][3]:.2f}\")"
      ],
      "metadata": {
        "id": "vL751pNTeAy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCyHbC09eAy5"
      },
      "source": [
        "## 11. Visualize Decision Boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BQBAsNGeAy5"
      },
      "outputs": [],
      "source": [
        "# Create mesh for decision boundary visualization\n",
        "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                        np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "# Prepare mesh\n",
        "X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "X_mesh_poly = poly.transform(X_mesh)\n",
        "Phi_mesh = np.c_[np.ones(X_mesh_poly.shape[0]), X_mesh_poly]\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, lam in enumerate(lambdas):\n",
        "    ax = axes[idx]\n",
        "    weights = results[lam]['weights']\n",
        "\n",
        "    # Compute probabilities for mesh\n",
        "    probs_mesh = expit(Phi_mesh @ weights).reshape(xx1.shape)\n",
        "\n",
        "    # Plot contours\n",
        "    ax.contourf(xx1, xx2, probs_mesh, levels=20, cmap='RdBu_r', alpha=0.6)\n",
        "    ax.contour(xx1, xx2, probs_mesh, levels=[0.5], colors='black', linewidths=2.5)\n",
        "\n",
        "    # Plot data\n",
        "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='orange',\n",
        "               edgecolors='k', s=50, alpha=0.8, label='Class 0')\n",
        "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='skyblue',\n",
        "               edgecolors='k', s=50, alpha=0.8, label='Class 1')\n",
        "\n",
        "    # Compute accuracy\n",
        "    probs = expit(Phi @ weights)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    accuracy = np.mean(preds == y)\n",
        "\n",
        "    # Title\n",
        "    if lam == 0:\n",
        "        title = f'λ = {lam} (No Regularisation)\\nAcc = {accuracy:.2f} - Overfitting'\n",
        "    elif lam == 1:\n",
        "        title = f'λ = {lam} (Moderate)\\nAcc = {accuracy:.2f} - Balanced ✓'\n",
        "    else:\n",
        "        title = f'λ = {lam} (Heavy)\\nAcc = {accuracy:.2f} - Underfitting'\n",
        "\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('$x_1$', fontsize=11)\n",
        "    ax.set_ylabel('$x_2$', fontsize=11)\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.axis('equal')\n",
        "    ax.set_xlim(x1_min, x1_max)\n",
        "    ax.set_ylim(x2_min, x2_max)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVRDJovXeAzE"
      },
      "source": [
        "## 12. Comparison with scikit-learn\n",
        "\n",
        "Let's validate our implementation against scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w06STJJneAzE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Test with λ = 1.0\n",
        "lambda_test = 1.0\n",
        "\n",
        "# Our implementation\n",
        "our_weights = results[lambda_test]['weights']\n",
        "our_probs = expit(Phi @ our_weights)\n",
        "our_accuracy = np.mean((our_probs >= 0.5).astype(int) == y)\n",
        "\n",
        "# scikit-learn (C = 1/λ)\n",
        "sklearn_model = LogisticRegression(penalty='l2', C=1.0/lambda_test,\n",
        "                                   max_iter=2000, random_state=42)\n",
        "sklearn_model.fit(X_poly, y)\n",
        "sklearn_accuracy = sklearn_model.score(X_poly, y)\n",
        "sklearn_weights = np.concatenate([sklearn_model.intercept_, sklearn_model.coef_[0]])\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: Our Implementation vs scikit-learn\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nλ = {lambda_test}\")\n",
        "print(f\"\\nOur accuracy:      {our_accuracy:.4f}\")\n",
        "print(f\"sklearn accuracy:  {sklearn_accuracy:.4f}\")\n",
        "print(f\"\\nDifference: {abs(our_accuracy - sklearn_accuracy):.4f}\")\n",
        "\n",
        "print(f\"\\nOur weight magnitude:     {np.linalg.norm(our_weights):.2f}\")\n",
        "print(f\"sklearn weight magnitude: {np.linalg.norm(sklearn_weights):.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if abs(our_accuracy - sklearn_accuracy) < 0.02:\n",
        "    print(\"✓ Results match! L2 regularisation implemented correctly.\")\n",
        "else:\n",
        "    print(\"⚠ Results differ. Check implementation.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd6CjWT-eAzE"
      },
      "source": [
        "## 13. The Bias-Variance Tradeoff\n",
        "\n",
        "Let's create a visualization showing how λ affects the bias-variance tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qJrBnTKeAzE"
      },
      "outputs": [],
      "source": [
        "# Test a range of λ values\n",
        "lambda_range = np.logspace(-3, 3, 20)  # From 0.001 to 1000\n",
        "accuracies = []\n",
        "weight_mags = []\n",
        "\n",
        "for lam in lambda_range:\n",
        "    weights, _, _, _ = gradient_descent_l2(Phi, y, lambda_reg=lam,\n",
        "                                           learning_rate=0.1, max_iter=2000)\n",
        "    probs = expit(Phi @ weights)\n",
        "    acc = np.mean((probs >= 0.5).astype(int) == y)\n",
        "    accuracies.append(acc)\n",
        "    weight_mags.append(np.linalg.norm(weights))\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Accuracy vs λ\n",
        "ax = axes[0]\n",
        "ax.plot(lambda_range, accuracies, 'o-', linewidth=2, markersize=6)\n",
        "ax.axvline(1.0, color='red', linestyle='--', linewidth=2, label='λ = 1.0 (optimal)')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('Regularisation Strength (λ)', fontsize=12)\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Accuracy vs λ: Finding the Sweet Spot', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(fontsize=11)\n",
        "\n",
        "# Add annotations\n",
        "ax.text(0.001, 0.5, 'Overfitting\\n(High Variance)', fontsize=10,\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='pink', alpha=0.5))\n",
        "ax.text(1000, 0.5, 'Underfitting\\n(High Bias)', fontsize=10,\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
        "\n",
        "# Plot 2: Weight magnitude vs λ\n",
        "ax = axes[1]\n",
        "ax.plot(lambda_range, weight_mags, 's-', linewidth=2, markersize=6, color='green')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('Regularisation Strength (λ)', fontsize=12)\n",
        "ax.set_ylabel('Weight Magnitude (||w||)', fontsize=12)\n",
        "ax.set_title('Weight Shrinkage Effect of Regularisation', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKEY INSIGHT:\")\n",
        "print(\"• Small λ → Large weights → Complex boundary → Overfitting\")\n",
        "print(\"• Optimal λ → Moderate weights → Smooth boundary → Good fit\")\n",
        "print(\"• Large λ → Small weights → Simple boundary → Underfitting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi6iz9bZeAzE"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We've walked through all the computational steps of L2 Regularisation:\n",
        "\n",
        "1. ✅ **Demonstrated overfitting** - high-degree polynomials create large weights\n",
        "2. ✅ **Added L2 penalty** - modified loss function: $J = NLL + \\lambda||\\vec{w}||^2$\n",
        "3. ✅ **Computed regularised gradient** - added $2\\lambda\\vec{w}$ term (excluding bias)\n",
        "4. ✅ **Implemented gradient descent** with regularisation\n",
        "5. ✅ **Visualised weight shrinkage** - regularisation reduces weight magnitudes\n",
        "6. ✅ **Explored bias-variance tradeoff** - λ controls model complexity\n",
        "7. ✅ **Validated implementation** - matched scikit-learn results\n",
        "\n",
        "### Key L2 Regularisation Concepts\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **L2 Penalty** | $\\lambda||\\vec{w}||^2 = \\lambda\\sum_{j=1}^{d} w_j^2$ |\n",
        "| **Regularised Gradient** | $\\nabla J = \\Phi^T(p - y) + 2\\lambda\\vec{w}$ |\n",
        "| **Weight Shrinkage** | Larger λ → Smaller weights |\n",
        "| **Bias-Variance** | Small λ → High variance (overfit)<br>Large λ → High bias (underfit) |\n",
        "| **Exclude Bias** | Don't regularise intercept term $w_0$ |\n",
        "\n",
        "### When to Use L2 Regularisation\n",
        "\n",
        "✅ **Use L2 when:**\n",
        "- Using polynomial or interaction features\n",
        "- Model has many parameters relative to data\n",
        "- Observing overfitting (train >> test accuracy)\n",
        "- Want to keep all features but reduce their influence\n",
        "\n",
        "❌ **Don't use L2 when:**\n",
        "- Model is already underfitting\n",
        "- Dataset is very large relative to features\n",
        "- Need sparse feature selection (use L1 instead)\n",
        "\n",
        "### Choosing λ\n",
        "\n",
        "Use **cross-validation** to find optimal λ:\n",
        "1. Try range: [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "2. Train model on training folds\n",
        "3. Validate on validation fold\n",
        "4. Select λ with best validation accuracy\n",
        "5. Retrain on full training set with optimal λ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}