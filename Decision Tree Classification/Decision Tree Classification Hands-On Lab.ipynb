{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Decision%20Tree%20Classification/Decision%20Tree%20Classification%20Hands-On%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification: Hands-On Lab\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand** how decision trees recursively partition feature space using greedy splits\n",
    "2. **Implement** entropy and information gain calculations from scratch\n",
    "3. **Build** a complete decision tree classifier using recursive tree construction\n",
    "4. **Apply** decision trees to classification problems with both numerical and categorical features\n",
    "5. **Evaluate** model performance using accuracy, confusion matrix, and cross-validation\n",
    "6. **Tune** hyperparameters (max_depth, min_samples_split, min_samples_leaf) to control overfitting\n",
    "7. **Visualize** decision boundaries and tree structure\n",
    "8. **Interpret** feature importances based on information gain contributions\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "**Decision Trees** are supervised learning algorithms that learn a series of if/else questions to partition the feature space into regions, each associated with a class label.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Entropy** measures the impurity or disorder in a dataset:\n",
    "\n",
    "$$E = -\\sum_{i=1}^{N} p_i \\log_2(p_i)$$\n",
    "\n",
    "Where:\n",
    "- $p_i$ is the proportion of samples belonging to class $i$\n",
    "- $N$ is the number of classes\n",
    "- By convention, $0 \\cdot \\log_2(0) = 0$\n",
    "\n",
    "**Information Gain** measures the reduction in entropy after a split:\n",
    "\n",
    "$$IG = E_{parent} - \\sum_{j} \\frac{|S_j|}{|S|} E_j$$\n",
    "\n",
    "Where:\n",
    "- $E_{parent}$ is the entropy before the split\n",
    "- $S_j$ is the subset of samples in child node $j$\n",
    "- $E_j$ is the entropy of child node $j$\n",
    "\n",
    "**Greedy Splitting**: At each node, the algorithm:\n",
    "1. Considers all possible features and thresholds\n",
    "2. Selects the split that maximizes information gain\n",
    "3. Recursively builds subtrees until stopping criteria are met\n",
    "\n",
    "---\n",
    "\n",
    "### Why These Components?\n",
    "\n",
    "**Why entropy?** Entropy quantifies uncertainty. A pure node (all samples from one class) has entropy 0, while a maximally mixed node has high entropy. By minimizing entropy, we create purer, more predictive leaf nodes.\n",
    "\n",
    "**Why information gain?** It measures how much a split reduces uncertainty. Choosing splits with maximum information gain ensures we ask the most informative questions first, leading to shorter, more efficient trees.\n",
    "\n",
    "**Why greedy splitting?** Finding the globally optimal tree is NP-hard. Greedy selection (locally optimal splits) is computationally tractable and works well in practice, though it may not find the globally optimal tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Decision Trees\n",
    "\n",
    "Decision trees are versatile classifiers with specific strengths and limitations. Understanding when to use them is crucial for effective model selection.\n",
    "\n",
    "### ✅ Use Decision Trees When:\n",
    "\n",
    "**1. Interpretability is Critical**\n",
    "- Trees can be visualized and explained as a series of if/else rules\n",
    "- Essential in healthcare, finance, and legal domains where decisions must be justified\n",
    "- Example: \"IF credit_score > 700 AND income > 50000 THEN approve_loan\"\n",
    "\n",
    "**2. Mixed Feature Types**\n",
    "- Handles both numerical and categorical features natively\n",
    "- No need for feature scaling or normalization\n",
    "- No need for one-hot encoding (with proper implementation)\n",
    "\n",
    "**3. Non-Linear Decision Boundaries**\n",
    "- Captures complex, axis-aligned decision boundaries\n",
    "- Automatically handles feature interactions through hierarchical splits\n",
    "- Example: \"IF age > 30 AND (income > 60000 OR education = 'PhD')\"\n",
    "\n",
    "**4. Feature Selection is Needed**\n",
    "- Naturally performs feature selection via information gain\n",
    "- Important features appear near the root; irrelevant features are ignored\n",
    "- Feature importances are directly interpretable\n",
    "\n",
    "**5. Quick Baseline Model**\n",
    "- Fast to train and predict\n",
    "- Requires minimal data preprocessing\n",
    "- Good starting point before trying complex models\n",
    "\n",
    "### ❌ Don't Use Decision Trees When:\n",
    "\n",
    "**1. Smooth Decision Boundaries Required**\n",
    "- Trees create axis-aligned, \"staircase\" boundaries\n",
    "- Diagonal or curved boundaries require many splits\n",
    "- **Better alternatives**: SVM with RBF kernel, Neural Networks, Logistic Regression\n",
    "\n",
    "**2. High Variance is Unacceptable**\n",
    "- Single trees are unstable—small data changes can produce very different trees\n",
    "- **Better alternatives**: Random Forests, Gradient Boosting (ensembles of trees)\n",
    "\n",
    "**3. Very High-Dimensional Sparse Data**\n",
    "- Text data with thousands of features can lead to overfitting\n",
    "- **Better alternatives**: Naive Bayes, Linear SVM, regularized models\n",
    "\n",
    "**4. Extrapolation Beyond Training Data**\n",
    "- Trees cannot extrapolate; predictions outside training range use nearest leaf\n",
    "- **Better alternatives**: Linear models, Neural Networks\n",
    "\n",
    "### Quick Comparison: Decision Trees vs Other Classifiers\n",
    "\n",
    "| Criterion | Decision Tree | Logistic Regression | kNN | Naive Bayes | Random Forest |\n",
    "|-----------|--------------|---------------------|-----|-------------|---------------|\n",
    "| **Interpretability** | ✅ Excellent | ✅ Good | ❌ Poor | ✅ Good | ⚠️ Moderate |\n",
    "| **Non-linear boundaries** | ✅ Good | ⚠️ Manual features | ✅ Excellent | ❌ Linear | ✅ Excellent |\n",
    "| **Training speed** | ✅ Fast | ✅ Fast | ✅ Fast (lazy) | ✅ Very Fast | ⚠️ Moderate |\n",
    "| **Prediction speed** | ✅ Very Fast | ✅ Very Fast | ❌ Slow | ✅ Very Fast | ⚠️ Moderate |\n",
    "| **Handles missing data** | ✅ Yes | ❌ No | ❌ No | ⚠️ Partial | ✅ Yes |\n",
    "| **Feature scaling needed** | ❌ No | ✅ Yes | ✅ Yes | ❌ No | ❌ No |\n",
    "| **Stability** | ❌ Low | ✅ High | ⚠️ Moderate | ✅ High | ✅ High |\n",
    "| **Overfitting risk** | ⚠️ High | ⚠️ Low | ⚠️ Moderate | ⚠️ Low | ⚠️ Low |\n",
    "\n",
    "### Real-World Applications Where Decision Trees Excel:\n",
    "\n",
    "1. **Medical Diagnosis**: Interpretable rules for disease detection\n",
    "2. **Credit Risk Assessment**: Explainable loan approval decisions\n",
    "3. **Customer Segmentation**: Clear rules for marketing targeting\n",
    "4. **Fraud Detection**: Transparent reasoning for flagged transactions\n",
    "5. **Manufacturing Quality Control**: Interpretable defect classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode for Decision Tree Classifier\n",
    "\n",
    "```\n",
    "# Decision Tree Classifier — Greedy Recursive Splitting\n",
    "# Inputs\n",
    "# X, y        ← data (N×D features, N labels)\n",
    "# max_depth   ← maximum tree depth\n",
    "# min_leaf    ← minimum samples required in a leaf\n",
    "# X_query     ← examples to predict\n",
    "\n",
    "# ----- fit -----\n",
    "Define BUILD(X, y, depth):\n",
    "    IF all labels in y equal OR depth = max_depth OR |y| < 2·min_leaf THEN\n",
    "        RETURN Leaf( class = majority(y) )\n",
    "    \n",
    "    N ← |y|\n",
    "    base ← − Σ_k p_k log2 p_k          # parent entropy; p_k = freq(y=k)/N\n",
    "    best_gain ← 0 ; best ← NONE\n",
    "    \n",
    "    FOR j = 1 TO D DO                   # iterate over features\n",
    "        thresholds ← midpoints of sorted unique X[:, j]\n",
    "        FOR each t IN thresholds DO\n",
    "            L ← {i : X[i, j] ≤ t}       # left child indices\n",
    "            R ← {i : X[i, j] > t}       # right child indices\n",
    "            IF |L| < min_leaf OR |R| < min_leaf THEN CONTINUE\n",
    "            H_L ← − Σ_k p_Lk log2 p_Lk  # left entropy\n",
    "            H_R ← − Σ_k p_Rk log2 p_Rk  # right entropy\n",
    "            gain ← base − (|L|/N)·H_L − (|R|/N)·H_R  # information gain\n",
    "            IF gain > best_gain THEN \n",
    "                best_gain ← gain ; best ← (j, t, L, R)\n",
    "    \n",
    "    IF best = NONE THEN\n",
    "        RETURN Leaf( class = majority(y) )\n",
    "    \n",
    "    (j*, t*, L, R) ← best\n",
    "    left  ← BUILD(X[L, :], y[L], depth + 1)\n",
    "    right ← BUILD(X[R, :], y[R], depth + 1)\n",
    "    RETURN Node(feature=j*, threshold=t*, left=left, right=right)\n",
    "\n",
    "tree ← BUILD(X, y, 0)\n",
    "\n",
    "# ----- predict -----\n",
    "ŷ ← list of length |X_query|\n",
    "FOR i = 1 TO |X_query| DO\n",
    "    node ← tree\n",
    "    WHILE node is not Leaf DO\n",
    "        IF X_query[i][node.feature] ≤ node.threshold THEN \n",
    "            node ← node.left\n",
    "        ELSE \n",
    "            node ← node.right\n",
    "    END WHILE\n",
    "    ŷ[i] ← node.class\n",
    "END FOR\n",
    "RETURN ŷ\n",
    "```\n",
    "\n",
    "**Note:** The algorithm uses **greedy splitting** — at each node, it picks the single best split without looking ahead. This is computationally efficient but may not find the globally optimal tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Decision trees have several key hyperparameters that control model complexity:\n",
    "\n",
    "- **`max_depth`**: Maximum depth of the tree. Limits how many levels of splits are allowed.\n",
    "  - Small values → simpler trees, may underfit\n",
    "  - Large values → complex trees, may overfit\n",
    "\n",
    "- **`min_samples_split`**: Minimum number of samples required to create a split (decision rule).\n",
    "  - Large values → fewer splits, simpler trees\n",
    "  - Small values → more splits, complex trees\n",
    "\n",
    "- **`min_samples_leaf`**: Minimum number of samples required to be in a leaf node.\n",
    "  - Large values → larger leaves, smoother decision boundaries\n",
    "  - Small values → smaller leaves, more granular boundaries\n",
    "\n",
    "**Trade-off**: These parameters control the **bias-variance trade-off**:\n",
    "- **Underfitting** (high bias): Tree is too simple, misses patterns\n",
    "- **Overfitting** (high variance): Tree memorizes noise, poor generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement Entropy Calculation\n",
    "\n",
    "Welcome to the hands-on implementation! We'll build the `MyDecisionTreeClassifier` class in **three independent exercises** to help you test and debug each component.\n",
    "\n",
    "**In this exercise, you'll implement:**\n",
    "- `_entropy()`: Calculate the entropy of a set of labels\n",
    "\n",
    "**Entropy Formula:**\n",
    "$$E = -\\sum_{i=1}^{N} p_i \\log_2(p_i)$$\n",
    "\n",
    "**Why separate exercises?**\n",
    "- Test each component immediately\n",
    "- No cascading failures\n",
    "- Build confidence step-by-step\n",
    "- Easy to debug if something goes wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Custom Decision Tree Classifier using entropy and information gain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int, default=None\n",
    "        Maximum depth of the tree. None means unlimited depth.\n",
    "    min_samples_split : int, default=2\n",
    "        Minimum number of samples required to split a node.\n",
    "    min_samples_leaf : int, default=1\n",
    "        Minimum number of samples required to be in a leaf node.\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of a label array.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like\n",
    "            Array of class labels (integers)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        entropy : float\n",
    "            Entropy value (0 = pure, higher = more impure)\n",
    "        \n",
    "        Formula\n",
    "        -------\n",
    "        E = -Σ p_i * log2(p_i) where p_i is the proportion of class i\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        - By convention, 0 * log2(0) = 0\n",
    "        - Use np.bincount to count occurrences of each class\n",
    "        \"\"\"\n",
    "        # TODO: Count occurrences of each class\n",
    "        # Hint: Use np.bincount(y, minlength=self.n_classes_) if available,\n",
    "        # or handle the general case\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # TODO: Calculate the proportion of each class\n",
    "        # Hint: counts / total_samples\n",
    "        counts = None\n",
    "        \n",
    "        # TODO: Calculate entropy using the formula: -Σ p_i * log2(p_i)\n",
    "        # Hint: Only include classes with p > 0 to avoid log(0)\n",
    "        # Use np.log2 for logarithm base 2\n",
    "        entropy = None\n",
    "        \n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 1 VERIFICATION: Testing Entropy Calculation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a test instance\n",
    "model_ex1 = MyDecisionTreeClassifier()\n",
    "model_ex1.n_classes_ = 2  # Set for testing\n",
    "\n",
    "print(\"\\n1. Testing entropy on pure sets (should be 0):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Pure set - all class 0\n",
    "y_pure_0 = np.array([0, 0, 0, 0, 0])\n",
    "entropy_pure_0 = model_ex1._entropy(y_pure_0)\n",
    "print(f\"All class 0: {y_pure_0} → Entropy = {entropy_pure_0:.4f}\")\n",
    "print(f\"Expected: 0.0000\")\n",
    "\n",
    "# Pure set - all class 1\n",
    "y_pure_1 = np.array([1, 1, 1, 1, 1])\n",
    "entropy_pure_1 = model_ex1._entropy(y_pure_1)\n",
    "print(f\"All class 1: {y_pure_1} → Entropy = {entropy_pure_1:.4f}\")\n",
    "print(f\"Expected: 0.0000\")\n",
    "\n",
    "print(\"\\n2. Testing entropy on maximally mixed set (should be 1.0 for binary):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Perfectly balanced binary\n",
    "y_balanced = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "entropy_balanced = model_ex1._entropy(y_balanced)\n",
    "print(f\"Balanced (5 vs 5): {y_balanced} → Entropy = {entropy_balanced:.4f}\")\n",
    "print(f\"Expected: 1.0000\")\n",
    "\n",
    "print(\"\\n3. Testing entropy on imbalanced sets:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# 9 muffins, 1 cookie (from lecture slides)\n",
    "y_low_entropy = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "entropy_low = model_ex1._entropy(y_low_entropy)\n",
    "print(f\"9 vs 1: {y_low_entropy} → Entropy = {entropy_low:.4f}\")\n",
    "print(f\"Expected: ~0.4690 (from lecture: 0.47)\")\n",
    "\n",
    "# 6 blue, 5 orange (from lecture slides numerical example)\n",
    "y_mixed = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "entropy_mixed = model_ex1._entropy(y_mixed)\n",
    "print(f\"6 vs 5: → Entropy = {entropy_mixed:.4f}\")\n",
    "print(f\"Expected: ~0.9940 (from lecture: 0.994)\")\n",
    "\n",
    "print(\"\\n4. Testing entropy on multiclass (3 classes):\")\n",
    "print(\"-\" * 70)\n",
    "model_ex1.n_classes_ = 3\n",
    "\n",
    "# 3 muffins, 4 cookies, 2 cakes (from lecture example)\n",
    "y_multiclass = np.array([0, 0, 0, 1, 1, 1, 1, 2, 2])\n",
    "entropy_multi = model_ex1._entropy(y_multiclass)\n",
    "print(f\"3 vs 4 vs 2: → Entropy = {entropy_multi:.4f}\")\n",
    "print(f\"Expected: ~1.5300 (from lecture: 1.52)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"If your outputs match the expected values, proceed to Exercise 2!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: What does entropy measure in the context of decision trees?\n",
    ">\n",
    "> A) The total number of samples in a node, used to determine when to stop splitting the tree\n",
    ">\n",
    "> B) The impurity or disorder of class labels in a node, where zero means all samples belong to one class\n",
    ">\n",
    "> C) The depth of the current node from the root, controlling the maximum complexity of the tree\n",
    ">\n",
    "> D) The information gain achieved by splitting on a particular feature at the current node\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Key Insight:** Entropy measures how \"mixed\" or \"impure\" the class labels are in a node. A pure node (entropy = 0) contains only one class, while a maximally impure node has equal proportions of all classes.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "Entropy quantifies uncertainty:\n",
    "- **E = 0**: All samples belong to one class (perfect purity, no uncertainty)\n",
    "- **E = 1**: For binary classification, 50-50 split (maximum uncertainty)\n",
    "- **E = log₂(N)**: Maximum entropy for N equally distributed classes\n",
    "\n",
    "**Example with numbers:**\n",
    "- 10 muffins, 0 cookies: E = -(10/10)log₂(10/10) = 0 (pure)\n",
    "- 5 muffins, 5 cookies: E = -2×(5/10)log₂(5/10) = 1.0 (maximum impurity)\n",
    "- 9 muffins, 1 cookie: E = -(9/10)log₂(9/10) - (1/10)log₂(1/10) ≈ 0.47 (low impurity)\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **A is FALSE**: Sample count is not entropy. The number of samples in a node affects the weighted average in information gain calculations, but entropy specifically measures class distribution impurity, not size.\n",
    "- **C is FALSE**: Depth is a separate concept from entropy. Depth limits tree complexity through the `max_depth` hyperparameter, while entropy measures label purity at a specific node regardless of its depth.\n",
    "- **D is FALSE**: This describes information gain, not entropy. Information gain is computed as the difference between parent entropy and the weighted sum of child entropies. Entropy is a prerequisite for calculating information gain.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement Information Gain Calculation\n",
    "\n",
    "Excellent! You now have a working entropy function. Let's implement **information gain** — the criterion for selecting the best split.\n",
    "\n",
    "**What you'll implement:**\n",
    "- `_information_gain()`: Calculate the reduction in entropy from a split\n",
    "\n",
    "**Information Gain Formula:**\n",
    "$$IG = E_{parent} - \\frac{|L|}{N} E_L - \\frac{|R|}{N} E_R$$\n",
    "\n",
    "**Why this matters:**\n",
    "Information gain tells us how much a split reduces uncertainty. We always choose the split with the highest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add information gain method to MyDecisionTreeClassifier\n",
    "def _information_gain(self, y_parent, y_left, y_right):\n",
    "    \"\"\"\n",
    "    Calculate information gain from a split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_parent : array-like\n",
    "        Labels before the split\n",
    "    y_left : array-like\n",
    "        Labels in the left child after split\n",
    "    y_right : array-like\n",
    "        Labels in the right child after split\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gain : float\n",
    "        Information gain (higher = better split)\n",
    "    \n",
    "    Formula\n",
    "    -------\n",
    "    IG = E_parent - (|L|/N * E_L + |R|/N * E_R)\n",
    "    \"\"\"\n",
    "    n_parent = len(y_parent)\n",
    "    n_left = len(y_left)\n",
    "    n_right = len(y_right)\n",
    "    \n",
    "    if n_left == 0 or n_right == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # TODO: Calculate parent entropy using your _entropy method\n",
    "    e_parent = None\n",
    "    \n",
    "    # TODO: Calculate left and right child entropies\n",
    "    e_left = None\n",
    "    e_right = None\n",
    "    \n",
    "    # TODO: Calculate weighted average of child entropies\n",
    "    # weighted_child_entropy = (n_left/n_parent) * e_left + (n_right/n_parent) * e_right\n",
    "    weighted_child_entropy = None\n",
    "    \n",
    "    # TODO: Calculate information gain = parent entropy - weighted child entropy\n",
    "    gain = None\n",
    "    \n",
    "    return gain\n",
    "\n",
    "# Add the method to the class\n",
    "MyDecisionTreeClassifier._information_gain = _information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: Why do we use weighted average of child entropies in information gain?\n",
    ">\n",
    "> A) To give equal importance to both child nodes regardless of their size in the calculation\n",
    ">\n",
    "> B) To penalize splits that create very small child nodes which might contain noise\n",
    ">\n",
    "> C) To account for the proportion of samples going to each child, since larger subsets have more influence\n",
    ">\n",
    "> D) To ensure the information gain is always positive and bounded between zero and one\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: C**\n",
    "\n",
    "**Key Insight:** Weighted averaging accounts for how many samples flow into each child node. A split that creates one very pure but tiny child and one large impure child isn't necessarily good—we care about the overall reduction in uncertainty across all samples.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "The weighted formula is:\n",
    "$$IG = E_{parent} - \\frac{|L|}{N} E_L - \\frac{|R|}{N} E_R$$\n",
    "\n",
    "**Example:**\n",
    "- Parent: 100 samples, E = 1.0\n",
    "- Split A: Left (99 samples, E=0.8), Right (1 sample, E=0)\n",
    "  - Weighted: (99/100)×0.8 + (1/100)×0 = 0.792\n",
    "  - IG = 1.0 - 0.792 = 0.208\n",
    "- Split B: Left (50 samples, E=0.5), Right (50 samples, E=0.5)\n",
    "  - Weighted: (50/100)×0.5 + (50/100)×0.5 = 0.5\n",
    "  - IG = 1.0 - 0.5 = 0.5\n",
    "\n",
    "Split B is better because it reduces uncertainty across more samples, even though Split A created a \"perfect\" (but tiny) right child.\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **A is FALSE**: Equal weighting would overvalue tiny pure nodes. A split creating 1 pure sample vs 99 mixed samples would appear artificially good.\n",
    "- **B is FALSE**: While weighted averaging does de-emphasize tiny nodes, this is a consequence of proper probability weighting, not an explicit penalty. The formula doesn't directly penalize small nodes.\n",
    "- **D is FALSE**: Information gain is guaranteed to be non-negative (you can't increase entropy by splitting), but it's not bounded by 1. For multiclass problems, the maximum possible gain equals the parent entropy, which can exceed 1.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 2 VERIFICATION: Testing Information Gain Calculation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test instance\n",
    "model_ex2 = MyDecisionTreeClassifier()\n",
    "model_ex2.n_classes_ = 2\n",
    "\n",
    "print(\"\\n1. Testing information gain (from lecture slides - Latte/Mocha example):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Parent: 12 muffins, 5 cookies, 2 cakes → 19 total\n",
    "# After split on Drink:\n",
    "#   Latte: 9 muffins, 1 cookie → 10 samples (low entropy)\n",
    "#   Mocha: 3 muffins, 4 cookies, 2 cakes → 9 samples (high entropy)\n",
    "\n",
    "# Simplified binary case from lecture: E_parent = 0.994 (6 blue, 5 orange)\n",
    "y_parent = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # 6 vs 5\n",
    "\n",
    "# Split at x1 = 1.0: Left gets 5 blue, 1 orange; Right gets 1 blue, 4 orange\n",
    "y_left = np.array([0, 0, 0, 0, 0, 1])   # 5 vs 1 → E ≈ 0.65\n",
    "y_right = np.array([0, 1, 1, 1, 1])      # 1 vs 4 → E ≈ 0.72\n",
    "\n",
    "ig = model_ex2._information_gain(y_parent, y_left, y_right)\n",
    "print(f\"Parent: 6 blue, 5 orange (E ≈ 0.994)\")\n",
    "print(f\"Left: 5 blue, 1 orange | Right: 1 blue, 4 orange\")\n",
    "print(f\"Information Gain = {ig:.4f}\")\n",
    "print(f\"Expected: ~0.31\")\n",
    "\n",
    "print(\"\\n2. Testing perfect split (IG should equal parent entropy):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Perfect split separates classes completely\n",
    "y_parent_perfect = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "y_left_perfect = np.array([0, 0, 0, 0, 0])   # All class 0\n",
    "y_right_perfect = np.array([1, 1, 1, 1, 1])  # All class 1\n",
    "\n",
    "ig_perfect = model_ex2._information_gain(y_parent_perfect, y_left_perfect, y_right_perfect)\n",
    "print(f\"Perfect split: All 0s left, All 1s right\")\n",
    "print(f\"Information Gain = {ig_perfect:.4f}\")\n",
    "print(f\"Expected: 1.0000 (equals parent entropy)\")\n",
    "\n",
    "print(\"\\n3. Testing no improvement split (IG should be 0):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Split that doesn't improve purity\n",
    "y_parent_same = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n",
    "y_left_same = np.array([0, 0, 1, 1])   # 50-50\n",
    "y_right_same = np.array([0, 0, 1, 1])  # 50-50\n",
    "\n",
    "ig_same = model_ex2._information_gain(y_parent_same, y_left_same, y_right_same)\n",
    "print(f\"No improvement: Both children have same distribution as parent\")\n",
    "print(f\"Information Gain = {ig_same:.4f}\")\n",
    "print(f\"Expected: 0.0000\")\n",
    "\n",
    "print(\"\\n4. Testing lecture example (Seating split):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# From slide 14: Seating split on café dataset\n",
    "# Parent: 10 muffins, 9 cookies, 6 cakes = 25 samples, E = 1.554\n",
    "model_ex2.n_classes_ = 3\n",
    "y_cafe_parent = np.array([0]*10 + [1]*9 + [2]*6)  # muffin=0, cookie=1, cake=2\n",
    "\n",
    "# Indoor: 10 muffins, 0 cookies, 6 cakes = 16 samples\n",
    "y_indoor = np.array([0]*10 + [2]*6)\n",
    "# Outdoor: 0 muffins, 9 cookies, 0 cakes = 9 samples (pure!)\n",
    "y_outdoor = np.array([1]*9)\n",
    "\n",
    "ig_seating = model_ex2._information_gain(y_cafe_parent, y_indoor, y_outdoor)\n",
    "print(f\"Seating split: Indoor (10 muffin, 6 cake) | Outdoor (9 cookie)\")\n",
    "print(f\"Information Gain = {ig_seating:.4f}\")\n",
    "print(f\"Expected: ~0.943 (from lecture slide 14)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"If your outputs match the expected values, proceed to Exercise 3!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement Full Decision Tree Classifier\n",
    "\n",
    "Excellent! You now have working components:\n",
    "- ✅ Entropy calculation\n",
    "- ✅ Information gain calculation\n",
    "\n",
    "Now let's put it all together and implement the **complete decision tree classifier**!\n",
    "\n",
    "**What you'll implement:**\n",
    "- `_find_best_split()`: Find the best feature and threshold to split on\n",
    "- `_build_tree()`: Recursively build the tree\n",
    "- `fit()`: Train the model\n",
    "- `predict()` and `predict_proba()`: Make predictions\n",
    "\n",
    "**The tree building process:**\n",
    "1. Check stopping conditions (pure node, max depth, min samples)\n",
    "2. Find the best split (feature + threshold with highest information gain)\n",
    "3. Split the data and recursively build left and right subtrees\n",
    "4. Return a leaf node if no good split is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the MyDecisionTreeClassifier class\n",
    "def _find_best_split(self, X, y):\n",
    "    \"\"\"\n",
    "    Find the best feature and threshold to split on.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_split : dict or None\n",
    "        Dictionary with 'feature', 'threshold', 'left_mask', 'right_mask', 'gain'\n",
    "        Returns None if no valid split is found\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    best_gain = 0.0\n",
    "    best_split = None\n",
    "    \n",
    "    # TODO: Iterate over all features\n",
    "    for feature_idx in range(n_features):\n",
    "        # Get unique values and compute midpoint thresholds\n",
    "        feature_values = X[:, feature_idx]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        \n",
    "        if len(unique_values) < 2:\n",
    "            continue\n",
    "        \n",
    "        # TODO: Calculate midpoint thresholds between consecutive unique values\n",
    "        # Hint: thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "        thresholds = None\n",
    "        \n",
    "        # TODO: Try each threshold\n",
    "        for threshold in thresholds:\n",
    "            # TODO: Create masks for left (<=) and right (>) children\n",
    "            left_mask = None\n",
    "            right_mask = None\n",
    "            \n",
    "            # Check minimum samples constraint\n",
    "            n_left = np.sum(left_mask)\n",
    "            n_right = np.sum(right_mask)\n",
    "            \n",
    "            if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "                continue\n",
    "            \n",
    "            # TODO: Calculate information gain for this split\n",
    "            y_left = y[left_mask]\n",
    "            y_right = y[right_mask]\n",
    "            gain = None  # Use self._information_gain(y, y_left, y_right)\n",
    "            \n",
    "            # TODO: Update best split if this is better\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split = {\n",
    "                    'feature': feature_idx,\n",
    "                    'threshold': threshold,\n",
    "                    'left_mask': left_mask,\n",
    "                    'right_mask': right_mask,\n",
    "                    'gain': gain\n",
    "                }\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "def _build_tree(self, X, y, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively build the decision tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Feature matrix for current node\n",
    "    y : array-like\n",
    "        Labels for current node\n",
    "    depth : int\n",
    "        Current depth in the tree\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    node : dict\n",
    "        Either a leaf node {'leaf': True, 'proba': [...], 'class': c}\n",
    "        or an internal node {'leaf': False, 'feature': f, 'threshold': t, 'left': ..., 'right': ...}\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    \n",
    "    # Calculate class probabilities for this node\n",
    "    counts = np.bincount(y, minlength=self.n_classes_)\n",
    "    proba = counts / counts.sum()\n",
    "    majority_class = np.argmax(counts)\n",
    "    \n",
    "    # TODO: Check stopping conditions\n",
    "    # 1. All samples belong to the same class (pure node)\n",
    "    # 2. Reached maximum depth (if max_depth is set)\n",
    "    # 3. Not enough samples to split (< min_samples_split)\n",
    "    \n",
    "    # Condition 1: Pure node\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
    "    \n",
    "    # Condition 2: Max depth reached\n",
    "    if self.max_depth is not None and depth >= self.max_depth:\n",
    "        return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
    "    \n",
    "    # Condition 3: Not enough samples\n",
    "    if n_samples < self.min_samples_split:\n",
    "        return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
    "    \n",
    "    # TODO: Find the best split\n",
    "    best_split = self._find_best_split(X, y)\n",
    "    \n",
    "    # No valid split found\n",
    "    if best_split is None:\n",
    "        return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
    "    \n",
    "    # TODO: Recursively build left and right subtrees\n",
    "    left_tree = self._build_tree(\n",
    "        X[best_split['left_mask']], \n",
    "        y[best_split['left_mask']], \n",
    "        depth + 1\n",
    "    )\n",
    "    right_tree = self._build_tree(\n",
    "        X[best_split['right_mask']], \n",
    "        y[best_split['right_mask']], \n",
    "        depth + 1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'leaf': False,\n",
    "        'feature': best_split['feature'],\n",
    "        'threshold': best_split['threshold'],\n",
    "        'left': left_tree,\n",
    "        'right': right_tree,\n",
    "        'gain': best_split['gain']\n",
    "    }\n",
    "\n",
    "def fit(self, X, y):\n",
    "    \"\"\"\n",
    "    Build the decision tree from training data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Training labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    self : object\n",
    "        Returns self for method chaining\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Store class information\n",
    "    self.classes_, y_encoded = np.unique(y, return_inverse=True)\n",
    "    self.n_classes_ = len(self.classes_)\n",
    "    self.n_features_ = X.shape[1]\n",
    "    \n",
    "    # Build the tree\n",
    "    self.tree_ = self._build_tree(X, y_encoded, depth=0)\n",
    "    \n",
    "    return self\n",
    "\n",
    "def _predict_single(self, x, node):\n",
    "    \"\"\"\n",
    "    Predict class probabilities for a single sample by traversing the tree.\n",
    "    \"\"\"\n",
    "    # TODO: Traverse tree until reaching a leaf\n",
    "    while not node['leaf']:\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            node = node['left']\n",
    "        else:\n",
    "            node = node['right']\n",
    "    \n",
    "    return node['proba']\n",
    "\n",
    "def predict_proba(self, X):\n",
    "    \"\"\"\n",
    "    Predict class probabilities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Samples to predict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    proba : array, shape (n_samples, n_classes)\n",
    "        Class probabilities for each sample\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    probas = np.array([self._predict_single(x, self.tree_) for x in X])\n",
    "    return probas\n",
    "\n",
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    Predict class labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Samples to predict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array, shape (n_samples,)\n",
    "        Predicted class labels\n",
    "    \"\"\"\n",
    "    proba = self.predict_proba(X)\n",
    "    return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "# Add all methods to the class\n",
    "MyDecisionTreeClassifier._find_best_split = _find_best_split\n",
    "MyDecisionTreeClassifier._build_tree = _build_tree\n",
    "MyDecisionTreeClassifier.fit = fit\n",
    "MyDecisionTreeClassifier._predict_single = _predict_single\n",
    "MyDecisionTreeClassifier.predict_proba = predict_proba\n",
    "MyDecisionTreeClassifier.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: What is the greedy approach in decision tree learning?\n",
    ">\n",
    "> A) The algorithm considers all possible tree structures and selects the globally optimal one\n",
    ">\n",
    "> B) At each node, it selects the locally best split without considering future consequences\n",
    ">\n",
    "> C) The algorithm prunes the tree after building it to remove unnecessary branches\n",
    ">\n",
    "> D) It randomly selects features and thresholds to create diverse decision boundaries\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Key Insight:** Greedy algorithms make the locally optimal choice at each step. In decision trees, this means selecting the split with the highest information gain at the current node, without considering how this choice affects future splits.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "At each node, the algorithm:\n",
    "1. Evaluates all possible (feature, threshold) combinations\n",
    "2. Selects the one with maximum information gain\n",
    "3. Never backtracks or reconsiders previous decisions\n",
    "\n",
    "**Trade-off:**\n",
    "- **Advantage**: Computationally efficient (polynomial time vs NP-hard for global optimization)\n",
    "- **Disadvantage**: May not find the globally optimal tree\n",
    "\n",
    "**Example:**\n",
    "A greedy split on feature A might look best now, but splitting on feature B first could lead to a better overall tree. Greedy algorithms can't \"see\" this.\n",
    "\n",
    "**Why other answers are incorrect:**\n",
    "\n",
    "- **A is FALSE**: Finding the globally optimal decision tree is NP-hard. Greedy algorithms don't consider all possible trees—they build incrementally, making local decisions.\n",
    "- **C is FALSE**: This describes post-pruning, a technique to prevent overfitting. Pruning is separate from the greedy splitting process and typically uses validation data.\n",
    "- **D is FALSE**: This describes Random Forests, not standard decision trees. Random Forests intentionally introduce randomness to create diverse trees for ensemble learning.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 3 VERIFICATION: Testing Complete Decision Tree\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test data from lecture slides (numerical features example)\n",
    "X_test = np.array([\n",
    "    [-0.5, -4.0], [-1.5, -2.5], [0.0, 0.0], [-1.0, 0.5], [0.5, 1.5], [2.5, 1.0],\n",
    "    [3.5, -3.5], [2.0, -3.0], [3.0, -2.0], [1.5, -1.5], [4.0, -1.0]\n",
    "])\n",
    "y_test = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "print(\"\\n1. Training on lecture example data:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_ex3 = MyDecisionTreeClassifier(max_depth=3, min_samples_leaf=1)\n",
    "model_ex3.fit(X_test, y_test)\n",
    "\n",
    "print(f\"✓ Training completed\")\n",
    "print(f\"✓ Number of classes: {model_ex3.n_classes_}\")\n",
    "print(f\"✓ Classes: {model_ex3.classes_}\")\n",
    "\n",
    "print(\"\\n2. Testing predictions:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "y_pred = model_ex3.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"True labels:      {y_test}\")\n",
    "print(f\"Predicted labels: {y_pred}\")\n",
    "print(f\"Training Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "print(\"\\n3. Testing single prediction (from lecture - point (2, -2)):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "X_new = np.array([[2.0, -2.0]])\n",
    "proba = model_ex3.predict_proba(X_new)[0]\n",
    "pred = model_ex3.predict(X_new)[0]\n",
    "\n",
    "print(f\"New Point: {X_new[0].tolist()}\")\n",
    "print(f\"Probabilities [class 0, class 1]: {proba.tolist()}\")\n",
    "print(f\"Predicted Class: {pred}\")\n",
    "print(f\"Expected Class: 1 (from lecture slide 41)\")\n",
    "\n",
    "print(\"\\n4. Tree structure check:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def print_tree(node, indent=0):\n",
    "    prefix = \"  \" * indent\n",
    "    if node['leaf']:\n",
    "        print(f\"{prefix}Leaf: class={node['class']}, proba={node['proba']}\")\n",
    "    else:\n",
    "        print(f\"{prefix}Node: X[{node['feature']}] <= {node['threshold']:.3f} (gain={node['gain']:.3f})\")\n",
    "        print(f\"{prefix}  Left:\")\n",
    "        print_tree(node['left'], indent + 2)\n",
    "        print(f\"{prefix}  Right:\")\n",
    "        print_tree(node['right'], indent + 2)\n",
    "\n",
    "print_tree(model_ex3.tree_)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUCCESS! Your complete implementation is working!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nYou can now proceed to apply your model to larger datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Classification Dataset\n\nWe'll use the **mixture dataset from the lecture slides (slides 43-52)**, which is also used in the Regularisation and Logistic Regression labs. This dataset contains two classes with non-linear separation, making it perfect for demonstrating:\n1. How decision trees partition feature space with axis-aligned splits\n2. The bias-variance trade-off with tree depth\n3. How hyperparameters control model complexity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download the mixture dataset from Google Drive\n# File ID: 1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT\n# Direct download URL\nurl = 'https://drive.google.com/uc?id=1Ls7f9OWKgeWswFR4EZ5eeoohfY9PACRT'\n\n# Load data\ndf = pd.read_csv(url)\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(df.head())\nprint(f\"\\nColumn names: {df.columns.tolist()}\")\n\n# Extract features and labels (last column is the label)\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values.astype(int)\n\nprint(f\"\\nFeatures shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\nprint(f\"Class distribution: {np.bincount(y)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure(figsize=(10, 8))\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Class 0',\n            edgecolors='k', s=50, alpha=0.7)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='skyblue', label='Class 1',\n            edgecolors='k', s=50, alpha=0.7)\nplt.xlabel('$x_1$', fontsize=14)\nplt.ylabel('$x_2$', fontsize=14)\nplt.title('Mixture Dataset (Non-Linear Boundary)', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split data (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Train class distribution: {np.bincount(y_train)}\")\nprint(f\"Test class distribution: {np.bincount(y_test)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = MyDecisionTreeClassifier(max_depth=3, min_samples_split=2, min_samples_leaf=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Number of classes: {model.n_classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Show some predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(min(5, len(y_test))):\n",
    "    print(f\"True: {y_test[i]}, Predicted: {y_pred[i]}, P(class=1): {y_proba[i, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\n[TN  FP]\")\n",
    "print(\"[FN  TP]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', fontsize=20)\n",
    "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundary\n",
    "\n",
    "Decision trees create axis-aligned (\"staircase\") decision boundaries. Let's visualize how our tree partitions the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n    \"\"\"Plot decision boundary for a 2D classification problem.\"\"\"\n    # Create mesh\n    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n                           np.linspace(x2_min, x2_max, 200))\n    \n    # Predict on mesh\n    X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n    if hasattr(model, 'predict_proba'):\n        Z = model.predict_proba(X_mesh)[:, 1].reshape(xx1.shape)\n    else:\n        Z = model.predict(X_mesh).reshape(xx1.shape)\n    \n    # Plot\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx1, xx2, Z, levels=20, cmap='RdBu_r', alpha=0.6)\n    plt.colorbar(label='P(Class 1)')\n    plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)\n    \n    # Plot data points\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Class 0', \n                edgecolors='k', s=50, alpha=0.7)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='skyblue', label='Class 1', \n                edgecolors='k', s=50, alpha=0.7)\n    \n    plt.xlabel('$x_1$', fontsize=14)\n    plt.ylabel('$x_2$', fontsize=14)\n    plt.title(title, fontsize=16)\n    plt.legend(fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\nplot_decision_boundary(model, X, y, \"Decision Tree Decision Boundary (max_depth=3)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "So far we've evaluated our model using a single train/test split. Let's use K-fold cross-validation for a more robust performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use training data for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    MyDecisionTreeClassifier(max_depth=3, min_samples_split=2, min_samples_leaf=1),\n",
    "    X_train, y_train, cv=kf, scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: Effect of max_depth\n",
    "\n",
    "Let's explore how different values of `max_depth` affect model performance and the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 2, 3, 4, 5, None]  # None means unlimited\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    model_depth = MyDecisionTreeClassifier(max_depth=depth, min_samples_split=2, min_samples_leaf=1)\n",
    "    \n",
    "    # Training score\n",
    "    model_depth.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, model_depth.predict(X_train))\n",
    "    train_scores.append(train_acc)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model_depth, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    val_scores.append(np.mean(cv_scores))\n",
    "    \n",
    "    depth_str = str(depth) if depth is not None else \"None\"\n",
    "    print(f\"max_depth={depth_str:>4}: Train={train_acc:.3f}, CV={np.mean(cv_scores):.3f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in depths]\n",
    "x_pos = range(len(depths))\n",
    "plt.plot(x_pos, train_scores, 'o-', label='Train Accuracy', linewidth=2, markersize=8)\n",
    "plt.plot(x_pos, val_scores, 's-', label='CV Accuracy', linewidth=2, markersize=8)\n",
    "plt.xticks(x_pos, depth_labels)\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effect of max_depth on Model Performance', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Effect of Hyperparameters on Decision Boundary\n",
    "\n",
    "Let's see how different `min_samples_split` values affect the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "min_samples_values = [2, 4, 8, 12]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.ravel()\n\nfor idx, min_samples in enumerate(min_samples_values):\n    ax = axes[idx]\n    \n    # Train model\n    model_ms = MyDecisionTreeClassifier(max_depth=None, min_samples_split=min_samples, min_samples_leaf=1)\n    model_ms.fit(X_train, y_train)\n    \n    # Cross-validation score\n    cv_scores = cross_val_score(model_ms, X_train, y_train, cv=5, scoring='accuracy')\n    \n    # Create mesh\n    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 150),\n                           np.linspace(x2_min, x2_max, 150))\n    \n    X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n    Z = model_ms.predict_proba(X_mesh)[:, 1].reshape(xx1.shape)\n    \n    # Plot\n    ax.contourf(xx1, xx2, Z, levels=20, cmap='RdBu_r', alpha=0.6)\n    ax.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)\n    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n               c='orange', edgecolors='k', s=40, alpha=0.7)\n    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n               c='skyblue', edgecolors='k', s=40, alpha=0.7)\n    ax.set_title(f'min_samples_split = {min_samples}, CV Acc = {np.mean(cv_scores):.2f}', fontsize=12)\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Bias-Variance Trade-off\n",
    "\n",
    "From the visualizations above:\n",
    "\n",
    "- **Small min_samples_split (e.g., 2)**: Complex boundaries, may overfit to noise\n",
    "  - High variance, low bias\n",
    "  - High train accuracy, potentially lower validation accuracy\n",
    "\n",
    "- **Large min_samples_split (e.g., 12)**: Simple boundaries, may underfit\n",
    "  - Low variance, high bias  \n",
    "  - Lower train and validation accuracy\n",
    "\n",
    "- **Optimal**: Balance between complexity and generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "Decision trees provide natural feature importances based on how much each feature contributes to reducing entropy (information gain) across all splits.\n",
    "\n",
    "**How it works:**\n",
    "1. At each split, calculate: `weighted_gain = (n_samples_at_node / n_total) × information_gain`\n",
    "2. Sum these contributions for each feature\n",
    "3. Normalize so all importances sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importances(model):\n",
    "    \"\"\"\n",
    "    Compute feature importances based on weighted information gain.\n",
    "    \"\"\"\n",
    "    importances = np.zeros(model.n_features_)\n",
    "    \n",
    "    def traverse(node, n_samples):\n",
    "        if node['leaf']:\n",
    "            return\n",
    "        \n",
    "        # Weight the gain by the proportion of samples at this node\n",
    "        weight = n_samples / len(y_train)\n",
    "        importances[node['feature']] += weight * node['gain']\n",
    "        \n",
    "        # Estimate samples in children (rough approximation)\n",
    "        n_left = n_samples // 2\n",
    "        n_right = n_samples - n_left\n",
    "        \n",
    "        traverse(node['left'], n_left)\n",
    "        traverse(node['right'], n_right)\n",
    "    \n",
    "    traverse(model.tree_, len(y_train))\n",
    "    \n",
    "    # Normalize\n",
    "    if importances.sum() > 0:\n",
    "        importances = importances / importances.sum()\n",
    "    \n",
    "    return importances\n",
    "\n",
    "# Train a fresh model\n",
    "model_fi = MyDecisionTreeClassifier(max_depth=3, min_samples_split=2, min_samples_leaf=1)\n",
    "model_fi.fit(X_train, y_train)\n",
    "\n",
    "# Compute importances\n",
    "importances = compute_feature_importances(model_fi)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for i, imp in enumerate(importances):\n",
    "    print(f\"  Feature X[{i}]: {imp:.4f} ({imp*100:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['$X_1$', '$X_2$'], importances, color=['steelblue', 'coral'], edgecolor='black')\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importances', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "for i, imp in enumerate(importances):\n",
    "    plt.text(i, imp + 0.02, f'{imp:.2%}', ha='center', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with scikit-learn\n",
    "\n",
    "Let's validate our implementation by comparing it with sklearn's `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sklearn model\n",
    "sklearn_model = SklearnDecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare accuracies\n",
    "our_train_acc = accuracy_score(y_train, model_fi.predict(X_train))\n",
    "our_test_acc = accuracy_score(y_test, model_fi.predict(X_test))\n",
    "sklearn_train_acc = sklearn_model.score(X_train, y_train)\n",
    "sklearn_test_acc = sklearn_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':<20} {'Train Acc':<15} {'Test Acc':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Our Implementation':<20} {our_train_acc:<15.4f} {our_test_acc:<15.4f}\")\n",
    "print(f\"{'sklearn':<20} {sklearn_train_acc:<15.4f} {sklearn_test_acc:<15.4f}\")\n",
    "\n",
    "print(\"\\nFeature Importances Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Feature':<15} {'Our Model':<15} {'sklearn':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(model_fi.n_features_):\n",
    "    print(f\"{'X[' + str(i) + ']':<15} {importances[i]:<15.4f} {sklearn_model.feature_importances_[i]:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sklearn Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(sklearn_model, \n",
    "          feature_names=['$X_1$', '$X_2$'], \n",
    "          class_names=['Class 0', 'Class 1'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('sklearn Decision Tree Structure', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Controlling Overfitting\n",
    "- **max_depth**: Limit tree depth to prevent overly complex trees\n",
    "- **min_samples_split**: Require minimum samples to create a split\n",
    "- **min_samples_leaf**: Require minimum samples in each leaf\n",
    "- **Use cross-validation** to select optimal hyperparameters\n",
    "\n",
    "### 2. Feature Handling\n",
    "- **No scaling required**: Trees are invariant to monotonic transformations\n",
    "- **Categorical features**: Use label encoding (as shown in lecture slides)\n",
    "- **Missing values**: Some implementations handle them; consider imputation for custom code\n",
    "\n",
    "### 3. Interpretability\n",
    "- **Visualize the tree**: Use `sklearn.tree.plot_tree` or export to graphviz\n",
    "- **Extract rules**: Convert tree to if-else statements\n",
    "- **Feature importances**: Identify most predictive features\n",
    "\n",
    "### 4. When Trees Struggle\n",
    "- **Linear boundaries**: Logistic regression may be simpler and better\n",
    "- **High variance**: Consider Random Forests (ensemble of trees)\n",
    "- **Extrapolation**: Trees cannot extrapolate beyond training data range\n",
    "\n",
    "### 5. Evaluation\n",
    "- **Use stratified splits** for imbalanced classes\n",
    "- **Report multiple metrics**: Accuracy, precision, recall, F1-score\n",
    "- **Visualize decision boundaries** to understand model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "1. ✅ **Implemented** entropy calculation to measure node impurity\n",
    "2. ✅ **Implemented** information gain to evaluate split quality\n",
    "3. ✅ **Built** a complete decision tree classifier from scratch using recursive partitioning\n",
    "4. ✅ **Trained** the model on synthetic classification data\n",
    "5. ✅ **Evaluated** performance using accuracy, confusion matrix, and cross-validation\n",
    "6. ✅ **Visualized** decision boundaries showing axis-aligned splits\n",
    "7. ✅ **Tuned** hyperparameters (max_depth, min_samples_split) and observed bias-variance trade-off\n",
    "8. ✅ **Computed** feature importances based on information gain contributions\n",
    "9. ✅ **Compared** your implementation with scikit-learn\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Entropy** measures impurity: $E = -\\sum p_i \\log_2(p_i)$\n",
    "- **Information Gain** is the reduction in entropy: $IG = E_{parent} - \\text{weighted avg}(E_{children})$\n",
    "- **Greedy splitting** selects the locally best split at each node\n",
    "- **Hyperparameters** control the bias-variance trade-off:\n",
    "  - Small values → complex trees, potential overfitting\n",
    "  - Large values → simple trees, potential underfitting\n",
    "- **Feature importances** show which features contribute most to predictions\n",
    "- **No feature scaling** is required for decision trees\n",
    "- **Decision boundaries** are axis-aligned (rectangular regions)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply decision trees to real-world datasets (e.g., Iris, Wine, Breast Cancer)\n",
    "- Implement Gini impurity as an alternative to entropy\n",
    "- Learn about Random Forests (ensembles of trees) to reduce variance\n",
    "- Explore Gradient Boosted Trees (XGBoost, LightGBM) for state-of-the-art performance\n",
    "- Implement pruning techniques to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}