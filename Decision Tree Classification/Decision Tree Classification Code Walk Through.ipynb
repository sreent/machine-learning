{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/machine-learning/blob/main/Decision%20Tree%20Classification/Decision%20Tree%20Classification%20Code%20Walk%20Through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeMstry0h7af"
      },
      "source": [
        "# Decision Tree Classification: Code Walk Through\n",
        "\n",
        "This notebook walks through the **computational steps** of Decision Tree Classification from scratch.\n",
        "\n",
        "## What We'll Cover:\n",
        "1. **Calculate Entropy** - measure impurity/disorder in a dataset\n",
        "2. **Calculate Information Gain** - measure how much a split reduces entropy\n",
        "3. **Find Best Split** - evaluate all possible thresholds for numerical features\n",
        "4. **Build the Tree** - recursively partition the feature space\n",
        "5. **Make Predictions** - traverse the tree to classify new points\n",
        "6. **Visualize Decision Boundaries** - see the axis-aligned partitions\n",
        "\n",
        "We'll show **both manual calculations** (to understand the logic) and **vectorized NumPy versions** (for efficiency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xd4hNXmh7af"
      },
      "source": [
        "## Step 1: Import Libraries\n",
        "\n",
        "We need:\n",
        "- **NumPy** for numerical operations\n",
        "- **Matplotlib** for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjpXv2njh7af"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBpOlvmxh7ag"
      },
      "source": [
        "## Step 2: Create Training Data\n",
        "\n",
        "We'll use the same dataset from the lecture slides (slides 21-27):\n",
        "- **11 training points** with **2 numerical features** ($x_1$ and $x_2$)\n",
        "- **2 classes**: class 0 (blue) and class 1 (orange)\n",
        "\n",
        "This dataset has a clear pattern that decision trees can capture with axis-aligned splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4VlQk7Wh7ah"
      },
      "outputs": [],
      "source": [
        "# Training data from lecture slides (numerical features example)\n",
        "X_train = np.array([\n",
        "    [-0.5, -4.0],   # Point 0, class 0\n",
        "    [-1.5, -2.5],   # Point 1, class 0\n",
        "    [ 0.0,  0.0],   # Point 2, class 0\n",
        "    [-1.0,  0.5],   # Point 3, class 0\n",
        "    [ 0.5,  1.5],   # Point 4, class 0\n",
        "    [ 2.5,  1.0],   # Point 5, class 0\n",
        "    [ 3.5, -3.5],   # Point 6, class 1\n",
        "    [ 2.0, -3.0],   # Point 7, class 1\n",
        "    [ 3.0, -2.0],   # Point 8, class 1\n",
        "    [ 1.5, -1.5],   # Point 9, class 1\n",
        "    [ 4.0, -1.0]    # Point 10, class 1\n",
        "])\n",
        "\n",
        "# Labels: which class each point belongs to\n",
        "y_train = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)  # (11, 2) = 11 points, 2 features\n",
        "print(\"Labels shape:\", y_train.shape)         # (11,) = 11 labels\n",
        "print(f\"\\nClass distribution: {np.bincount(y_train)}\")\n",
        "print(f\"  Class 0: {np.sum(y_train == 0)} samples\")\n",
        "print(f\"  Class 1: {np.sum(y_train == 1)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbGd1194h7ah"
      },
      "source": [
        "## Step 3: Visualize the Data\n",
        "\n",
        "Let's plot our training data to see how it's distributed in 2D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xPwrN-hh7ah"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', s=150, edgecolors='black', linewidths=1.5,\n",
        "            label='Class 0')\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', s=150, edgecolors='black', linewidths=1.5,\n",
        "            label='Class 1')\n",
        "plt.xlabel('$x_1$', fontsize=14)\n",
        "plt.ylabel('$x_2$', fontsize=14)\n",
        "plt.title('Training Data Visualization', fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "print(f\"We have {len(X_train)} training points in 2D space\")\n",
        "print(\"Notice: Class 0 (blue) tends to be on the left/top, Class 1 (orange) on the right/bottom\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jafg3xRWh7ai"
      },
      "source": [
        "## Step 4: Understanding Entropy\n",
        "\n",
        "**Entropy** measures the **impurity** or **disorder** in a dataset. It tells us how \"mixed\" the classes are.\n",
        "\n",
        "$$E = -\\sum_{i=1}^{N} p_i \\log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "- $p_i$ is the proportion of samples belonging to class $i$\n",
        "- $N$ is the number of classes\n",
        "- By convention, $0 \\cdot \\log_2(0) = 0$\n",
        "\n",
        "**Key insights:**\n",
        "- **Pure node** (all same class): Entropy = 0\n",
        "- **Maximally mixed** (equal proportions): Entropy = $\\log_2(N)$\n",
        "- For binary classification: max entropy = 1.0 (when 50-50 split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJBcbsrh7ai"
      },
      "source": [
        "### Manual Entropy Calculation\n",
        "\n",
        "Let's calculate the entropy of our full training set step by step.\n",
        "\n",
        "We have 6 samples of class 0 and 5 samples of class 1 (total = 11)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Lqwj8Yh7ai"
      },
      "outputs": [],
      "source": [
        "# Step 1: Count samples in each class\n",
        "n_total = len(y_train)\n",
        "n_class_0 = np.sum(y_train == 0)\n",
        "n_class_1 = np.sum(y_train == 1)\n",
        "\n",
        "print(\"Step 1 - Count samples:\")\n",
        "print(f\"  Total samples: {n_total}\")\n",
        "print(f\"  Class 0: {n_class_0}\")\n",
        "print(f\"  Class 1: {n_class_1}\")\n",
        "print()\n",
        "\n",
        "# Step 2: Calculate proportions\n",
        "p_0 = n_class_0 / n_total\n",
        "p_1 = n_class_1 / n_total\n",
        "\n",
        "print(\"Step 2 - Calculate proportions:\")\n",
        "print(f\"  p_0 = {n_class_0}/{n_total} = {p_0:.4f}\")\n",
        "print(f\"  p_1 = {n_class_1}/{n_total} = {p_1:.4f}\")\n",
        "print()\n",
        "\n",
        "# Step 3: Calculate each term: p_i * log2(p_i)\n",
        "term_0 = p_0 * np.log2(p_0)\n",
        "term_1 = p_1 * np.log2(p_1)\n",
        "\n",
        "print(\"Step 3 - Calculate p_i √ó log‚ÇÇ(p_i):\")\n",
        "print(f\"  p_0 √ó log‚ÇÇ(p_0) = {p_0:.4f} √ó log‚ÇÇ({p_0:.4f}) = {p_0:.4f} √ó {np.log2(p_0):.4f} = {term_0:.4f}\")\n",
        "print(f\"  p_1 √ó log‚ÇÇ(p_1) = {p_1:.4f} √ó log‚ÇÇ({p_1:.4f}) = {p_1:.4f} √ó {np.log2(p_1):.4f} = {term_1:.4f}\")\n",
        "print()\n",
        "\n",
        "# Step 4: Sum and negate\n",
        "entropy_manual = -(term_0 + term_1)\n",
        "\n",
        "print(\"Step 4 - Sum and negate:\")\n",
        "print(f\"  E = -({term_0:.4f} + {term_1:.4f})\")\n",
        "print(f\"  E = -({term_0 + term_1:.4f})\")\n",
        "print(f\"  E = {entropy_manual:.4f}\")\n",
        "print()\n",
        "print(f\"Root Entropy = {entropy_manual:.3f} (matches lecture slide 21: 0.994)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CQOT7F-h7ai"
      },
      "source": [
        "### Vectorized Entropy Function\n",
        "\n",
        "Now let's create an efficient function to calculate entropy for any label array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3q0Wj8jh7ai"
      },
      "outputs": [],
      "source": [
        "def entropy(y):\n",
        "    \"\"\"\n",
        "    Calculate entropy of a label array.\n",
        "\n",
        "    E = -Œ£ p_i √ó log‚ÇÇ(p_i)\n",
        "    \"\"\"\n",
        "    if len(y) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Count occurrences of each class\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    # Calculate proportions\n",
        "    proportions = counts / len(y)\n",
        "\n",
        "    # Calculate entropy (only for non-zero proportions to avoid log(0))\n",
        "    return -np.sum(proportions * np.log2(proportions))\n",
        "\n",
        "# Verify it matches our manual calculation\n",
        "entropy_vectorized = entropy(y_train)\n",
        "print(f\"Vectorized entropy: {entropy_vectorized:.4f}\")\n",
        "print(f\"Manual entropy:     {entropy_manual:.4f}\")\n",
        "print(f\"Results match: {np.isclose(entropy_vectorized, entropy_manual)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0xVoAUh7aj"
      },
      "source": [
        "### Visualizing Entropy: Low vs High Impurity\n",
        "\n",
        "Let's see how entropy varies with class proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwrsB-Izh7aj"
      },
      "outputs": [],
      "source": [
        "# Test entropy on different distributions\n",
        "print(\"Entropy Examples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Pure node (all class 0)\n",
        "y_pure = np.array([0, 0, 0, 0, 0])\n",
        "print(f\"Pure (all 0s):     {y_pure} ‚Üí E = {entropy(y_pure):.4f}\")\n",
        "\n",
        "# Almost pure (9 muffins, 1 cookie - from lecture slide 9)\n",
        "y_low_entropy = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
        "print(f\"Low entropy (9:1): {y_low_entropy} ‚Üí E = {entropy(y_low_entropy):.4f}\")\n",
        "\n",
        "# Our dataset (6:5)\n",
        "print(f\"Our data (6:5):    ‚Üí E = {entropy(y_train):.4f}\")\n",
        "\n",
        "# Perfectly balanced\n",
        "y_balanced = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "print(f\"Balanced (5:5):    {y_balanced} ‚Üí E = {entropy(y_balanced):.4f}\")\n",
        "\n",
        "# Plot entropy curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "p_values = np.linspace(0.001, 0.999, 100)\n",
        "entropy_values = [-p * np.log2(p) - (1-p) * np.log2(1-p) for p in p_values]\n",
        "\n",
        "plt.plot(p_values, entropy_values, 'b-', linewidth=2)\n",
        "plt.axvline(x=6/11, color='red', linestyle='--', label=f'Our data (p={6/11:.3f})')\n",
        "plt.scatter([6/11], [entropy(y_train)], color='red', s=100, zorder=5)\n",
        "plt.xlabel('Proportion of Class 0', fontsize=12)\n",
        "plt.ylabel('Entropy', fontsize=12)\n",
        "plt.title('Entropy vs Class Proportion (Binary Classification)', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSz0tw1Oh7aj"
      },
      "source": [
        "## Step 5: Understanding Information Gain\n",
        "\n",
        "**Information Gain (IG)** measures how much a split reduces entropy.\n",
        "\n",
        "$$IG = E_{parent} - \\frac{|L|}{N} E_L - \\frac{|R|}{N} E_R$$\n",
        "\n",
        "Where:\n",
        "- $E_{parent}$ is the entropy before the split\n",
        "- $|L|, |R|$ are the number of samples in left and right children\n",
        "- $E_L, E_R$ are the entropies of the children\n",
        "- $N$ is the total number of samples\n",
        "\n",
        "**Higher information gain = better split!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9dqS6W9h7aj"
      },
      "outputs": [],
      "source": [
        "def information_gain(y_parent, y_left, y_right):\n",
        "    \"\"\"\n",
        "    Calculate information gain from a split.\n",
        "\n",
        "    IG = E_parent - (|L|/N √ó E_L + |R|/N √ó E_R)\n",
        "    \"\"\"\n",
        "    n_parent = len(y_parent)\n",
        "    n_left = len(y_left)\n",
        "    n_right = len(y_right)\n",
        "\n",
        "    if n_left == 0 or n_right == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Parent entropy\n",
        "    e_parent = entropy(y_parent)\n",
        "\n",
        "    # Weighted child entropy\n",
        "    e_left = entropy(y_left)\n",
        "    e_right = entropy(y_right)\n",
        "    weighted_child = (n_left / n_parent) * e_left + (n_right / n_parent) * e_right\n",
        "\n",
        "    return e_parent - weighted_child\n",
        "\n",
        "print(\"Information Gain Function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZckHaKEh7aj"
      },
      "source": [
        "## Step 6: Finding the Best Split for Numerical Features\n",
        "\n",
        "For numerical features, we need to find the **best threshold** to split on.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Sort the unique values of the feature\n",
        "2. Consider **midpoints** between consecutive values as candidate thresholds\n",
        "3. For each threshold, split the data into left ($\\leq$ threshold) and right ($>$ threshold)\n",
        "4. Calculate information gain for each split\n",
        "5. Choose the threshold with the highest information gain\n",
        "\n",
        "From lecture slide 22:\n",
        "- Unique values of $x_1$: [-1.5, -1.0, -0.5, 0.0, 0.5, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
        "- Candidate thresholds: [-1.25, -0.75, -0.25, 0.25, 1.0, 1.75, 2.25, 2.75, 3.25, 3.75]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AliMqMg4h7aj"
      },
      "source": [
        "### Manual Calculation: Evaluating Split on $x_1$ at threshold 1.0\n",
        "\n",
        "Let's manually evaluate the split $x_1 \\leq 1.0$ (from lecture slide 23)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0tNsDJvh7aj"
      },
      "outputs": [],
      "source": [
        "# Feature x1 values\n",
        "x1 = X_train[:, 0]\n",
        "print(\"Feature x‚ÇÅ values:\")\n",
        "for i, (val, label) in enumerate(zip(x1, y_train)):\n",
        "    print(f\"  Point {i}: x‚ÇÅ = {val:5.1f}, class = {label}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING SPLIT: x‚ÇÅ ‚â§ 1.0\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Split at threshold 1.0\n",
        "threshold = 1.0\n",
        "left_mask = x1 <= threshold\n",
        "right_mask = x1 > threshold\n",
        "\n",
        "y_left = y_train[left_mask]\n",
        "y_right = y_train[right_mask]\n",
        "\n",
        "print(f\"\\nThreshold: x‚ÇÅ ‚â§ {threshold}\")\n",
        "print(f\"\\nLeft child (x‚ÇÅ ‚â§ {threshold}):\")\n",
        "print(f\"  Samples: {np.sum(left_mask)} points\")\n",
        "print(f\"  Labels: {y_left}\")\n",
        "print(f\"  Class 0: {np.sum(y_left == 0)}, Class 1: {np.sum(y_left == 1)}\")\n",
        "\n",
        "print(f\"\\nRight child (x‚ÇÅ > {threshold}):\")\n",
        "print(f\"  Samples: {np.sum(right_mask)} points\")\n",
        "print(f\"  Labels: {y_right}\")\n",
        "print(f\"  Class 0: {np.sum(y_right == 0)}, Class 1: {np.sum(y_right == 1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRPzRIG1h7aj"
      },
      "outputs": [],
      "source": [
        "# Calculate entropies\n",
        "print(\"\\nEntropy Calculations:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "e_parent = entropy(y_train)\n",
        "print(f\"Parent Entropy: E = {e_parent:.4f}\")\n",
        "\n",
        "e_left = entropy(y_left)\n",
        "print(f\"\\nLeft Child (5 class 0, 1 class 1):\")\n",
        "print(f\"  E_left = -{5/6:.4f} √ó log‚ÇÇ({5/6:.4f}) - {1/6:.4f} √ó log‚ÇÇ({1/6:.4f})\")\n",
        "print(f\"  E_left = {e_left:.4f}\")\n",
        "\n",
        "e_right = entropy(y_right)\n",
        "print(f\"\\nRight Child (1 class 0, 4 class 1):\")\n",
        "print(f\"  E_right = -{1/5:.4f} √ó log‚ÇÇ({1/5:.4f}) - {4/5:.4f} √ó log‚ÇÇ({4/5:.4f})\")\n",
        "print(f\"  E_right = {e_right:.4f}\")\n",
        "\n",
        "# Calculate weighted entropy\n",
        "n_left = len(y_left)\n",
        "n_right = len(y_right)\n",
        "n_total = len(y_train)\n",
        "\n",
        "weighted_entropy = (n_left / n_total) * e_left + (n_right / n_total) * e_right\n",
        "print(f\"\\nWeighted Child Entropy:\")\n",
        "print(f\"  E = ({n_left}/{n_total}) √ó {e_left:.4f} + ({n_right}/{n_total}) √ó {e_right:.4f}\")\n",
        "print(f\"  E = {n_left/n_total:.4f} √ó {e_left:.4f} + {n_right/n_total:.4f} √ó {e_right:.4f}\")\n",
        "print(f\"  E = {weighted_entropy:.4f}\")\n",
        "\n",
        "# Calculate information gain\n",
        "ig = e_parent - weighted_entropy\n",
        "print(f\"\\nInformation Gain:\")\n",
        "print(f\"  IG = E_parent - E_weighted\")\n",
        "print(f\"  IG = {e_parent:.4f} - {weighted_entropy:.4f}\")\n",
        "print(f\"  IG = {ig:.4f}\")\n",
        "print(f\"\\n‚Üí Split at x‚ÇÅ ‚â§ 1.0 gives IG = {ig:.3f} (matches lecture slide 23: 0.629)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCX76dd2h7ak"
      },
      "source": [
        "### Visualize the Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ne9Y27ph7ak"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Before split\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', s=150, edgecolors='black', linewidths=1.5, label='Class 0')\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', s=150, edgecolors='black', linewidths=1.5, label='Class 1')\n",
        "plt.axvline(x=1.0, color='green', linewidth=3, linestyle='-', label='Split: x‚ÇÅ = 1.0')\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title(f'Split at x‚ÇÅ ‚â§ 1.0 (IG = {ig:.3f})', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: After split\n",
        "plt.subplot(1, 2, 2)\n",
        "# Left region (x1 <= 1.0)\n",
        "plt.axvspan(-2.5, 1.0, alpha=0.2, color='blue', label='Left: x‚ÇÅ ‚â§ 1.0')\n",
        "# Right region (x1 > 1.0)\n",
        "plt.axvspan(1.0, 4.5, alpha=0.2, color='red', label='Right: x‚ÇÅ > 1.0')\n",
        "\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', s=150, edgecolors='black', linewidths=1.5)\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', s=150, edgecolors='black', linewidths=1.5)\n",
        "plt.axvline(x=1.0, color='green', linewidth=3, linestyle='-')\n",
        "plt.xlabel('$x_1$', fontsize=12)\n",
        "plt.ylabel('$x_2$', fontsize=12)\n",
        "plt.title('Regions After Split', fontsize=14)\n",
        "plt.legend(fontsize=10, loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Left region: 5 blue (class 0), 1 orange (class 1) ‚Üí E = {e_left:.3f}\")\n",
        "print(f\"Right region: 1 blue (class 0), 4 orange (class 1) ‚Üí E = {e_right:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeUKbvRgh7ak"
      },
      "source": [
        "### Evaluating All Possible Splits\n",
        "\n",
        "Let's find the **best split** by evaluating all candidate thresholds for both features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK0FfdfVh7ak"
      },
      "outputs": [],
      "source": [
        "def find_best_split(X, y):\n",
        "    \"\"\"\n",
        "    Find the best feature and threshold to split on.\n",
        "\n",
        "    Returns: (best_feature, best_threshold, best_gain, split_info)\n",
        "    \"\"\"\n",
        "    n_samples, n_features = X.shape\n",
        "    best_gain = 0.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    all_splits = []\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        unique_values = np.unique(feature_values)\n",
        "\n",
        "        if len(unique_values) < 2:\n",
        "            continue\n",
        "\n",
        "        # Midpoint thresholds\n",
        "        thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            left_mask = feature_values <= threshold\n",
        "            right_mask = ~left_mask\n",
        "\n",
        "            y_left = y[left_mask]\n",
        "            y_right = y[right_mask]\n",
        "\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "\n",
        "            gain = information_gain(y, y_left, y_right)\n",
        "            all_splits.append((feature_idx, threshold, gain))\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold, best_gain, all_splits\n",
        "\n",
        "# Find best split for root node\n",
        "best_feat, best_thresh, best_ig, all_splits = find_best_split(X_train, y_train)\n",
        "\n",
        "print(\"All Candidate Splits:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Feature':<10} {'Threshold':<12} {'Info Gain':<12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Sort by feature then threshold\n",
        "all_splits_sorted = sorted(all_splits, key=lambda x: (x[0], x[1]))\n",
        "for feat, thresh, gain in all_splits_sorted:\n",
        "    marker = \" ‚Üê BEST\" if feat == best_feat and thresh == best_thresh else \"\"\n",
        "    print(f\"x[{feat}]      {thresh:<12.3f} {gain:<12.4f}{marker}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"BEST SPLIT: x[{best_feat}] ‚â§ {best_thresh:.1f} with IG = {best_ig:.4f}\")\n",
        "print(\"(Matches lecture slide 25: x‚ÇÅ ‚â§ 1.0 has highest IG = 0.629)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-CCtvYZh7ak"
      },
      "source": [
        "## Step 7: Building the Decision Tree\n",
        "\n",
        "Now let's build the complete decision tree using **recursive splitting**.\n",
        "\n",
        "**Stopping conditions:**\n",
        "1. Node is **pure** (all samples same class)\n",
        "2. Reached **maximum depth**\n",
        "3. Not enough samples to split (< `min_samples_split`)\n",
        "4. No valid split improves purity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-YLIYcbh7ak"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier:\n",
        "    \"\"\"\n",
        "    Decision Tree Classifier using entropy and information gain.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.tree_ = None\n",
        "        self.n_classes_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"Calculate entropy of label array.\"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0.0\n",
        "        counts = np.bincount(y, minlength=self.n_classes_)\n",
        "        proportions = counts / len(y)\n",
        "        proportions = proportions[proportions > 0]\n",
        "        return -np.sum(proportions * np.log2(proportions))\n",
        "\n",
        "    def _information_gain(self, y, y_left, y_right):\n",
        "        \"\"\"Calculate information gain from a split.\"\"\"\n",
        "        n = len(y)\n",
        "        n_left, n_right = len(y_left), len(y_right)\n",
        "\n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0.0\n",
        "\n",
        "        e_parent = self._entropy(y)\n",
        "        e_left = self._entropy(y_left)\n",
        "        e_right = self._entropy(y_right)\n",
        "\n",
        "        return e_parent - (n_left/n * e_left + n_right/n * e_right)\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        best_gain = 0.0\n",
        "        best_split = None\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            feature_values = X[:, feature_idx]\n",
        "            unique_values = np.unique(feature_values)\n",
        "\n",
        "            if len(unique_values) < 2:\n",
        "                continue\n",
        "\n",
        "            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_mask = feature_values <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                n_left = np.sum(left_mask)\n",
        "                n_right = np.sum(right_mask)\n",
        "\n",
        "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                y_left = y[left_mask]\n",
        "                y_right = y[right_mask]\n",
        "\n",
        "                gain = self._information_gain(y, y_left, y_right)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split = {\n",
        "                        'feature': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_mask': left_mask,\n",
        "                        'right_mask': right_mask,\n",
        "                        'gain': gain\n",
        "                    }\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively build the decision tree.\"\"\"\n",
        "        n_samples = len(y)\n",
        "\n",
        "        # Calculate class probabilities\n",
        "        counts = np.bincount(y, minlength=self.n_classes_)\n",
        "        proba = counts / counts.sum()\n",
        "        majority_class = np.argmax(counts)\n",
        "\n",
        "        # Stopping conditions\n",
        "        # 1. Pure node\n",
        "        if len(np.unique(y)) == 1:\n",
        "            return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
        "\n",
        "        # 2. Max depth reached\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
        "\n",
        "        # 3. Not enough samples\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
        "\n",
        "        # Find best split\n",
        "        best_split = self._find_best_split(X, y)\n",
        "\n",
        "        # 4. No valid split\n",
        "        if best_split is None:\n",
        "            return {'leaf': True, 'proba': proba, 'class': majority_class}\n",
        "\n",
        "        # Recursively build children\n",
        "        left_tree = self._build_tree(\n",
        "            X[best_split['left_mask']],\n",
        "            y[best_split['left_mask']],\n",
        "            depth + 1\n",
        "        )\n",
        "        right_tree = self._build_tree(\n",
        "            X[best_split['right_mask']],\n",
        "            y[best_split['right_mask']],\n",
        "            depth + 1\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'leaf': False,\n",
        "            'feature': best_split['feature'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left': left_tree,\n",
        "            'right': right_tree,\n",
        "            'gain': best_split['gain'],\n",
        "            'n_samples': n_samples,\n",
        "            'entropy': self._entropy(y)\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build decision tree from training data.\"\"\"\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_, y_encoded = np.unique(y, return_inverse=True)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "        self.n_features_ = X.shape[1]\n",
        "\n",
        "        self.tree_ = self._build_tree(X, y_encoded, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _predict_single(self, x, node):\n",
        "        \"\"\"Traverse tree for single sample.\"\"\"\n",
        "        while not node['leaf']:\n",
        "            if x[node['feature']] <= node['threshold']:\n",
        "                node = node['left']\n",
        "            else:\n",
        "                node = node['right']\n",
        "        return node['proba']\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities.\"\"\"\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        return np.array([self._predict_single(x, self.tree_) for x in X])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels.\"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return self.classes_[np.argmax(proba, axis=1)]\n",
        "\n",
        "print(\"DecisionTreeClassifier class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIlGmMoVh7ak"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mk2bSVUh7ak"
      },
      "outputs": [],
      "source": [
        "# Train our decision tree\n",
        "model = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Number of classes: {model.n_classes_}\")\n",
        "print(f\"Classes: {model.classes_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB33ORToh7ak"
      },
      "source": [
        "### Visualize the Tree Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU4KFVBPh7al"
      },
      "outputs": [],
      "source": [
        "def print_tree(node, feature_names=None, indent=0):\n",
        "    \"\"\"Pretty print the decision tree structure.\"\"\"\n",
        "    prefix = \"  \" * indent\n",
        "\n",
        "    if node['leaf']:\n",
        "        class_counts = node['proba'] * 11  # Approximate counts\n",
        "        print(f\"{prefix}üçÉ Leaf: predict class {node['class']}\")\n",
        "        print(f\"{prefix}   proba = {node['proba']}\")\n",
        "    else:\n",
        "        feat_name = f\"x[{node['feature']}]\" if feature_names is None else feature_names[node['feature']]\n",
        "        print(f\"{prefix}üìä {feat_name} ‚â§ {node['threshold']:.2f}\")\n",
        "        print(f\"{prefix}   (IG = {node['gain']:.4f}, E = {node['entropy']:.4f}, n = {node['n_samples']})\")\n",
        "        print(f\"{prefix}   ‚îú‚îÄ True (left):\")\n",
        "        print_tree(node['left'], feature_names, indent + 2)\n",
        "        print(f\"{prefix}   ‚îî‚îÄ False (right):\")\n",
        "        print_tree(node['right'], feature_names, indent + 2)\n",
        "\n",
        "print(\"Decision Tree Structure:\")\n",
        "print(\"=\" * 70)\n",
        "print_tree(model.tree_, feature_names=['$x_1$', '$x_2$'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WvhU1t6h7al"
      },
      "source": [
        "## Step 8: Making Predictions\n",
        "\n",
        "Let's classify a new point: $(2.0, -2.0)$ (from lecture slide 41)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xh9xYOth7al"
      },
      "outputs": [],
      "source": [
        "# Test point from lecture\n",
        "X_test_point = np.array([[2.0, -2.0]])\n",
        "\n",
        "print(\"Prediction Walk-Through for point (2.0, -2.0):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Manual traversal\n",
        "node = model.tree_\n",
        "x = X_test_point[0]\n",
        "step = 1\n",
        "\n",
        "while not node['leaf']:\n",
        "    feat = node['feature']\n",
        "    thresh = node['threshold']\n",
        "\n",
        "    print(f\"\\nStep {step}: At node 'x[{feat}] ‚â§ {thresh:.2f}'\")\n",
        "    print(f\"  Test: x[{feat}] = {x[feat]:.2f} ‚â§ {thresh:.2f}?\")\n",
        "\n",
        "    if x[feat] <= thresh:\n",
        "        print(f\"  Result: {x[feat]:.2f} ‚â§ {thresh:.2f} is TRUE ‚Üí go LEFT\")\n",
        "        node = node['left']\n",
        "    else:\n",
        "        print(f\"  Result: {x[feat]:.2f} ‚â§ {thresh:.2f} is FALSE ‚Üí go RIGHT\")\n",
        "        node = node['right']\n",
        "    step += 1\n",
        "\n",
        "print(f\"\\nStep {step}: Reached LEAF node\")\n",
        "print(f\"  Probabilities: {node['proba']}\")\n",
        "print(f\"  Predicted class: {node['class']}\")\n",
        "\n",
        "# Verify with predict method\n",
        "pred = model.predict(X_test_point)[0]\n",
        "proba = model.predict_proba(X_test_point)[0]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREDICTION RESULT:\")\n",
        "print(f\"  Point: {X_test_point[0].tolist()}\")\n",
        "print(f\"  Probabilities [class 0, class 1]: {proba.tolist()}\")\n",
        "print(f\"  Predicted Class: {pred}\")\n",
        "print(f\"\\n(Matches lecture slide 41: Expected class 1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBcHVyxQh7al"
      },
      "source": [
        "### Evaluate on Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g0rYPoyh7al"
      },
      "outputs": [],
      "source": [
        "# Predict on all training points\n",
        "y_pred = model.predict(X_train)\n",
        "\n",
        "print(\"Predictions on Training Data:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Point':<8} {'x‚ÇÅ':>6} {'x‚ÇÇ':>6}   {'True':>5} {'Pred':>5} {'Match':>6}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(X_train)):\n",
        "    match = \"‚úì\" if y_train[i] == y_pred[i] else \"‚úó\"\n",
        "    if y_train[i] == y_pred[i]:\n",
        "        correct += 1\n",
        "    print(f\"  {i:<6} {X_train[i, 0]:>6.1f} {X_train[i, 1]:>6.1f}   {y_train[i]:>5} {y_pred[i]:>5} {match:>6}\")\n",
        "\n",
        "accuracy = correct / len(y_train)\n",
        "print(\"-\" * 50)\n",
        "print(f\"Training Accuracy: {correct}/{len(y_train)} = {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTj6AaZ4h7al"
      },
      "source": [
        "## Step 9: Visualize Decision Boundary\n",
        "\n",
        "Decision trees create **axis-aligned** (rectangular) decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCnrmPF3h7al"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(model, X, y, title=\"Decision Tree Decision Boundary\"):\n",
        "    \"\"\"Plot decision boundary for 2D classification.\"\"\"\n",
        "    # Create mesh\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                           np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "    # Predict on mesh\n",
        "    X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "    Z = model.predict_proba(X_mesh)[:, 1].reshape(xx1.shape)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(xx1, xx2, Z, levels=20, cmap='RdBu_r', alpha=0.6)\n",
        "    plt.colorbar(label='P(Class 1)')\n",
        "    plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2, linestyles='dashed')\n",
        "\n",
        "    # Plot data points\n",
        "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='skyblue', s=150,\n",
        "                edgecolors='black', linewidths=1.5, label='Class 0')\n",
        "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='orange', s=150,\n",
        "                edgecolors='black', linewidths=1.5, label='Class 1')\n",
        "\n",
        "    plt.xlabel('$x_1$', fontsize=14)\n",
        "    plt.ylabel('$x_2$', fontsize=14)\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(model, X_train, y_train,\n",
        "                       \"Decision Tree: Axis-Aligned Decision Boundary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuWF8afyh7al"
      },
      "source": [
        "### Understanding the Rectangular Regions\n",
        "\n",
        "Notice how the decision boundary consists of **horizontal and vertical lines** ‚Äî this is because each split is based on a single feature threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3RRPFFBh7al"
      },
      "outputs": [],
      "source": [
        "# Visualize the splits explicitly\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Background regions\n",
        "plt.axvspan(-2.5, 1.0, alpha=0.15, color='blue', label='Region: x‚ÇÅ ‚â§ 1.0')\n",
        "plt.axvspan(1.0, 5.0, ymin=0, ymax=0.55, alpha=0.15, color='red')  # x1 > 1.0, x2 <= 0\n",
        "plt.axvspan(1.0, 5.0, ymin=0.55, ymax=1, alpha=0.15, color='blue')  # x1 > 1.0, x2 > 0\n",
        "\n",
        "# Data points\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
        "            c='skyblue', s=150, edgecolors='black', linewidths=1.5, label='Class 0', zorder=5)\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
        "            c='orange', s=150, edgecolors='black', linewidths=1.5, label='Class 1', zorder=5)\n",
        "\n",
        "# Split lines\n",
        "plt.axvline(x=1.0, color='green', linewidth=3, linestyle='-', label='Split 1: x‚ÇÅ = 1.0')\n",
        "plt.axhline(y=0.0, color='purple', linewidth=3, linestyle='-', xmin=0.54, label='Split 2: x‚ÇÇ = 0.0')\n",
        "\n",
        "# Annotations\n",
        "plt.annotate('Predict 0\\n(5 blue, 1 orange)', xy=(-0.5, 2), fontsize=11, ha='center',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "plt.annotate('Predict 1\\n(0 blue, 5 orange)', xy=(3, -2.5), fontsize=11, ha='center',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
        "plt.annotate('Predict 0\\n(1 blue, 0 orange)', xy=(3.5, 1), fontsize=11, ha='center',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "plt.xlabel('$x_1$', fontsize=14)\n",
        "plt.ylabel('$x_2$', fontsize=14)\n",
        "plt.title('Decision Tree Splits (from lecture slide 27)', fontsize=16)\n",
        "plt.legend(fontsize=10, loc='lower left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(-2.5, 5)\n",
        "plt.ylim(-5, 3)\n",
        "plt.show()\n",
        "\n",
        "print(\"Decision Rules (from lecture slide 27):\")\n",
        "print(\"  IF x‚ÇÅ ‚â§ 1.0 ‚Üí predict 0\")\n",
        "print(\"  ELSE IF x‚ÇÇ ‚â§ 0.0 ‚Üí predict 1\")\n",
        "print(\"  ELSE ‚Üí predict 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xrPjbPmh7aq"
      },
      "source": [
        "## Step 10: Comparison with scikit-learn\n",
        "\n",
        "Let's verify our implementation matches sklearn's `DecisionTreeClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuOgq1sTh7aq"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTree\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Train sklearn model with same parameters\n",
        "sklearn_model = SklearnDecisionTree(\n",
        "    criterion='entropy',\n",
        "    max_depth=3,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=42\n",
        ")\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "\n",
        "# Compare predictions\n",
        "our_pred = model.predict(X_train)\n",
        "sklearn_pred = sklearn_model.predict(X_train)\n",
        "\n",
        "print(\"Comparison: Our Implementation vs scikit-learn\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOur predictions:     {our_pred}\")\n",
        "print(f\"sklearn predictions: {sklearn_pred}\")\n",
        "print(f\"\\nPredictions match: {np.all(our_pred == sklearn_pred)}\")\n",
        "\n",
        "# Compare accuracies\n",
        "our_accuracy = np.mean(our_pred == y_train)\n",
        "sklearn_accuracy = sklearn_model.score(X_train, y_train)\n",
        "\n",
        "print(f\"\\nOur Training Accuracy:     {our_accuracy:.2%}\")\n",
        "print(f\"sklearn Training Accuracy: {sklearn_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o27ZzJxth7aq"
      },
      "outputs": [],
      "source": [
        "# Visualize sklearn tree\n",
        "plt.figure(figsize=(16, 10))\n",
        "plot_tree(sklearn_model,\n",
        "          feature_names=['$x_1$', '$x_2$'],\n",
        "          class_names=['Class 0', 'Class 1'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=11)\n",
        "plt.title('sklearn DecisionTreeClassifier Structure', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HscrIa50h7aq"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We've walked through all the computational steps of Decision Tree Classification:\n",
        "\n",
        "1. ‚úÖ **Calculated Entropy** - measured impurity using $E = -\\sum p_i \\log_2(p_i)$\n",
        "2. ‚úÖ **Calculated Information Gain** - measured split quality as reduction in entropy\n",
        "3. ‚úÖ **Found Best Splits** - evaluated all (feature, threshold) pairs using midpoints\n",
        "4. ‚úÖ **Built the Tree** - recursively partitioned feature space\n",
        "5. ‚úÖ **Made Predictions** - traversed tree from root to leaf\n",
        "6. ‚úÖ **Visualized Decision Boundaries** - saw axis-aligned rectangular regions\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "| Concept | Formula | Description |\n",
        "|---------|---------|-------------|\n",
        "| **Entropy** | $E = -\\sum_{i} p_i \\log_2(p_i)$ | Measures impurity (0 = pure, higher = more mixed) |\n",
        "| **Information Gain** | $IG = E_{parent} - \\sum_j \\frac{|S_j|}{N} E_j$ | Reduction in entropy from a split |\n",
        "| **Greedy Splitting** | Select $\\arg\\max$ IG | Choose locally best split at each node |\n",
        "| **Threshold Selection** | Midpoints of unique values | $t_k = \\frac{v_k + v_{k+1}}{2}$ |\n",
        "\n",
        "### Key NumPy Operations Used\n",
        "\n",
        "| Operation | Purpose |\n",
        "|-----------|--------|\n",
        "| `np.unique(y, return_counts=True)` | Count class occurrences |\n",
        "| `np.bincount(y)` | Fast counting for integer labels |\n",
        "| `np.log2(p)` | Logarithm base 2 for entropy |\n",
        "| `X[:, feature_idx] <= threshold` | Create boolean mask for split |\n",
        "| `np.argmax(proba)` | Find class with highest probability |\n",
        "\n",
        "### Decision Tree Characteristics\n",
        "\n",
        "| Property | Decision Trees |\n",
        "|----------|---------------|\n",
        "| **Decision Boundary** | Axis-aligned (rectangular regions) |\n",
        "| **Feature Scaling** | Not required |\n",
        "| **Interpretability** | High (can extract IF-ELSE rules) |\n",
        "| **Training Speed** | Fast (O(n √ó d √ó log n) per split) |\n",
        "| **Prediction Speed** | Very fast (O(depth)) |\n",
        "| **Overfitting Risk** | High (control with max_depth, min_samples) |\n",
        "\n",
        "### Comparison with Other Algorithms\n",
        "\n",
        "| Algorithm | Boundary Type | Scaling Needed | Interpretable | Handles Non-linear |\n",
        "|-----------|--------------|----------------|---------------|--------------------|\n",
        "| **Decision Trees** | Axis-aligned | No | Yes | Yes |\n",
        "| **Logistic Regression** | Linear | Yes | Yes | No (manual features) |\n",
        "| **KNN** | Non-parametric | Yes | Limited | Yes |\n",
        "| **Naive Bayes** | Linear/Quadratic | No | Yes | Limited |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}